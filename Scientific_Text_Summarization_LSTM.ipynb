{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRPciZ1jatuQ",
        "outputId": "3e6ece08-62e2-4187-ae89-aeeef72ca0ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os"
      ],
      "metadata": {
        "id": "MkSGRGyNeCK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists('Analysis/'):\n",
        "  os.makedirs('Analysis/')\n",
        "SAVE_DIR = 'Analysis/'"
      ],
      "metadata": {
        "id": "O9nkQBks0rAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "VZSyswqzLoUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = pd.read_csv('train.csv')\n",
        "dataset_test = pd.read_csv('test.csv')\n",
        "dataset_val = pd.read_csv('val.csv')"
      ],
      "metadata": {
        "id": "6peBtjZ0ecMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "tF4F3oxBefD2",
        "outputId": "c7a99cc9-2350-4a1d-a8b3-3469c53c8846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 source  \\\n",
              "0     ['Due to the success of deep learning to solvi...   \n",
              "1     ['The backpropagation (BP) algorithm is often ...   \n",
              "2     ['We introduce the 2-simplicial Transformer, a...   \n",
              "3     ['We present Tensor-Train RNN (TT-RNN), a nove...   \n",
              "4     ['Recent efforts on combining deep models with...   \n",
              "...                                                 ...   \n",
              "1987  ['Semi-supervised learning, i.e. jointly learn...   \n",
              "1988  ['Model-free reinforcement learning (RL) has b...   \n",
              "1989  ['We introduce a neural architecture to perfor...   \n",
              "1990  ['Machine learned large-scale retrieval system...   \n",
              "1991  ['The ability to autonomously explore and navi...   \n",
              "\n",
              "                 source_labels  \\\n",
              "0           [0, 0, 0, 0, 1, 0]   \n",
              "1     [0, 0, 0, 1, 0, 0, 0, 0]   \n",
              "2                       [0, 1]   \n",
              "3           [0, 0, 0, 1, 0, 0]   \n",
              "4        [0, 1, 0, 0, 0, 0, 0]   \n",
              "...                        ...   \n",
              "1987     [0, 0, 0, 1, 0, 0, 0]   \n",
              "1988     [0, 0, 0, 0, 1, 0, 0]   \n",
              "1989              [1, 0, 0, 0]   \n",
              "1990        [0, 0, 0, 1, 0, 0]   \n",
              "1991     [0, 0, 0, 0, 0, 0, 1]   \n",
              "\n",
              "                                           rouge_scores    paper_id  \\\n",
              "0     [0.30188679695129395, 0.3720930218696594, 0.60...   SysEexbRb   \n",
              "1     [0.0, 0.0, 0.1304347813129425, 0.1428571343421...  SygvZ209F7   \n",
              "2              [0.3333333432674408, 0.8888888955116272]  rkecJ6VFvr   \n",
              "3     [0.06666666269302368, 0.06451612710952759, 0.0...   HJJ0w--0W   \n",
              "4     [0.277777761220932, 0.5714285373687744, 0.0952...   HyH9lbZAW   \n",
              "...                                                 ...         ...   \n",
              "1987  [0.07999999821186066, 0.11538460850715637, 0.1...  rJel41BtDH   \n",
              "1988  [0.09302324801683426, 0.08695651590824127, 0.0...   Skw0n-W0Z   \n",
              "1989  [0.46666666865348816, 0.0714285671710968, 0.0,...  rJgFtkhEtr   \n",
              "1990  [0.277777761220932, 0.0, 0.2857142686843872, 0...  SJxPVcSonN   \n",
              "1991  [0.10169491171836853, 0.0937499925494194, 0.16...  BJgMFxrYPB   \n",
              "\n",
              "                                                 target  \n",
              "0     ['We provide necessary and sufficient analytic...  \n",
              "1     ['Biologically plausible learning algorithms, ...  \n",
              "2     ['We introduce the 2-simplicial Transformer an...  \n",
              "3     ['Accurate forecasting over very long time hor...  \n",
              "4     ['We propose a variational message-passing alg...  \n",
              "...                                                 ...  \n",
              "1987  ['Pseudo-labeling has shown to be a weak alter...  \n",
              "1988  ['We show that a special goal-condition value ...  \n",
              "1989  ['A novel neural architecture for efficient am...  \n",
              "1990  ['We propose a novel two-tower shared-bottom m...  \n",
              "1991  ['We address the task of autonomous exploratio...  \n",
              "\n",
              "[1992 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bd702a19-0f95-4349-9c4d-32b38bd135d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>source_labels</th>\n",
              "      <th>rouge_scores</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>['Due to the success of deep learning to solvi...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0]</td>\n",
              "      <td>[0.30188679695129395, 0.3720930218696594, 0.60...</td>\n",
              "      <td>SysEexbRb</td>\n",
              "      <td>['We provide necessary and sufficient analytic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>['The backpropagation (BP) algorithm is often ...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
              "      <td>[0.0, 0.0, 0.1304347813129425, 0.1428571343421...</td>\n",
              "      <td>SygvZ209F7</td>\n",
              "      <td>['Biologically plausible learning algorithms, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>['We introduce the 2-simplicial Transformer, a...</td>\n",
              "      <td>[0, 1]</td>\n",
              "      <td>[0.3333333432674408, 0.8888888955116272]</td>\n",
              "      <td>rkecJ6VFvr</td>\n",
              "      <td>['We introduce the 2-simplicial Transformer an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>['We present Tensor-Train RNN (TT-RNN), a nove...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.06666666269302368, 0.06451612710952759, 0.0...</td>\n",
              "      <td>HJJ0w--0W</td>\n",
              "      <td>['Accurate forecasting over very long time hor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>['Recent efforts on combining deep models with...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0.277777761220932, 0.5714285373687744, 0.0952...</td>\n",
              "      <td>HyH9lbZAW</td>\n",
              "      <td>['We propose a variational message-passing alg...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>['Semi-supervised learning, i.e. jointly learn...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
              "      <td>[0.07999999821186066, 0.11538460850715637, 0.1...</td>\n",
              "      <td>rJel41BtDH</td>\n",
              "      <td>['Pseudo-labeling has shown to be a weak alter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1988</th>\n",
              "      <td>['Model-free reinforcement learning (RL) has b...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.09302324801683426, 0.08695651590824127, 0.0...</td>\n",
              "      <td>Skw0n-W0Z</td>\n",
              "      <td>['We show that a special goal-condition value ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989</th>\n",
              "      <td>['We introduce a neural architecture to perfor...</td>\n",
              "      <td>[1, 0, 0, 0]</td>\n",
              "      <td>[0.46666666865348816, 0.0714285671710968, 0.0,...</td>\n",
              "      <td>rJgFtkhEtr</td>\n",
              "      <td>['A novel neural architecture for efficient am...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990</th>\n",
              "      <td>['Machine learned large-scale retrieval system...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.277777761220932, 0.0, 0.2857142686843872, 0...</td>\n",
              "      <td>SJxPVcSonN</td>\n",
              "      <td>['We propose a novel two-tower shared-bottom m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991</th>\n",
              "      <td>['The ability to autonomously explore and navi...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n",
              "      <td>[0.10169491171836853, 0.0937499925494194, 0.16...</td>\n",
              "      <td>BJgMFxrYPB</td>\n",
              "      <td>['We address the task of autonomous exploratio...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1992 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd702a19-0f95-4349-9c4d-32b38bd135d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bd702a19-0f95-4349-9c4d-32b38bd135d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bd702a19-0f95-4349-9c4d-32b38bd135d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mcp0h5dDbdA2",
        "outputId": "6953a270-43de-46d5-d6cf-ed4dc40279f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "source           0\n",
              "source_labels    0\n",
              "rouge_scores     0\n",
              "paper_id         0\n",
              "target           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train['source'] = [i.lstrip('[').rstrip(']\\n ') for i in dataset_train['source']]\n",
        "dataset_train['target'] = [i.lstrip('[').rstrip(']\\n ') for i in dataset_train['target']]\n",
        "dataset_train['source'] = [i.lstrip('\\'').rstrip('\\'') for i in dataset_train['source']]\n",
        "dataset_train['target'] = [i.lstrip('\\'').rstrip('\\'') for i in dataset_train['target']]"
      ],
      "metadata": {
        "id": "OGKH4eDktRLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train['source'] = [i.lower() for i in dataset_train['source']]\n",
        "dataset_train['target'] = [i.lower() for i in dataset_train['target']]"
      ],
      "metadata": {
        "id": "2MRSwrVdmcgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "sX4CgzSftmVN",
        "outputId": "17caf3ae-9b50-4bf1-ea64-5dbbaf77c7cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 source  \\\n",
              "0     due to the success of deep learning to solving...   \n",
              "1     the backpropagation (bp) algorithm is often th...   \n",
              "2     we introduce the 2-simplicial transformer, an ...   \n",
              "3     we present tensor-train rnn (tt-rnn), a novel ...   \n",
              "4     recent efforts on combining deep models with p...   \n",
              "...                                                 ...   \n",
              "1987  semi-supervised learning, i.e. jointly learnin...   \n",
              "1988  model-free reinforcement learning (rl) has bee...   \n",
              "1989  we introduce a neural architecture to perform ...   \n",
              "1990  machine learned large-scale retrieval systems ...   \n",
              "1991  the ability to autonomously explore and naviga...   \n",
              "\n",
              "                 source_labels  \\\n",
              "0           [0, 0, 0, 0, 1, 0]   \n",
              "1     [0, 0, 0, 1, 0, 0, 0, 0]   \n",
              "2                       [0, 1]   \n",
              "3           [0, 0, 0, 1, 0, 0]   \n",
              "4        [0, 1, 0, 0, 0, 0, 0]   \n",
              "...                        ...   \n",
              "1987     [0, 0, 0, 1, 0, 0, 0]   \n",
              "1988     [0, 0, 0, 0, 1, 0, 0]   \n",
              "1989              [1, 0, 0, 0]   \n",
              "1990        [0, 0, 0, 1, 0, 0]   \n",
              "1991     [0, 0, 0, 0, 0, 0, 1]   \n",
              "\n",
              "                                           rouge_scores    paper_id  \\\n",
              "0     [0.30188679695129395, 0.3720930218696594, 0.60...   SysEexbRb   \n",
              "1     [0.0, 0.0, 0.1304347813129425, 0.1428571343421...  SygvZ209F7   \n",
              "2              [0.3333333432674408, 0.8888888955116272]  rkecJ6VFvr   \n",
              "3     [0.06666666269302368, 0.06451612710952759, 0.0...   HJJ0w--0W   \n",
              "4     [0.277777761220932, 0.5714285373687744, 0.0952...   HyH9lbZAW   \n",
              "...                                                 ...         ...   \n",
              "1987  [0.07999999821186066, 0.11538460850715637, 0.1...  rJel41BtDH   \n",
              "1988  [0.09302324801683426, 0.08695651590824127, 0.0...   Skw0n-W0Z   \n",
              "1989  [0.46666666865348816, 0.0714285671710968, 0.0,...  rJgFtkhEtr   \n",
              "1990  [0.277777761220932, 0.0, 0.2857142686843872, 0...  SJxPVcSonN   \n",
              "1991  [0.10169491171836853, 0.0937499925494194, 0.16...  BJgMFxrYPB   \n",
              "\n",
              "                                                 target  \n",
              "0     we provide necessary and sufficient analytical...  \n",
              "1     biologically plausible learning algorithms, pa...  \n",
              "2     we introduce the 2-simplicial transformer and ...  \n",
              "3     accurate forecasting over very long time horiz...  \n",
              "4     we propose a variational message-passing algor...  \n",
              "...                                                 ...  \n",
              "1987  pseudo-labeling has shown to be a weak alterna...  \n",
              "1988  we show that a special goal-condition value fu...  \n",
              "1989  a novel neural architecture for efficient amor...  \n",
              "1990  we propose a novel two-tower shared-bottom mod...  \n",
              "1991  we address the task of autonomous exploration ...  \n",
              "\n",
              "[1992 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-df2823cc-763b-49a1-a97d-79f9927a9997\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>source_labels</th>\n",
              "      <th>rouge_scores</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>due to the success of deep learning to solving...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0]</td>\n",
              "      <td>[0.30188679695129395, 0.3720930218696594, 0.60...</td>\n",
              "      <td>SysEexbRb</td>\n",
              "      <td>we provide necessary and sufficient analytical...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the backpropagation (bp) algorithm is often th...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
              "      <td>[0.0, 0.0, 0.1304347813129425, 0.1428571343421...</td>\n",
              "      <td>SygvZ209F7</td>\n",
              "      <td>biologically plausible learning algorithms, pa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>we introduce the 2-simplicial transformer, an ...</td>\n",
              "      <td>[0, 1]</td>\n",
              "      <td>[0.3333333432674408, 0.8888888955116272]</td>\n",
              "      <td>rkecJ6VFvr</td>\n",
              "      <td>we introduce the 2-simplicial transformer and ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>we present tensor-train rnn (tt-rnn), a novel ...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.06666666269302368, 0.06451612710952759, 0.0...</td>\n",
              "      <td>HJJ0w--0W</td>\n",
              "      <td>accurate forecasting over very long time horiz...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>recent efforts on combining deep models with p...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0.277777761220932, 0.5714285373687744, 0.0952...</td>\n",
              "      <td>HyH9lbZAW</td>\n",
              "      <td>we propose a variational message-passing algor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>semi-supervised learning, i.e. jointly learnin...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
              "      <td>[0.07999999821186066, 0.11538460850715637, 0.1...</td>\n",
              "      <td>rJel41BtDH</td>\n",
              "      <td>pseudo-labeling has shown to be a weak alterna...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1988</th>\n",
              "      <td>model-free reinforcement learning (rl) has bee...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.09302324801683426, 0.08695651590824127, 0.0...</td>\n",
              "      <td>Skw0n-W0Z</td>\n",
              "      <td>we show that a special goal-condition value fu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989</th>\n",
              "      <td>we introduce a neural architecture to perform ...</td>\n",
              "      <td>[1, 0, 0, 0]</td>\n",
              "      <td>[0.46666666865348816, 0.0714285671710968, 0.0,...</td>\n",
              "      <td>rJgFtkhEtr</td>\n",
              "      <td>a novel neural architecture for efficient amor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990</th>\n",
              "      <td>machine learned large-scale retrieval systems ...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.277777761220932, 0.0, 0.2857142686843872, 0...</td>\n",
              "      <td>SJxPVcSonN</td>\n",
              "      <td>we propose a novel two-tower shared-bottom mod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991</th>\n",
              "      <td>the ability to autonomously explore and naviga...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n",
              "      <td>[0.10169491171836853, 0.0937499925494194, 0.16...</td>\n",
              "      <td>BJgMFxrYPB</td>\n",
              "      <td>we address the task of autonomous exploration ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1992 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-df2823cc-763b-49a1-a97d-79f9927a9997')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-df2823cc-763b-49a1-a97d-79f9927a9997 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-df2823cc-763b-49a1-a97d-79f9927a9997');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def cleaning(text,num):\n",
        "  str = text.lower()\n",
        "  str = re.sub('\"','', str)\n",
        "\n",
        "  str = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in str.split(\" \")])\n",
        "  str = re.sub(r\"'s\\b\",\"\",str)\n",
        "  str = re.sub(\"[^a-zA-Z]\", \" \", str)\n",
        "  str = re.sub('[m]{2,}', 'mm', str)\n",
        "  if(num==0):\n",
        "    str = re.sub(r'\\.',' . ',str)\n",
        "  if(num==0):\n",
        "      tokens = [w for w in str.split() if not w in stop_words]\n",
        "\n",
        "  else:\n",
        "      tokens=str.split()\n",
        "  long_words=[]\n",
        "  for i in tokens:\n",
        "      if len(i)>1:                                                 #removing short words\n",
        "          long_words.append(i)\n",
        "  return (\" \".join(long_words)).strip()"
      ],
      "metadata": {
        "id": "wJCJid3Ztto_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\"you're\": \"you are\", \"you've\": \"you have\"}"
      ],
      "metadata": {
        "id": "FOkSEryKyJgj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_source = []\n",
        "for t in dataset_train['source']:\n",
        "    clean_source.append(cleaning(t,0))\n",
        "\n",
        "clean_target = []\n",
        "for t in dataset_train['target']:\n",
        "    clean_target.append(cleaning(t,0))"
      ],
      "metadata": {
        "id": "4mM7YJB0yM15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train['source'] = clean_source\n",
        "dataset_train['target'] = clean_target\n",
        "\n",
        "dataset_train.replace('', np.nan, inplace=True)\n",
        "dataset_train.dropna(axis=0,inplace=True)"
      ],
      "metadata": {
        "id": "P7LuuEkqyQ0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "coEH1oAEyQy-",
        "outputId": "78df7130-5a51-4e0f-f578-5a21b3244078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 source  \\\n",
              "0     due success deep learning solving variety chal...   \n",
              "1     backpropagation bp algorithm often thought bio...   \n",
              "2     introduce simplicial transformer extension tra...   \n",
              "3     present tensor train rnn tt rnn novel family n...   \n",
              "4     recent efforts combining deep models probabili...   \n",
              "...                                                 ...   \n",
              "1987  semi supervised learning jointly learning labe...   \n",
              "1988  model free reinforcement learning rl proven po...   \n",
              "1989  introduce neural architecture perform amortize...   \n",
              "1990  machine learned large scale retrieval systems ...   \n",
              "1991  ability autonomously explore navigate physical...   \n",
              "\n",
              "                 source_labels  \\\n",
              "0           [0, 0, 0, 0, 1, 0]   \n",
              "1     [0, 0, 0, 1, 0, 0, 0, 0]   \n",
              "2                       [0, 1]   \n",
              "3           [0, 0, 0, 1, 0, 0]   \n",
              "4        [0, 1, 0, 0, 0, 0, 0]   \n",
              "...                        ...   \n",
              "1987     [0, 0, 0, 1, 0, 0, 0]   \n",
              "1988     [0, 0, 0, 0, 1, 0, 0]   \n",
              "1989              [1, 0, 0, 0]   \n",
              "1990        [0, 0, 0, 1, 0, 0]   \n",
              "1991     [0, 0, 0, 0, 0, 0, 1]   \n",
              "\n",
              "                                           rouge_scores    paper_id  \\\n",
              "0     [0.30188679695129395, 0.3720930218696594, 0.60...   SysEexbRb   \n",
              "1     [0.0, 0.0, 0.1304347813129425, 0.1428571343421...  SygvZ209F7   \n",
              "2              [0.3333333432674408, 0.8888888955116272]  rkecJ6VFvr   \n",
              "3     [0.06666666269302368, 0.06451612710952759, 0.0...   HJJ0w--0W   \n",
              "4     [0.277777761220932, 0.5714285373687744, 0.0952...   HyH9lbZAW   \n",
              "...                                                 ...         ...   \n",
              "1987  [0.07999999821186066, 0.11538460850715637, 0.1...  rJel41BtDH   \n",
              "1988  [0.09302324801683426, 0.08695651590824127, 0.0...   Skw0n-W0Z   \n",
              "1989  [0.46666666865348816, 0.0714285671710968, 0.0,...  rJgFtkhEtr   \n",
              "1990  [0.277777761220932, 0.0, 0.2857142686843872, 0...  SJxPVcSonN   \n",
              "1991  [0.10169491171836853, 0.0937499925494194, 0.16...  BJgMFxrYPB   \n",
              "\n",
              "                                                 target  \n",
              "0     provide necessary sufficient analytical forms ...  \n",
              "1     biologically plausible learning algorithms par...  \n",
              "2     introduce simplicial transformer show architec...  \n",
              "3     accurate forecasting long time horizons using ...  \n",
              "4     propose variational message passing algorithm ...  \n",
              "...                                                 ...  \n",
              "1987  pseudo labeling shown weak alternative semi su...  \n",
              "1988  show special goal condition value function tra...  \n",
              "1989  novel neural architecture efficient amortized ...  \n",
              "1990  propose novel two tower shared bottom model ar...  \n",
              "1991  address task autonomous exploration navigation...  \n",
              "\n",
              "[1992 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4b80084f-a762-42ee-9b47-96db91da822d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>source_labels</th>\n",
              "      <th>rouge_scores</th>\n",
              "      <th>paper_id</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>due success deep learning solving variety chal...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0]</td>\n",
              "      <td>[0.30188679695129395, 0.3720930218696594, 0.60...</td>\n",
              "      <td>SysEexbRb</td>\n",
              "      <td>provide necessary sufficient analytical forms ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>backpropagation bp algorithm often thought bio...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
              "      <td>[0.0, 0.0, 0.1304347813129425, 0.1428571343421...</td>\n",
              "      <td>SygvZ209F7</td>\n",
              "      <td>biologically plausible learning algorithms par...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>introduce simplicial transformer extension tra...</td>\n",
              "      <td>[0, 1]</td>\n",
              "      <td>[0.3333333432674408, 0.8888888955116272]</td>\n",
              "      <td>rkecJ6VFvr</td>\n",
              "      <td>introduce simplicial transformer show architec...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>present tensor train rnn tt rnn novel family n...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.06666666269302368, 0.06451612710952759, 0.0...</td>\n",
              "      <td>HJJ0w--0W</td>\n",
              "      <td>accurate forecasting long time horizons using ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>recent efforts combining deep models probabili...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[0.277777761220932, 0.5714285373687744, 0.0952...</td>\n",
              "      <td>HyH9lbZAW</td>\n",
              "      <td>propose variational message passing algorithm ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1987</th>\n",
              "      <td>semi supervised learning jointly learning labe...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0, 0]</td>\n",
              "      <td>[0.07999999821186066, 0.11538460850715637, 0.1...</td>\n",
              "      <td>rJel41BtDH</td>\n",
              "      <td>pseudo labeling shown weak alternative semi su...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1988</th>\n",
              "      <td>model free reinforcement learning rl proven po...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.09302324801683426, 0.08695651590824127, 0.0...</td>\n",
              "      <td>Skw0n-W0Z</td>\n",
              "      <td>show special goal condition value function tra...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1989</th>\n",
              "      <td>introduce neural architecture perform amortize...</td>\n",
              "      <td>[1, 0, 0, 0]</td>\n",
              "      <td>[0.46666666865348816, 0.0714285671710968, 0.0,...</td>\n",
              "      <td>rJgFtkhEtr</td>\n",
              "      <td>novel neural architecture efficient amortized ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1990</th>\n",
              "      <td>machine learned large scale retrieval systems ...</td>\n",
              "      <td>[0, 0, 0, 1, 0, 0]</td>\n",
              "      <td>[0.277777761220932, 0.0, 0.2857142686843872, 0...</td>\n",
              "      <td>SJxPVcSonN</td>\n",
              "      <td>propose novel two tower shared bottom model ar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1991</th>\n",
              "      <td>ability autonomously explore navigate physical...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1]</td>\n",
              "      <td>[0.10169491171836853, 0.0937499925494194, 0.16...</td>\n",
              "      <td>BJgMFxrYPB</td>\n",
              "      <td>address task autonomous exploration navigation...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1992 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4b80084f-a762-42ee-9b47-96db91da822d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4b80084f-a762-42ee-9b47-96db91da822d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4b80084f-a762-42ee-9b47-96db91da822d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "source_word_count = []\n",
        "target_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in dataset_train['source']:\n",
        "      temp=i.split()\n",
        "      source_word_count.append(len(temp))\n",
        "\n",
        "for j in dataset_train['target']:\n",
        "  temp1=j.split()\n",
        "  target_word_count.append(len(temp1))"
      ],
      "metadata": {
        "id": "LMX6AIiB1KfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(source_word_count)\n",
        "plt.title('Source Distribution')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Number of Source Sentences')\n",
        "plt.show()\n",
        "\n",
        "sns.histplot(target_word_count)\n",
        "plt.title('Target Distribution')\n",
        "plt.xlabel('Word Count')\n",
        "plt.ylabel('Number of Target Sentence')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        },
        "id": "bznzRhgK3abl",
        "outputId": "ecd5282e-8640-4947-a338-11b9339b96ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABE5klEQVR4nO3de1wVdf7H8fdBuYgiiIpAoeAl73dXcrWEsAxbtdVK03bRXC1X84KZ8tv1uraQlpmuq2WmVppZlqZtqHnXEC9JVioJopairDdQVOQyvz96eLYToBw53KbX8/GYxzLf7/fMfM6s4duZ78xYDMMwBAAAYFJOZV0AAABASSLsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsADCNkJAQhYSElMq+LBaLpk6dal2fOnWqLBaLzp8/Xyr7DwwM1KBBg0plX0BFR9gBKhiLxVKkZdu2bWVdqo2vvvpKU6dO1eXLl4s0ftCgQTbfp1q1aqpfv76eeOIJrV69Wnl5eWVSV2kqz7UBFUnlsi4AgH3ee+89m/V3331XmzZtytfetGnT0izrjr766itNmzZNgwYNkpeXV5E+4+rqqrfffluSdP36dZ08eVLr1q3TE088oZCQEK1du1bVq1e3jt+4cWOp1HWrnsqVS/ZX6O1qS0xMlJMT/14FioKwA1QwzzzzjM36nj17tGnTpnztd8MwDN24cUNVqlQp9rYcoXLlyvm+14wZMxQTE6OoqCgNHTpUH374obXPxcWlROvJy8vTzZs35ebmJjc3txLd1524urqW6f6BioR/FgAmtGTJEj300EPy8fGRq6urmjVrpgULFuQbFxgYqD/84Q/asGGDOnTooCpVqujNN9+UJJ08eVK9evVS1apV5ePjo7Fjx2rDhg0FXiKLj4/Xo48+Kk9PT7m7u6tr167avXu3tX/q1KkaP368JCkoKMh6aerEiRN39f0mTpyoRx55RB999JF++OEHa3tBc3bmzZun5s2by93dXTVq1FCHDh20YsWKItVlsVg0cuRILV++XM2bN5erq6tiY2Otfb+cs3PL+fPn9dRTT6l69eqqWbOmRo8erRs3blj7T5w4IYvFoqVLl+b77C+3eafaCpqzc/z4cT355JPy9vaWu7u77r//fn3++ec2Y7Zt2yaLxaJVq1bp5Zdf1r333is3NzeFhYUpKSmp0GMOVGSc2QFMaMGCBWrevLl69eqlypUra926dfrrX/+qvLw8jRgxwmZsYmKinn76aT333HMaOnSoGjdurMzMTD300ENKTU3V6NGj5evrqxUrVmjr1q359rVlyxaFh4erffv2mjJlipycnKxha+fOnerYsaP69OmjH374QR988IFef/111apVS5JUu3btu/6Of/rTn7Rx40Zt2rRJ9913X4FjFi1apFGjRumJJ56who5Dhw4pPj5eAwYMKFJdW7Zs0apVqzRy5EjVqlVLgYGBt63rqaeeUmBgoKKjo7Vnzx7NnTtXly5d0rvvvmvX97P3mJ07d06///3vde3aNY0aNUo1a9bUsmXL1KtXL3388cf64x//aDM+JiZGTk5OevHFF5Wenq6ZM2dq4MCBio+Pt6tOoEIwAFRoI0aMMH79n/K1a9fyjevevbtRv359m7Z69eoZkozY2Fib9tdee82QZKxZs8badv36daNJkyaGJGPr1q2GYRhGXl6e0ahRI6N79+5GXl6ezf6DgoKMhx9+2No2a9YsQ5KRkpJSpO8VERFhVK1atdD+gwcPGpKMsWPHWtu6du1qdO3a1breu3dvo3nz5rfdz+3qkmQ4OTkZ33//fYF9U6ZMsa5PmTLFkGT06tXLZtxf//pXQ5LxzTffGIZhGCkpKYYkY8mSJXfc5u1qq1evnhEREWFdHzNmjCHJ2Llzp7XtypUrRlBQkBEYGGjk5uYahmEYW7duNSQZTZs2NbKysqxj33jjDUOS8e233+bbF1DRcRkLMKFfzrlJT0/X+fPn1bVrVx0/flzp6ek2Y4OCgtS9e3ebttjYWN1zzz3q1auXtc3NzU1Dhw61GZeQkKBjx45pwIABunDhgs6fP6/z588rMzNTYWFh2rFjh8Pumvq1atWqSZKuXLlS6BgvLy/99NNP2rdv313vp2vXrmrWrFmRx//6zNkLL7wgSfrPf/5z1zUUxX/+8x917NhRXbp0sbZVq1ZNw4YN04kTJ3T48GGb8YMHD7aZ4/TAAw9I+vlSGGA2XMYCTGj37t2aMmWK4uLidO3aNZu+9PR0eXp6WteDgoLyff7kyZNq0KCBLBaLTXvDhg1t1o8dOyZJioiIKLSW9PR01ahRw+7vcCdXr16VJHl4eBQ6ZsKECfryyy/VsWNHNWzYUI888ogGDBigzp07F3k/BR2f22nUqJHNeoMGDeTk5HTX85OK6uTJkwoODs7XfuuuvJMnT6pFixbW9rp169qMu/X/0aVLl0qwSqBsEHYAk0lOTlZYWJiaNGmi2bNnKyAgQC4uLvrPf/6j119/Pd+ZluLceXVrW7NmzVKbNm0KHHPrDIyjfffdd5LyB7Bfatq0qRITE7V+/XrFxsZq9erV+ve//63Jkydr2rRpRdpPce9M+3Vg/PX6Lbm5ucXaj70qVapUYLthGKVaB1AaCDuAyaxbt05ZWVn67LPPbP71XtDk4sLUq1dPhw8flmEYNn85//punQYNGkiSqlevrm7dut12m4X9JX+33nvvPVksFj388MO3HVe1alX169dP/fr1082bN9WnTx+9/PLLioqKkpubm8PrOnbsmM3ZoKSkJOXl5VknNt86g/LrBwWePHky37bsqa1evXpKTEzM13706FFrP/BbxZwdwGRu/Yv9l/9CT09P15IlS4q8je7du+v06dP67LPPrG03btzQokWLbMa1b99eDRo00Kuvvmq9rPRL//3vf60/V61aVVL+v+TvRkxMjDZu3Kh+/frlu2z0SxcuXLBZd3FxUbNmzWQYhrKzsx1elyTNnz/fZn3evHmSpPDwcEk/B8NatWppx44dNuP+/e9/59uWPbX16NFDe/fuVVxcnLUtMzNTb731lgIDA+2adwSYDWd2AJN55JFH5OLiop49e+q5557T1atXtWjRIvn4+Cg1NbVI23juuef0r3/9S08//bRGjx4tPz8/LV++3PogvVtnHJycnPT2228rPDxczZs31+DBg3XPPffo9OnT2rp1q6pXr65169ZJ+jkYSdLf/vY39e/fX87OzurZs6f1L/SC5OTk6P3335f0c9g6efKkPvvsMx06dEihoaF666237ngsfH191blzZ9WpU0dHjhzRv/71Lz322GPWuT53U9ftpKSkqFevXnr00UcVFxen999/XwMGDFDr1q2tY/7yl78oJiZGf/nLX9ShQwft2LHD5nlBt9hT28SJE/XBBx8oPDxco0aNkre3t5YtW6aUlBStXr2apy3jt61sbwYDUFwF3Xr+2WefGa1atTLc3NyMwMBA45VXXjHeeeedfLcx16tXz3jssccK3O7x48eNxx57zKhSpYpRu3ZtY9y4ccbq1asNScaePXtsxh48eNDo06ePUbNmTcPV1dWoV6+e8dRTTxmbN2+2GfePf/zDuOeeewwnJ6c73oYeERFhSLIu7u7uRmBgoNG3b1/j448/tt5K/Uu/vvX8zTffNB588EFrXQ0aNDDGjx9vpKenF6kuScaIESMKrE+F3Hp++PBh44knnjA8PDyMGjVqGCNHjjSuX79u89lr164ZQ4YMMTw9PQ0PDw/jqaeeMtLS0vJt83a1/frWc8MwjOTkZOOJJ54wvLy8DDc3N6Njx47G+vXrbcbcuvX8o48+smm/3S3xQEVnMQxmowEomjlz5mjs2LH66aefdM8995R1OQBQJIQdAAW6fv26zZ1IN27cUNu2bZWbm1vgJRcAKK+YswOgQH369FHdunXVpk0bpaen6/3339fRo0e1fPnysi4NAOxC2AFQoO7du+vtt9/W8uXLlZubq2bNmmnlypXq169fWZcGAHbhMhYAADA17kUEAACmRtgBAACmxpwd/fx+nzNnzsjDw8Phj44HAAAlwzAMXblyRf7+/rd9cCZhR9KZM2cUEBBQ1mUAAIC78OOPP+ree+8ttJ+wI1kfG//jjz+qevXqZVwNAAAoioyMDAUEBFj/Hi8MYUf/e89P9erVCTsAAFQwd5qCwgRlAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgaoQdAABgamUadnbs2KGePXvK399fFotFa9assem3WCwFLrNmzbKOCQwMzNcfExNTyt8EAACUV2UadjIzM9W6dWvNnz+/wP7U1FSb5Z133pHFYlHfvn1txk2fPt1m3AsvvFAa5QMAgAqgTF8EGh4ervDw8EL7fX19bdbXrl2r0NBQ1a9f36bdw8Mj31gAAACpAs3ZOXfunD7//HMNGTIkX19MTIxq1qyptm3batasWcrJySmDCgEAQHlUpmd27LFs2TJ5eHioT58+Nu2jRo1Su3bt5O3tra+++kpRUVFKTU3V7NmzC91WVlaWsrKyrOsZGRklVjcAAChbFSbsvPPOOxo4cKDc3Nxs2iMjI60/t2rVSi4uLnruuecUHR0tV1fXArcVHR2tadOmlWi9KJrw3n2VmnahwL4zp3+U/z0BBfb5+dTUF2tXl2RpAACTqBBhZ+fOnUpMTNSHH354x7HBwcHKycnRiRMn1Lhx4wLHREVF2YSkjIwMBQQU/JcqSlZq2gU1GRxdYF/SpH6F9h1dElWSZQEATKRChJ3Fixerffv2at269R3HJiQkyMnJST4+PoWOcXV1LfSsDwAAMJcyDTtXr15VUlKSdT0lJUUJCQny9vZW3bp1Jf181uWjjz7Sa6+9lu/zcXFxio+PV2hoqDw8PBQXF6exY8fqmWeeUY0aNUrtewAAgPKrTMPO/v37FRoaal2/dWkpIiJCS5culSStXLlShmHo6aefzvd5V1dXrVy5UlOnTlVWVpaCgoI0duxYm0tUAADgt61Mw05ISIgMw7jtmGHDhmnYsGEF9rVr10579uwpidIAAIBJVJjn7AAAANwNwg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADA1wg4AADC1ymVdACq+8N59lZp2odB+P5+a+mLt6lKsCACA/yHsoNhS0y6oyeDoQvuPLokqxWoAALDFZSwAAGBqhB0AAGBqhB0AAGBqhB0AAGBqTFBGiUs5flxtOoUU2Hfi1Ck1Kd1yAAC/MYQdlLhcqdC7tZIm9SvdYgAAvzlcxgIAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZG2AEAAKZWpmFnx44d6tmzp/z9/WWxWLRmzRqb/kGDBslisdgsjz76qM2YixcvauDAgapevbq8vLw0ZMgQXb16tRS/BQAAKM/KNOxkZmaqdevWmj9/fqFjHn30UaWmplqXDz74wKZ/4MCB+v7777Vp0yatX79eO3bs0LBhw0q6dAAAUEFULsudh4eHKzw8/LZjXF1d5evrW2DfkSNHFBsbq3379qlDhw6SpHnz5qlHjx569dVX5e/v7/CaAQBAxVLu5+xs27ZNPj4+aty4sYYPH64LFy5Y++Li4uTl5WUNOpLUrVs3OTk5KT4+vtBtZmVlKSMjw2YBAADmVK7DzqOPPqp3331Xmzdv1iuvvKLt27crPDxcubm5kqSzZ8/Kx8fH5jOVK1eWt7e3zp49W+h2o6Oj5enpaV0CAgJK9HsAAICyU6aXse6kf//+1p9btmypVq1aqUGDBtq2bZvCwsLuertRUVGKjIy0rmdkZBB4AAAwqXJ9ZufX6tevr1q1aikpKUmS5Ovrq7S0NJsxOTk5unjxYqHzfKSf5wFVr17dZgEAAOZUocLOTz/9pAsXLsjPz0+S1KlTJ12+fFkHDhywjtmyZYvy8vIUHBxcVmUCAIBypEwvY129etV6lkaSUlJSlJCQIG9vb3l7e2vatGnq27evfH19lZycrJdeekkNGzZU9+7dJUlNmzbVo48+qqFDh2rhwoXKzs7WyJEj1b9/f+7EMrmU48fVplNIgX1+PjX1xdrVpVsQAKDcKtOws3//foWGhlrXb82jiYiI0IIFC3To0CEtW7ZMly9flr+/vx555BH94x//kKurq/Uzy5cv18iRIxUWFiYnJyf17dtXc+fOLfXvgtKVK6nJ4OgC+44uiSrdYgAA5VqZhp2QkBAZhlFo/4YNG+64DW9vb61YscKRZQEAABOpUHN2AAAA7EXYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApnZXYWfnzp165pln1KlTJ50+fVqS9N5772nXrl0OLQ4AAKC47A47q1evVvfu3VWlShUdPHhQWVlZkqT09HT985//dHiBAAAAxWF32JkxY4YWLlyoRYsWydnZ2dreuXNnff311w4tDgAAoLgq2/uBxMREPfjgg/naPT09dfnyZUfUBJSY8N59lZp2odB+P5+a+mLt6lKsCABQ0uwOO76+vkpKSlJgYKBN+65du1S/fn1H1QWUiNS0C2oyOLrQ/qNLokqxGgBAabD7MtbQoUM1evRoxcfHy2Kx6MyZM1q+fLlefPFFDR8+vCRqBAAAuGt2n9mZOHGi8vLyFBYWpmvXrunBBx+Uq6urXnzxRb3wwgslUSMAAMBdszvsWCwW/e1vf9P48eOVlJSkq1evqlmzZqpWrVpJ1AcAAFAsdoed9PR05ebmytvbW82aNbO2X7x4UZUrV1b16tUdWiAAAEBx2D1np3///lq5cmW+9lWrVql///4OKQoAAMBR7A478fHxCg0NzdceEhKi+Ph4hxQFAADgKHaHnaysLOXk5ORrz87O1vXr1x1SFAAAgKPYHXY6duyot956K1/7woUL1b59e4cUBQAA4Ch2T1CeMWOGunXrpm+++UZhYWGSpM2bN2vfvn3auHGjwwsEAAAoDrvP7HTu3FlxcXEKCAjQqlWrtG7dOjVs2FCHDh3SAw88UBI1AgAA3DW7z+xIUps2bbR8+XJH1wIAAOBwdxV28vLylJSUpLS0NOXl5dn0FfSSUAAAgLJid9jZs2ePBgwYoJMnT8owDJs+i8Wi3NxchxUHAABQXHaHneeff14dOnTQ559/Lj8/P1kslpKoCwAAwCHsDjvHjh3Txx9/rIYNG5ZEPQAAAA5l991YwcHBSkpKKolaAAAAHM7uMzsvvPCCxo0bp7Nnz6ply5Zydna26W/VqpXDigPuRsrx42rTKaTAvhOnTqlJ6ZYDAChjdoedvn37SpKeffZZa5vFYpFhGExQRrmQK6nJ4OgC+5Im9SvdYgAAZc7uy1gpKSn5luPHj1v/1x47duxQz5495e/vL4vFojVr1lj7srOzNWHCBLVs2VJVq1aVv7+//vznP+vMmTM22wgMDJTFYrFZYmJi7P1aAADApOw+s1OvXj2H7TwzM1OtW7fWs88+qz59+tj0Xbt2TV9//bUmTZqk1q1b69KlSxo9erR69eql/fv324ydPn26hg4dal338PBwWI0AAKBiu6uHCr733ntauHChUlJSFBcXp3r16mnOnDkKCgpS7969i7yd8PBwhYeHF9jn6empTZs22bT961//UseOHXXq1CnVrVvX2u7h4SFfX9+7+SooovDefZWadqHAPubBAADKM7svYy1YsECRkZHq0aOHLl++bJ2j4+XlpTlz5ji6Phvp6emyWCzy8vKyaY+JiVHNmjXVtm1bzZo1Szk5ObfdTlZWljIyMmwW3F5q2gU1GRxd4HKn4w0AQFmyO+zMmzdPixYt0t/+9jdVqlTJ2t6hQwd9++23Di3ul27cuKEJEybo6aefVvXq1a3to0aN0sqVK7V161Y999xz+uc//6mXXnrpttuKjo6Wp6endQkICCixugEAQNmy+zJWSkqK2rZtm6/d1dVVmZmZDinq17Kzs/XUU0/JMAwtWLDApi8yMtL6c6tWreTi4qLnnntO0dHRcnV1LXB7UVFRNp/LyMgg8AAAYFJ2n9kJCgpSQkJCvvbY2Fg1bdrUETXZuBV0Tp48qU2bNtmc1SlIcHCwcnJydOLEiULHuLq6qnr16jYLAAAwJ7vP7ERGRmrEiBG6ceOGDMPQ3r179cEHHyg6Olpvv/22Q4u7FXSOHTumrVu3qmbNmnf8TEJCgpycnOTj4+PQWgAAQMVkd9j5y1/+oipVqujvf/+7rl27pgEDBsjf319vvPGG+vfvb9e2rl69avPqiZSUFCUkJMjb21t+fn564okn9PXXX2v9+vXKzc3V2bNnJUne3t5ycXFRXFyc4uPjFRoaKg8PD8XFxWns2LF65plnVKNGDXu/GgAAMKG7uvV84MCBGjhwoK5du6arV6/e9VmU/fv3KzQ01Lp+ax5NRESEpk6dqs8++0yS1KZNG5vPbd26VSEhIXJ1ddXKlSs1depUZWVlKSgoSGPHjrWZjwMAAH7b7A47Dz30kD755BN5eXnJ3d1d7u7ukn6e5Pv4449ry5YtRd5WSEiIDMMotP92fZLUrl077dmzp8j7AwAAvz12T1Detm2bbt68ma/9xo0b2rlzp0OKAgAAcJQin9k5dOiQ9efDhw9b589IUm5urmJjY3XPPfc4tjoAAIBiKnLYadOmjfVFmw899FC+/ipVqmjevHkOLQ4AAKC4ihx2UlJSZBiG6tevr71796p27drWPhcXF/n4+Ng8URkAAKA8KHLYufW287y8vBIrBgAAwNHu6tbzWw/5S0tLyxd+Jk+e7JDCUPp4szkAwIzsDjuLFi3S8OHDVatWLfn6+spisVj7LBYLYacCu/Vm84IkTepXytUAAOAYdoedGTNm6OWXX9aECRNKoh4AAACHsvs5O5cuXdKTTz5ZErUAAAA4nN1h58knn9TGjRtLohYAAACHs/syVsOGDTVp0iTt2bNHLVu2lLOzs03/qFGjHFYcAABAcdkddt566y1Vq1ZN27dv1/bt2236LBYLYQcAAJQrdoedlJSUkqgDAACgRNg9Z+eWmzdvKjExUTk5OY6sBwAAwKHsDjvXrl3TkCFD5O7urubNm+vUqVOSpBdeeEExMTEOLxAAAKA47A47UVFR+uabb7Rt2za5ublZ27t166YPP/zQocUBAAAUl91zdtasWaMPP/xQ999/v83Tk5s3b67k5GSHFgcAAFBcdp/Z+e9//ysfH5987ZmZmTbhBwAAoDywO+x06NBBn3/+uXX9VsB5++231alTJ8dVBgAA4AB2X8b65z//qfDwcB0+fFg5OTl64403dPjwYX311Vf5nrsDAABQ1uw+s9OlSxclJCQoJydHLVu21MaNG+Xj46O4uDi1b9++JGoEAAC4a3af2ZGkBg0aaNGiRY6uBQAAwOGKHHZycnKUm5srV1dXa9u5c+e0cOFCZWZmqlevXurSpUuJFAkAAHC3ihx2hg4dKhcXF7355puSpCtXruh3v/udbty4IT8/P73++utau3atevToUWLFAgAA2KvIc3Z2796tvn37Wtffffdd5ebm6tixY/rmm28UGRmpWbNmlUiRAAAAd6vIYef06dNq1KiRdX3z5s3q27evPD09JUkRERH6/vvvHV8hAABAMRQ57Li5uen69evW9T179ig4ONim/+rVq46tDgAAoJiKHHbatGmj9957T5K0c+dOnTt3Tg899JC1Pzk5Wf7+/o6vEAAAoBiKPEF58uTJCg8P16pVq5SamqpBgwbJz8/P2v/pp5+qc+fOJVIkAADA3Spy2OnatasOHDigjRs3ytfXV08++aRNf5s2bdSxY0eHFwgAAFAcdj1UsGnTpmratGmBfcOGDXNIQQAAAI5k9+siAAAAKhLCDgAAMDXCDgAAMDXCDgAAMDW7w079+vV14cKFfO2XL19W/fr1HVIUAACAo9gddk6cOKHc3Nx87VlZWTp9+rRDigIAAHCUIt96/tlnn1l/3rBhg/WdWJKUm5urzZs3KzAw0KHFAQAAFFeRw87jjz8uSbJYLIqIiLDpc3Z2VmBgoF577TWHFgcAAFBcRb6MlZeXp7y8PNWtW1dpaWnW9by8PGVlZSkxMVF/+MMf7Nr5jh071LNnT/n7+8tisWjNmjU2/YZhaPLkyfLz81OVKlXUrVs3HTt2zGbMxYsXNXDgQFWvXl1eXl4aMmQILyQFAABWds/ZSUlJUa1atSRJN27cKNbOMzMz1bp1a82fP7/A/pkzZ2ru3LlauHCh4uPjVbVqVXXv3t1mvwMHDtT333+vTZs2af369dqxYwdPcwYAAFZ2h528vDz94x//0D333KNq1arp+PHjkqRJkyZp8eLFdm0rPDxcM2bM0B//+Md8fYZhaM6cOfr73/+u3r17q1WrVnr33Xd15swZ6xmgI0eOKDY2Vm+//baCg4PVpUsXzZs3TytXrtSZM2fs/WoAAMCE7A47M2bM0NKlSzVz5ky5uLhY21u0aKG3337bYYWlpKTo7Nmz6tatm7XN09NTwcHBiouLkyTFxcXJy8tLHTp0sI7p1q2bnJycFB8f77BaAABAxWV32Hn33Xf11ltvaeDAgapUqZK1vXXr1jp69KjDCjt79qwkqU6dOjbtderUsfadPXtWPj4+Nv2VK1eWt7e3dUxBsrKylJGRYbMAAABzsjvsnD59Wg0bNszXnpeXp+zsbIcUVdKio6Pl6elpXQICAsq6JAAAUELsDjvNmjXTzp0787V//PHHatu2rUOKkiRfX19J0rlz52zaz507Z+3z9fVVWlqaTX9OTo4uXrxoHVOQqKgopaenW5cff/zRYXUDAIDypcjP2bll8uTJioiI0OnTp5WXl6dPPvlEiYmJevfdd7V+/XqHFRYUFCRfX19t3rxZbdq0kSRlZGQoPj5ew4cPlyR16tRJly9f1oEDB9S+fXtJ0pYtW5SXl6fg4OBCt+3q6ipXV1eH1QoAAMovu8NO7969tW7dOk2fPl1Vq1bV5MmT1a5dO61bt04PP/ywXdu6evWqkpKSrOspKSlKSEiQt7e36tatqzFjxmjGjBlq1KiRgoKCNGnSJPn7+1sfcNi0aVM9+uijGjp0qBYuXKjs7GyNHDlS/fv3l7+/v71fDQAAmJDdYUeSHnjgAW3atKnYO9+/f79CQ0Ot65GRkZKkiIgILV26VC+99JIyMzM1bNgwXb58WV26dFFsbKzc3Nysn1m+fLlGjhypsLAwOTk5qW/fvpo7d26xawMAAOZwV2HHUUJCQmQYRqH9FotF06dP1/Tp0wsd4+3trRUrVpREeQAAwATsDjs1atSQxWLJ126xWOTm5qaGDRtq0KBBGjx4sEMKBAAAKI67mqD88ssvKzw8XB07dpQk7d27V7GxsRoxYoRSUlI0fPhw5eTkaOjQoQ4vGAAAwB52h51du3ZpxowZev75523a33zzTW3cuFGrV69Wq1atNHfuXMIOAAAoc3Y/Z2fDhg02r3C4JSwsTBs2bJAk9ejRw/rOLAAAgLJk95kdb29vrVu3TmPHjrVpX7dunby9vSX9/DZzDw8Px1QIlBPhvfsqNe1CgX1+PjX1xdrVpVwRAKAo7A47kyZN0vDhw7V161brnJ19+/bpP//5jxYuXChJ2rRpk7p27erYSoEylpp2QU0GRxfYd3RJVClXAwAoKrvDztChQ9WsWTP961//0ieffCJJaty4sbZv367f//73kqRx48Y5tkoAAIC7ZFfYyc7O1nPPPadJkybpgw8+KKmaAAAAHMauCcrOzs5avZp5CQAAoOKw+26sxx9/XGvWrCmBUgAAABzP7jk7jRo10vTp07V79261b99eVatWtekfNWqUw4oDAAAoLrvDzuLFi+Xl5aUDBw7owIEDNn0Wi4WwAwAAyhW7w05KSkpJ1AEAAFAi7J6zAwAAUJHYfWZHkn766Sd99tlnOnXqlG7evGnTN3v2bIcUBgAA4Ah2h53NmzerV69eql+/vo4ePaoWLVroxIkTMgxD7dq1K4kaAQAA7prdl7GioqL04osv6ttvv5Wbm5tWr16tH3/8UV27dtWTTz5ZEjUCAADcNbvDzpEjR/TnP/9ZklS5cmVdv35d1apV0/Tp0/XKK684vEAAAIDisPsyVtWqVa3zdPz8/JScnKzmzZtLks6fP+/Y6oBSlnL8uNp0Cimw78SpU2pSuuUAABygyGFn+vTpGjdunO6//37t2rVLTZs2VY8ePTRu3Dh9++23+uSTT3T//feXZK1AicuVCn2zedKkfqVbDADAIYp8GWvatGnKzMzU7NmzFRwcbG0LCwvThx9+qMDAQC1evLjECgUAALgbRT6zYxiGJKl+/frWtqpVq2rhwoWOrwoAAMBB7JqgbLFYSqoOAACAEmHXBOX77rvvjoHn4sWLxSoIAADAkewKO9OmTZOnp2dJ1QIAAOBwdoWd/v37y8fHp6RqAQAAcLgiz9lhvg4AAKiIihx2bt2NBQAAUJEU+TJWXl5eSdYBAABQIux+NxYAAEBFYve7sVCxhffuq9S0CwX28e4nAIAZEXZ+Y1LTLvDuJwDAb0qRLmO1a9dOly5dkvTzC0GvXbtWokUBAAA4SpHCzpEjR5SZmSnp5wcLXr16tUSLAgAAcJQiXcZq06aNBg8erC5dusgwDL366quqVq1agWMnT57s0AIBAACKo0hhZ+nSpZoyZYrWr18vi8WiL774QpUr5/+oxWIh7AAAgHKlSGGncePGWrlypSTJyclJmzdv5rURAACgQrD7biweLggAACqSu7r1PDk5WXPmzNGRI0ckSc2aNdPo0aPVoEEDhxYHAABQXHY/QXnDhg1q1qyZ9u7dq1atWqlVq1aKj49X8+bNtWnTppKoEQAA4K7ZHXYmTpyosWPHKj4+XrNnz9bs2bMVHx+vMWPGaMKECQ4vMDAwUBaLJd8yYsQISVJISEi+vueff97hdQAAgIrJ7stYR44c0apVq/K1P/vss5ozZ44jarKxb98+5ebmWte/++47Pfzww3ryySetbUOHDtX06dOt6+7u7g6vAwAAVEx2h53atWsrISFBjRo1smlPSEgokTu0ateubbMeExOjBg0aqGvXrtY2d3d3+fr6OnzfAACg4rM77AwdOlTDhg3T8ePH9fvf/16StHv3br3yyiuKjIx0eIG/dPPmTb3//vuKjIyUxWKxti9fvlzvv/++fH191bNnT02aNOm2Z3eysrKUlZVlXc/IyCjRugEAQNmxO+xMmjRJHh4eeu211xQVFSVJ8vf319SpUzVq1CiHF/hLa9as0eXLlzVo0CBr24ABA1SvXj35+/vr0KFDmjBhghITE/XJJ58Uup3o6GhNmzatRGsFAADlg91hx2KxaOzYsRo7dqyuXLkiSfLw8HB4YQVZvHixwsPD5e/vb20bNmyY9eeWLVvKz89PYWFhSk5OLvRW+KioKJuzUBkZGQoICCi5wgEAQJm5q+fs3FJaIUeSTp48qS+//PK2Z2wkKTg4WJKUlJRUaNhxdXWVq6urw2sEAADlj923npeVJUuWyMfHR4899thtxyUkJEiS/Pz8SqEqAABQ3hXrzE5pycvL05IlSxQREWHzAtLk5GStWLFCPXr0UM2aNXXo0CGNHTtWDz74oFq1alWGFQMAgPKiQoSdL7/8UqdOndKzzz5r0+7i4qIvv/xSc+bMUWZmpgICAtS3b1/9/e9/L6NKAQBAeWNX2MnOztajjz6qhQsX5nvOTkl65JFHZBhGvvaAgABt37691OoACpNy/LjadAopsM/Pp6a+WLu6dAsCAFjZFXacnZ116NChkqoFqLByJTUZHF1g39ElUaVbDADAht0TlJ955hktXry4JGoBAABwOLvn7OTk5Oidd97Rl19+qfbt26tq1ao2/bNnz3ZYcQAAAMVld9j57rvv1K5dO0nSDz/8YNP3y1c4AAAAlAd2h52tW7eWRB0AAAAl4q4fKpiUlKQNGzbo+vXrklTg3VIAAABlze6wc+HCBYWFhem+++5Tjx49lJqaKkkaMmSIxo0b5/ACAQAAisPusDN27Fg5Ozvr1KlTcnd3t7b369dPsbGxDi0OAACguOyes7Nx40Zt2LBB9957r017o0aNdPLkSYcVBgAA4Ah2n9nJzMy0OaNzy8WLF3mTOAAAKHfsDjsPPPCA3n33Xeu6xWJRXl6eZs6cqdDQUIcWBwAAUFx2X8aaOXOmwsLCtH//ft28eVMvvfSSvv/+e128eFG7d+8uiRoBAADumt1ndlq0aKEffvhBXbp0Ue/evZWZmak+ffro4MGDatCgQUnUCAAAcNfsPrMjSZ6envrb3/7m6FoAAAAc7q7CzqVLl7R48WIdOXJEktSsWTMNHjxY3t7eDi0OAACguOy+jLVjxw4FBgZq7ty5unTpki5duqS5c+cqKChIO3bsKIkaAQAA7prdZ3ZGjBihfv36acGCBapUqZIkKTc3V3/96181YsQIffvttw4vEgAA4G7ZfWYnKSlJ48aNswYdSapUqZIiIyOVlJTk0OIAAACKy+6w065dO+tcnV86cuSIWrdu7ZCiAAAAHKVIl7EOHTpk/XnUqFEaPXq0kpKSdP/990uS9uzZo/nz5ysmJqZkqgQAALhLRQo7bdq0kcVikWEY1raXXnop37gBAwaoX79+jqsOAACgmIoUdlJSUkq6DgAAgBJRpLBTr169kq4DAACgRNzVQwXPnDmjXbt2KS0tTXl5eTZ9o0aNckhhAAAAjmB32Fm6dKmee+45ubi4qGbNmrJYLNY+i8VC2AEAAOWK3WFn0qRJmjx5sqKiouTkZPed6wAAAKXK7rRy7do19e/fn6ADAAAqBLvP7AwZMkQfffSRJk6cWBL1wAHCe/dVatqFAvtOnDqlJqVcDwAAZcnusBMdHa0//OEPio2NVcuWLeXs7GzTP3v2bIcVh7uTmnZBTQZHF9iXNInnIAEAflvuKuxs2LBBjRs3lqR8E5QBAADKE7vDzmuvvaZ33nlHgwYNKoFyAAAAHMvuWcaurq7q3LlzSdQCAADgcHaHndGjR2vevHklUQsAAIDD2X0Za+/evdqyZYvWr1+v5s2b55ug/MknnzisOAAAgOKyO+x4eXmpT58+JVELAACAw9kddpYsWVISdQAAAJQIHoMMAABMze4zO0FBQbd9ns7x48eLVRBgNinHj6tNp5AC+/x8auqLtatLtyAA+I2xO+yMGTPGZj07O1sHDx5UbGysxo8f76i6ANPIlQp9ovXRJVGlWwwA/AbZHXZGjx5dYPv8+fO1f//+Yhf0S1OnTtW0adNs2ho3bqyjR49Kkm7cuKFx48Zp5cqVysrKUvfu3fXvf/9bderUcWgdAACg4nLYnJ3w8HCtXu340/HNmzdXamqqddm1a5e1b+zYsVq3bp0++ugjbd++XWfOnOFOMQAAYMPuMzuF+fjjj+Xt7e2ozVlVrlxZvr6++drT09O1ePFirVixQg899JCkn+8Ua9q0qfbs2aP777/f4bUAAICKx+6w07ZtW5sJyoZh6OzZs/rvf/+rf//73w4tTpKOHTsmf39/ubm5qVOnToqOjlbdunV14MABZWdnq1u3btaxTZo0Ud26dRUXF3fbsJOVlaWsrCzrekZGhsPrBgAA5YPdYefxxx+3WXdyclLt2rUVEhKiJk2aOKouSVJwcLCWLl2qxo0bKzU1VdOmTdMDDzyg7777TmfPnpWLi4u8vLxsPlOnTh2dPXv2ttuNjo7ONxcIAACYk91hZ8qUKSVRR4HCw8OtP7dq1UrBwcGqV6+eVq1apSpVqtz1dqOiohQZGWldz8jIUEBAQLFqBQAA5VOFeqigl5eX7rvvPiUlJcnX11c3b97U5cuXbcacO3euwDk+v+Tq6qrq1avbLAAAwJyKHHacnJxUqVKl2y6VKztsvnOBrl69quTkZPn5+al9+/ZydnbW5s2brf2JiYk6deqUOnXqVKJ1AACAiqPI6eTTTz8ttC8uLk5z585VXl6eQ4q65cUXX1TPnj1Vr149nTlzRlOmTFGlSpX09NNPy9PTU0OGDFFkZKS8vb1VvXp1vfDCC+rUqRN3YqHC4OnKAFDyihx2evfuna8tMTFREydO1Lp16zRw4EBNnz7docX99NNPevrpp3XhwgXVrl1bXbp00Z49e1S7dm1J0uuvvy4nJyf17dvX5qGCQEXB05UBoOTd1XWnW2dZli1bpu7duyshIUEtWrRwdG1auXLlbfvd3Nw0f/58zZ8/3+H7BgAA5mDXBOX09HRNmDBBDRs21Pfff6/Nmzdr3bp1JRJ0AAAAHKHIZ3ZmzpypV155Rb6+vvrggw8KvKwFAABQ3hQ57EycOFFVqlRRw4YNtWzZMi1btqzAcZ988onDigMAACiuIoedP//5zzaviQAAAKgIihx2li5dWoJlAAAAlIySfQoggBIT3ruvUtMuFNjHM3oA4H8IO0AFlZp2gWf0AEARVKh3YwEAANiLsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEyNsAMAAEytclkXAMDxUo4fV5tOIQX2+fnU1BdrV5duQQBQhgg7gAnlSmoyOLrAvqNLokq3GAAoY1zGAgAApkbYAQAApkbYAQAApkbYAQAApkbYAQAApkbYAQAAplauw050dLR+97vfycPDQz4+Pnr88ceVmJhoMyYkJEQWi8Vmef7558uoYgAAUN6U67Czfft2jRgxQnv27NGmTZuUnZ2tRx55RJmZmTbjhg4dqtTUVOsyc+bMMqoYAACUN+X6oYKxsbE260uXLpWPj48OHDigBx980Nru7u4uX1/f0i4PAABUAOX6zM6vpaenS5K8vb1t2pcvX65atWqpRYsWioqK0rVr1267naysLGVkZNgsAADAnMr1mZ1fysvL05gxY9S5c2e1aNHC2j5gwADVq1dP/v7+OnTokCZMmKDExER98sknhW4rOjpa06ZNK42yAQBAGaswYWfEiBH67rvvtGvXLpv2YcOGWX9u2bKl/Pz8FBYWpuTkZDVo0KDAbUVFRSkyMtK6npGRoYCAgJIpHAAAlKkKEXZGjhyp9evXa8eOHbr33ntvOzY4OFiSlJSUVGjYcXV1laurq8PrLE3hvfsqNe1CgX0nTp1Sk1KuBwCA8qpchx3DMPTCCy/o008/1bZt2xQUFHTHzyQkJEiS/Pz8Sri6spWadqHQt1onTepXytUAAFB+leuwM2LECK1YsUJr166Vh4eHzp49K0ny9PRUlSpVlJycrBUrVqhHjx6qWbOmDh06pLFjx+rBBx9Uq1atyrh6AABQHpTrsLNgwQJJPz848JeWLFmiQYMGycXFRV9++aXmzJmjzMxMBQQEqG/fvvr73/9eBtUCAIDyqFyHHcMwbtsfEBCg7du3l1I1AACgIqpQz9kBAACwF2EHAACYGmEHAACYWrmeswPA8VKOH1ebTiEF9vn51NQXa1eXbkEAUMIIO8BvTK5U6DOaji6JKt1iAKAUcBkLAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYGmEHAACYWuWyLgBA+ZFy/LjadAopsM/Pp6a+WLu6dAsCAAcg7ACwypXUZHB0gX1Hl0SVbjEA4CBcxgIAAKZG2AEAAKbGZaxyLLx3X6WmXSiw78SpU2pSyvUAAFAREXbKsdS0C4XOn0ia1K+UqwEAoGIi7JSh2525kTh7AwCAIxB2ytDtztxInL0BAMARmKAMAABMjbADAABMjctYAIqEpysDqKgIOwCKhKcrA6iouIwFAABMjbADAABMjbADAABMjbADAABMjQnKAIqNO7UAlGemCTvz58/XrFmzdPbsWbVu3Vrz5s1Tx44dy7os4DeBO7UAlGemCDsffvihIiMjtXDhQgUHB2vOnDnq3r27EhMT5ePjU9blAYAp3O59fpzBQ3lmirAze/ZsDR06VIMHD5YkLVy4UJ9//rneeecdTZw4sYyrAwBzuN37/DiDh/Kswoedmzdv6sCBA4qK+t9/aE5OTurWrZvi4uLKsLKf3e5fQrzVHCgZnIEASl95/u+uwoed8+fPKzc3V3Xq1LFpr1Onjo4ePVrgZ7KyspSVlWVdT09PlyRlZGQ4vL6fzpzTfc9MLbDv2IxByr6eWehnjby8QvsrSl95q6ci1Zqbk3PbP5O5OTnlptbifI+ScLv/7n54f2qp12MWt/szVxb/P6N8KYv/7m5t0zCM2w80KrjTp08bkoyvvvrKpn38+PFGx44dC/zMlClTDEksLCwsLCwsJlh+/PHH22aFCn9mp1atWqpUqZLOnTtn037u3Dn5+voW+JmoqChFRkZa1/Py8nTx4kXVrFlTFovFZmxGRoYCAgL0448/qnr16o7/AibCsSo6jpV9OF5Fx7EqOo5V0ZXXY2UYhq5cuSJ/f//bjqvwYcfFxUXt27fX5s2b9fjjj0v6Obxs3rxZI0eOLPAzrq6ucnV1tWnz8vK67X6qV69erv4PLs84VkXHsbIPx6voOFZFx7EquvJ4rDw9Pe84psKHHUmKjIxURESEOnTooI4dO2rOnDnKzMy03p0FAAB+u0wRdvr166f//ve/mjx5ss6ePas2bdooNjY236RlAADw22OKsCNJI0eOLPSyVXG4urpqypQp+S57IT+OVdFxrOzD8So6jlXRcayKrqIfK4th3Ol+LQAAgIqLt54DAABTI+wAAABTI+wAAABTI+wAAABTI+zcwfz58xUYGCg3NzcFBwdr7969ZV1SmduxY4d69uwpf39/WSwWrVmzxqbfMAxNnjxZfn5+qlKlirp166Zjx46VTbFlLDo6Wr/73e/k4eEhHx8fPf7440pMTLQZc+PGDY0YMUI1a9ZUtWrV1Ldv33xPBP8tWLBggVq1amV9aFmnTp30xRdfWPs5ToWLiYmRxWLRmDFjrG0cr59NnTpVFovFZmnS5H+vYOY42Tp9+rSeeeYZ1axZU1WqVFHLli21f/9+a39F/f1O2LmNDz/8UJGRkZoyZYq+/vprtW7dWt27d1daWlpZl1amMjMz1bp1a82fP7/A/pkzZ2ru3LlauHCh4uPjVbVqVXXv3l03btwo5UrL3vbt2zVixAjt2bNHmzZtUnZ2th555BFlZv7vZYpjx47VunXr9NFHH2n79u06c+aM+vTpU4ZVl417771XMTExOnDggPbv36+HHnpIvXv31vfffy+J41SYffv26c0331SrVq1s2jle/9O8eXOlpqZal127dln7OE7/c+nSJXXu3FnOzs764osvdPjwYb322muqUaOGdUyF/f3uiJdxmlXHjh2NESNGWNdzc3MNf39/Izo6ugyrKl8kGZ9++ql1PS8vz/D19TVmzZplbbt8+bLh6upqfPDBB2VQYfmSlpZmSDK2b99uGMbPx8bZ2dn46KOPrGOOHDliSDLi4uLKqsxyo0aNGsbbb7/NcSrElStXjEaNGhmbNm0yunbtaowePdowDP5c/dKUKVOM1q1bF9jHcbI1YcIEo0uXLoX2V+Tf75zZKcTNmzd14MABdevWzdrm5OSkbt26KS4urgwrK99SUlJ09uxZm+Pm6emp4OBgjpuk9PR0SZK3t7ck6cCBA8rOzrY5Xk2aNFHdunV/08crNzdXK1euVGZmpjp16sRxKsSIESP02GOP2RwXiT9Xv3bs2DH5+/urfv36GjhwoE6dOiWJ4/Rrn332mTp06KAnn3xSPj4+atu2rRYtWmTtr8i/3wk7hTh//rxyc3PzvXKiTp06Onv2bBlVVf7dOjYct/zy8vI0ZswYde7cWS1atJD08/FycXHJ9yLa3+rx+vbbb1WtWjW5urrq+eef16effqpmzZpxnAqwcuVKff3114qOjs7Xx/H6n+DgYC1dulSxsbFasGCBUlJS9MADD+jKlSscp185fvy4FixYoEaNGmnDhg0aPny4Ro0apWXLlkmq2L/fTfO6CKC8GzFihL777jub+QKw1bhxYyUkJCg9PV0ff/yxIiIitH379rIuq9z58ccfNXr0aG3atElubm5lXU65Fh4ebv25VatWCg4OVr169bRq1SpVqVKlDCsrf/Ly8tShQwf985//lCS1bdtW3333nRYuXKiIiIgyrq54OLNTiFq1aqlSpUr5ZuWfO3dOvr6+ZVRV+Xfr2HDcbI0cOVLr16/X1q1bde+991rbfX19dfPmTV2+fNlm/G/1eLm4uKhhw4Zq3769oqOj1bp1a73xxhscp185cOCA0tLS1K5dO1WuXFmVK1fW9u3bNXfuXFWuXFl16tTheBXCy8tL9913n5KSkvhz9St+fn5q1qyZTVvTpk2tl/0q8u93wk4hXFxc1L59e23evNnalpeXp82bN6tTp05lWFn5FhQUJF9fX5vjlpGRofj4+N/kcTMMQyNHjtSnn36qLVu2KCgoyKa/ffv2cnZ2tjleiYmJOnXq1G/yeP1aXl6esrKyOE6/EhYWpm+//VYJCQnWpUOHDho4cKD1Z45Xwa5evark5GT5+fnx5+pXOnfunO/RGD/88IPq1asnqYL/fi/rGdLl2cqVKw1XV1dj6dKlxuHDh41hw4YZXl5extmzZ8u6tDJ15coV4+DBg8bBgwcNScbs2bONgwcPGidPnjQMwzBiYmIMLy8vY+3atcahQ4eM3r17G0FBQcb169fLuPLSN3z4cMPT09PYtm2bkZqaal2uXbtmHfP8888bdevWNbZs2WLs37/f6NSpk9GpU6cyrLpsTJw40di+fbuRkpJiHDp0yJg4caJhsViMjRs3GobBcbqTX96NZRgcr1vGjRtnbNu2zUhJSTF2795tdOvWzahVq5aRlpZmGAbH6Zf27t1rVK5c2Xj55ZeNY8eOGcuXLzfc3d2N999/3zqmov5+J+zcwbx584y6desaLi4uRseOHY09e/aUdUllbuvWrYakfEtERIRhGD/fnjhp0iSjTp06hqurqxEWFmYkJiaWbdFlpKDjJMlYsmSJdcz169eNv/71r0aNGjUMd3d3449//KORmppadkWXkWeffdaoV6+e4eLiYtSuXdsICwuzBh3D4Djdya/DDsfrZ/369TP8/PwMFxcX45577jH69etnJCUlWfs5TrbWrVtntGjRwnB1dTWaNGlivPXWWzb9FfX3u8UwDKNszikBAACUPObsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAAAAUyPsAKjwQkJCNGbMmLIuA0A5RdgBUCwLFy6Uh4eHcnJyrG1Xr16Vs7OzQkJCbMZu27ZNFotFycnJpVyldPPmTc2cOVOtW7eWu7u7atWqpc6dO2vJkiXKzs4u1VoIZ0DpqlzWBQCo2EJDQ3X16lXt379f999/vyRp586d8vX1VXx8vG7cuCE3NzdJ0tatW1W3bl01aNDA7v0YhqHc3FxVrmz/r62bN2+qe/fu+uabb/SPf/xDnTt3VvXq1bVnzx69+uqratu2rdq0aWP3dgFUDJzZAVAsjRs3lp+fn7Zt22Zt27Ztm3r37q2goCDt2bPHpj00NFSSlJWVpVGjRsnHx0dubm7q0qWL9u3bZzPWYrHoiy++UPv27eXq6qpdu3YpMzNTf/7zn1WtWjX5+fnptddeu2ONc+bM0Y4dO7R582aNGDFCbdq0Uf369TVgwADFx8erUaNGRapp6dKl8vLystn2mjVrZLFYrOtTp05VmzZt9N577ykwMFCenp7q37+/rly5IkkaNGiQtm/frjfeeEMWi0UWi0UnTpwo8vEGYD/CDoBiCw0N1datW63rW7duVUhIiLp27Wptv379uuLj461h56WXXtLq1au1bNkyff3112rYsKG6d++uixcv2mx74sSJiomJ0ZEjR9SqVSuNHz9e27dv19q1a7Vx40Zt27ZNX3/99W3rW758ubp166a2bdvm63N2dlbVqlXtqulOkpOTtWbNGq1fv17r16/X9u3bFRMTI0l644031KlTJw0dOlSpqalKTU1VQECAXdsHYB/CDoBiCw0N1e7du5WTk6MrV67o4MGD6tq1qx588EHrGZ+4uDhlZWUpNDRUmZmZWrBggWbNmqXw8HA1a9ZMixYtUpUqVbR48WKbbU+fPl0PP/ywGjRoIBcXFy1evFivvvqqwsLC1LJlSy1btsxmvlBBjh07piZNmtx2jD013UleXp6WLl2qFi1a6IEHHtCf/vQnbd68WZLk6ekpFxcXubu7y9fXV76+vqpUqZJd2wdgH8IOgGILCQlRZmam9u3bp507d+q+++5T7dq11bVrV+u8nW3btql+/fqqW7eukpOTlZ2drc6dO1u34ezsrI4dO+rIkSM22+7QoYP15+TkZN28eVPBwcHWNm9vbzVu3Pi29RmGccfvYE9NdxIYGCgPDw/rup+fn9LS0uzaBgDHYYIygGJr2LCh7r33Xm3dulWXLl1S165dJUn+/v4KCAjQV199pa1bt+qhhx6ye9u3LjEVx3333aejR48WeztOTk75glNBd3I5OzvbrFssFuXl5RV7/wDuDmd2ADhEaGiotm3bpm3bttnccv7ggw/qiy++0N69e63zdW5dktq9e7d1XHZ2tvbt26dmzZoVuo8GDRrI2dlZ8fHx1rZLly7phx9+uG1tAwYM0JdffqmDBw/m68vOzlZmZmaRaqpdu7auXLmizMxM65iEhITb7rsgLi4uys3NtftzAO4OYQeAQ4SGhmrXrl1KSEiwntmRpK5du+rNN9/UzZs3rWGnatWqGj58uMaPH6/Y2FgdPnxYQ4cO1bVr1zRkyJBC91GtWjUNGTJE48eP15YtW/Tdd99p0KBBcnK6/a+yMWPGqHPnzgoLC9P8+fP1zTff6Pjx41q1apXuv/9+HTt2rEg1BQcHy93dXf/3f/+n5ORkrVixQkuXLrX7WAUGBio+Pl4nTpzQ+fPnOesDlDAuYwFwiNDQUF2/fl1NmjRRnTp1rO1du3bVlStXrLeo3xITE6O8vDz96U9/0pUrV9ShQwdt2LBBNWrUuO1+Zs2apatXr6pnz57y8PDQuHHjlJ6eftvPuLq6atOmTXr99df15ptv6sUXX5S7u7uaNm2qUaNGqUWLFkWqydvbW++//77Gjx+vRYsWKSwsTFOnTtWwYcPsOlYvvviiIiIi1KxZM12/fl0pKSkKDAy0axsAis5iFGXmHgAAQAXFZSwAAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBqhB0AAGBq/w9F8an4oovG6gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test['source'] = [i.lstrip('[').rstrip(']\\n ') for i in dataset_test['source']]\n",
        "dataset_test['target'] = [i.lstrip('[').rstrip(']\\n ') for i in dataset_test['target']]\n",
        "dataset_test['source'] = [i.lstrip('\\'').rstrip('\\'') for i in dataset_test['source']]\n",
        "dataset_test['target'] = [i.lstrip('\\'').rstrip('\\'') for i in dataset_test['target']]\n",
        "dataset_val['source'] = [i.lstrip('[').rstrip(']\\n ') for i in dataset_val['source']]\n",
        "dataset_val['target'] = [i.lstrip('[').rstrip(']\\n ') for i in dataset_val['target']]\n",
        "dataset_val['source'] = [i.lstrip('\\'').rstrip('\\'') for i in dataset_val['source']]\n",
        "dataset_val['target'] = [i.lstrip('\\'').rstrip('\\'') for i in dataset_val['target']]"
      ],
      "metadata": {
        "id": "wfYnQp5-6y_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test['source'] = [i.lower() for i in dataset_test['source']]\n",
        "dataset_test['target'] = [i.lower() for i in dataset_test['target']]\n",
        "dataset_val['source'] = [i.lower() for i in dataset_val['source']]\n",
        "dataset_val['target'] = [i.lower() for i in dataset_val['target']]"
      ],
      "metadata": {
        "id": "RKGILcY67cIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_source_test = []\n",
        "for t in dataset_test['source']:\n",
        "    clean_source_test.append(cleaning(t,0))\n",
        "\n",
        "clean_target_test = []\n",
        "for t in dataset_test['target']:\n",
        "    clean_target_test.append(cleaning(t,0))\n",
        "\n",
        "clean_source_val = []\n",
        "for t in dataset_val['source']:\n",
        "    clean_source_val.append(cleaning(t,0))\n",
        "\n",
        "clean_target_val = []\n",
        "for t in dataset_val['target']:\n",
        "    clean_target_val.append(cleaning(t,0))"
      ],
      "metadata": {
        "id": "viOe_eaH7gHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_test['source'] = clean_source_test\n",
        "dataset_test['target'] = clean_target_test\n",
        "dataset_val['source'] = clean_source_val\n",
        "dataset_val['target'] = clean_target_val\n",
        "\n",
        "dataset_test.replace('', np.nan, inplace=True)\n",
        "dataset_test.dropna(axis=0,inplace=True)\n",
        "dataset_val.replace('', np.nan, inplace=True)\n",
        "dataset_val.dropna(axis=0,inplace=True)"
      ],
      "metadata": {
        "id": "lCSwH3Fp7i3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, Y_train = dataset_train['source'], dataset_train['target']\n",
        "X_test, Y_test = dataset_test['source'], dataset_test['target']"
      ],
      "metadata": {
        "id": "CvbZcxF_FBeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X_train))\n",
        "print(len(Y_train))\n",
        "print(len(X_test))\n",
        "print(len(Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHcDCJTlGEsu",
        "outputId": "04ad9d5d-5d84-4ffa-ab07-a942903140ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1992\n",
            "1992\n",
            "618\n",
            "618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n"
      ],
      "metadata": {
        "id": "z7wmrW6ALqV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def readLangs(text, summary, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    text=np.array(text)\n",
        "    summary=np.array(summary)\n",
        "    pairs = [[text[i],summary[i]] for i in range(len(text))]\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(summary)\n",
        "        output_lang = Lang(text)\n",
        "    else:\n",
        "        input_lang = Lang(text)\n",
        "        output_lang = Lang(summary)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "metadata": {
        "id": "laIuDMHZLu9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name,\"--------------------\", input_lang.n_words)\n",
        "    #print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n"
      ],
      "metadata": {
        "id": "8PH-nIEkLwVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData(X_train, Y_train , False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8BwnaAELyxZ",
        "outputId": "be78df26-bc32-4bb3-c88a-476747e90183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 1992 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "['due success deep learning solving variety challenging machine learning tasks rising interest understanding loss functions training neural networks theoretical aspect particularly properties critical points landscape around importance determine convergence performance optimization algorithms paper provide necessary sufficient characterization analytical forms critical points well global minimizers square loss functions linear neural networks show analytical forms critical points characterize values corresponding loss functions well necessary sufficient conditions achieve global minimum furthermore exploit analytical forms critical points characterize landscape properties loss functions linear neural networks shallow relu networks one particular conclusion loss function linear networks spurious local minimum loss function one hidden layer nonlinear networks relu activation function local minimum global minimum'\n",
            " 'backpropagation bp algorithm often thought biologically implausible brain one main reasons bp requires symmetric weight matrices feedforward feedback pathways address weight transport problem grossberg two biologically plausible algorithms proposed liao et al lillicrap et al relax bp weight symmetry requirements demonstrate comparable learning capabilities bp small datasets however recent study bartunov et al finds although feedback alignment fa variants target propagation tp perform well mnist cifar perform significantly worse bp imagenet additionally evaluate sign symmetry ss algorithm liao et al differs bp fa feedback feedforward weights share magnitudes share signs examined performance sign symmetry feedback alignment imagenet ms coco datasets using different network architectures resnet alexnet imagenet retinanet ms coco surprisingly networks trained sign symmetry attain classification performance approaching bp trained networks results complement study bartunov et al establish new benchmark future biologically plausible learning algorithms difficult datasets complex architectures'\n",
            " 'introduce simplicial transformer extension transformer includes form higher dimensional attention generalising dot product attention uses attention update entity representations tensor products value vectors show architecture useful inductive bias logical reasoning context deep reinforcement learning'\n",
            " ...\n",
            " 'introduce neural architecture perform amortized approximate bayesian inference latent random permutations two sets objects method involves approximating permanents matrices pairwise probabilities using recent ideas functions defined sets sampled permutation comes probability estimate quantity unavailable mcmc approaches illustrate method sets points mnist images'\n",
            " 'machine learned large scale retrieval systems require large amount training data representing query item relevance however collecting users explicit feedback costly paper propose leverage user logs implicit feedback auxiliary objectives improve relevance modeling retrieval systems specifically adopt two tower neural net architecture model query item relevance given collaborative content information introducing auxiliary tasks trained much richer implicit user feedback data improve quality resolution learned representations queries items applying learned representations industrial retrieval system delivered significant improvements'\n",
            " 'ability autonomously explore navigate physical space fundamental requirement virtually mobile autonomous agent household robotic vacuums autonomous vehicles traditional slam based approaches exploration navigation largely focus leveraging scene geometry fail model dynamic objects agents semantic constraints wet floors doorways learning based rl agents attractive alternative incorporate semantic geometric information notoriously sample inefficient difficult generalize novel settings difficult interpret paper combine best worlds modular approach em learns spatial representation scene trained effective coupled traditional geometric planners specifically design agent learns predict spatial affordance map elucidates parts scene navigable active self supervised experience gathering contrast simulation environments assume static world evaluate approach vizdoom simulator using large scale randomly generated maps containing variety dynamic actors hazards show learned affordance maps used augment traditional approaches exploration navigation providing significant improvements performance'] -------------------- 11477\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pairs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xK5oesvKREBt",
        "outputId": "c60760ba-ae07-48be-e68c-b65ce4c4b2d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['backpropagation bp algorithm often thought biologically implausible brain one main reasons bp requires symmetric weight matrices feedforward feedback pathways address weight transport problem grossberg two biologically plausible algorithms proposed liao et al lillicrap et al relax bp weight symmetry requirements demonstrate comparable learning capabilities bp small datasets however recent study bartunov et al finds although feedback alignment fa variants target propagation tp perform well mnist cifar perform significantly worse bp imagenet additionally evaluate sign symmetry ss algorithm liao et al differs bp fa feedback feedforward weights share magnitudes share signs examined performance sign symmetry feedback alignment imagenet ms coco datasets using different network architectures resnet alexnet imagenet retinanet ms coco surprisingly networks trained sign symmetry attain classification performance approaching bp trained networks results complement study bartunov et al establish new benchmark future biologically plausible learning algorithms difficult datasets complex architectures',\n",
              " 'biologically plausible learning algorithms particularly sign symmetry work well imagenet']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1"
      ],
      "metadata": {
        "id": "nd8NFp3FONF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "metadata": {
        "id": "zrts86z2NZ2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.LSTM = nn.LSTM(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.LSTM(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return (torch.zeros(1, 1, self.hidden_size, device=device),torch.zeros(1, 1, self.hidden_size, device=device))"
      ],
      "metadata": {
        "id": "TafB8WADNduD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.LSTM = nn.LSTM(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.LSTM(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "HKNaFpEeNfkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 257"
      ],
      "metadata": {
        "id": "q5ZJsQWFu3R-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size*2 , self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size*2 , self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.LSTM = nn.LSTM(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        #print('Decoder --- atndecoder')\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        #print(\"inside forward decoder\")\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        #print(\"embedded size\", embedded.size())\n",
        "        #print(embedded[0].size()) #1,300\n",
        "        #print(hidden[0].size()) # 1, 1, 300\n",
        "        #print(\"diff or not\")\n",
        "\n",
        "        #temp = torch.cat((embedded[0], hidden[0]), 1)\n",
        "\n",
        "        #print(temp)\n",
        "        #print(temp.size())\n",
        "\n",
        "        attn_weights = F.softmax(self.attn(torch.cat((embedded[0], hidden[0][0]), 1)), dim=1)\n",
        "        #print(\"after getting attn weights softmax\")\n",
        "        #print(attn_weights.size())\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.LSTM(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "metadata": {
        "id": "POZIhZ-fDOC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "    #print('bbbbbbb-->>> input length', input_length)\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "        #print(\"priting before error\")\n",
        "        #print(encoder_output.size())\n",
        "        #print(encoder_outputs.size())\n",
        "        temp = encoder_output[0, 0]\n",
        "        #print(temp)\n",
        "        encoder_outputs[ei] = temp\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    #use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    #Without teacher forcing: use its own predictions as the next input\n",
        "\n",
        "    #print('aaaaa-->>>')\n",
        "\n",
        "    for di in range(target_length):\n",
        "      decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "      decoder_input, decoder_hidden, encoder_outputs)\n",
        "      topv, topi = decoder_output.topk(1)\n",
        "\n",
        "      decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "      loss += criterion(decoder_output, target_tensor[di])\n",
        "\n",
        "      if decoder_input.item() == EOS_token:\n",
        "             break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "metadata": {
        "id": "eRt_M9KNNutz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "metadata": {
        "id": "l_8OiFmkN1rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "NLyTlybCN3cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    print(\"Training....\")\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        if iter% 1000 == 0:\n",
        "            print(iter,\"/\",n_iters + 1)\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        input_length = input_tensor.size(0)\n",
        "        if(input_length > 150):\n",
        "          #print(input_length)\n",
        "          continue\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    log_frame = pd.DataFrame(columns = [\"LOSS\"])\n",
        "    log_frame[\"LOSS\"] = plot_losses\n",
        "    log_frame.to_csv(os.path.join('./Analysis/', \"Loss_Values.csv\"), index = False)"
      ],
      "metadata": {
        "id": "bF0DS9RMN55n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        #print('cccccc->>>>')\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "metadata": {
        "id": "_3VnxUVBN8bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluateRandomly(encoder, decoder, n=5):\n",
        "    text=list()\n",
        "    headline=list()\n",
        "    pred_headline=list()\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "\n",
        "        if(len(pair[0].split())>=150):\n",
        "          continue\n",
        "        else:\n",
        "          if(i%1000==0):\n",
        "            print(i*100/n,\"% complete\")\n",
        "\n",
        "          #print('>', pair[0])\n",
        "          text.append(pair[0])\n",
        "          #print('=', pair[1])\n",
        "          headline.append(pair[1])\n",
        "          output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "          output_sentence = ' '.join(output_words)\n",
        "          pred_headline.append(output_sentence)\n",
        "          #print('<', output_sentence)\n",
        "          #print('')\n",
        "    return(text,headline,pred_headline)"
      ],
      "metadata": {
        "id": "x1GvKUl3OEWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size = 300\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 150000, print_every=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2MjJ_hUOHAK",
        "outputId": "3ce9aa32-a15b-44b8-dfad-3e92ac2eea9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training....\n",
            "1000 / 150001\n",
            "1m 26s (- 215m 36s) (1000 0%) 3.2012\n",
            "2000 / 150001\n",
            "3000 / 150001\n",
            "3m 53s (- 191m 4s) (3000 2%) 6.9037\n",
            "4000 / 150001\n",
            "5m 8s (- 187m 22s) (4000 2%) 3.5420\n",
            "5000 / 150001\n",
            "6m 23s (- 185m 30s) (5000 3%) 3.6509\n",
            "6000 / 150001\n",
            "7m 37s (- 182m 52s) (6000 4%) 3.3846\n",
            "7000 / 150001\n",
            "8m 50s (- 180m 47s) (7000 4%) 3.3210\n",
            "8000 / 150001\n",
            "10m 3s (- 178m 39s) (8000 5%) 3.4855\n",
            "9000 / 150001\n",
            "11m 17s (- 176m 53s) (9000 6%) 3.3709\n",
            "10000 / 150001\n",
            "12m 31s (- 175m 16s) (10000 6%) 3.4090\n",
            "11000 / 150001\n",
            "12000 / 150001\n",
            "14m 59s (- 172m 28s) (12000 8%) 6.8721\n",
            "13000 / 150001\n",
            "16m 13s (- 171m 3s) (13000 8%) 3.4253\n",
            "14000 / 150001\n",
            "15000 / 150001\n",
            "18m 40s (- 168m 2s) (15000 10%) 6.6549\n",
            "16000 / 150001\n",
            "19m 54s (- 166m 44s) (16000 10%) 3.2350\n",
            "17000 / 150001\n",
            "21m 9s (- 165m 32s) (17000 11%) 3.2146\n",
            "18000 / 150001\n",
            "22m 23s (- 164m 12s) (18000 12%) 3.2127\n",
            "19000 / 150001\n",
            "23m 38s (- 163m 2s) (19000 12%) 3.2195\n",
            "20000 / 150001\n",
            "24m 54s (- 161m 52s) (20000 13%) 2.9763\n",
            "21000 / 150001\n",
            "26m 10s (- 160m 48s) (21000 14%) 3.1167\n",
            "22000 / 150001\n",
            "27m 24s (- 159m 29s) (22000 14%) 2.9788\n",
            "23000 / 150001\n",
            "28m 41s (- 158m 28s) (23000 15%) 2.9441\n",
            "24000 / 150001\n",
            "25000 / 150001\n",
            "31m 13s (- 156m 9s) (25000 16%) 5.5748\n",
            "26000 / 150001\n",
            "32m 31s (- 155m 8s) (26000 17%) 2.6707\n",
            "27000 / 150001\n",
            "33m 50s (- 154m 9s) (27000 18%) 2.6548\n",
            "28000 / 150001\n",
            "35m 7s (- 153m 4s) (28000 18%) 2.5492\n",
            "29000 / 150001\n",
            "36m 25s (- 152m 0s) (29000 19%) 2.5056\n",
            "30000 / 150001\n",
            "37m 44s (- 150m 56s) (30000 20%) 2.3427\n",
            "31000 / 150001\n",
            "32000 / 150001\n",
            "40m 19s (- 148m 43s) (32000 21%) 4.4743\n",
            "33000 / 150001\n",
            "41m 38s (- 147m 37s) (33000 22%) 2.1579\n",
            "34000 / 150001\n",
            "42m 56s (- 146m 29s) (34000 22%) 2.0386\n",
            "35000 / 150001\n",
            "44m 15s (- 145m 26s) (35000 23%) 2.0480\n",
            "36000 / 150001\n",
            "45m 35s (- 144m 22s) (36000 24%) 1.9330\n",
            "37000 / 150001\n",
            "46m 55s (- 143m 19s) (37000 24%) 1.9398\n",
            "38000 / 150001\n",
            "48m 15s (- 142m 15s) (38000 25%) 1.8232\n",
            "39000 / 150001\n",
            "49m 35s (- 141m 9s) (39000 26%) 1.7641\n",
            "40000 / 150001\n",
            "50m 56s (- 140m 5s) (40000 26%) 1.7947\n",
            "41000 / 150001\n",
            "52m 16s (- 138m 58s) (41000 27%) 1.7448\n",
            "42000 / 150001\n",
            "53m 36s (- 137m 51s) (42000 28%) 1.6972\n",
            "43000 / 150001\n",
            "54m 55s (- 136m 41s) (43000 28%) 1.5874\n",
            "44000 / 150001\n",
            "56m 15s (- 135m 32s) (44000 29%) 1.5715\n",
            "45000 / 150001\n",
            "57m 36s (- 134m 25s) (45000 30%) 1.5337\n",
            "46000 / 150001\n",
            "58m 56s (- 133m 14s) (46000 30%) 1.5381\n",
            "47000 / 150001\n",
            "60m 17s (- 132m 7s) (47000 31%) 1.4915\n",
            "48000 / 150001\n",
            "61m 37s (- 130m 57s) (48000 32%) 1.3993\n",
            "49000 / 150001\n",
            "50000 / 150001\n",
            "64m 17s (- 128m 35s) (50000 33%) 2.8197\n",
            "51000 / 150001\n",
            "65m 38s (- 127m 25s) (51000 34%) 1.3549\n",
            "52000 / 150001\n",
            "66m 58s (- 126m 14s) (52000 34%) 1.3179\n",
            "53000 / 150001\n",
            "68m 19s (- 125m 3s) (53000 35%) 1.3260\n",
            "54000 / 150001\n",
            "55000 / 150001\n",
            "71m 1s (- 122m 40s) (55000 36%) 2.5134\n",
            "56000 / 150001\n",
            "72m 25s (- 121m 34s) (56000 37%) 1.2160\n",
            "57000 / 150001\n",
            "73m 47s (- 120m 23s) (57000 38%) 1.1924\n",
            "58000 / 150001\n",
            "75m 8s (- 119m 10s) (58000 38%) 1.1746\n",
            "59000 / 150001\n",
            "76m 29s (- 117m 58s) (59000 39%) 1.1656\n",
            "60000 / 150001\n",
            "77m 52s (- 116m 49s) (60000 40%) 1.1561\n",
            "61000 / 150001\n",
            "79m 13s (- 115m 35s) (61000 40%) 1.1191\n",
            "62000 / 150001\n",
            "80m 35s (- 114m 23s) (62000 41%) 1.1264\n",
            "63000 / 150001\n",
            "81m 56s (- 113m 10s) (63000 42%) 1.0453\n",
            "64000 / 150001\n",
            "83m 17s (- 111m 55s) (64000 42%) 1.0018\n",
            "65000 / 150001\n",
            "84m 38s (- 110m 41s) (65000 43%) 0.9984\n",
            "66000 / 150001\n",
            "67000 / 150001\n",
            "87m 23s (- 108m 15s) (67000 44%) 1.9812\n",
            "68000 / 150001\n",
            "88m 46s (- 107m 3s) (68000 45%) 1.0087\n",
            "69000 / 150001\n",
            "90m 9s (- 105m 50s) (69000 46%) 0.9575\n",
            "70000 / 150001\n",
            "91m 32s (- 104m 37s) (70000 46%) 0.9191\n",
            "71000 / 150001\n",
            "92m 56s (- 103m 24s) (71000 47%) 0.9078\n",
            "72000 / 150001\n",
            "94m 18s (- 102m 10s) (72000 48%) 0.8612\n",
            "73000 / 150001\n",
            "95m 39s (- 100m 54s) (73000 48%) 0.8546\n",
            "74000 / 150001\n",
            "97m 2s (- 99m 39s) (74000 49%) 0.8741\n",
            "75000 / 150001\n",
            "98m 26s (- 98m 26s) (75000 50%) 0.8587\n",
            "76000 / 150001\n",
            "99m 51s (- 97m 13s) (76000 50%) 0.8466\n",
            "77000 / 150001\n",
            "78000 / 150001\n",
            "102m 38s (- 94m 44s) (78000 52%) 1.5809\n",
            "79000 / 150001\n",
            "104m 0s (- 93m 28s) (79000 52%) 0.7984\n",
            "80000 / 150001\n",
            "105m 25s (- 92m 14s) (80000 53%) 0.7788\n",
            "81000 / 150001\n",
            "106m 48s (- 90m 58s) (81000 54%) 0.7212\n",
            "82000 / 150001\n",
            "108m 12s (- 89m 43s) (82000 54%) 0.7308\n",
            "83000 / 150001\n",
            "109m 35s (- 88m 28s) (83000 55%) 0.7405\n",
            "84000 / 150001\n",
            "110m 58s (- 87m 11s) (84000 56%) 0.7061\n",
            "85000 / 150001\n",
            "112m 21s (- 85m 55s) (85000 56%) 0.7127\n",
            "86000 / 150001\n",
            "113m 45s (- 84m 39s) (86000 57%) 0.6689\n",
            "87000 / 150001\n",
            "115m 10s (- 83m 23s) (87000 57%) 0.7268\n",
            "88000 / 150001\n",
            "116m 35s (- 82m 8s) (88000 58%) 0.6699\n",
            "89000 / 150001\n",
            "117m 59s (- 80m 51s) (89000 59%) 0.6923\n",
            "90000 / 150001\n",
            "119m 21s (- 79m 34s) (90000 60%) 0.6541\n",
            "91000 / 150001\n",
            "120m 43s (- 78m 16s) (91000 60%) 0.6383\n",
            "92000 / 150001\n",
            "122m 8s (- 76m 59s) (92000 61%) 0.6749\n",
            "93000 / 150001\n",
            "123m 32s (- 75m 43s) (93000 62%) 0.6438\n",
            "94000 / 150001\n",
            "124m 56s (- 74m 25s) (94000 62%) 0.6529\n",
            "95000 / 150001\n",
            "126m 19s (- 73m 8s) (95000 63%) 0.5980\n",
            "96000 / 150001\n",
            "127m 41s (- 71m 49s) (96000 64%) 0.6066\n",
            "97000 / 150001\n",
            "98000 / 150001\n",
            "130m 25s (- 69m 12s) (98000 65%) 1.1636\n",
            "99000 / 150001\n",
            "131m 51s (- 67m 55s) (99000 66%) 0.5740\n",
            "100000 / 150001\n",
            "101000 / 150001\n",
            "134m 40s (- 65m 20s) (101000 67%) 1.1066\n",
            "102000 / 150001\n",
            "136m 5s (- 64m 2s) (102000 68%) 0.5425\n",
            "103000 / 150001\n",
            "137m 29s (- 62m 44s) (103000 68%) 0.5218\n",
            "104000 / 150001\n",
            "138m 53s (- 61m 25s) (104000 69%) 0.5219\n",
            "105000 / 150001\n",
            "140m 17s (- 60m 7s) (105000 70%) 0.5007\n",
            "106000 / 150001\n",
            "141m 41s (- 58m 49s) (106000 70%) 0.4946\n",
            "107000 / 150001\n",
            "143m 4s (- 57m 29s) (107000 71%) 0.4483\n",
            "108000 / 150001\n",
            "144m 28s (- 56m 11s) (108000 72%) 0.4694\n",
            "109000 / 150001\n",
            "145m 52s (- 54m 52s) (109000 72%) 0.4498\n",
            "110000 / 150001\n",
            "147m 16s (- 53m 33s) (110000 73%) 0.4332\n",
            "111000 / 150001\n",
            "148m 40s (- 52m 14s) (111000 74%) 0.4463\n",
            "112000 / 150001\n",
            "150m 4s (- 50m 55s) (112000 74%) 0.4101\n",
            "113000 / 150001\n",
            "151m 28s (- 49m 35s) (113000 75%) 0.4112\n",
            "114000 / 150001\n",
            "152m 51s (- 48m 16s) (114000 76%) 0.3950\n",
            "115000 / 150001\n",
            "154m 15s (- 46m 56s) (115000 76%) 0.3782\n",
            "116000 / 150001\n",
            "155m 40s (- 45m 37s) (116000 77%) 0.3895\n",
            "117000 / 150001\n",
            "157m 4s (- 44m 18s) (117000 78%) 0.3615\n",
            "118000 / 150001\n",
            "119000 / 150001\n",
            "159m 54s (- 41m 39s) (119000 79%) 0.7033\n",
            "120000 / 150001\n",
            "161m 17s (- 40m 19s) (120000 80%) 0.3490\n",
            "121000 / 150001\n",
            "162m 42s (- 38m 59s) (121000 80%) 0.3257\n",
            "122000 / 150001\n",
            "164m 5s (- 37m 39s) (122000 81%) 0.3387\n",
            "123000 / 150001\n",
            "165m 30s (- 36m 19s) (123000 82%) 0.3072\n",
            "124000 / 150001\n",
            "166m 52s (- 34m 59s) (124000 82%) 0.3014\n",
            "125000 / 150001\n",
            "168m 16s (- 33m 39s) (125000 83%) 0.2995\n",
            "126000 / 150001\n",
            "169m 42s (- 32m 19s) (126000 84%) 0.2880\n",
            "127000 / 150001\n",
            "171m 8s (- 30m 59s) (127000 84%) 0.2645\n",
            "128000 / 150001\n",
            "172m 34s (- 29m 39s) (128000 85%) 0.2689\n",
            "129000 / 150001\n",
            "173m 57s (- 28m 19s) (129000 86%) 0.2572\n",
            "130000 / 150001\n",
            "175m 23s (- 26m 59s) (130000 86%) 0.2541\n",
            "131000 / 150001\n",
            "176m 47s (- 25m 38s) (131000 87%) 0.2312\n",
            "132000 / 150001\n",
            "178m 13s (- 24m 18s) (132000 88%) 0.2220\n",
            "133000 / 150001\n",
            "179m 40s (- 22m 57s) (133000 88%) 0.2037\n",
            "134000 / 150001\n",
            "181m 4s (- 21m 37s) (134000 89%) 0.2029\n",
            "135000 / 150001\n",
            "182m 29s (- 20m 16s) (135000 90%) 0.1894\n",
            "136000 / 150001\n",
            "137000 / 150001\n",
            "185m 21s (- 17m 35s) (137000 91%) 0.4054\n",
            "138000 / 150001\n",
            "186m 46s (- 16m 14s) (138000 92%) 0.1604\n",
            "139000 / 150001\n",
            "140000 / 150001\n",
            "141000 / 150001\n",
            "191m 0s (- 12m 11s) (141000 94%) 0.4838\n",
            "142000 / 150001\n",
            "192m 26s (- 10m 50s) (142000 94%) 0.1512\n",
            "143000 / 150001\n",
            "193m 51s (- 9m 29s) (143000 95%) 0.1553\n",
            "144000 / 150001\n",
            "145000 / 150001\n",
            "146000 / 150001\n",
            "198m 4s (- 5m 25s) (146000 97%) 0.3687\n",
            "147000 / 150001\n",
            "199m 31s (- 4m 4s) (147000 98%) 0.1179\n",
            "148000 / 150001\n",
            "200m 57s (- 2m 42s) (148000 98%) 0.1216\n",
            "149000 / 150001\n",
            "202m 21s (- 1m 21s) (149000 99%) 0.1070\n",
            "150000 / 150001\n",
            "203m 45s (- 0m 0s) (150000 100%) 0.0945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text,headline,pred_headline=evaluateRandomly(encoder1, attn_decoder1,15000)\n",
        "\n",
        "pred_df_LSTM=pd.DataFrame()\n",
        "\n",
        "pred_df_LSTM['text']=text\n",
        "pred_df_LSTM['headline']=headline\n",
        "pred_df_LSTM['pred_headline']=pred_headline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSUEobbkOe84",
        "outputId": "e69c0263-05af-4207-f2d8-69f8bcb31dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.666666666666667 % complete\n",
            "13.333333333333334 % complete\n",
            "20.0 % complete\n",
            "26.666666666666668 % complete\n",
            "33.333333333333336 % complete\n",
            "40.0 % complete\n",
            "46.666666666666664 % complete\n",
            "53.333333333333336 % complete\n",
            "60.0 % complete\n",
            "66.66666666666667 % complete\n",
            "73.33333333333333 % complete\n",
            "80.0 % complete\n",
            "93.33333333333333 % complete\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10,110):\n",
        "  print(\"original Summary>>>\",pred_df_LSTM.iloc[i]['headline'])\n",
        "  print(\"Predicted Summary>>>\",pred_df_LSTM.iloc[i]['pred_headline'])\n",
        "  print('-----------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "ZPEzaY87fLSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0,1500):\n",
        "  print(\"original Summary>>>\",pred_df_LSTM.iloc[i]['headline'])\n",
        "  print(\"Predicted Summary>>>\",pred_df_LSTM.iloc[i]['pred_headline'])\n",
        "  print('-----------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1JmSEK99S8q",
        "outputId": "0bf315a3-54b8-4620-a12e-d1baa333091e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original Summary>>> explore using background knowledge query reformulation help retrieve better supporting evidence answering multiple choice science questions\n",
            "Predicted Summary>>> explore using background knowledge query reformulation help retrieve better supporting evidence answering multiple choice science questions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> sgd adam single spiked model tensor pca\n",
            "Predicted Summary>>> sgd adam single spiked model tensor pca \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> non asymptotic analysis sgd svrg showing strength algorithm convergence speed computational cost parametrized parametrized settings\n",
            "Predicted Summary>>> non asymptotic analysis sgd svrg showing strength algorithm convergence speed computational cost parametrized parametrized settings \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improved gan based pixel inpainting network compressed seismic image recovery andproposed xa non uniform sampling survey recommendatio easily applied medical domains compressive sensing technique\n",
            "Predicted Summary>>> improved gan based pixel inpainting network compressed seismic image recovery andproposed xa non uniform sampling survey recommendatio easily applied medical domains technique \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose generative latent variable model unsupervised scene decomposition provides factorized object representation per foreground object also decomposing background segments complex morphology\n",
            "Predicted Summary>>> propose generative latent variable model unsupervised scene decomposition provides factorized object representation per foreground object also decomposing background segments complex morphology \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> regularizing optimization trajectory fisher information old tasks reduces catastrophic forgetting greatly\n",
            "Predicted Summary>>> regularizing optimization trajectory fisher information old tasks reduces catastrophic forgetting greatly \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new approach learning noisy rewards reinforcement learning\n",
            "Predicted Summary>>> new approach learning noisy rewards reinforcement learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper presents empirical analysis role different types image representations probes properties representations task image captioning\n",
            "Predicted Summary>>> paper presents empirical analysis role different types image representations probes properties representations task image captioning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fine tuning bert legal corpora provides marginal valuable improvements nlp tasks legal domain\n",
            "Predicted Summary>>> fine tuning bert legal corpora provides marginal valuable improvements nlp tasks legal domain \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose learn synthesizing shot classifiers many shot classifiers using one single objective function gfsl\n",
            "Predicted Summary>>> propose learn synthesizing shot classifiers many shot classifiers using one single objective function gfsl \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose special weakly supervised multi label learning problem along newly tailored algorithm learns underlying classifier learning assign pseudo labels\n",
            "Predicted Summary>>> propose special weakly supervised multi label learning problem along newly tailored algorithm learns underlying classifier learning assign pseudo labels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce reclor reading comprehension dataset requiring logical reasoning find current state art models struggle real logical reasoning poor performance near random guess\n",
            "Predicted Summary>>> introduce reclor reading comprehension dataset requiring logical reasoning find current state art models struggle real logical reasoning poor performance near random \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> method persistent latent states resblocks demonstrated super resolution alised image sequences\n",
            "Predicted Summary>>> method persistent latent states resblocks demonstrated super resolution alised image sequences \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new methodology novelty detection utilizing hidden space activation values obtained deep autoencoder\n",
            "Predicted Summary>>> new methodology novelty detection utilizing hidden space activation values obtained deep autoencoder \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> graphon good search space neural architecture search empirically produces good networks\n",
            "Predicted Summary>>> graphon good search space neural architecture search empirically produces good networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> input structuring along chaos stability\n",
            "Predicted Summary>>> input structuring along chaos stability \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce machine learning model uses domain independent features estimate criticality current state cause known undesirable state\n",
            "Predicted Summary>>> introduce machine learning model uses domain independent features estimate criticality current state cause known state state state state \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce capacity exploit information degree arbitrary goal achieved another goal intended policy gradient methods\n",
            "Predicted Summary>>> introduce capacity exploit information degree arbitrary goal achieved another goal intended policy gradient methods \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes new method neural network learning online bandit settings marginalizing last layer\n",
            "Predicted Summary>>> paper proposes new method neural network learning online bandit settings marginalizing last layer \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> secret transfer method rl based transfer credit assignment\n",
            "Predicted Summary>>> secret transfer method rl based transfer credit assignment \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> asymptotic convergence stochastic subgradien method momentum general parallel asynchronous computation general nonconvex nonsmooth optimization\n",
            "Predicted Summary>>> asymptotic convergence stochastic subgradien method momentum general parallel asynchronous computation general nonconvex nonsmooth optimization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study deep ensembles lens loss landscape space predictions demonstrating decorrelation power random initializations unmatched subspace sampling explores single mode\n",
            "Predicted Summary>>> study deep ensembles lens loss landscape space predictions demonstrating decorrelation power random initializations unmatched subspace single single \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning imitate expert absence optimal actions learning dynamics model exploring environment\n",
            "Predicted Summary>>> learning imitate expert absence optimal actions learning dynamics model exploring environment \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new way learning semantic program embedding\n",
            "Predicted Summary>>> new way learning semantic program embedding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel method leverages gradients differentiable simulators improve performance rl robotics control\n",
            "Predicted Summary>>> propose novel method leverages gradients differentiable simulators improve performance rl robotics control \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning embedding control high dimensional observations\n",
            "Predicted Summary>>> learning embedding control high dimensional observations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> parameters trained neural network permuted produce completely separate model different task enabling embedding trojan horse networks inside another network\n",
            "Predicted Summary>>> parameters trained neural network permuted produce completely separate model different task enabling embedding trojan horse networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> operations gan latent space induce distribution mismatch compared training distribution address using optimal transport match distributions\n",
            "Predicted Summary>>> operations gan latent space induce distribution mismatch compared training distribution address using optimal transport match distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> benchmark neural linear model uci uci gap datasets\n",
            "Predicted Summary>>> benchmark neural linear model uci uci gap datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> set modifications loc get policy actor critic outperforms performs similarly acer modifications large batchsizes aggressive clamping policy forcing gumbel noise\n",
            "Predicted Summary>>> set modifications loc get policy actor critic outperforms performs similarly acer modifications large batchsizes aggressive clamping policy forcing gumbel \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose hurricane address challenge hardware diversity one shot neural architecture search\n",
            "Predicted Summary>>> propose hurricane address challenge hardware diversity one shot neural architecture search \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> accelerate secure dnn inference trusted execution environments factor selectively outsourcing computation linear layers faster yet untrusted co processor\n",
            "Predicted Summary>>> accelerate secure dnn inference trusted execution environments factor selectively outsourcing computation linear layers faster yet untrusted co processor \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improved likelihood estimates variational autoencoders using self supervised feature learning\n",
            "Predicted Summary>>> improved likelihood estimates variational autoencoders using self supervised feature learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposed novel framework graph similarity learning inductive unsupervised scenario\n",
            "Predicted Summary>>> paper proposed novel framework graph similarity learning inductive unsupervised scenario \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel deep network architecture dynamically decide network capacity trains lifelong learning scenario\n",
            "Predicted Summary>>> propose novel deep network architecture dynamically decide network capacity trains lifelong learning scenario \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel approach spike sorting using neural clustering process ncp recently introduced neural architecture performs scalable amortized approximate bayesian inference efficient probabilistic clustering\n",
            "Predicted Summary>>> present novel approach spike sorting using neural clustering process ncp recently introduced neural architecture performs scalable amortized approximate bayesian inference efficient probabilistic \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> keras infinite neural networks\n",
            "Predicted Summary>>> keras infinite neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> verify deterministic probabilistic properties neural networks using non convex relaxations visible transformations specified generative models\n",
            "Predicted Summary>>> verify deterministic probabilistic properties neural networks using non convex relaxations visible transformations specified generative models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose repeated reference benchmark task regularized continual learning approach adaptive communication humans unfamiliar domains\n",
            "Predicted Summary>>> propose repeated reference benchmark task regularized continual learning approach adaptive communication humans unfamiliar domains \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> tailoring predictions sequence models ldss rnns via explicit latent code\n",
            "Predicted Summary>>> tailoring predictions sequence models ldss rnns via explicit latent code \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose model learns discover informative frames future video sequence represent video via keyframes\n",
            "Predicted Summary>>> propose model learns discover informative frames future video sequence represent video via keyframes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use empirical tools mode connectivity svcca investigate neural network training heuristics learning rate restarts warmup knowledge distillation\n",
            "Predicted Summary>>> use empirical tools mode connectivity svcca investigate neural network training heuristics learning rate restarts warmup knowledge distillation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new theoretical explanation existence adversarial examples\n",
            "Predicted Summary>>> new theoretical explanation existence adversarial examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> focus creating universal adversaries fool object detectors hide objects detectors\n",
            "Predicted Summary>>> focus creating universal adversaries fool object detectors hide objects detectors \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed implementation accelerate dnn data parallel training reducing communication bandwidth requirement\n",
            "Predicted Summary>>> proposed implementation accelerate dnn data parallel training reducing communication bandwidth requirement \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel iterative algorithm based generalized low rank models computing interpreting word embedding models\n",
            "Predicted Summary>>> present novel iterative algorithm based generalized low rank models computing interpreting word embedding models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel method calibrate knowledge graph embedding models without need negative examples\n",
            "Predicted Summary>>> propose novel method calibrate knowledge graph embedding models without need negative examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new application seq seq modelling automating sciene journalism highly abstractive dataset transfer learning tricks automatic evaluation measure\n",
            "Predicted Summary>>> new application seq seq modelling automating sciene journalism highly abstractive dataset transfer learning tricks automatic evaluation measure \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> control topic sentiment text generation almost without training\n",
            "Predicted Summary>>> control topic sentiment text generation almost without training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> conditional vae top latent spaces pre trained generative models enables transfer drastically different domains preserving locality semantic alignment\n",
            "Predicted Summary>>> conditional vae top latent spaces pre trained generative models enables transfer drastically different domains preserving locality semantic alignment \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> variational inference infering discrete distribution low precision neural network derived\n",
            "Predicted Summary>>> variational inference infering discrete distribution low precision neural network derived \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neural phrase based machine translation linear decoding time\n",
            "Predicted Summary>>> neural phrase based machine translation linear decoding time \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning rank using transformer architecture\n",
            "Predicted Summary>>> learning rank using transformer architecture \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> first principled weight initialization method hypernetworks\n",
            "Predicted Summary>>> first principled weight initialization method hypernetworks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> rnns implicitly implement tensor product representations principled interpretable method representing symbolic structures continuous space\n",
            "Predicted Summary>>> rnns implicitly implement tensor product representations principled interpretable method representing symbolic structures continuous space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel two tower shared bottom model architecture transferring knowledge rich implicit feedbacks predict relevance large scale retrieval systems\n",
            "Predicted Summary>>> propose novel two tower shared bottom model architecture transferring knowledge rich implicit feedbacks predict relevance large scale retrieval systems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose ae based gan alleviates mode collapse gans\n",
            "Predicted Summary>>> propose ae based gan alleviates mode collapse gans \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analysis expressivity generality recurrent neural networks relu nonlinearities using tensor train decomposition\n",
            "Predicted Summary>>> analysis expressivity generality recurrent neural networks relu nonlinearities using tensor train decomposition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combine splines neural networks obtain novel distribution functions use model intensity functions point processes\n",
            "Predicted Summary>>> combine splines neural networks obtain novel distribution functions use model intensity functions point processes processes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work enforces hamiltonian dynamics control learn system models embedded position velocity data exploits physically consistent dynamics synthesize model based control via energy shaping\n",
            "Predicted Summary>>> work enforces hamiltonian dynamics control learn system models embedded position velocity data exploits physically consistent dynamics model based control via energy \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce flipout efficient method decorrelating gradients computed stochastic neural net weights within mini batch implicitly sampling pseudo independent weight perturbations example\n",
            "Predicted Summary>>> introduce flipout efficient method decorrelating gradients computed stochastic neural net weights within mini batch implicitly sampling pseudo independent weight perturbations example \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present attribution technique leveraging sparsity inducing norms achieve interpretability\n",
            "Predicted Summary>>> present attribution technique leveraging sparsity inducing norms achieve interpretability \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> discover structure functional causal models generative neural networks\n",
            "Predicted Summary>>> discover structure functional causal models generative neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> spiking recurrent neural networks performing working memory task utilize long heterogeneous timescales strikingly similar observed prefrontal cortex\n",
            "Predicted Summary>>> spiking recurrent neural networks performing working memory task utilize long heterogeneous timescales strikingly similar observed prefrontal cortex \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gan based method joint image per pixel annotation synthesis\n",
            "Predicted Summary>>> gan based method joint image per pixel annotation synthesis \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> design incremental sequence action parsers text sql task achieve sota results improve using non deterministic oracles allow multiple correct action sequences\n",
            "Predicted Summary>>> design incremental sequence action parsers text sql task achieve sota results improve using non deterministic oracles allow multiple correct correct correct \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new auto encoder incorporated multiway delay embedding transform toward interpreting deep image prior\n",
            "Predicted Summary>>> propose new auto encoder incorporated multiway delay embedding transform toward interpreting deep image prior \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> estimation training data distribution trained classifier using gan\n",
            "Predicted Summary>>> estimation training data distribution trained classifier using gan \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposed simple yet effective baseline learning noisy labels\n",
            "Predicted Summary>>> paper proposed simple yet effective baseline learning noisy labels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose use separate exploration policy collect pre adaptation trajectories maml also show using self supervised objective inner loop leads stable training much better performance\n",
            "Predicted Summary>>> propose use separate exploration policy collect pre adaptation trajectories maml also show using self supervised objective inner loop leads stable training much better performance performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> predicting auction price vehicle license plates hong kong deep recurrent neural network based characters plates\n",
            "Predicted Summary>>> predicting auction price vehicle license plates hong kong deep recurrent neural network based characters plates \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> generalizing backward propagation using formal methods supersymmetry\n",
            "Predicted Summary>>> generalizing backward propagation using formal methods supersymmetry \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose extension multi output learning continuum tasks using operator valued kernels\n",
            "Predicted Summary>>> propose extension multi output learning continuum tasks using operator valued kernels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present method interpreting black box models using instance wise backward selection identify minimal subsets features alone suffice justify particular decision made model\n",
            "Predicted Summary>>> present method interpreting black box models using instance wise backward selection identify minimal subsets features alone suffice justify particular decision made made made \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> recent distribution detection method helps measure confidence rnn predictions nlp tasks\n",
            "Predicted Summary>>> recent distribution detection method helps measure confidence rnn predictions nlp tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work proposed universal visual representation neural machine translation nmt using retrieved images similar topics source sentence extending image applicability nmt\n",
            "Predicted Summary>>> work proposed universal visual representation neural machine translation nmt using retrieved images similar topics source sentence extending image applicability image \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show minimizing cross entropy loss using gradient method could lead poor margin features dataset lie low dimensional subspace\n",
            "Predicted Summary>>> show minimizing cross entropy loss using gradient method could lead poor margin features dataset lie low dimensional \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose framework incorporates planning efficient exploration learning complex environments\n",
            "Predicted Summary>>> propose framework incorporates planning efficient exploration learning complex environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fully connected architecture used produce word embeddings character representations outperforms traditional embeddings provides insight sparsity dropout\n",
            "Predicted Summary>>> fully connected architecture used produce word embeddings character representations outperforms traditional embeddings provides insight sparsity dropout \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop general framework establish certified robustness ml models various classes adversarial perturbations\n",
            "Predicted Summary>>> develop general framework establish certified robustness ml models various classes adversarial perturbations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop new deep generative model semi supervised learning propose new max min cross entropy training cnns\n",
            "Predicted Summary>>> develop new deep generative model semi supervised learning propose new max min cross entropy training cnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> compress deep cnns reusing single convolutional layer iterative manner thereby reducing parameter counts factor proportional depth whilst leaving accuracies largely unaffected\n",
            "Predicted Summary>>> compress deep cnns reusing single convolutional layer iterative manner thereby reducing parameter counts factor proportional depth whilst accuracies unaffected unaffected \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> oe teaches anomaly detectors learn heuristics detecting unseen anomalies experiments classification density estimation calibration nlp vision settings tune test distribution samples unlike previous work\n",
            "Predicted Summary>>> oe teaches anomaly detectors learn heuristics detecting unseen anomalies experiments classification density estimation calibration nlp vision settings tune test distribution samples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> built android keyboard lexical word based semantic meaning based emoji suggestion capabilities compared effects two different chat studies\n",
            "Predicted Summary>>> built android keyboard lexical word based semantic meaning based emoji suggestion capabilities compared effects two different chat studies \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> understand transferability perspectives improved generalization optimization feasibility transferability\n",
            "Predicted Summary>>> understand transferability perspectives improved generalization optimization feasibility transferability \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show possible recover parameters layer relu generative model looking samples generated\n",
            "Predicted Summary>>> show possible recover parameters layer relu generative model looking samples generated \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> previous vaes text cannot learn controllable latent representation images well fix enable first success towards controlled text generation without supervision\n",
            "Predicted Summary>>> previous vaes text cannot learn controllable latent representation images well fix enable first success towards controlled text generation without \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new framework preconditioner learning derive new forms preconditioners learning methods reveal relationship methods like rmsprop adam adagrad esgd kfac batch normalization etc\n",
            "Predicted Summary>>> propose new framework preconditioner learning derive new forms preconditioners learning methods reveal relationship methods like rmsprop adagrad adagrad adagrad adagrad adagrad kfac batch \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> inference large transformers expensive due self attention multiple layers show simple decomposition technique yield faster low memory footprint model accurate original models\n",
            "Predicted Summary>>> inference large transformers expensive due self attention multiple layers show simple decomposition technique yield faster memory memory model model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> limiting state information default policy improvement performance kl regularized rl framework agent default policy optimized together\n",
            "Predicted Summary>>> limiting state information default policy improvement performance kl regularized rl framework agent default policy optimized together \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel approach improve given cross surface mapping local refinement new iterative method deform mesh order meet user constraints\n",
            "Predicted Summary>>> propose novel approach improve given cross surface mapping local refinement new iterative method deform mesh order meet meet constraints \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> way generate training corpora neural code synthesis using discriminator trained unlabelled data\n",
            "Predicted Summary>>> way generate training corpora neural code synthesis using discriminator trained unlabelled data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose first attack independent robustness metric clever applied neural network classifier\n",
            "Predicted Summary>>> propose first attack independent robustness metric clever applied neural network classifier \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel multi task framework learns table detection semantic component recognition cell type classification spreadsheet tables promising results\n",
            "Predicted Summary>>> propose novel multi task framework learns table detection semantic component recognition cell type classification spreadsheet tables promising results \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present provable algorithm exactly recovering factors dictionary learning model\n",
            "Predicted Summary>>> present provable algorithm exactly recovering factors dictionary learning model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> stochastic variational video prediction real world settings\n",
            "Predicted Summary>>> stochastic variational video prediction real world settings \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose instance cross entropy ice measures difference estimated instance level matching distribution ground truth one\n",
            "Predicted Summary>>> propose instance cross entropy ice measures difference estimated instance level matching distribution ground truth one \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present method adapting hyperparameters probabilistic models using optimal transport applications robotics\n",
            "Predicted Summary>>> present method adapting hyperparameters probabilistic models using optimal transport applications robotics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose choco sgd decentralized sgd compressed communication non convex objectives show strong performance various deep learning applications device learning datacenter case\n",
            "Predicted Summary>>> propose choco sgd decentralized sgd compressed communication non convex objectives show strong performance various deep learning applications device learning datacenter case \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose tucker relatively simple powerful linear model link prediction knowledge graphs based tucker decomposition binary tensor representation knowledge graph triples\n",
            "Predicted Summary>>> propose tucker relatively simple powerful linear model link prediction knowledge graphs based tucker decomposition binary tensor representation knowledge triples triples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> reproduced alphazero google cloud platform\n",
            "Predicted Summary>>> reproduced alphazero google cloud platform \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> general detailed neural network gdnn multi task learning incorporating cross domain sketch cds semantic parsing\n",
            "Predicted Summary>>> general detailed neural network gdnn multi task learning incorporating cross domain sketch cds semantic parsing \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use dynamic rewards train event extractors\n",
            "Predicted Summary>>> use dynamic rewards train event extractors \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> describe improve image generative model according slow difficult evaluate objective human feedback could many applications like making aesthetic images\n",
            "Predicted Summary>>> describe improve image generative model according slow difficult evaluate objective human feedback could many applications like making aesthetic images \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> strokenet novel architecture agent trained draw strokes differentiable simulation environment could effectively exploit power back propagation\n",
            "Predicted Summary>>> strokenet novel architecture agent trained draw strokes differentiable simulation environment could effectively exploit power back propagation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> explored novel method compositional set embeddings perceive represent single class entire set classes associated input data\n",
            "Predicted Summary>>> explored novel method compositional set embeddings perceive represent single class entire set classes associated input data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> minimum set exponentially distributed hashes useful collision probability generalizes jaccard index probability distributions\n",
            "Predicted Summary>>> minimum set exponentially distributed hashes useful collision probability generalizes jaccard index probability distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces coloring scheme node disambiguation graph neural networks based separability proven universal mpnn extension\n",
            "Predicted Summary>>> paper introduces coloring scheme node disambiguation graph neural networks based separability proven universal mpnn extension \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide fast principled adversarial training procedure computational statistical performance guarantees\n",
            "Predicted Summary>>> provide fast principled adversarial training procedure computational statistical performance guarantees \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> cascade adversarial training low level similarity learning improve robustness white box black box attacks\n",
            "Predicted Summary>>> cascade adversarial training low level similarity learning improve robustness white box black box attacks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> practical adaptive algorithms gradient based meta learning provable guarantees\n",
            "Predicted Summary>>> practical adaptive algorithms gradient based meta learning provable guarantees \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> apply ordinary differential equation model graph structured data\n",
            "Predicted Summary>>> apply ordinary differential equation model graph structured data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes novel actor critic method uses hessians critic update actor\n",
            "Predicted Summary>>> paper proposes novel actor critic method uses hessians critic update actor \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel state space time series model capability capture structure change points anomaly points better forecasting performance exist change points anomalies time series\n",
            "Predicted Summary>>> propose novel state space time series model capability capture structure change points anomaly points better forecasting performance exist change points anomalies time series series \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> strategy repair damaged neural networks\n",
            "Predicted Summary>>> strategy repair damaged neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate space efficiency memory augmented neural nets learning set membership\n",
            "Predicted Summary>>> investigate space efficiency memory augmented neural nets learning set membership \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn training neural networks treat layer gradient boosting problem\n",
            "Predicted Summary>>> learn training neural networks treat layer gradient boosting problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> using bert encoder sequential prediction labels multi label text classification task\n",
            "Predicted Summary>>> using bert encoder sequential prediction labels multi label text classification task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> backward model previous state action given next state used simulate additional trajectories terminating states interest improves rl learning efficiency\n",
            "Predicted Summary>>> backward model previous state action given next state used simulate additional trajectories terminating states improves improves rl rl efficiency \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised spectral clustering using deep neural networks\n",
            "Predicted Summary>>> unsupervised spectral clustering using deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simple augmentation method overcomes robustness accuracy trade observed literature opens questions effect training distribution distribution generalization\n",
            "Predicted Summary>>> simple augmentation method overcomes robustness accuracy trade observed literature opens questions effect training distribution distribution generalization generalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show autoregressive flows used improve sequential latent variable models\n",
            "Predicted Summary>>> show autoregressive flows used improve sequential latent variable models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extend quantum svms semi supervised setting deal likely problem many missing class labels huge datasets\n",
            "Predicted Summary>>> extend quantum svms semi supervised setting deal likely problem many missing class labels huge datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce statistical approach assessing neural network robustness provides informative notion robust network rather conventional binary assertion whether property violated\n",
            "Predicted Summary>>> introduce statistical approach assessing neural network robustness provides informative notion robust network rather conventional binary assertion whether property violated \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised learning approach separating two structured signals superposition\n",
            "Predicted Summary>>> unsupervised learning approach separating two structured signals superposition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use unrolled simulator end end differentiable model protein structure show sometimes hierarchically generalize unseen fold topologies\n",
            "Predicted Summary>>> use unrolled simulator end end differentiable model protein structure show sometimes hierarchically generalize unseen fold topologies \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze tasks best learned together one network best learn separately\n",
            "Predicted Summary>>> analyze tasks best learned together one network best learn separately \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> wasserstein autoencoder hyperbolic latent space\n",
            "Predicted Summary>>> wasserstein autoencoder hyperbolic latent space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new certified adversarial training method crown ibp achieves state art robustness inf norm adversarial perturbations\n",
            "Predicted Summary>>> propose new certified adversarial training method crown ibp achieves state art robustness inf norm adversarial perturbations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> distance kernel embedding via random features structured inputs\n",
            "Predicted Summary>>> distance kernel embedding via random features structured inputs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose automatic metrics holistically evaluate open dialogue generation strongly correlate human evaluation\n",
            "Predicted Summary>>> propose automatic metrics holistically evaluate open dialogue generation strongly correlate human evaluation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show pre training untrained neural network examples improve reconstruction results compressed sensing semantic recovery problems like colorization\n",
            "Predicted Summary>>> show pre training untrained neural network examples improve reconstruction results compressed sensing semantic recovery problems like colorization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> bayesian meta learning using pac bayes framework implicit prior distributions\n",
            "Predicted Summary>>> bayesian meta learning using pac bayes framework implicit prior distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep net deep neural network cyber security use cases\n",
            "Predicted Summary>>> deep net deep neural network cyber security use cases \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes novel actor critic method uses hessians critic update actor\n",
            "Predicted Summary>>> paper proposes novel actor critic method uses hessians critic update actor \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new state art imagenet mobile setting\n",
            "Predicted Summary>>> new state art imagenet mobile setting \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use mixture density networks full conditional density estimation spatial offset regression apply human pose estimation task\n",
            "Predicted Summary>>> use mixture density networks full conditional density estimation spatial offset regression apply human pose estimation task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent\n",
            "Predicted Summary>>> new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neural simulation universal turing machine\n",
            "Predicted Summary>>> neural simulation universal turing machine \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose generalization visit counters evaluate propagating exploratory value trajectories enabling efficient exploration model free rl\n",
            "Predicted Summary>>> propose generalization visit counters evaluate propagating exploratory value trajectories enabling efficient exploration model free rl \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> initiate push towards building er systems recognize thousands types providing method automatically construct suitable datasets based type hierarchy\n",
            "Predicted Summary>>> initiate push towards building er systems recognize thousands types providing method automatically construct suitable datasets based hierarchy \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> adversarial attacks unsupervised node embeddings based eigenvalue perturbation theory\n",
            "Predicted Summary>>> adversarial attacks unsupervised node embeddings based eigenvalue perturbation theory \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> sober view current state gans practical perspective\n",
            "Predicted Summary>>> sober view current state gans practical perspective \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show transformer architecture neural gpu turing complete\n",
            "Predicted Summary>>> show transformer architecture neural gpu turing complete \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present software framework transforming distributions demonstrate flexibility relaxing mean field assumptions variational inference use coupling flows replicate structure target generative model\n",
            "Predicted Summary>>> present software framework transforming distributions demonstrate flexibility relaxing mean field assumptions variational inference use coupling flows replicate structure target generative model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide first time rigorous proof orthogonal initialization speeds convergence relative gaussian initialization deep linear networks\n",
            "Predicted Summary>>> provide first time rigorous proof orthogonal initialization speeds convergence relative gaussian initialization deep linear networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes attention module captures inter channel relationships offers large performance gains\n",
            "Predicted Summary>>> paper proposes attention module captures inter channel relationships offers large performance gains \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce new gradient detach based complementary objective training strategy domain adaptive object detection\n",
            "Predicted Summary>>> introduce new gradient detach based complementary objective training strategy domain adaptive object detection \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose extension multi output learning continuum tasks using operator valued kernels\n",
            "Predicted Summary>>> propose extension multi output learning continuum tasks using operator valued kernels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> submitted emnlp\n",
            "Predicted Summary>>> submitted emnlp \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> trellis networks new sequence modeling architecture bridges recurrent convolutional models sets new state art word character level language modeling\n",
            "Predicted Summary>>> trellis networks new sequence modeling architecture bridges recurrent convolutional models sets new state art word character language language modeling \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> stable domain adversarial training approach robust comprehensive domain adaptation\n",
            "Predicted Summary>>> stable domain adversarial training approach robust comprehensive domain adaptation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel cnn structure video level representation learning surpassing recent cnns\n",
            "Predicted Summary>>> novel cnn structure video level representation learning surpassing recent cnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised structure learning method parsimonious deep feed forward networks\n",
            "Predicted Summary>>> unsupervised structure learning method parsimonious deep feed forward networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> patch based bottleneck formulation vae framework learns unsupervised representations better suited visual recognition\n",
            "Predicted Summary>>> patch based bottleneck formulation vae framework learns unsupervised representations better suited visual recognition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> loopgan extends cycle length cyclegan enable unaligned sequential transformation two time steps\n",
            "Predicted Summary>>> loopgan extends cycle length cyclegan enable unaligned sequential transformation two time steps \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep learning adaptation randomized least squares value iteration\n",
            "Predicted Summary>>> deep learning adaptation randomized least squares value iteration \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> classification problems classes show gradient tends live tiny slowly evolving subspace spanned eigenvectors corresponding largest eigenvalues hessian\n",
            "Predicted Summary>>> classification problems classes show gradient tends live tiny slowly evolving subspace spanned eigenvectors corresponding largest eigenvalues hessian \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show special goal condition value function trained model free methods used within model based control resulting substantially better sample efficiency performance\n",
            "Predicted Summary>>> show special goal condition value function trained model free methods used within model based control resulting substantially better sample efficiency performance performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> adapting ucb exploration ensemble learning improves prior methods double dqn atari benchmark\n",
            "Predicted Summary>>> adapting ucb exploration ensemble learning improves prior methods double dqn atari benchmark \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel variants optimization methods combine benefits adaptive non adaptive methods\n",
            "Predicted Summary>>> novel variants optimization methods combine benefits adaptive non adaptive methods \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces partial grounding tackle problem arises full grounding process translation pddl input task ground representation like strips infeasible due memory time constraints\n",
            "Predicted Summary>>> paper introduces partial grounding tackle problem arises full grounding process translation pddl input task input ground representation like due like due memory due \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new sparse structured attention mechanism tvmax promotes sparsity encourages weight related adjacent locations\n",
            "Predicted Summary>>> propose new sparse structured attention mechanism tvmax promotes sparsity encourages weight related adjacent locations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> self ensembling based algorithm visual domain adaptation state art results visda image classification domain adaptation challenge\n",
            "Predicted Summary>>> self ensembling based algorithm visual domain adaptation state art results visda image classification domain adaptation challenge \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> existing deep convolutional networks image classification tasks sensitive gabor noise patterns small structured changes input cause large changes output\n",
            "Predicted Summary>>> existing deep convolutional networks image classification tasks sensitive gabor noise patterns small structured changes changes large changes large output output \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised self imitation algorithm capable inference single expert demonstration\n",
            "Predicted Summary>>> unsupervised self imitation algorithm capable inference single expert demonstration \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> data efficient deep reinforcement learning used learning precise stacking policies\n",
            "Predicted Summary>>> data efficient deep reinforcement learning used learning precise stacking policies \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gans successful adversarial training use convnets show convnet generator trained simple reconstruction loss learnable noise vectors leads many desirable properties gan\n",
            "Predicted Summary>>> gans successful adversarial training use convnets show convnet generator trained simple reconstruction loss learnable vectors vectors many leads many properties gan \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce new method synthesizing adversarial examples robust physical world use fabricate first adversarial objects\n",
            "Predicted Summary>>> introduce new method synthesizing adversarial examples robust physical world use fabricate first adversarial objects \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce attention mechanism improve feature extraction deep active learning al semi supervised setting\n",
            "Predicted Summary>>> introduce attention mechanism improve feature extraction deep active learning al semi supervised setting \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose algorithm learning useful skills without reward function show skills used solve downstream tasks\n",
            "Predicted Summary>>> propose algorithm learning useful skills without reward function show skills used solve downstream tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop agent call distributional deterministic deep policy gradient algorithm achieves state art performance number challenging continuous control problems\n",
            "Predicted Summary>>> develop agent call distributional deterministic deep policy gradient algorithm achieves state art performance number challenging continuous control problems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work presents scalable algorithm non linear offline system identification partial observations\n",
            "Predicted Summary>>> work presents scalable algorithm non linear offline system identification partial observations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose regularizer improves interpolation autoencoders show also improves learned representation downstream tasks\n",
            "Predicted Summary>>> propose regularizer improves interpolation autoencoders show also improves learned representation downstream tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use siamese networks guide disentangle generation process gans without labeled data\n",
            "Predicted Summary>>> use siamese networks guide disentangle generation process gans without labeled data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel approach using mode connectivity loss landscapes mitigate adversarial effects repair tampered models evaluate adversarial robustness\n",
            "Predicted Summary>>> novel approach using mode connectivity loss landscapes mitigate adversarial effects repair tampered models evaluate adversarial robustness \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present end end design methodology efficient deep learning deployment\n",
            "Predicted Summary>>> present end end design methodology efficient deep learning deployment \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce fedprox framework tackle statistical heterogeneity federated settings convergence guarantees improved robustness stability\n",
            "Predicted Summary>>> introduce fedprox framework tackle statistical heterogeneity federated settings convergence guarantees improved robustness stability \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn task agnostic world graph abstraction environment show using structured exploration significantly accelerate downstream task specific rl\n",
            "Predicted Summary>>> learn task agnostic world graph abstraction environment show using structured exploration significantly accelerate downstream task specific rl \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> high object detection accuracy obtained training domain specific compact models training short\n",
            "Predicted Summary>>> high object detection accuracy obtained training domain specific compact models training short \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> variant capsule networks used pairwise learning tasks results shows siamese capsule networks work well shot learning setting\n",
            "Predicted Summary>>> variant capsule networks used pairwise learning tasks results shows siamese capsule networks work well shot learning setting \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dynamic precision technique train deep neural networks\n",
            "Predicted Summary>>> dynamic precision technique train deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> tackle goal conditioned tasks combining hindsight experience replay imitation learning algorithms showing faster convergence first higher final performance second\n",
            "Predicted Summary>>> tackle goal conditioned tasks combining hindsight experience replay imitation learning algorithms showing faster convergence first higher second performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> sparse mobilenets faster dense ones appropriate kernels\n",
            "Predicted Summary>>> sparse mobilenets faster dense ones appropriate kernels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup\n",
            "Predicted Summary>>> present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> limiting state information default policy improvement performance kl regularized rl framework agent default policy optimized together\n",
            "Predicted Summary>>> limiting state information default policy improvement performance kl regularized rl framework agent default policy optimized together \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> framework links deep network layers stochastic optimization algorithms used improve model accuracy inform network design\n",
            "Predicted Summary>>> framework links deep network layers stochastic optimization algorithms used improve model accuracy inform network design \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> find environment settings sota agents trained navigation tasks display extreme failures suggesting failures generalization\n",
            "Predicted Summary>>> find environment settings sota agents trained navigation tasks display extreme failures suggesting failures generalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose transint novel interpretable kg embedding method isomorphically preserves implication ordering among relations embedding space explainable robust geometrically coherent way\n",
            "Predicted Summary>>> propose transint novel interpretable kg embedding method isomorphically preserves implication ordering among relations embedding space explainable robust geometrically coherent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simplex based geometric method proposed cope shot learning problems\n",
            "Predicted Summary>>> simplex based geometric method proposed cope shot learning problems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose differentially private laplacian smoothing stochastic gradient descent train machine learning models better utility maintain differential privacy guarantees\n",
            "Predicted Summary>>> propose differentially private laplacian smoothing stochastic gradient descent train machine learning models better utility maintain differential privacy guarantees \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper extends multi agent generative adversarial imitation learning extensive form markov games\n",
            "Predicted Summary>>> paper extends multi agent generative adversarial imitation learning extensive form markov games \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose high performance lstms binary ternary weights greatly reduce implementation complexity\n",
            "Predicted Summary>>> propose high performance lstms binary ternary weights greatly reduce implementation complexity \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose approach learn decentralized policies multi agent settings using attention based critics demonstrate promising results environments complex interactions\n",
            "Predicted Summary>>> propose approach learn decentralized policies multi agent settings using attention based critics demonstrate promising results environments complex interactions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed cooperative training novel training algorithm generative modeling discrete data\n",
            "Predicted Summary>>> proposed cooperative training novel training algorithm generative modeling discrete data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> generative model reaction prediction learns mechanistic electron steps reaction directly raw reaction data\n",
            "Predicted Summary>>> generative model reaction prediction learns mechanistic electron steps reaction directly raw reaction data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> two novel gans constructed generate high quality fmri brain images synthetic brain images greatly help improve downstream classification tasks\n",
            "Predicted Summary>>> two novel gans constructed generate high quality fmri brain images synthetic brain images greatly help improve classification tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper describes three techniques allow non backtracking computationally limited scheduler consider small number alternative activities based resource availability\n",
            "Predicted Summary>>> paper describes three techniques allow non backtracking computationally limited scheduler consider small number alternative activities based resource availability \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> practical proposal ethical responsive nlp technology operationalizing transparency test training data\n",
            "Predicted Summary>>> practical proposal ethical responsive nlp technology operationalizing transparency test training data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn high quality denoising using single instances corrupted images training data\n",
            "Predicted Summary>>> learn high quality denoising using single instances corrupted images training data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show minimizing cross entropy loss using gradient method could lead poor margin features dataset lie low dimensional subspace\n",
            "Predicted Summary>>> show minimizing cross entropy loss using gradient method could lead poor margin features dataset lie low dimensional \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop analytical method study bayesian inference finite width neural networks find renormalization group flow picture naturally emerges\n",
            "Predicted Summary>>> develop analytical method study bayesian inference finite width neural networks find renormalization group flow picture naturally emerges \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> utilized within subjects study evaluate four paced breathing visuals common mobile apps understand effective providing breathing exercise guidance\n",
            "Predicted Summary>>> utilized within subjects study evaluate four paced breathing visuals common mobile apps understand effective providing breathing exercise guidance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel neural architecture efficient amortized inference latent permutations\n",
            "Predicted Summary>>> novel neural architecture efficient amortized inference latent permutations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose generative latent variable model unsupervised scene decomposition provides factorized object representation per foreground object also decomposing background segments complex morphology\n",
            "Predicted Summary>>> propose generative latent variable model unsupervised scene decomposition provides factorized object representation per foreground object also decomposing background segments complex morphology \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> weakly supervised learning based clustering framework performs comparable fully supervised learning models exploiting unique class count\n",
            "Predicted Summary>>> weakly supervised learning based clustering framework performs comparable fully supervised learning models exploiting unique class count \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improve existing dialogue systems responding people sharing personal stories incorporating emotion prediction representations also release new benchmark dataset empathetic dialogues\n",
            "Predicted Summary>>> improve existing dialogue systems responding people sharing personal stories incorporating emotion prediction representations also release new benchmark dataset empathetic dialogues \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep neural networks trained data augmentation require explicit regularization weight decay dropout exhibit greater adaptaibility changes architecture amount training data\n",
            "Predicted Summary>>> deep neural networks trained data augmentation require explicit regularization weight decay dropout exhibit greater adaptaibility changes architecture amount training training training data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose simple self supervised data augmentation technique improves performance fully supervised scenarios including shot learning imbalanced classification\n",
            "Predicted Summary>>> propose simple self supervised data augmentation technique improves performance fully supervised scenarios including shot learning imbalanced classification \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> method active anomaly detection present new layer attached deep learning model designed unsupervised anomaly detection transform active method\n",
            "Predicted Summary>>> method active anomaly detection present new layer attached deep learning model designed unsupervised anomaly detection transform active method \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> monte carlo methods quantizing pre trained models without additional training\n",
            "Predicted Summary>>> monte carlo methods quantizing pre trained models without additional training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neural probabilistic motor primitives compress motion capture tracking policies one flexible model capable one shot imitation reuse low level controller\n",
            "Predicted Summary>>> neural probabilistic motor primitives compress motion capture tracking policies one flexible model capable one shot imitation reuse low level controller \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper presents method stochastically generating video frames given key frames using direct convolutions\n",
            "Predicted Summary>>> paper presents method stochastically generating video frames given key frames using direct convolutions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> regularization based approach continual learning using bayesian neural networks predict parameters importance\n",
            "Predicted Summary>>> regularization based approach continual learning using bayesian neural networks predict parameters importance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce three generic point cloud processing blocks improve accuracy memory consumption multiple state art networks thus allowing design deeper accurate networks\n",
            "Predicted Summary>>> introduce three generic point cloud processing blocks improve accuracy memory consumption multiple state networks networks thus design design deeper networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> immersive visualization classical non euclidean spaces using real time ray tracing\n",
            "Predicted Summary>>> immersive visualization classical non euclidean spaces using real time ray tracing \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new algorithm leveraging expressiveness generative neural networks improve evolutionary strategies algorithms\n",
            "Predicted Summary>>> propose new algorithm leveraging expressiveness generative neural networks improve evolutionary strategies algorithms \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning embedding control high dimensional observations\n",
            "Predicted Summary>>> learning embedding control high dimensional observations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unify support estimation family adversarial imitation learning algorithms support guided adversarial imitation learning robust stable imitation learning framework\n",
            "Predicted Summary>>> unify support estimation family adversarial imitation learning algorithms support guided adversarial imitation learning robust stable imitation learning framework \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed flexible generative model learns stably directly minimizing exact empirical wasserstein distance\n",
            "Predicted Summary>>> proposed flexible generative model learns stably directly minimizing exact empirical wasserstein distance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> hierarchical generative model hybrid vae gan learns disentangled representation data without compromising generative quality\n",
            "Predicted Summary>>> hierarchical generative model hybrid vae gan learns disentangled representation data without compromising generative quality \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> theoretical analysis nonlinear wide autoencoder\n",
            "Predicted Summary>>> theoretical analysis nonlinear wide autoencoder \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> strong bias towards simple outpouts observed many simple input ouput maps parameter function map deep networks found biased way\n",
            "Predicted Summary>>> strong bias towards simple outpouts observed many simple input ouput maps parameter function map deep networks found biased biased biased \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> detecting overlapping communities graphs using graph neural networks\n",
            "Predicted Summary>>> detecting overlapping communities graphs using graph neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present approach redesign environment uninterpretable agent behaviors minimized eliminated\n",
            "Predicted Summary>>> present approach redesign environment uninterpretable agent behaviors minimized eliminated \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> conduct adversarial attacks binarized neural networks show reduce impact strongest attacks maintaining comparable accuracy black box setting\n",
            "Predicted Summary>>> conduct adversarial attacks binarized neural networks show reduce impact strongest attacks maintaining comparable accuracy black box setting \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> parameter space noise allows reinforcement learning algorithms explore perturbing parameters instead actions often leading significantly improved exploration performance\n",
            "Predicted Summary>>> parameter space noise allows reinforcement learning algorithms explore perturbing parameters instead actions often leading significantly improved exploration exploration performance performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> demonstrate gated recurrent asynchronous spiking neural network corresponds lstm unit\n",
            "Predicted Summary>>> demonstrate gated recurrent asynchronous spiking neural network corresponds lstm unit \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose four new ways collecting nli data help slightly pretraining data help reduce annotation artifacts\n",
            "Predicted Summary>>> propose four new ways collecting nli data help slightly pretraining data help reduce \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> distill language models representations syntax unsupervised metric learning\n",
            "Predicted Summary>>> distill language models representations syntax unsupervised metric learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> information bottleneck principle applied resnets using pixelcnn models decode mutual information conditionally generate images information illustration\n",
            "Predicted Summary>>> information bottleneck principle applied resnets using pixelcnn models decode mutual information conditionally generate images information illustration \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning based algorithms improve upon performance classical algorithms low rank approximation problem retaining worst case guarantee\n",
            "Predicted Summary>>> learning based algorithms improve upon performance classical algorithms low rank approximation problem retaining worst case guarantee \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel framework evaluate interpretability neural network\n",
            "Predicted Summary>>> propose novel framework evaluate interpretability neural network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> training method make deep learning algorithms work better neuromorphic computing chips uncertainty\n",
            "Predicted Summary>>> training method make deep learning algorithms work better neuromorphic computing chips uncertainty \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose efficient provable data independent method network compression via neural pruning using coresets neurons novel construction proposed paper\n",
            "Predicted Summary>>> propose efficient provable data independent method network compression via neural pruning using coresets neurons novel construction proposed paper \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> cnn lstm generate markup like code describing graphical user interface images\n",
            "Predicted Summary>>> cnn lstm generate markup like code describing graphical user interface images \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> hybrid model utilizing raw audio spectrogram information speech enhancement tasks\n",
            "Predicted Summary>>> hybrid model utilizing raw audio spectrogram information speech enhancement tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> topology based graph convolutional network gcn\n",
            "Predicted Summary>>> topology based graph convolutional network gcn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> soft quantization approach learn pure fixed point representations deep neural networks\n",
            "Predicted Summary>>> soft quantization approach learn pure fixed point representations deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present first verification neural network perception tasks produces correct output within specified tolerance every input interest\n",
            "Predicted Summary>>> present first verification neural network perception tasks produces correct output within specified tolerance every input \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> efficient video classification using frame based conditional gating module selecting dominant frames followed temporal modeling classifier\n",
            "Predicted Summary>>> efficient video classification using frame based conditional gating module selecting dominant frames temporal modeling classifier modeling \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> jointly train multilingual skip gram model cross lingual sentence similarity model learn high quality multilingual text embeddings perform well low resource scenario\n",
            "Predicted Summary>>> jointly train multilingual skip gram model cross lingual sentence similarity model learn high quality multilingual text embeddings perform well low resource scenario \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> modular method fully cooperative multi goal multi agent reinforcement learning based curriculum learning efficient exploration credit assignment action goal interactions\n",
            "Predicted Summary>>> modular method fully cooperative multi goal multi agent reinforcement learning based curriculum learning efficient exploration credit assignment action goal interactions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> controlled study role environments respect properties emergent communication protocols\n",
            "Predicted Summary>>> controlled study role environments respect properties emergent communication protocols \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> submitted emnlp\n",
            "Predicted Summary>>> submitted emnlp \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> modular approach consisting sentence selector module followed qa model made robust adversarial attacks comparison qa model trained full context\n",
            "Predicted Summary>>> modular approach consisting sentence selector module followed qa model made robust adversarial attacks comparison qa model trained full context \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces algorithm handle optimization problem multiple constraints vision manifold\n",
            "Predicted Summary>>> paper introduces algorithm handle optimization problem multiple constraints vision manifold \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> non reversible way making accept reject decisions beneficial\n",
            "Predicted Summary>>> non reversible way making accept reject decisions beneficial \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper compares statistical tests rl comparisons false positive statistical power checks robustness assumptions using simulated distributions empirical distributions sac td provides guidelines rl students researchers\n",
            "Predicted Summary>>> paper compares statistical tests rl comparisons false positive statistical power checks robustness assumptions using simulated empirical distributions empirical td td provides rl \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> manifold structured latent space generative models\n",
            "Predicted Summary>>> manifold structured latent space generative models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper propose imitative models combine benefits il goal directed planning probabilistic predictive models desirable behavior able plan interpretable expert like trajectories achieve specified goals\n",
            "Predicted Summary>>> paper propose imitative models combine benefits il goal directed planning probabilistic predictive models desirable behavior able plan interpretable expert like trajectories achieve specified specified \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> formulating sparse signal recovery sequential decision making problem develop method based rl mcts learns policy discover support sparse signal\n",
            "Predicted Summary>>> formulating sparse signal recovery sequential decision making problem develop method based rl mcts learns policy discover support sparse signal \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study empirically hard recover missing parts trained models\n",
            "Predicted Summary>>> study empirically hard recover missing parts trained models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce method computing intrinsic reward curiosity using metrics derived sampling latent variable model used estimate dynamics\n",
            "Predicted Summary>>> introduce method computing intrinsic reward curiosity using metrics derived sampling latent variable model used estimate dynamics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> convolution neural network multi view stereo matching whose design inspired best practices traditional geometry based approaches\n",
            "Predicted Summary>>> convolution neural network multi view stereo matching whose design inspired best practices traditional geometry based approaches \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> built physical simulation rodent trained solve set tasks analyzed resulting networks\n",
            "Predicted Summary>>> built physical simulation rodent trained solve set tasks analyzed resulting networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning synthesize raw waveform audio gans\n",
            "Predicted Summary>>> learning synthesize raw waveform audio gans \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> scale generative classifiers complex datasets evaluate effectiveness reject illegal inputs including distribution samples adversarial examples\n",
            "Predicted Summary>>> scale generative classifiers complex datasets evaluate effectiveness reject illegal inputs including distribution samples adversarial examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose use separate exploration policy collect pre adaptation trajectories maml also show using self supervised objective inner loop leads stable training much better performance\n",
            "Predicted Summary>>> propose use separate exploration policy collect pre adaptation trajectories maml also show using self supervised objective inner loop leads stable training much better performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new module improves resnet like architectures enforcing channel selective behavior convolutional layers\n",
            "Predicted Summary>>> propose new module improves resnet like architectures enforcing channel selective behavior convolutional layers \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce novel method train seq seq models language models converge faster generalize better almost completely transfer new domain using less labeled data\n",
            "Predicted Summary>>> introduce novel method train seq seq models language models converge faster generalize better almost completely transfer new domain using less less \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new dnn architecture deep learning tabular data\n",
            "Predicted Summary>>> propose new dnn architecture deep learning tabular data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce method train models provable robustness wrt norms geq simultaneously\n",
            "Predicted Summary>>> introduce method train models provable robustness wrt norms geq simultaneously \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose first methods exactly optimizing softmax distribution using stochastic gradient runtime independent number classes datapoints\n",
            "Predicted Summary>>> propose first methods exactly optimizing softmax distribution using stochastic gradient runtime independent number number datapoints \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose transint novel interpretable kg embedding method isomorphically preserves implication ordering among relations embedding space explainable robust geometrically coherent way\n",
            "Predicted Summary>>> propose transint novel interpretable kg embedding method isomorphically preserves implication ordering among relations embedding space explainable robust geometrically coherent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> approach overcomes scalability issues implies novel mathematical connections among quantum many body physics quantum information theory machine learning\n",
            "Predicted Summary>>> approach overcomes scalability issues implies novel mathematical connections among quantum many body physics quantum information theory machine learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> defense gan uses generative adversarial network defend white box black box attacks classification models\n",
            "Predicted Summary>>> defense gan uses generative adversarial network defend white box black box attacks classification models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning imitate expert absence optimal actions learning dynamics model exploring environment\n",
            "Predicted Summary>>> learning imitate expert absence optimal actions learning dynamics model exploring environment \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use vaes learn shared latent space embedding image features attributes thereby achieve state art results generalized zero shot learning\n",
            "Predicted Summary>>> use vaes learn shared latent space embedding image features attributes thereby achieve state art results generalized zero shot learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> systematically analyze convergence behaviour popular gradient algorithms solving bilinear games simultaneous alternating updates\n",
            "Predicted Summary>>> systematically analyze convergence behaviour popular gradient algorithms solving bilinear games simultaneous alternating updates \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> asal pool based active learning method generates high entropy samples retrieves matching samples pool sub linear time\n",
            "Predicted Summary>>> asal pool based active learning method generates high entropy samples retrieves matching samples pool sub linear time \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> entropy regularized policy optimization formalism subsumes set sequence prediction learning algorithms new interpolation algorithm improved results text generation game imitation learning\n",
            "Predicted Summary>>> entropy regularized policy optimization formalism subsumes set sequence prediction learning algorithms new interpolation algorithm improved results text generation game imitation learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose tendency rl efficiently solve goal oriented tasks large state space using automated curriculum learning discriminative shaping reward potential tackle robot manipulation tasks perception\n",
            "Predicted Summary>>> propose tendency rl efficiently solve goal oriented tasks large state space using automated curriculum learning discriminative shaping reward shaping shaping shaping robot shaping reward tasks tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> find irrationality expert demonstrator help learner infer preferences\n",
            "Predicted Summary>>> find irrationality expert demonstrator help learner infer preferences \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce variational intrinsic successor features visr novel algorithm learns controllable features leveraged provide fast task inference successor features framework\n",
            "Predicted Summary>>> introduce variational intrinsic successor features visr novel algorithm learns controllable features leveraged provide fast task inference features framework framework \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel gram gauss newton method train neural networks inspired neural tangent kernel gauss newton method fast convergence speed theoretically experimentally\n",
            "Predicted Summary>>> novel gram gauss newton method train neural networks inspired neural tangent kernel gauss newton method fast convergence speed theoretically experimentally \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> empirical investigation gan based alignment word vector spaces focusing cases linear transformations provably exist training unstable\n",
            "Predicted Summary>>> empirical investigation gan based alignment word vector spaces focusing cases linear transformations provably exist training unstable \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper provides multi stream end end approach learn unified embeddings query response pairs dialogue systems leveraging contextual syntactic semantic external information together\n",
            "Predicted Summary>>> paper provides multi stream end end approach learn unified embeddings query response pairs dialogue systems leveraging semantic semantic external information external \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> augmented bootstrapping approach combining information reference set iterative refinements soft labels improve name entity recognition biomedical literature\n",
            "Predicted Summary>>> augmented bootstrapping approach combining information reference set iterative refinements soft labels improve name entity recognition biomedical literature \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> training convnets mixed image size improve results across multiple sizes evaluation\n",
            "Predicted Summary>>> training convnets mixed image size improve results across multiple sizes evaluation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> first text adversarial defense method word level improved generic based attack method synonyms substitution based attacks\n",
            "Predicted Summary>>> first text adversarial defense method word level improved generic based attack method synonyms substitution based attacks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> flow based models non invertible also learn discrete variables\n",
            "Predicted Summary>>> flow based models non invertible also learn discrete variables \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel simple operator chopout neural networks trained even single training process truncated sub networks perform well possible\n",
            "Predicted Summary>>> present novel simple operator chopout neural networks trained even single training process truncated sub networks perform well possible \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> approximate inference algorithm deep learning\n",
            "Predicted Summary>>> approximate inference algorithm deep learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce dataset models training evaluation protocols collaborative drawing task allows studying goal driven perceptually actionably grounded language generation understanding\n",
            "Predicted Summary>>> introduce dataset models training evaluation protocols collaborative drawing task allows studying goal driven perceptually actionably grounded language generation understanding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce stochastic training method training binary neural network binary weights activations\n",
            "Predicted Summary>>> introduce stochastic training method training binary neural network binary weights activations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> graph neural network assisted monte carlo tree search approach traveling salesman problem\n",
            "Predicted Summary>>> graph neural network assisted monte carlo tree search approach traveling salesman problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show deep networks sensitive task irrelevant changes input also invariant wide range task relevant changes thus making vast regions input space vulnerable adversarial attacks\n",
            "Predicted Summary>>> show deep networks sensitive task irrelevant changes input also invariant wide range task relevant changes thus making vast regions vast input regions adversarial attacks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show drawing multiple samples predictions per input datapoint learn less data freely obtain reinforce baseline\n",
            "Predicted Summary>>> show drawing multiple samples predictions per input datapoint learn less data freely obtain reinforce baseline \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> asymptotic convergence stochastic subgradien method momentum general parallel asynchronous computation general nonconvex nonsmooth optimization\n",
            "Predicted Summary>>> asymptotic convergence stochastic subgradien method momentum general parallel asynchronous computation general nonconvex nonsmooth optimization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> using compressing tecniques encoding words possibility faster training cnn dimensionality reduction representation\n",
            "Predicted Summary>>> using compressing tecniques encoding words possibility faster training cnn dimensionality reduction representation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep model based rl works well\n",
            "Predicted Summary>>> deep model based rl works well \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> automatic learning data augmentation using gan based architecture improve image classifier\n",
            "Predicted Summary>>> automatic learning data augmentation using gan based architecture improve image classifier \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning sample via lower bounding acceptance rate metropolis hastings algorithm\n",
            "Predicted Summary>>> learning sample via lower bounding acceptance rate metropolis hastings algorithm \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extendable modular architecture proposed developing variety agent behaviors dqn\n",
            "Predicted Summary>>> extendable modular architecture proposed developing variety agent behaviors dqn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce mist rnns exhibit superior vanishing gradient properties comparison lstm improve performance substantially lstm clockwork rnns tasks requiring long term dependencies much efficient previously proposed narx rnns even fewer parameters operations lstm\n",
            "Predicted Summary>>> introduce mist rnns exhibit superior vanishing gradient properties comparison lstm improve performance substantially lstm clockwork rnns tasks requiring long term efficient much efficient previously even \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extracting finite state machine recurrent neural network via quantization purpose interpretability experiments atari\n",
            "Predicted Summary>>> extracting finite state machine recurrent neural network via quantization purpose interpretability experiments atari \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> variation network generative model able learn high level attributes without supervision used controlled input manipulation\n",
            "Predicted Summary>>> variation network generative model able learn high level attributes without supervision used controlled input manipulation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes new output layer deep networks permits use logged contextual bandit feedback training\n",
            "Predicted Summary>>> paper proposes new output layer deep networks permits use logged contextual bandit feedback training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> state art complex text sql parsing combining hard soft relational reasoning schema question encoding\n",
            "Predicted Summary>>> state art complex text sql parsing combining hard soft relational reasoning schema question encoding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use simple search algorithm involving rnn priority queue find solutions coding tasks\n",
            "Predicted Summary>>> use simple search algorithm involving rnn priority queue find solutions coding tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel end end model spnet incorporate semantic scaffolds improving abstractive dialog summarization\n",
            "Predicted Summary>>> propose novel end end model spnet incorporate semantic scaffolds improving abstractive dialog summarization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neumann networks end end sample efficient learning approach solving linear inverse problems imaging compatible mse optimal approach admit extension patch based learning\n",
            "Predicted Summary>>> neumann networks end end sample efficient learning approach solving linear inverse problems imaging compatible mse optimal approach admit admit extension learning learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> manifold structured latent space generative models\n",
            "Predicted Summary>>> manifold structured latent space generative models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose learn generalized policy natural language grounded navigation tasks via environment agnostic multitask learning\n",
            "Predicted Summary>>> propose learn generalized policy natural language grounded navigation tasks via environment agnostic multitask learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improving label efficiency multi task learning auditory data\n",
            "Predicted Summary>>> improving label efficiency multi task learning auditory data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show relatively simple black box adversarial attack scheme using bayesian optimization dimension upsampling preferable existing methods number available queries low\n",
            "Predicted Summary>>> show relatively simple black box adversarial attack scheme using bayesian optimization dimension upsampling preferable existing methods number number number low \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pruning vae proposed search disentangled variables intrinsic dimension\n",
            "Predicted Summary>>> pruning vae proposed search disentangled variables intrinsic dimension \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gans successful adversarial training use convnets show convnet generator trained simple reconstruction loss learnable noise vectors leads many desirable properties gan\n",
            "Predicted Summary>>> gans successful adversarial training use convnets show convnet generator trained simple reconstruction loss learnable vectors vectors vectors many properties gan many properties gan \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning preferences plan traces using active learning\n",
            "Predicted Summary>>> learning preferences plan traces using active learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper improves existing sample based evaluation gans contains insightful experiments\n",
            "Predicted Summary>>> paper improves existing sample based evaluation gans contains insightful experiments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present scalable approximation wide range ebm objectives applications implicit vaes waes\n",
            "Predicted Summary>>> present scalable approximation wide range ebm objectives applications implicit vaes waes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> solve gradient vanishing exploding problems proprose efficient parametrization transition matrix rnn loses expressive power converges faster good generalization\n",
            "Predicted Summary>>> solve gradient vanishing exploding problems proprose efficient parametrization transition matrix rnn loses expressive power converges faster good generalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> address sample inefficiency reward bias adversarial imitation learning algorithms gail airl\n",
            "Predicted Summary>>> address sample inefficiency reward bias adversarial imitation learning algorithms gail airl \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> give algorithm learning two layer neural network symmetric input distribution\n",
            "Predicted Summary>>> give algorithm learning two layer neural network symmetric input distribution \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose discrete wasserstein gan dwgan model based dual formulation wasserstein distance two discrete distributions\n",
            "Predicted Summary>>> propose discrete wasserstein gan dwgan model based dual formulation wasserstein distance two discrete distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> build knowledgeable conversational agents conditioning wikipedia new supervised task\n",
            "Predicted Summary>>> build knowledgeable conversational agents conditioning wikipedia new supervised task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning limited training data exploiting helpful instances rich data source\n",
            "Predicted Summary>>> learning limited training data exploiting helpful instances rich data source \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> operations gan latent space induce distribution mismatch compared training distribution address using optimal transport match distributions\n",
            "Predicted Summary>>> operations gan latent space induce distribution mismatch compared training distribution address using optimal transport match distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> without requiring constraints post processing show salient dimensions word vectors interpreted semantic features\n",
            "Predicted Summary>>> without requiring constraints post processing show salient dimensions word vectors interpreted semantic features \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn optimization algorithm generalizes unseen tasks\n",
            "Predicted Summary>>> learn optimization algorithm generalizes unseen tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improved pretraining analysing encoder output attention\n",
            "Predicted Summary>>> improved pretraining analysing encoder output attention \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present local ensembles method detecting extrapolation trained models approximates variance ensemble using local second order information\n",
            "Predicted Summary>>> present local ensembles method detecting extrapolation trained models approximates variance ensemble using local second order information \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> scalable general purpose factorization algorithm also helps circumvent cold start problem\n",
            "Predicted Summary>>> scalable general purpose factorization algorithm also helps circumvent cold start problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> handling uncertainty visual perception plan recognition\n",
            "Predicted Summary>>> handling uncertainty visual perception plan recognition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dcem learns latent domains optimization problems helps bridge gap model based model free rl create differentiable controller fine tune parts ppo\n",
            "Predicted Summary>>> dcem learns latent domains optimization problems helps bridge gap model based model free rl create differentiable controller fine tune parts ppo \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper address problem low shot network expansion learning\n",
            "Predicted Summary>>> paper address problem low shot network expansion learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze tasks best learned together one network best learn separately\n",
            "Predicted Summary>>> analyze tasks best learned together one network best learn separately \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show autoregressive flows used improve sequential latent variable models\n",
            "Predicted Summary>>> show autoregressive flows used improve sequential latent variable models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> automatically score essays sparse data comparing new essays known samples referee network\n",
            "Predicted Summary>>> automatically score essays sparse data comparing new essays known samples referee network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present new weight encoding scheme enables high compression ratio fast sparse dense matrix conversion\n",
            "Predicted Summary>>> present new weight encoding scheme enables high compression ratio fast sparse dense matrix conversion \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> lstms learn long range dependencies compositionally building shorter constituents course training\n",
            "Predicted Summary>>> lstms learn long range dependencies compositionally building shorter constituents course training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use self supervision domain align unsupervised domain adaptation\n",
            "Predicted Summary>>> use self supervision domain align unsupervised domain adaptation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simple text augmentation techniques significantly boost performance text classification tasks especially small datasets\n",
            "Predicted Summary>>> simple text augmentation techniques significantly boost performance text classification tasks especially small datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> perform depth investigation suitability self attention models character level neural machine translation\n",
            "Predicted Summary>>> perform depth investigation suitability self attention models character level neural machine translation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> disentangled representation learning\n",
            "Predicted Summary>>> disentangled representation learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes advanced policy optimization method hindsight experience sparse reward reinforcement learning\n",
            "Predicted Summary>>> paper proposes advanced policy optimization method hindsight experience sparse reward reinforcement learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analysis deep convolutional networks terms associated arrangement hyperplanes\n",
            "Predicted Summary>>> analysis deep convolutional networks terms associated arrangement hyperplanes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neural model predicting multi aspect sentiments generating probabilistic multi dimensional mask simultaneously model outperforms strong baselines generates masks strong feature predictors meaningful interpretable\n",
            "Predicted Summary>>> neural model predicting multi aspect sentiments generating probabilistic multi dimensional mask simultaneously model outperforms strong baselines generates strong strong strong feature predictors predictors predictors interpretable \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose neural cascades simple trivially parallelizable approach reading comprehension consisting feed forward nets attention achieves state art performance triviaqa dataset\n",
            "Predicted Summary>>> propose neural cascades simple trivially parallelizable approach reading comprehension consisting feed forward nets attention achieves state art performance triviaqa \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn permute set encode permuted set rnn obtain set representation\n",
            "Predicted Summary>>> learn permute set encode permuted set rnn obtain set representation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate properties recently introduced deep image prior ulyanov et al\n",
            "Predicted Summary>>> investigate properties recently introduced deep image prior ulyanov et al \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> address trade caused dependency classes domains improving domain adversarial nets\n",
            "Predicted Summary>>> address trade caused dependency classes domains improving domain adversarial nets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pointer network architecture ranking items learned click logs\n",
            "Predicted Summary>>> pointer network architecture ranking items learned click logs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> nonlinear unsupervised metric learning framework boost performance clustering algorithms\n",
            "Predicted Summary>>> nonlinear unsupervised metric learning framework boost performance clustering algorithms \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> rearranging terms maximum mean discrepancy yields much better loss function discriminator generative adversarial nets\n",
            "Predicted Summary>>> rearranging terms maximum mean discrepancy yields much better loss function discriminator generative adversarial nets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduction new optimization method application deep learning\n",
            "Predicted Summary>>> introduction new optimization method application deep learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> describe neuro ai interface technique evaluate generative adversarial networks\n",
            "Predicted Summary>>> describe neuro ai interface technique evaluate generative adversarial networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper uses principles field calibration machine learning logits neural network defend adversarial attacks\n",
            "Predicted Summary>>> paper uses principles field calibration machine learning logits neural network defend adversarial attacks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> using compressing tecniques encoding words possibility faster training cnn dimensionality reduction representation\n",
            "Predicted Summary>>> using compressing tecniques encoding words possibility faster training cnn dimensionality reduction representation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> causally correct partial models generate whole observation remain causally correct stochastic environments\n",
            "Predicted Summary>>> causally correct partial models generate whole observation remain causally correct stochastic environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> slowmo improves optimization generalization performance communication efficient decentralized algorithms without sacrificing speed\n",
            "Predicted Summary>>> slowmo improves optimization generalization performance communication efficient decentralized algorithms without sacrificing speed \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel bit format eliminates need loss scaling stochastic rounding low precision techniques\n",
            "Predicted Summary>>> propose novel bit format eliminates need loss scaling stochastic rounding low precision techniques \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method based adversarial training strategy learn discriminative features unbiased invariant confounder incorporating loss function encourages vanished correlation bias learned features\n",
            "Predicted Summary>>> propose method based adversarial training strategy learn discriminative features unbiased invariant confounder incorporating loss function encourages vanished correlation correlation learned features \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> theoretical framework deep relu network explains multiple puzzling phenomena like parameterization implicit regularization lottery tickets etc\n",
            "Predicted Summary>>> theoretical framework deep relu network explains multiple puzzling phenomena like parameterization implicit regularization lottery tickets etc \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose interpretable model detecting user chosen wakewords learns user examples\n",
            "Predicted Summary>>> propose interpretable model detecting user chosen wakewords learns user examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop vaes encoder takes model parameter vector input rapid inference many models\n",
            "Predicted Summary>>> develop vaes encoder takes model parameter vector input rapid inference many models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce statistical approach assessing neural network robustness provides informative notion robust network rather conventional binary assertion whether property violated\n",
            "Predicted Summary>>> introduce statistical approach assessing neural network robustness provides informative notion robust network rather conventional binary assertion whether property violated \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> understand transferability perspectives improved generalization optimization feasibility transferability\n",
            "Predicted Summary>>> understand transferability perspectives improved generalization optimization feasibility transferability \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> hierarchical generative model hybrid vae gan learns disentangled representation data without compromising generative quality\n",
            "Predicted Summary>>> hierarchical generative model hybrid vae gan learns disentangled representation data without compromising generative quality \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn convert hand drawn sketch high level program\n",
            "Predicted Summary>>> learn convert hand drawn sketch high level program \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> graph neural network assisted monte carlo tree search approach traveling salesman problem\n",
            "Predicted Summary>>> graph neural network assisted monte carlo tree search approach traveling salesman problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> beyond worst case analysis representational power relu nets polynomial kernels particular presence sparse latent structure\n",
            "Predicted Summary>>> beyond worst case analysis representational power relu nets polynomial kernels particular presence sparse latent structure \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study representations phonology neural network models spoken language several variants analytical techniques\n",
            "Predicted Summary>>> study representations phonology neural network models spoken language several variants analytical techniques \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop approach parcellate hidden layer dnn functionally related groups applying spectral coclustering attribution scores hidden neurons\n",
            "Predicted Summary>>> develop approach parcellate hidden layer dnn functionally related groups applying spectral coclustering attribution scores hidden neurons \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> exploring learnability learned neural networks\n",
            "Predicted Summary>>> exploring learnability learned neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose use lattices represent objects prove fundamental result train networks use\n",
            "Predicted Summary>>> propose use lattices represent objects prove fundamental result train networks use \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> explainable reinforcement learning model using novel combination mixture experts non differentiable decision tree experts\n",
            "Predicted Summary>>> explainable reinforcement learning model using novel combination mixture experts non differentiable decision tree experts \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop general framework establish certified robustness ml models various classes adversarial perturbations\n",
            "Predicted Summary>>> develop general framework establish certified robustness ml models various classes adversarial perturbations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> distributed latent space based knowledge sharing framework deep multi task learning\n",
            "Predicted Summary>>> distributed latent space based knowledge sharing framework deep multi task learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup\n",
            "Predicted Summary>>> present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> reframe generation problem one editing existing points result extrapolate better traditional gans\n",
            "Predicted Summary>>> reframe generation problem one editing existing points result extrapolate better traditional gans \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> derive nesterov method arises straightforward discretization ode different one su boyd candes prove acceleration stochastic case\n",
            "Predicted Summary>>> derive nesterov method arises straightforward discretization ode different one su boyd candes prove acceleration stochastic case \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dimensionality reduction algorithm visualise text network information example email corpus co authorships\n",
            "Predicted Summary>>> dimensionality reduction algorithm visualise text network information example email corpus co authorships \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose agile framework training agents perform instructions examples respective goal states\n",
            "Predicted Summary>>> propose agile framework training agents perform instructions examples respective goal states \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new objective measurement evaluating explanations based notion adversarial robustness evaluation criteria allows us derive new explanations capture pertinent features qualitatively quantitatively\n",
            "Predicted Summary>>> propose new objective measurement evaluating explanations based notion adversarial robustness evaluation criteria allows us derive new explanations capture pertinent features \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> residual ebm text whose formulation equivalent discriminating human machine generated text study generalization behavior\n",
            "Predicted Summary>>> residual ebm text whose formulation equivalent discriminating human machine generated text study generalization behavior \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combine hard handcrafted constraints deep prior weak constraint perform seismic imaging reap information posterior distribution leveraging multiplicity data\n",
            "Predicted Summary>>> combine hard handcrafted constraints deep prior weak constraint perform seismic imaging reap information posterior distribution leveraging multiplicity data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose top modulation network multi task learning applications several advantages current schemes\n",
            "Predicted Summary>>> propose top modulation network multi task learning applications several advantages current schemes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> approximation primate ventral stream convolutional network performs poorly object recognition multiple architectural features contribute\n",
            "Predicted Summary>>> approximation primate ventral stream convolutional network performs poorly object recognition multiple architectural features \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning hgns nd domains\n",
            "Predicted Summary>>> learning hgns nd domains \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study problem continuous control agents deep rl adversarial attacks proposed two step algorithm based learned model dynamics\n",
            "Predicted Summary>>> study problem continuous control agents deep rl adversarial attacks proposed two step algorithm based learned model dynamics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> multi generator gan framework additional network learn prior input noise\n",
            "Predicted Summary>>> multi generator gan framework additional network learn prior input noise \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate implicit syntactic knowledge sentence embeddings using new analysis set grammatically annotated sentences acceptability judgments\n",
            "Predicted Summary>>> investigate implicit syntactic knowledge sentence embeddings using new analysis set grammatically annotated sentences acceptability judgments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper analyses tremendous representational power networks especially skip connections may used method better generalization\n",
            "Predicted Summary>>> paper analyses tremendous representational power networks especially skip connections may used method better generalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> causally correct partial models generate whole observation remain causally correct stochastic environments\n",
            "Predicted Summary>>> causally correct partial models generate whole observation remain causally correct stochastic environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> mathematically analyze effect batch normalization simple model obtain key new insights applies general supervised learning\n",
            "Predicted Summary>>> mathematically analyze effect batch normalization simple model obtain key new insights applies general supervised learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose regularizer improves interpolation autoencoders show also improves learned representation downstream tasks\n",
            "Predicted Summary>>> propose regularizer improves interpolation autoencoders show also improves learned representation downstream tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present simple modification alternating sgd method called prediction step improves stability adversarial networks\n",
            "Predicted Summary>>> present simple modification alternating sgd method called prediction step improves stability adversarial networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present long time scale musical audio style transfer algorithm synthesizes audio time domain uses time frequency representations audio\n",
            "Predicted Summary>>> present long time scale musical audio style transfer algorithm synthesizes audio time domain uses time frequency representations audio \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fully parallelizable adversarial noise resistant metric learning algorithm theoretical guarantees\n",
            "Predicted Summary>>> fully parallelizable adversarial noise resistant metric learning algorithm theoretical guarantees \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> omniglot miniimagenet simple shot learning solve without using labels meta evaluation demonstrated method called centroid networks\n",
            "Predicted Summary>>> omniglot miniimagenet simple shot learning solve without using labels meta evaluation demonstrated method called centroid networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new efficient algorithm construct adversarial examples means deformations rather additive perturbations\n",
            "Predicted Summary>>> propose new efficient algorithm construct adversarial examples means deformations rather additive perturbations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study implicit bias gradient descent prove minimal set assumptions parameter direction homogeneous models converges kkt points natural margin maximization problem\n",
            "Predicted Summary>>> study implicit bias gradient descent prove minimal set assumptions parameter direction models models converges kkt points natural margin maximization problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose extension batch normalization show first kind convergence analysis extension show numerical experiments better performance original batch normalizatin\n",
            "Predicted Summary>>> propose extension batch normalization show first kind convergence analysis extension show numerical experiments better performance original batch \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> representing programs graphs including semantics helps generating programs\n",
            "Predicted Summary>>> representing programs graphs including semantics helps generating programs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> disentangled representation learning\n",
            "Predicted Summary>>> disentangled representation learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop efficient methods train neural embedding models dot product structure reformulating objective function terms generalized gram matrices maintaining estimates matrices\n",
            "Predicted Summary>>> develop efficient methods train neural embedding models dot product structure reformulating objective function terms generalized gram matrices matrices estimates \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> spoken term detection using structured prediction deep networks implementing new loss function maximizes auc ranks according predefined threshold\n",
            "Predicted Summary>>> spoken term detection using structured prediction deep networks implementing new loss function maximizes auc ranks according predefined threshold \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> explain bias situation mmd gans mmd gans work smaller critic networks wgan gps new gan evaluation metric\n",
            "Predicted Summary>>> explain bias situation mmd gans mmd gans work smaller critic networks wgan gps new gan evaluation metric \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extend deep sets functional embeddings neural processes include translation equivariant members\n",
            "Predicted Summary>>> extend deep sets functional embeddings neural processes include translation equivariant members \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> ensuring models learned federated fashion reveal client participation\n",
            "Predicted Summary>>> ensuring models learned federated fashion reveal client participation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> trellis networks new sequence modeling architecture bridges recurrent convolutional models sets new state art word character level language modeling\n",
            "Predicted Summary>>> trellis networks new sequence modeling architecture bridges recurrent convolutional models sets new state art word character level language modeling \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> models generate singing voices without lyrics scores take accompaniment input output singing voices\n",
            "Predicted Summary>>> models generate singing voices without lyrics scores take accompaniment input output singing voices \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> internal consistency constraints improve agents ability develop emergent protocols generalize across communicative roles\n",
            "Predicted Summary>>> internal consistency constraints improve agents ability develop emergent protocols generalize across communicative roles \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> modern deep cnns invariant translations scalings realistic image transformations lack invariance related subsampling operation biases contained image datasets\n",
            "Predicted Summary>>> modern deep cnns invariant translations scalings realistic image transformations lack invariance related operation operation contained image datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improve training stability semi supervised generative adversarial networks collaborative training\n",
            "Predicted Summary>>> improve training stability semi supervised generative adversarial networks collaborative training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose pocketflow automated framework model compression acceleration facilitate deep learning models deployment mobile devices\n",
            "Predicted Summary>>> propose pocketflow automated framework model compression acceleration facilitate deep learning models deployment mobile devices \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> attempt model drawing process fonts building sequential generative models vector graphics svgs highly structured representation font characters\n",
            "Predicted Summary>>> attempt model drawing process fonts building sequential generative models vector graphics svgs highly structured representation font font font font \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present state space lstm models combination state space models lstms propose inference algorithm based sequential monte carlo\n",
            "Predicted Summary>>> present state space lstm models combination state space models lstms propose inference algorithm based sequential monte carlo \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present simple modification alternating sgd method called prediction step improves stability adversarial networks\n",
            "Predicted Summary>>> present simple modification alternating sgd method called prediction step improves stability adversarial networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> demonstrated state art training results using bit floating point representation across resnet gnmt transformer\n",
            "Predicted Summary>>> demonstrated state art training results using bit floating point representation across resnet gnmt transformer \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gaussian normalization performs least squares fit back propagation zero centers decorrelates partial derivatives normalized activations\n",
            "Predicted Summary>>> gaussian normalization performs least squares fit back propagation zero centers decorrelates partial derivatives normalized activations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduced novel simple efficient data augmentation method boosts performances existing gans training data limited diverse\n",
            "Predicted Summary>>> introduced novel simple efficient data augmentation method boosts performances existing gans training data limited diverse \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> rederive wide class inference procedures global information bottleneck objective\n",
            "Predicted Summary>>> rederive wide class inference procedures global information bottleneck objective \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper tackles fault tolerance random adversarial stoppages\n",
            "Predicted Summary>>> paper tackles fault tolerance random adversarial stoppages \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> points problems loss function used irgan recently proposed gan framework information retrieval model motivated co training proposed achieves better performance\n",
            "Predicted Summary>>> points problems loss function used irgan recently proposed gan framework information retrieval model motivated motivated co training proposed performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel method handle image degradations different levels learning diffusion terminal time model generalize unseen degradation level different noise statistic\n",
            "Predicted Summary>>> propose novel method handle image degradations different levels learning diffusion terminal time model generalize unseen degradation level different statistic \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper explore internal dense yet external sparse network structure deep neural networks analyze key properties\n",
            "Predicted Summary>>> paper explore internal dense yet external sparse network structure deep neural networks analyze key properties \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> differentiable multi hop access textual knowledge base indexed contextual representations\n",
            "Predicted Summary>>> differentiable multi hop access textual knowledge base indexed contextual representations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> alternative gradient penalty\n",
            "Predicted Summary>>> alternative gradient penalty \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> cast gans variational inequality framework import techniques literature optimize gans better give algorithmic extensions empirically test performance training gans\n",
            "Predicted Summary>>> cast gans variational inequality framework import techniques literature optimize gans better give algorithmic extensions empirically test performance training gans \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose fidelity weighted learning semi supervised teacher student approach training neural networks using weakly labeled data\n",
            "Predicted Summary>>> propose fidelity weighted learning semi supervised teacher student approach training neural networks using weakly labeled data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> accelerate distributed optimization exploiting stragglers\n",
            "Predicted Summary>>> accelerate distributed optimization exploiting stragglers \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> graph generative models based generalization message passing continuous time using ordinary differential equations\n",
            "Predicted Summary>>> graph generative models based generalization message passing continuous time using ordinary differential equations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposing novel method based guided attention enforce sparisty deep neural networks\n",
            "Predicted Summary>>> proposing novel method based guided attention enforce sparisty deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combining ideas traditional algorithms design reinforcement learning introduce novel framework learning algorithms solve online combinatorial optimization problems\n",
            "Predicted Summary>>> combining ideas traditional algorithms design reinforcement learning introduce novel framework learning algorithms solve online combinatorial optimization problems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes novel actor critic method uses hessians critic update actor\n",
            "Predicted Summary>>> paper proposes novel actor critic method uses hessians critic update actor \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> reactor combines multiple algorithmic architectural contributions produce agent higher sample efficiency prioritized dueling dqn giving better run time performance\n",
            "Predicted Summary>>> reactor combines multiple algorithmic architectural contributions produce agent higher sample efficiency prioritized dueling dqn giving better run time performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> found adversarial training speeds gan training also increases image quality\n",
            "Predicted Summary>>> found adversarial training speeds gan training also increases image quality \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel black box targeted attack able fool state art speech text transcription\n",
            "Predicted Summary>>> present novel black box targeted attack able fool state art speech text transcription \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide estimator estimation algorithm class multi task regression problem provide statistical computational analysis\n",
            "Predicted Summary>>> provide estimator estimation algorithm class multi task regression problem provide statistical computational analysis \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> parametric manifold learning neural networks geometric framework\n",
            "Predicted Summary>>> parametric manifold learning neural networks geometric framework \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new neural architecture top dense layers standard convolutional architectures replaced approximation kernel function relying nystr approximation\n",
            "Predicted Summary>>> new neural architecture top dense layers standard convolutional architectures replaced approximation kernel function relying nystr approximation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce information theoretic viewpoint behavior deep networks optimization processes generalization abilities\n",
            "Predicted Summary>>> introduce information theoretic viewpoint behavior deep networks optimization processes generalization abilities \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extend classical label propation methods jointly model graph feature information graph filtering perspective show connections graph convlutional networks\n",
            "Predicted Summary>>> extend classical label propation methods jointly model graph feature information graph filtering perspective show connections graph convlutional networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> reinforcment practices machine translation performance gains might come better predictions\n",
            "Predicted Summary>>> reinforcment practices machine translation performance gains might come better predictions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn conditional autoregressive flow propose perturbations induce simulator failure improving inference performance\n",
            "Predicted Summary>>> learn conditional autoregressive flow propose perturbations induce simulator failure improving inference performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> compute saliency using strong generative model efficiently marginalize plausible alternative inputs revealing concentrated pixel areas preserve label information\n",
            "Predicted Summary>>> compute saliency using strong generative model efficiently marginalize plausible alternative inputs revealing concentrated pixel areas preserve label information \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show relatively simple black box adversarial attack scheme using bayesian optimization dimension upsampling preferable existing methods number available queries low\n",
            "Predicted Summary>>> show relatively simple black box adversarial attack scheme using bayesian optimization dimension upsampling preferable existing methods number number number low \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> wasserstein autoencoder hyperbolic latent space\n",
            "Predicted Summary>>> wasserstein autoencoder hyperbolic latent space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> agent trained curiosity extrinsic reward surprisingly well popular environments including suite atari games mario etc\n",
            "Predicted Summary>>> agent trained curiosity extrinsic reward surprisingly well popular environments including suite atari games mario etc \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> representing melodies images semantic units aligned generate using dcgan without recurrent components\n",
            "Predicted Summary>>> representing melodies images semantic units aligned generate using dcgan without recurrent components \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neural networks trained modify connectivity improving online learning performance challenging tasks\n",
            "Predicted Summary>>> neural networks trained modify connectivity improving online learning performance challenging tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce continuous logic network cln novel neural architecture automatically learning loop invariants general smt formulas\n",
            "Predicted Summary>>> introduce continuous logic network cln novel neural architecture automatically learning loop invariants general smt formulas \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper hypothesize superficially perturbed data points merely map class map representation\n",
            "Predicted Summary>>> paper hypothesize superficially perturbed data points merely map class map representation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel aritificial checkerboard enhancer ace module guides attacks pre specified pixel space successfully defends simple padding operation\n",
            "Predicted Summary>>> propose novel aritificial checkerboard enhancer ace module guides attacks pre specified pixel space successfully defends simple padding operation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> first posing solving sample efficiency optimization problem non parameterized policy space solving supervised regression problem find parameterized policy near optimal non parameterized policy\n",
            "Predicted Summary>>> first posing solving sample efficiency optimization problem non parameterized policy space solving supervised regression regression find parameterized policy non parameterized non optimal policy optimal \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use machine learning generate synonyms large shopping taxonomies\n",
            "Predicted Summary>>> use machine learning generate synonyms large shopping taxonomies \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> structured latent variable approach adds discrete control states within standard autoregressive neural paradigm provide arbitrary grounding internal model decisions without sacrificing representational power neural models\n",
            "Predicted Summary>>> structured latent variable approach adds discrete control states within standard autoregressive neural paradigm provide arbitrary internal model internal model internal without \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> prove first ever convergence proof asynchronous accelerated algorithm attains speedup\n",
            "Predicted Summary>>> prove first ever convergence proof asynchronous accelerated algorithm attains speedup \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose end end model building process universally applicable wide variety authorship verification corpora outperforms state art little modification fine tuning\n",
            "Predicted Summary>>> propose end end model building process universally applicable wide variety authorship verification corpora outperforms state art little modification fine tuning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel matrix completion based algorithm model disease progression events\n",
            "Predicted Summary>>> novel matrix completion based algorithm model disease progression events \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> focusing final predictions anytime predictors recent multi scale densenets make small anytime models outperform large ones focus\n",
            "Predicted Summary>>> focusing final predictions anytime predictors recent multi scale densenets make small anytime models outperform large ones focus \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel training scheme efficiently obtaining order aware sentence representations\n",
            "Predicted Summary>>> present novel training scheme efficiently obtaining order aware sentence representations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> breaking layer hierarchy propose step approach construction neuron hierarchy networks outperform nas smash hierarchical representation fewer parameters shorter searching time\n",
            "Predicted Summary>>> breaking layer hierarchy propose step approach construction neuron hierarchy networks outperform nas smash hierarchical representation fewer parameters shorter searching time \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce isrlu activation function continuously differentiable faster elu related isru replaces tanh sigmoid\n",
            "Predicted Summary>>> introduce isrlu activation function continuously differentiable faster elu related isru replaces tanh sigmoid \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> recovery guarantee stochastic gradient descent random initialization learning two layer neural network two hidden nodes unit norm weights relu activation functions gaussian inputs\n",
            "Predicted Summary>>> recovery guarantee stochastic gradient descent random initialization learning two layer neural network two hidden nodes unit norm weights relu activation functions functions gaussian inputs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> sensory deficits early training phases lead irreversible performance loss artificial neuronal networks suggesting information phenomena common cause point importance initial transient forgetting\n",
            "Predicted Summary>>> sensory deficits early training phases lead irreversible performance loss artificial neuronal networks suggesting information phenomena common cause point importance initial forgetting \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present new weight encoding scheme enables high compression ratio fast sparse dense matrix conversion\n",
            "Predicted Summary>>> present new weight encoding scheme enables high compression ratio fast sparse dense matrix conversion \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning based algorithms improve upon performance classical algorithms low rank approximation problem retaining worst case guarantee\n",
            "Predicted Summary>>> learning based algorithms improve upon performance classical algorithms low rank approximation problem retaining worst case guarantee \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel end end model spnet incorporate semantic scaffolds improving abstractive dialog summarization\n",
            "Predicted Summary>>> propose novel end end model spnet incorporate semantic scaffolds improving abstractive dialog summarization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study implicit bias gradient methods solving binary classification problem nonlinear relu models\n",
            "Predicted Summary>>> study implicit bias gradient methods solving binary classification problem nonlinear relu models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use non negative rank relu activation matrices complexity measure show negatively correlates good generalization\n",
            "Predicted Summary>>> use non negative rank relu activation matrices complexity measure show negatively correlates good generalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> embed nodes graph gaussian distributions allowing us capture uncertainty representation\n",
            "Predicted Summary>>> embed nodes graph gaussian distributions allowing us capture uncertainty representation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present multi task benchmark analysis platform evaluating generalization natural language understanding systems\n",
            "Predicted Summary>>> present multi task benchmark analysis platform evaluating generalization natural language understanding systems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> empirical investigation gan based alignment word vector spaces focusing cases linear transformations provably exist training unstable\n",
            "Predicted Summary>>> empirical investigation gan based alignment word vector spaces focusing cases linear transformations provably exist training unstable \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> toy dataset based critical percolation planar graph provides analytical window training dynamics deep neural networks\n",
            "Predicted Summary>>> toy dataset based critical percolation planar graph provides analytical window training dynamics deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improved fast weight network shows better results general toy task\n",
            "Predicted Summary>>> improved fast weight network shows better results general toy task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> generative adversarial network training continual learning problem\n",
            "Predicted Summary>>> generative adversarial network training continual learning problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> elastic infogan modification infogan learns without supervision disentangled representations class imbalanced data\n",
            "Predicted Summary>>> elastic infogan modification infogan learns without supervision disentangled representations class imbalanced data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improved likelihood estimates variational autoencoders using self supervised feature learning\n",
            "Predicted Summary>>> improved likelihood estimates variational autoencoders using self supervised feature learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring roughly computational cost\n",
            "Predicted Summary>>> study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring roughly cost \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel algorithm incremental learning vae fixed architecture\n",
            "Predicted Summary>>> novel algorithm incremental learning vae fixed architecture \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose simple randomization technique improving generalization deep reinforcement learning across tasks various unseen visual patterns\n",
            "Predicted Summary>>> propose simple randomization technique improving generalization deep reinforcement learning across tasks various unseen visual patterns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel optimization objective encourages fairness heterogeneous federated networks develop scalable method solve\n",
            "Predicted Summary>>> propose novel optimization objective encourages fairness heterogeneous federated networks develop scalable method solve \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> accelerate distributed optimization exploiting stragglers\n",
            "Predicted Summary>>> accelerate distributed optimization exploiting stragglers \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop theoretical foundations expressive power gnns design provably powerful gnn\n",
            "Predicted Summary>>> develop theoretical foundations expressive power gnns design provably powerful gnn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel meta learning model adaptively balances effect meta learning task specific learning also class specific learning within task\n",
            "Predicted Summary>>> novel meta learning model adaptively balances effect meta learning task specific learning also class specific learning within task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> semi supervised cross lingual document classification\n",
            "Predicted Summary>>> semi supervised cross lingual document classification \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gan based method learn important topological features arbitrary input graph\n",
            "Predicted Summary>>> gan based method learn important topological features arbitrary input graph \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose structure generator gan consider objects relations explicitly generate images means composition\n",
            "Predicted Summary>>> propose structure generator gan consider objects relations explicitly generate images means composition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> method learning image representations good disentangling factors variation obtaining faithful reconstructions\n",
            "Predicted Summary>>> method learning image representations good disentangling factors variation obtaining faithful reconstructions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel state space time series model capability capture structure change points anomaly points better forecasting performance exist change points anomalies time series\n",
            "Predicted Summary>>> propose novel state space time series model capability capture structure change points anomaly points better forecasting performance exist change points anomalies time series series \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work study problem learning representations identify novel objects exploring objects using tactile sensing key point query provided image domain\n",
            "Predicted Summary>>> work study problem learning representations identify novel objects exploring objects using tactile sensing key point query provided image domain \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fast iterative algorithm balance energy network staying functional equivalence class\n",
            "Predicted Summary>>> fast iterative algorithm balance energy network staying functional equivalence class \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel architecture traverses image pyramid top fashion visits informative regions along way\n",
            "Predicted Summary>>> propose novel architecture traverses image pyramid top fashion visits informative regions along way \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use deep rl learn policy directs search genetic algorithm better optimize execution cost computation graphs show improved results real world tensorflow graphs\n",
            "Predicted Summary>>> use deep rl learn policy directs search genetic algorithm better optimize execution cost computation graphs show improved real world graphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised spectral clustering using deep neural networks\n",
            "Predicted Summary>>> unsupervised spectral clustering using deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> first posing solving sample efficiency optimization problem non parameterized policy space solving supervised regression problem find parameterized policy near optimal non parameterized policy\n",
            "Predicted Summary>>> first posing solving sample efficiency optimization problem non parameterized policy space solving supervised regression regression find parameterized policy non parameterized policy optimal \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improve reconstruction time quality experimental mask based lensless imager using end end learning approach incorporates knowledge imaging model\n",
            "Predicted Summary>>> improve reconstruction time quality experimental mask based lensless imager using end end learning approach incorporates knowledge imaging model imaging \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show transformer architecture neural gpu turing complete\n",
            "Predicted Summary>>> show transformer architecture neural gpu turing complete \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> approach improving prediction accuracy learning deep features neighboring scene images satellite scene image analysis\n",
            "Predicted Summary>>> approach improving prediction accuracy learning deep features neighboring scene images satellite scene image analysis \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new anytime neural network allows partial evaluation subnetworks different widths well depths\n",
            "Predicted Summary>>> propose new anytime neural network allows partial evaluation subnetworks different widths well depths \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use empirical tools mode connectivity svcca investigate neural network training heuristics learning rate restarts warmup knowledge distillation\n",
            "Predicted Summary>>> use empirical tools mode connectivity svcca investigate neural network training heuristics learning rate restarts warmup knowledge distillation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> loopgan extends cycle length cyclegan enable unaligned sequential transformation two time steps\n",
            "Predicted Summary>>> loopgan extends cycle length cyclegan enable unaligned sequential transformation two time steps \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show adding constraint td updates stabilizes learning allows deep learning without target network\n",
            "Predicted Summary>>> show adding constraint td updates stabilizes learning allows deep learning without target network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate theoretical practical evidence policy reinforcement learning improvement reusing data several consecutive policies\n",
            "Predicted Summary>>> investigate theoretical practical evidence policy reinforcement learning improvement reusing data several consecutive policies \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> method incorporates wgan achieve occupancy measure matching transition learning\n",
            "Predicted Summary>>> method incorporates wgan achieve occupancy measure matching transition learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel simple operator chopout neural networks trained even single training process truncated sub networks perform well possible\n",
            "Predicted Summary>>> present novel simple operator chopout neural networks trained even single training process truncated sub networks perform well possible \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improved gan based pixel inpainting network compressed seismic image recovery andproposed xa non uniform sampling survey recommendatio easily applied medical domains compressive sensing technique\n",
            "Predicted Summary>>> improved gan based pixel inpainting network compressed seismic image recovery andproposed xa non uniform sampling survey recommendatio easily applied medical medical domains technique \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> distilling single task models multi task model improves natural language understanding performance\n",
            "Predicted Summary>>> distilling single task models multi task model improves natural language understanding performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel iterative algorithm based generalized low rank models computing interpreting word embedding models\n",
            "Predicted Summary>>> present novel iterative algorithm based generalized low rank models computing interpreting word embedding models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> representing melodies images semantic units aligned generate using dcgan without recurrent components\n",
            "Predicted Summary>>> representing melodies images semantic units aligned generate using dcgan without recurrent components \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> plan syntactic structural translation using codes\n",
            "Predicted Summary>>> plan syntactic structural translation using codes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> end end trainable model compression method optimizing accuracy jointly expected model size\n",
            "Predicted Summary>>> end end trainable model compression method optimizing accuracy jointly expected model size \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed bayesian meta sampling method adapting model uncertainty meta learning\n",
            "Predicted Summary>>> proposed bayesian meta sampling method adapting model uncertainty meta learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new method unsupervised representation learning graphs relying maximizing mutual information local global representations graph state art results competitive supervised learning\n",
            "Predicted Summary>>> new method unsupervised representation learning graphs relying maximizing mutual information local global representations graph state art results competitive supervised learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new drl policy algorithm achieving state art performance\n",
            "Predicted Summary>>> propose new drl policy algorithm achieving state art performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose weakly supervised learning method classification localization cancers extremely high resolution histopathology whole slide images using image wide labels\n",
            "Predicted Summary>>> propose weakly supervised learning method classification localization cancers extremely high resolution histopathology whole slide images using image wide labels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> obtain state art robustness data shifts maintain calibration data shift even though even accuracy drops\n",
            "Predicted Summary>>> obtain state art robustness data shifts maintain calibration data shift even though even accuracy drops \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep net deep neural network cyber security use cases\n",
            "Predicted Summary>>> deep net deep neural network cyber security use cases \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes novel complex masking method speech enhancement along loss function efficient phase estimation\n",
            "Predicted Summary>>> paper proposes novel complex masking method speech enhancement along loss function efficient phase estimation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose bayesian hypernetworks framework approximate bayesian inference neural networks\n",
            "Predicted Summary>>> propose bayesian hypernetworks framework approximate bayesian inference neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce stochastic training method training binary neural network binary weights activations\n",
            "Predicted Summary>>> introduce stochastic training method training binary neural network binary weights activations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> try design train classifier whose adversarial robustness resemblance robustness human\n",
            "Predicted Summary>>> try design train classifier whose adversarial robustness resemblance robustness human \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new method training deep hashing image retrieval using relational distance metric samples\n",
            "Predicted Summary>>> propose new method training deep hashing image retrieval using relational distance metric samples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces new dynamic feature representation approach provide efficient way inference deep neural networks\n",
            "Predicted Summary>>> paper introduces new dynamic feature representation approach provide efficient way inference deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> smooth regularization sample graph unpaired image image translation results significantly improved consistency\n",
            "Predicted Summary>>> smooth regularization sample graph unpaired image image translation results significantly improved consistency \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent\n",
            "Predicted Summary>>> new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new order decoder neural machine translation\n",
            "Predicted Summary>>> new order decoder neural machine translation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel structured class blind pruning technique produce highly compressed neural networks\n",
            "Predicted Summary>>> propose novel structured class blind pruning technique produce highly compressed neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce statistical approach assessing neural network robustness provides informative notion robust network rather conventional binary assertion whether property violated\n",
            "Predicted Summary>>> introduce statistical approach assessing neural network robustness provides informative notion robust network rather conventional binary assertion whether property violated \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> graph neural network assisted monte carlo tree search approach traveling salesman problem\n",
            "Predicted Summary>>> graph neural network assisted monte carlo tree search approach traveling salesman problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> apply copula transformation deep information bottleneck leads restored invariance properties disentangled latent space superior predictive capabilities\n",
            "Predicted Summary>>> apply copula transformation deep information bottleneck leads restored invariance properties disentangled latent space superior predictive capabilities \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper explores using wearable sensory augmenting technology facilitate first hand perspective taking like cat like whiskers\n",
            "Predicted Summary>>> paper explores using wearable sensory augmenting technology facilitate first hand perspective taking like cat like whiskers \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed new task datasets baselines conv cyclegan preserves object properties across frames batch structure frame level methods matters\n",
            "Predicted Summary>>> proposed new task datasets baselines conv cyclegan preserves object properties across frames batch structure frame level methods matters \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> convolutional neural networks behave compositional nearest neighbors\n",
            "Predicted Summary>>> convolutional neural networks behave compositional nearest neighbors \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> flow based models non invertible also learn discrete variables\n",
            "Predicted Summary>>> flow based models non invertible also learn discrete variables \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> describe two end end autoencoding parsers semi supervised graph based dependency parsing\n",
            "Predicted Summary>>> describe two end end autoencoding parsers semi supervised graph based dependency parsing \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose use lattices represent objects prove fundamental result train networks use\n",
            "Predicted Summary>>> propose use lattices represent objects prove fundamental result train networks use \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel approach using mode connectivity loss landscapes mitigate adversarial effects repair tampered models evaluate adversarial robustness\n",
            "Predicted Summary>>> novel approach using mode connectivity loss landscapes mitigate adversarial effects repair tampered models evaluate adversarial robustness \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> employ linear homomorphic compression schemes represent sufficient statistics conditional random field model coreference allows us scale inference improve speed order magnitude\n",
            "Predicted Summary>>> employ linear homomorphic compression schemes represent sufficient statistics conditional random field model coreference allows us scale inference improve speed \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new recurrent memory architecture track common sense state changes entities simulating causal effects actions\n",
            "Predicted Summary>>> propose new recurrent memory architecture track common sense state changes entities simulating causal effects actions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> using asynchronous gradient updates accelerate dynamic neural network training\n",
            "Predicted Summary>>> using asynchronous gradient updates accelerate dynamic neural network training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper tackles fault tolerance random adversarial stoppages\n",
            "Predicted Summary>>> paper tackles fault tolerance random adversarial stoppages \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze gradient descent deep linear neural networks providing guarantee convergence global optimum linear rate\n",
            "Predicted Summary>>> analyze gradient descent deep linear neural networks providing guarantee convergence global optimum linear rate \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> meta reinforcement learning approach embedding neural network controller applied autonomous driving carla simulator\n",
            "Predicted Summary>>> meta reinforcement learning approach embedding neural network controller applied autonomous driving carla simulator \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new variational hashing based collaborative filtering approach optimized novel self mask variant hamming distance outperforms state art ndcg\n",
            "Predicted Summary>>> propose new variational hashing based collaborative filtering approach optimized novel self mask variant hamming distance outperforms state art ndcg \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose neural hyperlink predictor nhp nhp adapts graph convolutional networks link prediction hypergraphs\n",
            "Predicted Summary>>> propose neural hyperlink predictor nhp nhp adapts graph convolutional networks link prediction hypergraphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> adversarial attacks unsupervised node embeddings based eigenvalue perturbation theory\n",
            "Predicted Summary>>> adversarial attacks unsupervised node embeddings based eigenvalue perturbation theory \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present use secondary encoder decoder loss function help train summarizer\n",
            "Predicted Summary>>> present use secondary encoder decoder loss function help train summarizer \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> routing procedures necessary capsnets\n",
            "Predicted Summary>>> routing procedures necessary capsnets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel method calibrate knowledge graph embedding models without need negative examples\n",
            "Predicted Summary>>> propose novel method calibrate knowledge graph embedding models without need negative examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce analyze phenomenon hallucinations nmt spurious translations unrelated source text propose methods reduce frequency\n",
            "Predicted Summary>>> introduce analyze phenomenon hallucinations nmt spurious translations unrelated source text propose methods reduce frequency \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simple rnn based meta learner achieves sota performance popular benchmarks\n",
            "Predicted Summary>>> simple rnn based meta learner achieves sota performance popular benchmarks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce crescendonet deep cnn architecture stacking simple building blocks without residual connections\n",
            "Predicted Summary>>> introduce crescendonet deep cnn architecture stacking simple building blocks without residual connections \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> ground language commands high dimensional visual environment learning language conditioned rewards using inverse reinforcement learning\n",
            "Predicted Summary>>> ground language commands high dimensional visual environment learning language conditioned rewards using inverse reinforcement learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop new likelihood free parameter estimation method equivalent maximum likelihood conditions\n",
            "Predicted Summary>>> develop new likelihood free parameter estimation method equivalent maximum likelihood conditions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> beyond worst case analysis representational power relu nets polynomial kernels particular presence sparse latent structure\n",
            "Predicted Summary>>> beyond worst case analysis representational power relu nets polynomial kernels particular presence sparse latent structure \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop efficient methods train neural embedding models dot product structure reformulating objective function terms generalized gram matrices maintaining estimates matrices\n",
            "Predicted Summary>>> develop efficient methods train neural embedding models dot product structure reformulating objective function terms generalized gram matrices estimates matrices \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extension gans combining optimal transport primal form energy distance defined adversarially learned feature space\n",
            "Predicted Summary>>> extension gans combining optimal transport primal form energy distance defined adversarially learned feature space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> address active learning batch setting noisy oracles use model uncertainty encode decision quality active learning algorithm acquisition\n",
            "Predicted Summary>>> address active learning batch setting noisy oracles use model uncertainty encode decision quality active learning algorithm acquisition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> soft quantization approach learn pure fixed point representations deep neural networks\n",
            "Predicted Summary>>> soft quantization approach learn pure fixed point representations deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze tasks best learned together one network best learn separately\n",
            "Predicted Summary>>> analyze tasks best learned together one network best learn separately \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> data driven learning algorithm based unrolling alternating minimization optimization sparse graph recovery\n",
            "Predicted Summary>>> data driven learning algorithm based unrolling alternating minimization optimization sparse graph recovery \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed method end end neural svm optimized shot learning\n",
            "Predicted Summary>>> proposed method end end neural svm optimized shot learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce new pointwise convolution layers equipped extremely fast conventional transforms deep neural network\n",
            "Predicted Summary>>> introduce new pointwise convolution layers equipped extremely fast conventional transforms deep neural network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> scalable general purpose factorization algorithm also helps circumvent cold start problem\n",
            "Predicted Summary>>> scalable general purpose factorization algorithm also helps circumvent cold start problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose differentiable architecture search algorithm convolutional recurrent networks achieving competitive performance state art using orders magnitude less computation resources\n",
            "Predicted Summary>>> propose differentiable architecture search algorithm convolutional recurrent networks achieving competitive performance state art using orders magnitude less computation resources \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> decaying learning rate increasing batch size training equivalent\n",
            "Predicted Summary>>> decaying learning rate increasing batch size training equivalent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> give method generating type safe programs java like language given small amount syntactic information desired code\n",
            "Predicted Summary>>> give method generating type safe programs java like language given small amount syntactic information desired code \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze develop computationally efficient implementation jacobian regularization increases classification margins neural networks\n",
            "Predicted Summary>>> analyze develop computationally efficient implementation jacobian regularization increases classification margins neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> rectification deep neural networks naturally leads favor invariant representation\n",
            "Predicted Summary>>> rectification deep neural networks naturally leads favor invariant representation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce searnn novel algorithm rnn training inspired learning search approach structured prediction order avoid limitations mle training\n",
            "Predicted Summary>>> introduce searnn novel algorithm rnn training inspired learning search approach structured prediction order avoid limitations mle training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> devise novel depthwise separable graph convolution dsgc generic spatial domain data highly compatible depthwise separable convolution\n",
            "Predicted Summary>>> devise novel depthwise separable graph convolution dsgc generic spatial domain data highly compatible depthwise separable convolution \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> enhance existing transformation based defenses using distribution classifier distribution softmax obtained transformed images\n",
            "Predicted Summary>>> enhance existing transformation based defenses using distribution classifier distribution softmax obtained transformed images \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> finding shed lights preventing cancer progression\n",
            "Predicted Summary>>> finding shed lights preventing cancer progression \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combine matching network framework shot learning large scale multi label model genomic sequence classification\n",
            "Predicted Summary>>> combine matching network framework shot learning large scale multi label model genomic sequence classification \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose reinforcement learning based variable swapping recomputation algorithm reduce memory cost\n",
            "Predicted Summary>>> propose reinforcement learning based variable swapping recomputation algorithm reduce memory cost \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce new type conditional gan aims leverage structure target space generator augment generator new unsupervised pathway learn target structure\n",
            "Predicted Summary>>> introduce new type conditional gan aims leverage structure target space generator augment generator new new pathway learn target structure \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> automatically score essays sparse data comparing new essays known samples referee network\n",
            "Predicted Summary>>> automatically score essays sparse data comparing new essays known samples referee network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> caml instance maml conditional class dependencies\n",
            "Predicted Summary>>> caml instance maml conditional class dependencies \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use theory compressed sensing prove lstms least well linear text classification bag grams\n",
            "Predicted Summary>>> use theory compressed sensing prove lstms least well linear text classification bag grams \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> batch normalization reduces robustness test time common corruptions adversarial examples\n",
            "Predicted Summary>>> batch normalization reduces robustness test time common corruptions adversarial examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present new deep latent model natural images trained unlabeled datasets utilized solve various image restoration tasks\n",
            "Predicted Summary>>> present new deep latent model natural images trained unlabeled datasets utilized solve various image restoration tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> generic dynamic architecture employs problem specific differentiable forking mechanism encode hard data structure assumptions applied clevr vqa expression evaluation\n",
            "Predicted Summary>>> generic dynamic architecture employs problem specific differentiable forking mechanism encode hard data structure assumptions applied clevr vqa expression evaluation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show make predictions using deep networks without training deep networks\n",
            "Predicted Summary>>> show make predictions using deep networks without training deep networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> algorithm unifying sgd adam empirical study performance\n",
            "Predicted Summary>>> algorithm unifying sgd adam empirical study performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> argue convolutional networks considered default starting point sequence modeling tasks\n",
            "Predicted Summary>>> argue convolutional networks considered default starting point sequence modeling tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use siamese networks guide disentangle generation process gans without labeled data\n",
            "Predicted Summary>>> use siamese networks guide disentangle generation process gans without labeled data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extracting finite state machine recurrent neural network via quantization purpose interpretability experiments atari\n",
            "Predicted Summary>>> extracting finite state machine recurrent neural network via quantization purpose interpretability experiments atari \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed bayesian meta sampling method adapting model uncertainty meta learning\n",
            "Predicted Summary>>> proposed bayesian meta sampling method adapting model uncertainty meta learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> compare many tasks task combinations pretraining sentence level bilstms nlp tasks language modeling best single pretraining task simple baselines also well\n",
            "Predicted Summary>>> compare many tasks task combinations pretraining sentence level bilstms nlp tasks language modeling best single task baselines baselines baselines also \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose model based method called search amortized value estimates save leverages real planned experience combining learning monte carlo tree search achieving strong performance small search budgets\n",
            "Predicted Summary>>> propose model based method called search amortized value estimates save leverages real planned experience combining learning monte carlo tree search achieving strong performance search small search budgets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep rl agent learns hyperbolic non exponential values new multi horizon auxiliary task\n",
            "Predicted Summary>>> deep rl agent learns hyperbolic non exponential values new multi horizon auxiliary task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep innovation protection allows evolving complex world models end end tasks\n",
            "Predicted Summary>>> deep innovation protection allows evolving complex world models end end tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce new gradient detach based complementary objective training strategy domain adaptive object detection\n",
            "Predicted Summary>>> introduce new gradient detach based complementary objective training strategy domain adaptive object detection \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn permute set encode permuted set rnn obtain set representation\n",
            "Predicted Summary>>> learn permute set encode permuted set rnn obtain set representation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce reclor reading comprehension dataset requiring logical reasoning find current state art models struggle real logical reasoning poor performance near random guess\n",
            "Predicted Summary>>> introduce reclor reading comprehension dataset requiring logical reasoning find current state art models struggle real logical reasoning poor performance near random \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new method uses statistical leverage score information measure importance data samples every task adopts frequent directions approach enable life long learning property\n",
            "Predicted Summary>>> new method uses statistical leverage score information measure importance data samples every task adopts frequent directions approach enable life long long learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> distance kernel embedding via random features structured inputs\n",
            "Predicted Summary>>> distance kernel embedding via random features structured inputs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show special goal condition value function trained model free methods used within model based control resulting substantially better sample efficiency performance\n",
            "Predicted Summary>>> show special goal condition value function trained model free methods used within model based control resulting substantially better sample efficiency performance performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introducing response charactrization method interpreting cell dynamics learned long short term memory lstm networks\n",
            "Predicted Summary>>> introducing response charactrization method interpreting cell dynamics learned long short term memory lstm networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces algorithm handle optimization problem multiple constraints vision manifold\n",
            "Predicted Summary>>> paper introduces algorithm handle optimization problem multiple constraints vision manifold \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> method learning better representations acts regularizer despite significant additional computation cost achieves improvements strong baselines supervised semi supervised learning tasks\n",
            "Predicted Summary>>> method learning better representations acts regularizer despite significant additional computation cost achieves improvements strong baselines supervised supervised tasks learning tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show special goal condition value function trained model free methods used within model based control resulting substantially better sample efficiency performance\n",
            "Predicted Summary>>> show special goal condition value function trained model free methods used within model based control resulting substantially better sample efficiency performance performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose transint novel interpretable kg embedding method isomorphically preserves implication ordering among relations embedding space explainable robust geometrically coherent way\n",
            "Predicted Summary>>> propose transint novel interpretable kg embedding method isomorphically preserves implication ordering among relations embedding space explainable robust geometrically coherent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel method handle image degradations different levels learning diffusion terminal time model generalize unseen degradation level different noise statistic\n",
            "Predicted Summary>>> propose novel method handle image degradations different levels learning diffusion terminal time model generalize unseen degradation level different statistic \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> see abstract revision paper identical except page supplementary material serve stand along technical report version paper\n",
            "Predicted Summary>>> see abstract revision paper identical except page supplementary material serve stand along technical report version paper \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> inadequacy disentanglement metrics\n",
            "Predicted Summary>>> inadequacy disentanglement metrics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use cultural transmission encourage compositionality languages emerge interactions neural agents\n",
            "Predicted Summary>>> use cultural transmission encourage compositionality languages emerge interactions neural agents \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neural tangent kernel randomly initialized relu net non trivial fluctuations long depth width comparable\n",
            "Predicted Summary>>> neural tangent kernel randomly initialized relu net non trivial fluctuations long depth width comparable \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present leaf modular benchmarking framework learning federated data applications learning paradigms federated learning meta learning multi task learning\n",
            "Predicted Summary>>> present leaf modular benchmarking framework learning federated data applications learning paradigms federated learning meta learning multi task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces neuromodulation artificial neural networks\n",
            "Predicted Summary>>> paper introduces neuromodulation artificial neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present new deep latent model natural images trained unlabeled datasets utilized solve various image restoration tasks\n",
            "Predicted Summary>>> present new deep latent model natural images trained unlabeled datasets utilized solve various image restoration tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show age confounds cognitive impairment detection solve fair representation learning propose metrics models\n",
            "Predicted Summary>>> show age confounds cognitive impairment detection solve fair representation learning propose metrics models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> ensemble method reinforcement learning weights functions based accumulated td errors\n",
            "Predicted Summary>>> ensemble method reinforcement learning weights functions based accumulated td errors \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper aims provide empirical answer question whether well trained dialogue response model output malicious responses\n",
            "Predicted Summary>>> paper aims provide empirical answer question whether well trained dialogue response model output malicious responses \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> interactively generating image incrementally growing scene graphs multiple steps using gans preserving contents image generated previous steps\n",
            "Predicted Summary>>> interactively generating image incrementally growing scene graphs multiple steps using gans preserving contents image generated previous steps \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> print input sentence current response sentence onto image use fine tuned imagenet cnn model predict next response word\n",
            "Predicted Summary>>> print input sentence current response sentence onto image use fine tuned imagenet cnn model predict next response word \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel dynamic ridesharing framework form trips optimizes operational value service provider user value passengers factoring users social preferences decision making process\n",
            "Predicted Summary>>> propose novel dynamic ridesharing framework form trips optimizes operational value service provider user value passengers factoring users social preferences decision making process \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper presents methods disentangle interpret contextual effects encoded deep neural network\n",
            "Predicted Summary>>> paper presents methods disentangle interpret contextual effects encoded deep neural network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method enables cnn folding create recurrent connections\n",
            "Predicted Summary>>> propose method enables cnn folding create recurrent connections \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate properties recently introduced deep image prior ulyanov et al\n",
            "Predicted Summary>>> investigate properties recently introduced deep image prior ulyanov et al \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> train natural media painting agent using environment model based painting agent present novel approach train constrained painting agent follows command encoded observation\n",
            "Predicted Summary>>> train natural media painting agent using environment model based painting agent agent present novel approach constrained constrained agent follows \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> directional message passing incorporates spatial directional information improve graph neural networks\n",
            "Predicted Summary>>> directional message passing incorporates spatial directional information improve graph neural \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present encoder decoder framework language style transfer allows use non parallel data source data various unknown language styles\n",
            "Predicted Summary>>> present encoder decoder framework language style transfer allows use non parallel data source data various unknown language styles \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extend quantum svms semi supervised setting deal likely problem many missing class labels huge datasets\n",
            "Predicted Summary>>> extend quantum svms semi supervised setting deal likely problem many missing class labels huge datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent\n",
            "Predicted Summary>>> new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce machine learning model uses domain independent features estimate criticality current state cause known undesirable state\n",
            "Predicted Summary>>> introduce machine learning model uses domain independent features estimate criticality current state cause known state state state state \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce new pointwise convolution layers equipped extremely fast conventional transforms deep neural network\n",
            "Predicted Summary>>> introduce new pointwise convolution layers equipped extremely fast conventional transforms deep neural network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed method extract leverage interpretations feature interactions\n",
            "Predicted Summary>>> proposed method extract leverage interpretations feature interactions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learned energy based model score matching\n",
            "Predicted Summary>>> learned energy based model score matching \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gradient flow based dynamical system invertible generative modeling\n",
            "Predicted Summary>>> gradient flow based dynamical system invertible generative modeling \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> distilling single task models multi task model improves natural language understanding performance\n",
            "Predicted Summary>>> distilling single task models multi task model improves natural language understanding performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose assessment framework analyze learn graph convolutional filter\n",
            "Predicted Summary>>> propose assessment framework analyze learn graph convolutional filter \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> modular hierarchical approach learn policies exploring environments\n",
            "Predicted Summary>>> modular hierarchical approach learn policies exploring environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work propose sparse transformer improve concentration attention global context explicit selection relevant segments sequence sequence learning\n",
            "Predicted Summary>>> work propose sparse transformer improve concentration attention global context explicit selection relevant segments sequence sequence learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel simple operator chopout neural networks trained even single training process truncated sub networks perform well possible\n",
            "Predicted Summary>>> present novel simple operator chopout neural networks trained even single training process truncated sub networks perform well possible \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new auto encoder based wasserstein distance improves sampling properties vae\n",
            "Predicted Summary>>> propose new auto encoder based wasserstein distance improves sampling properties vae \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> sort encoder undo sorting decoder avoid responsibility problem set auto encoders\n",
            "Predicted Summary>>> sort encoder undo sorting decoder avoid responsibility problem set auto encoders \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> adversarial audio discrimination using temporal dependency\n",
            "Predicted Summary>>> adversarial audio discrimination using temporal dependency \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce capacity exploit information degree arbitrary goal achieved another goal intended policy gradient methods\n",
            "Predicted Summary>>> introduce capacity exploit information degree arbitrary goal achieved another goal intended policy gradient methods \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combine splines neural networks obtain novel distribution functions use model intensity functions point processes\n",
            "Predicted Summary>>> combine splines neural networks obtain novel distribution functions use model intensity functions point processes processes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel architecture shot classification capable dealing uncertainty\n",
            "Predicted Summary>>> novel architecture shot classification capable dealing uncertainty \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pairwise learned capsule network performs well face verification tasks given limited labeled data\n",
            "Predicted Summary>>> pairwise learned capsule network performs well face verification tasks given limited labeled data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present treeqn atreec new architectures deep reinforcement learning discrete action domains integrate differentiable line tree planning action value function policy\n",
            "Predicted Summary>>> present treeqn atreec new architectures deep reinforcement learning discrete action domains integrate differentiable line planning action value function function \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose repeated reference benchmark task regularized continual learning approach adaptive communication humans unfamiliar domains\n",
            "Predicted Summary>>> propose repeated reference benchmark task regularized continual learning approach adaptive communication humans unfamiliar domains \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pose generative models likelihoods excessively influenced input complexity propose way compensate detecting distribution inputs\n",
            "Predicted Summary>>> pose generative models likelihoods excessively influenced input complexity propose way compensate detecting distribution inputs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show ways train hierarchical video prediction model without needing pose labels\n",
            "Predicted Summary>>> show ways train hierarchical video prediction model without needing pose labels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> detecting overlapping communities graphs using graph neural networks\n",
            "Predicted Summary>>> detecting overlapping communities graphs using graph neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> using ensembles pseudo labels unsupervised clustering\n",
            "Predicted Summary>>> using ensembles pseudo labels unsupervised clustering \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> feature map compression method converts quantized activations binary vectors followed nonlinear dimensionality reduction layers embedded dnn\n",
            "Predicted Summary>>> feature map compression method converts quantized activations binary vectors followed nonlinear dimensionality reduction layers embedded \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel cluster based algorithm curriculum learning proposed solve robust training generative models\n",
            "Predicted Summary>>> novel cluster based algorithm curriculum learning proposed solve robust training generative models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper studies interactions fast learning slow prediction models demonstrate interactions improve machine capability solve joint lifelong shot learning problems\n",
            "Predicted Summary>>> paper studies interactions fast learning slow prediction models demonstrate interactions improve machine capability solve joint lifelong learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised structure learning method parsimonious deep feed forward networks\n",
            "Predicted Summary>>> unsupervised structure learning method parsimonious deep feed forward networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new framework using dual space generating images corresponding multiclass labels number class large\n",
            "Predicted Summary>>> new framework using dual space generating images corresponding multiclass labels number class large \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose approach learn decentralized policies multi agent settings using attention based critics demonstrate promising results environments complex interactions\n",
            "Predicted Summary>>> propose approach learn decentralized policies multi agent settings using attention based critics demonstrate promising results environments complex interactions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel two tower shared bottom model architecture transferring knowledge rich implicit feedbacks predict relevance large scale retrieval systems\n",
            "Predicted Summary>>> propose novel two tower shared bottom model architecture transferring knowledge rich implicit feedbacks predict relevance large scale retrieval systems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> detect statistical interactions captured feedforward multilayer neural network directly interpreting learned weights\n",
            "Predicted Summary>>> detect statistical interactions captured feedforward multilayer neural network directly interpreting learned weights \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose use lattices represent objects prove fundamental result train networks use\n",
            "Predicted Summary>>> propose use lattices represent objects prove fundamental result train networks use \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use monte carlo tree search homoglyphs generate indistinguishable adversarial samples text data\n",
            "Predicted Summary>>> use monte carlo tree search homoglyphs generate indistinguishable adversarial samples text data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> noise robust deep learning architecture\n",
            "Predicted Summary>>> noise robust deep learning architecture \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> flow based models non invertible also learn discrete variables\n",
            "Predicted Summary>>> flow based models non invertible also learn discrete variables \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose adversarial inverse reinforcement learning algorithm capable learning reward functions transfer new unseen environments\n",
            "Predicted Summary>>> propose adversarial inverse reinforcement learning algorithm capable learning reward functions transfer new unseen environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> system rewriting text conditioned multiple controllable attributes\n",
            "Predicted Summary>>> system rewriting text conditioned multiple controllable attributes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> embedded architecture deep learning optimized devices face detection emotion recognition\n",
            "Predicted Summary>>> embedded architecture deep learning optimized devices face detection emotion recognition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop agent call distributional deterministic deep policy gradient algorithm achieves state art performance number challenging continuous control problems\n",
            "Predicted Summary>>> develop agent call distributional deterministic deep policy gradient algorithm achieves state art performance number challenging continuous control problems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring roughly computational cost\n",
            "Predicted Summary>>> study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring requiring computational roughly \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work tensor based method preposition representation training\n",
            "Predicted Summary>>> work tensor based method preposition representation training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use simple search algorithm involving rnn priority queue find solutions coding tasks\n",
            "Predicted Summary>>> use simple search algorithm involving rnn priority queue find solutions coding tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> theoretical study multi task learning practical implications improving multi task training transfer learning\n",
            "Predicted Summary>>> theoretical study multi task learning practical implications improving multi task training transfer learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> generative model reaction prediction learns mechanistic electron steps reaction directly raw reaction data\n",
            "Predicted Summary>>> generative model reaction prediction learns mechanistic electron steps reaction directly raw reaction data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use siamese networks guide disentangle generation process gans without labeled data\n",
            "Predicted Summary>>> use siamese networks guide disentangle generation process gans without labeled data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> efficient multi view video summarization scheme advanced activity recognition iot environments\n",
            "Predicted Summary>>> efficient multi view video summarization scheme advanced activity recognition iot environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new framework data dependent dnn regularization prevent dnns overfitting random data random labels\n",
            "Predicted Summary>>> propose new framework data dependent dnn regularization prevent dnns overfitting random data random labels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose enhance deep scattering network order improve control stability given machine learning pipeline proposing continuous wavelet thresholding scheme\n",
            "Predicted Summary>>> propose enhance deep scattering network order improve control stability given machine learning pipeline proposing continuous wavelet thresholding scheme \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised structure learning method parsimonious deep feed forward networks\n",
            "Predicted Summary>>> unsupervised structure learning method parsimonious deep feed forward networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> prove large class functions exists interval certified robust network approximating arbitrary precision\n",
            "Predicted Summary>>> prove large class functions exists interval certified robust network approximating arbitrary precision \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> multi level spectral approach improving quality scalability unsupervised graph embedding\n",
            "Predicted Summary>>> multi level spectral approach improving quality scalability unsupervised graph embedding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> compositional attribute based planning generalizes long test tasks despite trained short simple tasks\n",
            "Predicted Summary>>> compositional attribute based planning generalizes long test tasks despite trained short simple tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novograd adaptive sgd method layer wise gradient normalization decoupled weight decay\n",
            "Predicted Summary>>> novograd adaptive sgd method layer wise gradient normalization decoupled weight decay \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new generative model discrete structured data proposed stochastic lazy attribute converts offline semantic check online guidance stochastic decoding effectively addresses constraints syntax semantics also achieves superior performance\n",
            "Predicted Summary>>> new generative model discrete structured data proposed stochastic lazy attribute converts offline semantic check online guidance stochastic decoding guidance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use deep reinforcement learning design physical attributes robot jointly control policy\n",
            "Predicted Summary>>> use deep reinforcement learning design physical attributes robot jointly control policy \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> disentanglement pytorch library variational representation learning\n",
            "Predicted Summary>>> disentanglement pytorch library variational representation learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combining imitation learning reinforcement learning learn outperform expert\n",
            "Predicted Summary>>> combining imitation learning reinforcement learning learn outperform expert \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> revisit simple idea pruning connections dnns ell regularization achieving state art results multiple datasets theoretic guarantees\n",
            "Predicted Summary>>> revisit simple idea pruning connections dnns ell regularization achieving state art results multiple datasets theoretic guarantees \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce nlprolog system performs rule based reasoning natural language leveraging pretrained sentence embeddings fine tuning evolution strategies apply two multi hop question answering tasks\n",
            "Predicted Summary>>> introduce nlprolog system performs rule based reasoning natural language leveraging pretrained sentence sentence embeddings fine tuning evolution apply two multi hop tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces marginattack stronger faster zero confidence adversarial attack\n",
            "Predicted Summary>>> paper introduces marginattack stronger faster zero confidence adversarial attack \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present use secondary encoder decoder loss function help train summarizer\n",
            "Predicted Summary>>> present use secondary encoder decoder loss function help train summarizer \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose nearest neighbor overlap procedure quantifies similarity embedders task agnostic manner use compare sentence embedders\n",
            "Predicted Summary>>> propose nearest neighbor overlap procedure quantifies similarity embedders task agnostic manner use compare sentence embedders \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new algorithm training neural networks compares favorably popular adaptive methods\n",
            "Predicted Summary>>> new algorithm training neural networks compares favorably popular adaptive methods \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning limited training data exploiting helpful instances rich data source\n",
            "Predicted Summary>>> learning limited training data exploiting helpful instances rich data source \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> solve ill posed inverse problems scarce ground truth examples estimating ensemble random projections model instead model\n",
            "Predicted Summary>>> solve ill posed inverse problems scarce ground truth examples estimating ensemble random projections model instead model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose agile framework training agents perform instructions examples respective goal states\n",
            "Predicted Summary>>> propose agile framework training agents perform instructions examples respective goal states \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduced novel gradient estimator using stein method compared methods learning implicit models approximate inference image generation\n",
            "Predicted Summary>>> introduced novel gradient estimator using stein method compared methods learning implicit models approximate inference image generation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> monte carlo methods quantizing pre trained models without additional training\n",
            "Predicted Summary>>> monte carlo methods quantizing pre trained models without additional training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> identify angle bias causes vanishing gradient problem deep nets propose efficient method reduce bias\n",
            "Predicted Summary>>> identify angle bias causes vanishing gradient problem deep nets propose efficient method reduce bias \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> approximate inference algorithm deep learning\n",
            "Predicted Summary>>> approximate inference algorithm deep learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper analyzes latent space learned model free approaches miniature incomplete information game trains forward model latent space apply monte carlo tree search yielding positive performance\n",
            "Predicted Summary>>> paper analyzes latent space learned model free approaches miniature incomplete information game trains forward model latent space apply monte carlo tree search positive positive positive \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> generate wikipedia articles abstractively conditioned source document text\n",
            "Predicted Summary>>> generate wikipedia articles abstractively conditioned source document text \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes simple procedure evaluating compositional structure learned representations uses procedure explore role compositionality four learning problems\n",
            "Predicted Summary>>> paper proposes simple procedure evaluating compositional structure learned representations uses procedure explore role compositionality four learning problems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper presents gan based framework learning distribution high dimensional incomplete data\n",
            "Predicted Summary>>> paper presents gan based framework learning distribution high dimensional incomplete data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose bayesian hypernetworks framework approximate bayesian inference neural networks\n",
            "Predicted Summary>>> propose bayesian hypernetworks framework approximate bayesian inference neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> accelerate secure dnn inference trusted execution environments factor selectively outsourcing computation linear layers faster yet untrusted co processor\n",
            "Predicted Summary>>> accelerate secure dnn inference trusted execution environments factor selectively outsourcing computation linear layers faster yet untrusted co processor \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new module improves resnet like architectures enforcing channel selective behavior convolutional layers\n",
            "Predicted Summary>>> propose new module improves resnet like architectures enforcing channel selective behavior convolutional layers \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> automatically construct explore small abstract markov decision process enabling us achieve state art results montezuma revenge pitfall private eye significant margin\n",
            "Predicted Summary>>> automatically construct explore small abstract markov decision process enabling us achieve state art results montezuma revenge pitfall private eye significant margin \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show transformer architecture neural gpu turing complete\n",
            "Predicted Summary>>> show transformer architecture neural gpu turing complete \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> without requiring constraints post processing show salient dimensions word vectors interpreted semantic features\n",
            "Predicted Summary>>> without requiring constraints post processing show salient dimensions word vectors interpreted semantic features \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose confidence based graph convolutional network semi supervised learning\n",
            "Predicted Summary>>> propose confidence based graph convolutional network semi supervised learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> mmo inspired research game platform studying emergent behaviors large populations complex environment\n",
            "Predicted Summary>>> mmo inspired research game platform studying emergent behaviors large populations complex environment \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> working toward generative knowledge graph models better estimate predictive uncertainty knowledge inference\n",
            "Predicted Summary>>> working toward generative knowledge graph models better estimate predictive uncertainty knowledge inference \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new method training deep hashing image retrieval using relational distance metric samples\n",
            "Predicted Summary>>> propose new method training deep hashing image retrieval using relational distance metric samples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> leverage deterministic autoencoders generative models proposing mixing functions combine hidden states pairs images mixes made look realistic adversarial framework\n",
            "Predicted Summary>>> leverage deterministic autoencoders generative models proposing mixing functions combine hidden states pairs images mixes made look realistic framework framework \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> give method generating type safe programs java like language given small amount syntactic information desired code\n",
            "Predicted Summary>>> give method generating type safe programs java like language given small amount syntactic information desired code \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed implementation accelerate dnn data parallel training reducing communication bandwidth requirement\n",
            "Predicted Summary>>> proposed implementation accelerate dnn data parallel training reducing communication bandwidth requirement \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed new task datasets baselines conv cyclegan preserves object properties across frames batch structure frame level methods matters\n",
            "Predicted Summary>>> proposed new task datasets baselines conv cyclegan preserves object properties across frames batch structure frame level methods matters \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed rnn based algorithm estimate predictive distribution one multi step forecasts time series prediction problems\n",
            "Predicted Summary>>> proposed rnn based algorithm estimate predictive distribution one multi step forecasts time series prediction problems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use transformer encoder translation training style masked translation model\n",
            "Predicted Summary>>> use transformer encoder translation training style masked translation model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose prob vec method problem embedding used personalized learning tool addition data level classification method called negative pre training cases training data set imbalanced\n",
            "Predicted Summary>>> propose prob vec method problem embedding used personalized learning tool addition data level classification method called negative pre training cases training data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel algorithm solving reinforcement learning bandit structured prediction problems sparse loss feedback\n",
            "Predicted Summary>>> present novel algorithm solving reinforcement learning bandit structured prediction problems sparse loss feedback \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> benchmark method measure compositional generalization maximizing divergence compound frequency small divergence atom frequency\n",
            "Predicted Summary>>> benchmark method measure compositional generalization maximizing divergence compound frequency small divergence atom frequency \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> relaxing constraint shared hierarchies enables effective deep multitask learning\n",
            "Predicted Summary>>> relaxing constraint shared hierarchies enables effective deep multitask learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> prove deep neural networks exponentially efficient shallow ones approximating sparse multivariate polynomials\n",
            "Predicted Summary>>> prove deep neural networks exponentially efficient shallow ones approximating sparse multivariate polynomials \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> define filter level pruning problem binary neural networks first time propose method solve\n",
            "Predicted Summary>>> define filter level pruning problem binary neural networks first time propose method solve \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel hrl framework formulate temporal abstraction problem learning latent representation action sequence\n",
            "Predicted Summary>>> propose novel hrl framework formulate temporal abstraction problem learning latent representation action sequence \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> approximate inference algorithm deep learning\n",
            "Predicted Summary>>> approximate inference algorithm deep learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose agile framework training agents perform instructions examples respective goal states\n",
            "Predicted Summary>>> propose agile framework training agents perform instructions examples respective goal states \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use continuous time dynamics define generative model exact likelihoods efficient sampling parameterized unrestricted neural networks\n",
            "Predicted Summary>>> use continuous time dynamics define generative model exact likelihoods efficient sampling parameterized unrestricted neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> decoding pixels still work representation learning images\n",
            "Predicted Summary>>> decoding pixels still work representation learning images \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> ideas future ickeps\n",
            "Predicted Summary>>> ideas future ickeps \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provably recover span deep multi layered neural network latent structure empirically apply efficient span recovery algorithms attack networks obfuscating inputs\n",
            "Predicted Summary>>> provably recover span deep multi layered neural network latent structure empirically apply efficient span recovery algorithms attack networks obfuscating inputs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> inspired trial trial variability brain result multiple noise sources introduce variability noise knowledge distillation framework studied effect generalization robustness\n",
            "Predicted Summary>>> inspired trial trial variability brain result multiple noise sources introduce variability noise knowledge distillation framework studied effect generalization robustness \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces algorithm handle optimization problem multiple constraints vision manifold\n",
            "Predicted Summary>>> paper introduces algorithm handle optimization problem multiple constraints vision manifold \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> utilized deep reinforcement learning teach agents ride sharing fleet style coordination\n",
            "Predicted Summary>>> utilized deep reinforcement learning teach agents ride sharing fleet style coordination \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> efficient multi view video summarization scheme advanced activity recognition iot environments\n",
            "Predicted Summary>>> efficient multi view video summarization scheme advanced activity recognition iot environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> first field show craft effective sparse kernel design three aspects composition performance efficiency\n",
            "Predicted Summary>>> first field show craft effective sparse kernel design three aspects composition performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce extra data dependent gaussian prior objective augment current mle training designed capture prior knowledge ground truth data\n",
            "Predicted Summary>>> introduce extra data dependent gaussian prior objective augment current mle training designed capture prior knowledge ground truth data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel dynamic ridesharing framework form trips optimizes operational value service provider user value passengers factoring users social preferences decision making process\n",
            "Predicted Summary>>> propose novel dynamic ridesharing framework form trips optimizes operational value service provider user value passengers factoring users social preferences decision making process \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pairwise learned capsule network performs well face verification tasks given limited labeled data\n",
            "Predicted Summary>>> pairwise learned capsule network performs well face verification tasks given limited labeled data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> convergence theory biased consistent gradient estimators stochastic optimization application graph convolutional networks\n",
            "Predicted Summary>>> convergence theory biased consistent gradient estimators stochastic optimization application graph convolutional networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new type end end trainable attention module applies global weight balances among layers utilizing co propagating rnn cnn\n",
            "Predicted Summary>>> propose new type end end trainable attention module applies global weight balances among layers utilizing co rnn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fix classifier neural networks without losing accuracy\n",
            "Predicted Summary>>> fix classifier neural networks without losing accuracy \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> important consider optimization function space parameter space introduce learning rule reduces distance traveled function space like sgd limits distance traveled parameter space\n",
            "Predicted Summary>>> important consider optimization function space parameter space introduce learning rule reduces distance traveled function space like sgd limits distance traveled parameter space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose generative neural network approach temporally coherent point clouds\n",
            "Predicted Summary>>> propose generative neural network approach temporally coherent point clouds \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> approach perform htn planning using external procedures evaluate predicates runtime semantic attachments\n",
            "Predicted Summary>>> approach perform htn planning using external procedures evaluate predicates runtime semantic attachments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose several general debiasing strategies address common biases seen different datasets obtain substantial improved domain performance settings\n",
            "Predicted Summary>>> propose several general debiasing strategies address common biases seen different datasets obtain substantial improved domain performance settings \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> variation network generative model able learn high level attributes without supervision used controlled input manipulation\n",
            "Predicted Summary>>> variation network generative model able learn high level attributes without supervision used controlled input manipulation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new module improves resnet like architectures enforcing channel selective behavior convolutional layers\n",
            "Predicted Summary>>> propose new module improves resnet like architectures enforcing channel selective behavior convolutional layers \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose end end model building process universally applicable wide variety authorship verification corpora outperforms state art little modification fine tuning\n",
            "Predicted Summary>>> propose end end model building process universally applicable wide variety authorship verification corpora outperforms state art little modification fine tuning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new model latently invertible autoencoder proposed solve problem variational inference vae using invertible network two stage adversarial training\n",
            "Predicted Summary>>> new model latently invertible autoencoder proposed solve problem variational inference vae using invertible network two stage adversarial training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposing novel method based guided attention enforce sparisty deep neural networks\n",
            "Predicted Summary>>> proposing novel method based guided attention enforce sparisty deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> systematically examines well explain hidden features deep network terms logical rules\n",
            "Predicted Summary>>> systematically examines well explain hidden features deep network terms logical rules \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> routing procedures necessary capsnets\n",
            "Predicted Summary>>> routing procedures necessary capsnets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper presents improved training mechanism obtaining binary networks smaller accuracy drop helps close gap full precision counterpart\n",
            "Predicted Summary>>> paper presents improved training mechanism obtaining binary networks smaller accuracy drop helps close gap full precision \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> general framework distilling bayesian posterior expectations deep neural networks\n",
            "Predicted Summary>>> general framework distilling bayesian posterior expectations deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> addressing task heterogeneity problem meta learning introducing meta knowledge graph\n",
            "Predicted Summary>>> addressing task heterogeneity problem meta learning introducing meta knowledge graph \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce class player games suited gradient based methods\n",
            "Predicted Summary>>> introduce class player games suited gradient based methods \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide rigorous comparison different graph neural networks graph classification\n",
            "Predicted Summary>>> provide rigorous comparison different graph neural networks graph classification \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> modular method fully cooperative multi goal multi agent reinforcement learning based curriculum learning efficient exploration credit assignment action goal interactions\n",
            "Predicted Summary>>> modular method fully cooperative multi goal multi agent reinforcement learning based curriculum learning efficient exploration credit assignment action goal interactions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning detect objects without image labels minutes video\n",
            "Predicted Summary>>> learning detect objects without image labels minutes video \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> closed form results deep learning layer decoupling limit applicable residual networks\n",
            "Predicted Summary>>> closed form results deep learning layer decoupling limit applicable residual networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improved pretraining analysing encoder output attention\n",
            "Predicted Summary>>> improved pretraining analysing encoder output attention \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose several new attacks methodology measure robustness unforeseen adversarial attacks\n",
            "Predicted Summary>>> propose several new attacks methodology measure robustness unforeseen adversarial attacks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze determine precision requirements training neural networks tensors including back propagated signals weight accumulators quantized fixed point format\n",
            "Predicted Summary>>> analyze determine precision requirements training neural networks tensors including back propagated signals weight accumulators quantized fixed point format \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose scalable method approximate eigenvectors laplacian reinforcement learning context show learned representations improve performance rl agent\n",
            "Predicted Summary>>> propose scalable method approximate eigenvectors laplacian reinforcement learning context show learned representations improve performance rl agent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose macer provable defense algorithm trains robust models maximizing certified radius use adversarial training performs better existing provable defenses\n",
            "Predicted Summary>>> propose macer provable defense algorithm trains robust models maximizing certified radius use adversarial training performs better existing provable defenses \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> initial findings intersection network neuroscience deep learning elegans mouse visual cortex learn recognize handwritten digits\n",
            "Predicted Summary>>> initial findings intersection network neuroscience deep learning elegans mouse visual cortex learn recognize handwritten digits \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> characterize dimensional properties adversarial subspaces neighborhood adversarial examples via use local intrinsic dimensionality lid\n",
            "Predicted Summary>>> characterize dimensional properties adversarial subspaces neighborhood adversarial examples via use local intrinsic dimensionality lid \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new monte carlo tree search rollout algorithm relies width based search construct lookahead\n",
            "Predicted Summary>>> propose new monte carlo tree search rollout algorithm relies width based search construct lookahead \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> reward estimation game videos\n",
            "Predicted Summary>>> reward estimation game videos \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring roughly computational cost\n",
            "Predicted Summary>>> study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring roughly computational roughly \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> predict generalization error specify model attains across model data scales\n",
            "Predicted Summary>>> predict generalization error specify model attains across model data scales \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper provides game based abstraction scheme compute provably sound policies pomdps\n",
            "Predicted Summary>>> paper provides game based abstraction scheme compute provably sound policies pomdps \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study problem learning optimizing physical simulations via differentiable programming using proposed diffsim programming language compiler\n",
            "Predicted Summary>>> study problem learning optimizing physical simulations via differentiable programming using proposed diffsim programming language compiler \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present new algorithm learning curriculum based notion mastering rate outperforms previous algorithms\n",
            "Predicted Summary>>> present new algorithm learning curriculum based notion mastering rate outperforms previous algorithms \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> higher momentum parameter beta helps escaping saddle points faster\n",
            "Predicted Summary>>> higher momentum parameter beta helps escaping saddle points faster \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work aims provide quantitative answers relative importance concepts interest via concept activation vectors cav particular framework enables non machine learning experts express concepts interest test hypotheses using examples set pictures illustrate concept show cav learned given relatively small set examples hypothesis testing cav answer whether particular concept gender important predicting given class doctor sets concepts interpreting networks cav require retraining modification network\n",
            "Predicted Summary>>> work aims provide quantitative answers relative importance concepts interest via concept activation vectors cav framework non non non learning non framework \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new combination evolution strategy deep reinforcement learning takes best worlds\n",
            "Predicted Summary>>> propose new combination evolution strategy deep reinforcement learning takes best worlds \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dimensionality reduction cases examples represented soft probability distributions\n",
            "Predicted Summary>>> dimensionality reduction cases examples represented soft probability distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce novel end end approach learning cluster absence labeled examples define differentiable loss function equivalent expected normalized cuts\n",
            "Predicted Summary>>> introduce novel end end approach learning cluster absence labeled examples define differentiable loss function equivalent expected normalized cuts \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study introduce novel method relies svd discover number latent dimensions\n",
            "Predicted Summary>>> study introduce novel method relies svd discover number latent dimensions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce method train models provable robustness wrt norms geq simultaneously\n",
            "Predicted Summary>>> introduce method train models provable robustness wrt norms geq simultaneously \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> multi level spectral approach improving quality scalability unsupervised graph embedding\n",
            "Predicted Summary>>> multi level spectral approach improving quality scalability unsupervised graph embedding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method enables cnn folding create recurrent connections\n",
            "Predicted Summary>>> propose method enables cnn folding create recurrent connections \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show individual units cnn representations learned nlp tasks selectively responsive natural language concepts\n",
            "Predicted Summary>>> show individual units cnn representations learned nlp tasks selectively responsive natural language concepts \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> inspired information bottleneck theory propose new architecture gan disentangled representation learning\n",
            "Predicted Summary>>> inspired information bottleneck theory propose new architecture gan disentangled representation learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> efficient video classification using frame based conditional gating module selecting dominant frames followed temporal modeling classifier\n",
            "Predicted Summary>>> efficient video classification using frame based conditional gating module selecting dominant frames followed temporal modeling classifier \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> adversarially regularized autoencoders learn smooth representations discrete structures allowing interesting results text generation unaligned style transfer semi supervised learning latent space interpolation arithmetic\n",
            "Predicted Summary>>> adversarially regularized autoencoders learn smooth representations discrete structures allowing interesting results text generation unaligned style transfer semi supervised learning latent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose diversely stale parameters break lockings backpropoagation algorithm train cnn parallel\n",
            "Predicted Summary>>> propose diversely stale parameters break lockings backpropoagation algorithm train cnn parallel \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> ground language commands high dimensional visual environment learning language conditioned rewards using inverse reinforcement learning\n",
            "Predicted Summary>>> ground language commands high dimensional visual environment learning language conditioned rewards using inverse reinforcement learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> attention weights fully expose bert knows syntax\n",
            "Predicted Summary>>> attention weights fully expose bert knows syntax \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> policy gradient backpropagation time using learned models functions sota results reinforcement learning benchmark environments\n",
            "Predicted Summary>>> policy gradient backpropagation time using learned models functions sota results reinforcement learning benchmark environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proves universal approximability quantized relu neural networks puts forward complexity bound given arbitrary error\n",
            "Predicted Summary>>> paper proves universal approximability quantized relu neural networks puts forward complexity bound given arbitrary error \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> representations language models consistently perform better translation encoders syntactic auxiliary prediction tasks\n",
            "Predicted Summary>>> representations language models consistently perform better translation encoders syntactic auxiliary prediction tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use model free algorithms like dqn trpo solve short horizon problems model free iteratively policy value iteration fashion\n",
            "Predicted Summary>>> use model free algorithms like dqn trpo solve short horizon problems model free value iteration policy iteration \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show robust gan priors work better gan priors limited angle ct reconstruction highly determined inverse problem\n",
            "Predicted Summary>>> show robust gan priors work better gan priors limited angle ct reconstruction highly determined inverse problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn temporal point processes modeling conditional density conditional intensity\n",
            "Predicted Summary>>> learn temporal point processes modeling conditional density conditional intensity \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> achieving strong adversarial robustness comparable adversarial training without training adversarial examples\n",
            "Predicted Summary>>> achieving strong adversarial robustness comparable adversarial training without training adversarial examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop efficient multi scale approximate attributed network embedding procedures provable properties\n",
            "Predicted Summary>>> develop efficient multi scale approximate attributed network embedding procedures provable properties \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose imitation learning method learn diverse quality demonstrations collected demonstrators different level expertise\n",
            "Predicted Summary>>> propose imitation learning method learn diverse quality demonstrations collected demonstrators different level expertise \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fast variational approximations approximating user state learning product embeddings\n",
            "Predicted Summary>>> fast variational approximations approximating user state learning product embeddings \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> address task autonomous exploration navigation using spatial affordance maps learned self supervised manner outperform classic geometric baselines sample efficient contemporary rl algorithms\n",
            "Predicted Summary>>> address task autonomous exploration navigation using spatial affordance maps learned self supervised manner outperform classic geometric baselines sample efficient contemporary contemporary \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose simple general space efficient data format accelerate deep learning training allowing sample fidelity dynamically selected training time\n",
            "Predicted Summary>>> propose simple general space efficient data format accelerate deep learning training allowing sample fidelity dynamically selected training time \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neural phrase based machine translation linear decoding time\n",
            "Predicted Summary>>> neural phrase based machine translation linear decoding time \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposed ensemble method called interboost training neural networks small sample classification method better generalization performance ensemble methods reduces variances significantly\n",
            "Predicted Summary>>> paper proposed ensemble method called interboost training neural networks small sample classification method better performance ensemble methods reduces variances variances \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improving deep transfer learning regularization using attention based feature maps\n",
            "Predicted Summary>>> improving deep transfer learning regularization using attention based feature maps \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> personalized propagation neural predictions ppnp improves graph neural networks separating prediction propagation via personalized pagerank\n",
            "Predicted Summary>>> personalized propagation neural predictions ppnp improves graph neural networks separating prediction propagation via personalized pagerank \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new face image dataset balanced race gender age used bias measurement mitigation\n",
            "Predicted Summary>>> new face image dataset balanced race gender age used bias measurement mitigation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze impact latent space fully trained generators pseudo inverting\n",
            "Predicted Summary>>> analyze impact latent space fully trained generators pseudo inverting \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep boosting algorithm developed learn discriminative ensemble classifier seamlessly combining set base deep cnns\n",
            "Predicted Summary>>> deep boosting algorithm developed learn discriminative ensemble classifier seamlessly combining set base deep cnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> marthe new method fit task specific learning rate schedules perspective hyperparameter optimization\n",
            "Predicted Summary>>> marthe new method fit task specific learning rate schedules perspective hyperparameter optimization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel ensemble retrieval based generation based open domain conversation systems\n",
            "Predicted Summary>>> novel ensemble retrieval based generation based open domain conversation systems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> model control generation images gan beta vae regard scale position objects\n",
            "Predicted Summary>>> model control generation images gan beta vae regard scale position objects \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper studied pubn classification problem incorporate biased negative bn data negative data fully representative true underlying negative distribution positive unlabeled pu learning\n",
            "Predicted Summary>>> paper studied pubn classification problem incorporate biased negative bn data negative data fully representative true underlying negative distribution positive pu unlabeled \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> comparative study generative models continual learning scenarios\n",
            "Predicted Summary>>> comparative study generative models continual learning scenarios \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> handling uncertainty visual perception plan recognition\n",
            "Predicted Summary>>> handling uncertainty visual perception plan recognition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate deep representation untrained random weight cnn dcn architectures show image reconstruction quality possible applications\n",
            "Predicted Summary>>> investigate deep representation untrained random weight cnn dcn architectures show image reconstruction quality possible applications \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> aim exploit diversity linguistic structures build sentence representations\n",
            "Predicted Summary>>> aim exploit diversity linguistic structures build sentence representations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> rnns implicitly implement tensor product representations principled interpretable method representing symbolic structures continuous space\n",
            "Predicted Summary>>> rnns implicitly implement tensor product representations principled interpretable method representing symbolic structures continuous space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> benchmarks biologically plausible learning algorithms complex datasets architectures\n",
            "Predicted Summary>>> benchmarks biologically plausible learning algorithms complex datasets architectures \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> demonstrate residual blocks viewed gauss newton steps propose new residual block exploits second order information\n",
            "Predicted Summary>>> demonstrate residual blocks viewed gauss newton steps propose new residual block exploits second order information \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper propose novel regularized adversarial training framework atlpa namely adversarial tolerant logit pairing attention\n",
            "Predicted Summary>>> paper propose novel regularized adversarial training framework atlpa namely adversarial tolerant logit pairing attention \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel multi task framework learns table detection semantic component recognition cell type classification spreadsheet tables promising results\n",
            "Predicted Summary>>> propose novel multi task framework learns table detection semantic component recognition cell type classification spreadsheet tables promising results \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show make predictions using deep networks without training deep networks\n",
            "Predicted Summary>>> show make predictions using deep networks without training deep networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> train neural network agents develop language compositional properties raw pixel input\n",
            "Predicted Summary>>> train neural network agents develop language compositional properties raw pixel \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> jiffy convolutional approach learning distance metric multivariate time series outperforms existing methods terms nearest neighbor classification accuracy\n",
            "Predicted Summary>>> jiffy convolutional approach learning distance metric multivariate time series outperforms existing methods terms nearest neighbor classification accuracy \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide scalable solution multi agent evaluation linear rate complexity time memory terms number agents\n",
            "Predicted Summary>>> provide scalable solution multi agent evaluation linear rate complexity time memory terms number agents \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new form autoencoding model incorporates best properties variational autoencoders vae generative adversarial networks gan\n",
            "Predicted Summary>>> propose new form autoencoding model incorporates best properties variational autoencoders vae generative adversarial networks gan \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> self training different views input gives excellent results semi supervised image recognition sequence tagging dependency parsing\n",
            "Predicted Summary>>> self training different views input gives excellent results semi supervised image recognition sequence tagging dependency parsing \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> biologically plausible learning algorithms particularly sign symmetry work well imagenet\n",
            "Predicted Summary>>> biologically plausible learning algorithms particularly sign symmetry work well imagenet \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> efficiently solve multi task problems automatic curriculum generation algorithm based generative model tracks learning agent performance\n",
            "Predicted Summary>>> efficiently solve multi task problems automatic curriculum generation algorithm based generative model tracks learning agent performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neural sparsity enhanced topic model based vae\n",
            "Predicted Summary>>> neural sparsity enhanced topic model based vae \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce netscore new metric designed provide quantitative assessment balance accuracy computational complexity network architecture complexity deep neural network\n",
            "Predicted Summary>>> introduce netscore new metric designed provide quantitative assessment balance accuracy computational complexity network architecture complexity deep neural network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce novel larger context language model simultaneously captures syntax semantics making capable generating highly interpretable sentences paragraphs\n",
            "Predicted Summary>>> introduce novel larger context language model simultaneously captures syntax semantics making capable generating highly interpretable sentences paragraphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> weakly supervised learning based clustering framework performs comparable fully supervised learning models exploiting unique class count\n",
            "Predicted Summary>>> weakly supervised learning based clustering framework performs comparable fully supervised learning models exploiting unique class count \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised reinforcement learning method learning policy robustly achieve perceptually specified goals\n",
            "Predicted Summary>>> unsupervised reinforcement learning method learning policy robustly achieve perceptually specified goals \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> actor critic reinforcement learning approach multi step returns applied autonomous driving carla simulator\n",
            "Predicted Summary>>> actor critic reinforcement learning approach multi step returns applied autonomous driving carla simulator \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel combination adversarial training provable defenses produces model state art accuracy certified robustness cifar\n",
            "Predicted Summary>>> propose novel combination adversarial training provable defenses produces model state art accuracy certified robustness cifar \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> probing robustness redundancy deep neural networks reveals capacity constraining features help explain non overfitting\n",
            "Predicted Summary>>> probing robustness redundancy deep neural networks reveals capacity constraining features help explain non overfitting \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> mathematically analyze effect batch normalization simple model obtain key new insights applies general supervised learning\n",
            "Predicted Summary>>> mathematically analyze effect batch normalization simple model obtain key new insights applies general supervised learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces cloudlstm new branch recurrent neural models tailored forecasting data streams generated geospatial point cloud sources\n",
            "Predicted Summary>>> paper introduces cloudlstm new branch recurrent neural models tailored forecasting data streams generated geospatial point cloud sources \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose hypothesis gradient descent generalizes based per example gradients interact\n",
            "Predicted Summary>>> propose hypothesis gradient descent generalizes based per example gradients interact \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce isrlu activation function continuously differentiable faster elu related isru replaces tanh sigmoid\n",
            "Predicted Summary>>> introduce isrlu activation function continuously differentiable faster elu related isru replaces tanh sigmoid \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> automated mice training neuroscience online iterative latent strategy inference behavior prediction\n",
            "Predicted Summary>>> automated mice training neuroscience online iterative latent strategy inference behavior prediction \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning search efficient densenet layer wise pruning\n",
            "Predicted Summary>>> learning search efficient densenet layer wise pruning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dynamic lightweight convolutions competitive self attention language tasks\n",
            "Predicted Summary>>> dynamic lightweight convolutions competitive self attention language tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes effective coding scheme neural networks encodes random set weights variational distribution\n",
            "Predicted Summary>>> paper proposes effective coding scheme neural networks encodes random set weights variational distribution \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> self attention layer perform convolution often learns practice\n",
            "Predicted Summary>>> self attention layer perform convolution often learns practice \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dqn ddpg hybrid algorithm proposed deal discrete continuous hybrid action space\n",
            "Predicted Summary>>> dqn ddpg hybrid algorithm proposed deal discrete continuous hybrid action space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose diversely stale parameters break lockings backpropoagation algorithm train cnn parallel\n",
            "Predicted Summary>>> propose diversely stale parameters break lockings backpropoagation algorithm train cnn parallel \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce machine learning model uses domain independent features estimate criticality current state cause known undesirable state\n",
            "Predicted Summary>>> introduce machine learning model uses domain independent features estimate criticality current state cause known state state state state \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel shot learning method generate query specific classification weights via information maximization\n",
            "Predicted Summary>>> novel shot learning method generate query specific classification weights via information maximization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> using linear programming show computational complexity approximate deep neural network training depends polynomially data size several architectures\n",
            "Predicted Summary>>> using linear programming show computational complexity approximate deep neural network training depends polynomially data size several architectures \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> decoding pixels still work representation learning images\n",
            "Predicted Summary>>> decoding pixels still work representation learning images \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> asal pool based active learning method generates high entropy samples retrieves matching samples pool sub linear time\n",
            "Predicted Summary>>> asal pool based active learning method generates high entropy samples retrieves matching samples pool sub linear time \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new auto encoder incorporated multiway delay embedding transform toward interpreting deep image prior\n",
            "Predicted Summary>>> propose new auto encoder incorporated multiway delay embedding transform toward interpreting deep image prior \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show deep learning network derivatives low rank structure structure allows us use second order derivative information calculate learning rates adaptively computationally feasible manner\n",
            "Predicted Summary>>> show deep learning network derivatives low rank structure structure allows us use second order derivative information calculate learning rates adaptively computationally \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> convergence theory biased consistent gradient estimators stochastic optimization application graph convolutional networks\n",
            "Predicted Summary>>> convergence theory biased consistent gradient estimators stochastic optimization application graph convolutional networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> user level differential privacy recurrent neural network language models possible sufficiently large dataset\n",
            "Predicted Summary>>> user level differential privacy recurrent neural network language models possible sufficiently large dataset \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> quantify energy cost terms money cloud credits carbon footprint training recently successful neural network models nlp costs high\n",
            "Predicted Summary>>> quantify energy cost terms money cloud credits carbon footprint training recently successful neural network models nlp costs high \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce autoregressive generative model spectrograms demonstrate applications speech music generation\n",
            "Predicted Summary>>> introduce autoregressive generative model spectrograms demonstrate applications speech music generation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose data augmentation approach meta learning prove valid\n",
            "Predicted Summary>>> propose data augmentation approach meta learning prove valid \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> apply monte carlo tree search episode generation alpha zero\n",
            "Predicted Summary>>> apply monte carlo tree search episode generation alpha zero \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> state art complex text sql parsing combining hard soft relational reasoning schema question encoding\n",
            "Predicted Summary>>> state art complex text sql parsing combining hard soft relational reasoning schema question encoding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> apply greedy assignment projected samples instead sorting approximate wasserstein distance\n",
            "Predicted Summary>>> apply greedy assignment projected samples instead sorting approximate wasserstein distance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce contextual decompositions interpretation algorithm lstms capable extracting word phrase interaction level importance score\n",
            "Predicted Summary>>> introduce contextual decompositions interpretation algorithm lstms capable extracting word phrase interaction level importance score \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present open loop brain machine interface whose performance unconstrained traditionally used bag words approach\n",
            "Predicted Summary>>> present open loop brain machine interface whose performance unconstrained traditionally used bag words approach \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper finds algorithms directly use lossless compressed representations deep feedforward networks perform inference without full decompression\n",
            "Predicted Summary>>> paper finds algorithms directly use lossless compressed representations deep feedforward networks perform inference without full decompression decompression \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> train rnns famous twitter users determine whether general twitter population likely believe climate change natural disaster\n",
            "Predicted Summary>>> train rnns famous twitter users determine whether general twitter population likely believe climate change natural disaster \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose neural cascades simple trivially parallelizable approach reading comprehension consisting feed forward nets attention achieves state art performance triviaqa dataset\n",
            "Predicted Summary>>> propose neural cascades simple trivially parallelizable approach reading comprehension consisting feed forward nets attention achieves state art performance triviaqa \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show multi channel attention weight contains semantic feature solve natural language inference task\n",
            "Predicted Summary>>> show multi channel attention weight contains semantic feature solve natural language inference task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> input structuring along chaos stability\n",
            "Predicted Summary>>> input structuring along chaos stability \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> outputs modern nlp apis nonsensical text provide strong signals model internals allowing adversaries steal apis\n",
            "Predicted Summary>>> outputs modern nlp apis nonsensical text provide strong signals model internals allowing adversaries steal apis \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> sparse optimization algorithm deep cnn models\n",
            "Predicted Summary>>> sparse optimization algorithm deep cnn models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> apply gradient based meta learning graph domain introduce new graph specific transfer function bootstrap process\n",
            "Predicted Summary>>> apply gradient based meta learning graph domain introduce new graph specific transfer function bootstrap process \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose alternative measure determining effectiveness adversarial attacks nlp models according distance measure based method like incremental gain control theory\n",
            "Predicted Summary>>> propose alternative measure determining effectiveness adversarial attacks nlp models according distance measure based method like incremental gain control \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> binarized back propagation need completely binarized training inflate size network\n",
            "Predicted Summary>>> binarized back propagation need completely binarized training inflate size network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose meta learning approach low resource neural machine translation rapidly learn translate new language\n",
            "Predicted Summary>>> propose meta learning approach low resource neural machine translation rapidly learn translate new language \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new model latently invertible autoencoder proposed solve problem variational inference vae using invertible network two stage adversarial training\n",
            "Predicted Summary>>> new model latently invertible autoencoder proposed solve problem variational inference vae using invertible network two stage adversarial training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> understand class labels help gan training propose new evaluation metric generative models\n",
            "Predicted Summary>>> understand class labels help gan training propose new evaluation metric generative models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> demonstrate gated recurrent asynchronous spiking neural network corresponds lstm unit\n",
            "Predicted Summary>>> demonstrate gated recurrent asynchronous spiking neural network corresponds lstm unit \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> residual binary neural networks significantly improve convergence rate inference accuracy binary neural networks\n",
            "Predicted Summary>>> residual binary neural networks significantly improve convergence rate inference accuracy binary neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose nearest neighbor overlap procedure quantifies similarity embedders task agnostic manner use compare sentence embedders\n",
            "Predicted Summary>>> propose nearest neighbor overlap procedure quantifies similarity embedders task agnostic manner use compare sentence embedders \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> features auto generated bio mimetic mothnet model significantly improve test accuracy standard ml methods vectorized mnist mothnet generated features also outperform standard feature generators\n",
            "Predicted Summary>>> features auto generated bio mimetic mothnet model significantly improve test accuracy standard ml methods vectorized mnist mothnet generated features also outperform outperform feature \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neuron agent naaa enable us train multi agent communication without trusted third party\n",
            "Predicted Summary>>> neuron agent naaa enable us train multi agent communication without trusted third party \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> biologically plausible learning rule training recurrent neural networks\n",
            "Predicted Summary>>> biologically plausible learning rule training recurrent neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> attribution sometimes misleading\n",
            "Predicted Summary>>> attribution sometimes misleading \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose message passing encoder decode networks fast accurate way modelling label dependencies multi label classification\n",
            "Predicted Summary>>> propose message passing encoder decode networks fast accurate way modelling label dependencies multi label classification \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show neural encoding models trained capture signal spiking variability neural population data using gans\n",
            "Predicted Summary>>> show neural encoding models trained capture signal spiking variability neural population data using gans \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unify extended kalman filter ekf state space approach power expectation propagation pep solving intractable moment matching integrals pep via linearisation leads globally iterated extension ekf\n",
            "Predicted Summary>>> unify extended kalman filter ekf state space approach power expectation propagation pep solving intractable moment matching integrals pep via leads linearisation leads extension extension extension extension extension extension extension extension extension extension extension \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose physics aware difference graph networks designed effectively learn spatial differences modeling sparsely observed dynamics\n",
            "Predicted Summary>>> propose physics aware difference graph networks designed effectively learn spatial differences modeling sparsely observed dynamics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show using semi parametric prior estimations speed hpo significantly across datasets metrics\n",
            "Predicted Summary>>> show using semi parametric prior estimations speed hpo significantly across datasets metrics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> connections predictive coding vaes new frontiers\n",
            "Predicted Summary>>> connections predictive coding vaes new frontiers \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep representations combined gradient descent approximate learning algorithm\n",
            "Predicted Summary>>> deep representations combined gradient descent approximate learning algorithm \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce several datasets cyrillic ocr method recognition\n",
            "Predicted Summary>>> introduce several datasets cyrillic ocr method recognition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study problem continuous control agents deep rl adversarial attacks proposed two step algorithm based learned model dynamics\n",
            "Predicted Summary>>> study problem continuous control agents deep rl adversarial attacks proposed two step algorithm based learned model dynamics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel tensor based method graph convolutional networks dynamic graphs\n",
            "Predicted Summary>>> propose novel tensor based method graph convolutional networks dynamic graphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> consider new variants optimization algorithms training deep nets\n",
            "Predicted Summary>>> consider new variants optimization algorithms training deep nets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper tackles fault tolerance random adversarial stoppages\n",
            "Predicted Summary>>> paper tackles fault tolerance random adversarial stoppages \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> jointly train multilingual skip gram model cross lingual sentence similarity model learn high quality multilingual text embeddings perform well low resource scenario\n",
            "Predicted Summary>>> jointly train multilingual skip gram model cross lingual sentence similarity model learn high quality multilingual text embeddings perform well low resource scenario \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> revisit idea master slave architecture multi agent deep reinforcement learning outperforms state arts\n",
            "Predicted Summary>>> revisit idea master slave architecture multi agent deep reinforcement learning outperforms state arts \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised representations learned contrastive predictive coding enable data efficient image classification\n",
            "Predicted Summary>>> unsupervised representations learned contrastive predictive coding enable data efficient image classification \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use transformer encoder translation training style masked translation model\n",
            "Predicted Summary>>> use transformer encoder translation training style masked translation model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> identify angle bias causes vanishing gradient problem deep nets propose efficient method reduce bias\n",
            "Predicted Summary>>> identify angle bias causes vanishing gradient problem deep nets propose efficient method reduce bias \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce augmented robust feature space streaming wifi data capable tackling concept drift indoor localization\n",
            "Predicted Summary>>> introduce augmented robust feature space streaming wifi data capable tackling concept drift indoor localization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> explored novel method compositional set embeddings perceive represent single class entire set classes associated input data\n",
            "Predicted Summary>>> explored novel method compositional set embeddings perceive represent single class entire set classes associated input data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide rigorous comparison different graph neural networks graph classification\n",
            "Predicted Summary>>> provide rigorous comparison different graph neural networks graph classification \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce novel larger context language model simultaneously captures syntax semantics making capable generating highly interpretable sentences paragraphs\n",
            "Predicted Summary>>> introduce novel larger context language model simultaneously captures syntax semantics making capable generating highly interpretable sentences paragraphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> strokenet novel architecture agent trained draw strokes differentiable simulation environment could effectively exploit power back propagation\n",
            "Predicted Summary>>> strokenet novel architecture agent trained draw strokes differentiable simulation environment could effectively exploit power back propagation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> augmented bootstrapping approach combining information reference set iterative refinements soft labels improve name entity recognition biomedical literature\n",
            "Predicted Summary>>> augmented bootstrapping approach combining information reference set iterative refinements soft labels improve name entity recognition biomedical literature \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dimensionality reduction algorithm visualise text network information example email corpus co authorships\n",
            "Predicted Summary>>> dimensionality reduction algorithm visualise text network information example email corpus co authorships \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new algorithm online multi task learning learns without restarts task borders\n",
            "Predicted Summary>>> new algorithm online multi task learning learns without restarts task borders \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn neural network uniformizes input distribution leads competitive indexing performance high dimensional space\n",
            "Predicted Summary>>> learn neural network uniformizes input distribution leads competitive indexing performance high dimensional space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work presents scalable solution continuous visual speech recognition\n",
            "Predicted Summary>>> work presents scalable solution continuous visual speech recognition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose regularization term added reinforcement learning objective allows policy maximize reward simultaneously learn invariant irrelevant changes within input\n",
            "Predicted Summary>>> propose regularization term added reinforcement learning objective allows policy maximize reward simultaneously learn invariant irrelevant changes within input \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide another novel explanation learning rate decay initially large learning rate suppresses network memorizing noisy data decaying learning rate improves learning complex patterns\n",
            "Predicted Summary>>> provide another novel explanation learning rate decay initially large learning rate suppresses network memorizing noisy data learning learning improves improves learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose fvd new metric generative models video based fid large scale human study confirms fvd correlates well qualitative human judgment generated videos\n",
            "Predicted Summary>>> propose fvd new metric generative models video based fid large scale human study confirms fvd correlates well qualitative human judgment generated generated \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> scale lossless compression latent variables beating existing approaches full size imagenet images\n",
            "Predicted Summary>>> scale lossless compression latent variables beating existing approaches full size imagenet images \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper suggesting method transform style images using deep neural networks\n",
            "Predicted Summary>>> paper suggesting method transform style images using deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel watermark encoder decoder neural networks perform cooperative game define watermarking scheme people need design watermarking methods\n",
            "Predicted Summary>>> propose novel watermark encoder decoder neural networks perform cooperative game define watermarking scheme people need design methods methods \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose unsupervised way learn multiple embeddings sentences phrases\n",
            "Predicted Summary>>> propose unsupervised way learn multiple embeddings sentences phrases \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> max pooled word vectors fuzzy jaccard set similarity extremely competitive baseline semantic similarity propose simple dynamic variant performs even better\n",
            "Predicted Summary>>> max pooled word vectors fuzzy jaccard set similarity extremely competitive baseline semantic similarity propose simple dynamic variant performs even even better \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose arbitrarily conditioned data imputation framework built upon variational autoencoders normalizing flows\n",
            "Predicted Summary>>> propose arbitrarily conditioned data imputation framework built upon variational autoencoders normalizing flows \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce capacity exploit information degree arbitrary goal achieved another goal intended policy gradient methods\n",
            "Predicted Summary>>> introduce capacity exploit information degree arbitrary goal achieved another goal intended policy gradient methods \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> adaptive gradient methods done right incur generalization penalty\n",
            "Predicted Summary>>> adaptive gradient methods done right incur generalization penalty \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> empirical theoretical study effects staleness non synchronous execution machine learning algorithms\n",
            "Predicted Summary>>> empirical theoretical study effects staleness non synchronous execution machine learning algorithms \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> conditional entropy bottleneck information theoretic objective function learning optimal representations\n",
            "Predicted Summary>>> conditional entropy bottleneck information theoretic objective function learning optimal representations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes novel lightweight transformer character level language modeling utilizing group wise operations\n",
            "Predicted Summary>>> paper proposes novel lightweight transformer character level language modeling utilizing group wise operations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised learning method uses meta learning enable efficient learning downstream image classification tasks outperforming state art methods\n",
            "Predicted Summary>>> unsupervised learning method uses meta learning enable efficient learning downstream image classification tasks outperforming state art methods \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> harmonic acoustic model\n",
            "Predicted Summary>>> harmonic acoustic model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> minimum set exponentially distributed hashes useful collision probability generalizes jaccard index probability distributions\n",
            "Predicted Summary>>> minimum set exponentially distributed hashes useful collision probability generalizes jaccard index probability distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extend recent insights related softmax consistency achieve state art results continuous control\n",
            "Predicted Summary>>> extend recent insights related softmax consistency achieve state art results continuous control \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose extension conditional variational autoencoder allows conditioning arbitrary subset features sampling remaining ones\n",
            "Predicted Summary>>> propose extension conditional variational autoencoder allows conditioning arbitrary subset features sampling remaining ones \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> techniques combining generalized policies search algorithms exploit strengths overcome weaknesses solving probabilistic planning problems\n",
            "Predicted Summary>>> techniques combining generalized policies search algorithms exploit strengths overcome weaknesses solving probabilistic planning problems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gan representations examined detail sets representation units found control generation semantic concepts output\n",
            "Predicted Summary>>> gan representations examined detail sets representation units found control generation semantic concepts output \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> representing programs graphs including semantics helps generating programs\n",
            "Predicted Summary>>> representing programs graphs including semantics helps generating programs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose alternative measure determining effectiveness adversarial attacks nlp models according distance measure based method like incremental gain control theory\n",
            "Predicted Summary>>> propose alternative measure determining effectiveness adversarial attacks nlp models according distance measure based method like incremental gain control \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new drl policy algorithm achieving state art performance\n",
            "Predicted Summary>>> propose new drl policy algorithm achieving state art performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper formalises problem online algorithm selection context reinforcement learning\n",
            "Predicted Summary>>> paper formalises problem online algorithm selection context reinforcement learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> generalized transformation based gradient model variational inference\n",
            "Predicted Summary>>> generalized transformation generalized transformation model variational inference \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> method transform dna sequences images using space filling hilbert curves enhance strengths cnns\n",
            "Predicted Summary>>> method transform dna sequences images using space filling hilbert curves enhance strengths cnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> leverage deterministic autoencoders generative models proposing mixing functions combine hidden states pairs images mixes made look realistic adversarial framework\n",
            "Predicted Summary>>> leverage deterministic autoencoders generative models proposing mixing functions combine hidden states pairs images mixes made look look framework framework \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new method assessing quaility similarity evaluators showing potential transformer based language models replacing bleu rouge\n",
            "Predicted Summary>>> new method assessing quaility similarity evaluators showing potential transformer based language models replacing bleu rouge \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> address active learning batch setting noisy oracles use model uncertainty encode decision quality active learning algorithm acquisition\n",
            "Predicted Summary>>> address active learning batch setting noisy oracles use model uncertainty encode decision quality active learning algorithm acquisition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> stochastic variational video prediction real world settings\n",
            "Predicted Summary>>> stochastic variational video prediction real world settings \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce amortized proximal optimization apo method adapt variety optimization hyperparameters online training including learning rates damping coefficients gradient variance exponents\n",
            "Predicted Summary>>> introduce amortized proximal optimization apo method adapt variety optimization hyperparameters online training including learning rates damping gradient gradient variance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show entropy sgd optimizes prior pac bayes bound violating requirement prior independent data use differential privacy resolve improve generalization\n",
            "Predicted Summary>>> show entropy sgd optimizes prior pac bayes bound violating requirement prior independent data use differential privacy resolve generalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> batch normalization reduces robustness test time common corruptions adversarial examples\n",
            "Predicted Summary>>> batch normalization reduces robustness test time common corruptions adversarial examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze determine precision requirements training neural networks tensors including back propagated signals weight accumulators quantized fixed point format\n",
            "Predicted Summary>>> analyze determine precision requirements training neural networks tensors including back propagated signals weight accumulators quantized fixed point format \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose neural cascades simple trivially parallelizable approach reading comprehension consisting feed forward nets attention achieves state art performance triviaqa dataset\n",
            "Predicted Summary>>> propose neural cascades simple trivially parallelizable approach reading comprehension consisting feed forward nets attention achieves state art performance triviaqa \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose cnn neuron ranking two different methods show consistency producing result allows interpret network deems important compress network keeping relevant nodes\n",
            "Predicted Summary>>> propose cnn neuron ranking two different methods show consistency producing result allows interpret network deems important compress network keeping relevant \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unify support estimation family adversarial imitation learning algorithms support guided adversarial imitation learning robust stable imitation learning framework\n",
            "Predicted Summary>>> unify support estimation family adversarial imitation learning algorithms support guided adversarial imitation learning robust stable imitation learning framework \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> opponent shaping powerful approach multi agent learning prevent convergence sos algorithm fixes strong guarantees differentiable games\n",
            "Predicted Summary>>> opponent shaping powerful approach multi agent learning prevent convergence sos algorithm fixes strong guarantees differentiable games \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show age confounds cognitive impairment detection solve fair representation learning propose metrics models\n",
            "Predicted Summary>>> show age confounds cognitive impairment detection solve fair representation learning propose metrics models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes new cnn model combines energy cost dynamic routing strategy enable adaptive energy efficient inference\n",
            "Predicted Summary>>> paper proposes new cnn model combines energy cost dynamic routing strategy enable adaptive energy efficient inference \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> construct theoretical framework weakly supervised disentanglement conducted lots experiments back theory\n",
            "Predicted Summary>>> construct theoretical framework weakly supervised disentanglement conducted lots experiments back \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present eligibility propagation alternative bptt compatible experimental data synaptic plasticity competes bptt machine learning benchmarks\n",
            "Predicted Summary>>> present eligibility propagation alternative bptt compatible experimental data synaptic plasticity competes bptt machine learning benchmarks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop analytical method study bayesian inference finite width neural networks find renormalization group flow picture naturally emerges\n",
            "Predicted Summary>>> develop analytical method study bayesian inference finite width neural networks find renormalization group flow picture naturally emerges \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposed model free policy il algorithm continuous control experimental results showed algorithm achieves competitive results gail significantly reducing environment interactions\n",
            "Predicted Summary>>> paper proposed model free policy il algorithm continuous control experimental results showed algorithm achieves competitive results gail significantly reducing interactions environment interactions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> method transform dna sequences images using space filling hilbert curves enhance strengths cnns\n",
            "Predicted Summary>>> method transform dna sequences images using space filling hilbert curves enhance strengths cnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> point important problems common practice using best single model performance comparing deep learning architectures propose method corrects flaws\n",
            "Predicted Summary>>> point important problems common practice using best single model performance comparing deep learning architectures propose method corrects flaws \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fast truly scalable full matrix adagrad adam theory adaptive stochastic non convex optimization\n",
            "Predicted Summary>>> fast truly scalable full matrix adagrad adam theory adaptive stochastic non convex optimization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> agent trained curiosity extrinsic reward surprisingly well popular environments including suite atari games mario etc\n",
            "Predicted Summary>>> agent trained curiosity extrinsic reward surprisingly well popular environments including suite atari games mario etc \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> demostarte using clinical notes conjuntion icu instruments data improves perfomance icu management benchmark tasks\n",
            "Predicted Summary>>> demostarte using clinical notes conjuntion icu instruments data improves perfomance icu management benchmark tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel approach improve given cross surface mapping local refinement new iterative method deform mesh order meet user constraints\n",
            "Predicted Summary>>> propose novel approach improve given cross surface mapping local refinement new iterative method deform mesh order meet meet constraints \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze gradient descent deep linear neural networks providing guarantee convergence global optimum linear rate\n",
            "Predicted Summary>>> analyze gradient descent deep linear neural networks providing guarantee convergence global optimum linear rate \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper tackles fault tolerance random adversarial stoppages\n",
            "Predicted Summary>>> paper tackles fault tolerance random adversarial stoppages \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present neuroscience inspired method based neural networks latent space search\n",
            "Predicted Summary>>> present neuroscience inspired method based neural networks latent space search \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> training dnns interface black box functions intermediate labels using estimator sub network replaced black box training\n",
            "Predicted Summary>>> training dnns interface black box functions intermediate labels using estimator sub network replaced black box training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learned phoneme embeddings multilingual neural speech synthesis network could represent relations phoneme pronunciation languages\n",
            "Predicted Summary>>> learned phoneme embeddings multilingual neural speech synthesis network could represent relations phoneme pronunciation languages \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> argue theoretically simply assuming weights relu network gaussian distributed without even bayesian formalism could fix issue calibrated uncertainty simple bayesian method could already sufficient\n",
            "Predicted Summary>>> argue theoretically simply assuming weights relu network gaussian distributed without even bayesian formalism could fix issue uncertainty simple bayesian method bayesian could already already \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new method using ontology information improve performance massively multi label prediction classification problems\n",
            "Predicted Summary>>> propose new method using ontology information improve performance massively multi label prediction classification problems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel meta learning model adaptively balances effect meta learning task specific learning also class specific learning within task\n",
            "Predicted Summary>>> novel meta learning model adaptively balances effect meta learning task specific learning also class specific learning within task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel bit format eliminates need loss scaling stochastic rounding low precision techniques\n",
            "Predicted Summary>>> propose novel bit format eliminates need loss scaling stochastic rounding low precision techniques \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> previous vaes text cannot learn controllable latent representation images well fix enable first success towards controlled text generation without supervision\n",
            "Predicted Summary>>> previous vaes text cannot learn controllable latent representation images well fix enable first success towards controlled text generation without supervision \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose approach generate realistic high fidelity stock market data based generative adversarial networks\n",
            "Predicted Summary>>> propose approach generate realistic high fidelity stock market data based generative adversarial networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep learning adaptation randomized least squares value iteration\n",
            "Predicted Summary>>> deep learning adaptation randomized least squares value iteration \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide another novel explanation learning rate decay initially large learning rate suppresses network memorizing noisy data decaying learning rate improves learning complex patterns\n",
            "Predicted Summary>>> provide another novel explanation learning rate decay initially large learning rate suppresses network memorizing noisy data learning learning improves improves learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> train variational models quantized networks computational determinism enables using cross platform data compression\n",
            "Predicted Summary>>> train variational models quantized networks computational determinism enables using cross platform data compression \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> design analyze new zeroth order stochastic optimization algorithm zo signsgd demonstrate connection application black box adversarial attacks robust deep learning\n",
            "Predicted Summary>>> design analyze new zeroth order stochastic optimization algorithm zo signsgd demonstrate connection application black box adversarial attacks robust deep learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dqn ddpg hybrid algorithm proposed deal discrete continuous hybrid action space\n",
            "Predicted Summary>>> dqn ddpg hybrid algorithm proposed deal discrete continuous hybrid action space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent\n",
            "Predicted Summary>>> new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> ideas future ickeps\n",
            "Predicted Summary>>> ideas future ickeps \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dimensionality reduction algorithm visualise text network information example email corpus co authorships\n",
            "Predicted Summary>>> dimensionality reduction algorithm visualise text network information example email corpus co authorships \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep representations combined gradient descent approximate learning algorithm\n",
            "Predicted Summary>>> deep representations combined gradient descent approximate learning algorithm \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed implementation accelerate dnn data parallel training reducing communication bandwidth requirement\n",
            "Predicted Summary>>> proposed implementation accelerate dnn data parallel training reducing communication bandwidth requirement \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel end end model spnet incorporate semantic scaffolds improving abstractive dialog summarization\n",
            "Predicted Summary>>> propose novel end end model spnet incorporate semantic scaffolds improving abstractive dialog summarization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> exploit inversion scheme arbitrary deep neural networks develop new semi supervised learning framework applicable many topologies\n",
            "Predicted Summary>>> exploit inversion scheme arbitrary deep neural networks develop new semi supervised learning framework applicable many topologies \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> noise robust deep learning architecture\n",
            "Predicted Summary>>> noise robust deep learning architecture \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce approach allow agents learn ppddl action models incrementally multiple planning problems framework reinforcement learning\n",
            "Predicted Summary>>> introduce approach allow agents learn ppddl action models incrementally multiple planning problems framework reinforcement learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> information bottleneck principle applied resnets using pixelcnn models decode mutual information conditionally generate images information illustration\n",
            "Predicted Summary>>> information bottleneck principle applied resnets using pixelcnn models decode mutual information conditionally generate images information illustration illustration \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> address problem generalization reinforcement learning unseen action spaces\n",
            "Predicted Summary>>> address problem generalization reinforcement learning unseen action spaces \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> children use mutual exclusivity bias learn new words standard neural nets show opposite bias hindering learning naturalistic scenarios lifelong learning\n",
            "Predicted Summary>>> children use mutual exclusivity bias learn new words standard neural nets show opposite bias hindering learning scenarios lifelong lifelong \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> bayesian meta learning using pac bayes framework implicit prior distributions\n",
            "Predicted Summary>>> bayesian meta learning using pac bayes framework implicit prior distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> routing procedures necessary capsnets\n",
            "Predicted Summary>>> routing procedures necessary capsnets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> decompose gap marginal log likelihood evidence lower bound study effect approximate posterior true posterior distribution vaes\n",
            "Predicted Summary>>> decompose gap marginal log likelihood evidence lower bound study effect approximate posterior true posterior distribution vaes vaes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work aims provide quantitative answers relative importance concepts interest via concept activation vectors cav particular framework enables non machine learning experts express concepts interest test hypotheses using examples set pictures illustrate concept show cav learned given relatively small set examples hypothesis testing cav answer whether particular concept gender important predicting given class doctor sets concepts interpreting networks cav require retraining modification network\n",
            "Predicted Summary>>> work aims provide quantitative answers relative importance concepts interest via concept activation vectors cav non non non framework learning non framework \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> convolutional neural networks behave compositional nearest neighbors\n",
            "Predicted Summary>>> convolutional neural networks behave compositional nearest neighbors \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new objective training hybrid vae gans lead significant improvement mode coverage quality\n",
            "Predicted Summary>>> propose new objective training hybrid vae gans lead significant improvement mode coverage quality \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper suggesting method transform style images using deep neural networks\n",
            "Predicted Summary>>> paper suggesting method transform style images using deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> focusing final predictions anytime predictors recent multi scale densenets make small anytime models outperform large ones focus\n",
            "Predicted Summary>>> focusing final predictions anytime predictors recent multi scale densenets make small anytime models outperform large ones focus \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose neural hyperlink predictor nhp nhp adapts graph convolutional networks link prediction hypergraphs\n",
            "Predicted Summary>>> propose neural hyperlink predictor nhp nhp adapts graph convolutional networks link prediction hypergraphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper presents deep learning model combines self organizing maps convolutional neural networks representation learning multi omics data\n",
            "Predicted Summary>>> paper presents deep learning model combines self organizing maps convolutional neural networks representation learning multi omics data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> experimentally show transfer learning makes sparse features network thereby produces compressible network\n",
            "Predicted Summary>>> experimentally show transfer learning makes sparse features network thereby produces compressible network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel neural architecture efficient amortized inference latent permutations\n",
            "Predicted Summary>>> novel neural architecture efficient amortized inference latent permutations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> weakly supervised learning based clustering framework performs comparable fully supervised learning models exploiting unique class count\n",
            "Predicted Summary>>> weakly supervised learning based clustering framework performs comparable fully supervised learning models exploiting unique class count \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> interactive technique improve brushing dense trajectory datasets taking account shape brush\n",
            "Predicted Summary>>> interactive technique improve brushing dense trajectory datasets taking account shape brush \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work presents scalable solution continuous visual speech recognition\n",
            "Predicted Summary>>> work presents scalable solution continuous visual speech recognition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose imagenet measure classifier corruption robustness imagenet measure perturbation robustness\n",
            "Predicted Summary>>> propose imagenet measure classifier corruption robustness imagenet measure perturbation robustness \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> solve sparse rewards problem web ui tasks using exploration guided demonstrations\n",
            "Predicted Summary>>> solve sparse rewards problem web ui tasks using exploration guided demonstrations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learned energy based model score matching\n",
            "Predicted Summary>>> learned energy based model score matching \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neumann networks end end sample efficient learning approach solving linear inverse problems imaging compatible mse optimal approach admit extension patch based learning\n",
            "Predicted Summary>>> neumann networks end end sample efficient learning approach solving linear inverse problems imaging compatible mse optimal approach admit admit extension learning learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> theory connecting hessian solution generalization power model\n",
            "Predicted Summary>>> theory connecting hessian solution generalization power model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose semantic aware neural abstractive summarization model novel automatic summarization evaluation scheme measures well model identifies topic information adversarial samples\n",
            "Predicted Summary>>> propose semantic aware neural abstractive summarization model novel automatic summarization evaluation scheme measures well model identifies topic information adversarial samples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present local ensembles method detecting extrapolation trained models approximates variance ensemble using local second order information\n",
            "Predicted Summary>>> present local ensembles method detecting extrapolation trained models approximates variance ensemble using local second order information \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel state space time series model capability capture structure change points anomaly points better forecasting performance exist change points anomalies time series\n",
            "Predicted Summary>>> propose novel state space time series model capability capture structure change points anomaly points better forecasting performance exist change points anomalies time series series \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use deep rl learn policy directs search genetic algorithm better optimize execution cost computation graphs show improved results real world tensorflow graphs\n",
            "Predicted Summary>>> use deep rl learn policy directs search genetic algorithm better optimize execution cost computation graphs show improved results real tensorflow \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose model based method called search amortized value estimates save leverages real planned experience combining learning monte carlo tree search achieving strong performance small search budgets\n",
            "Predicted Summary>>> propose model based method called search amortized value estimates save leverages real planned experience combining learning monte carlo tree search achieving strong performance search small search budgets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> theoretical study multi task learning practical implications improving multi task training transfer learning\n",
            "Predicted Summary>>> theoretical study multi task learning practical implications improving multi task training transfer learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new drl policy algorithm achieving state art performance\n",
            "Predicted Summary>>> propose new drl policy algorithm achieving state art performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> framework links deep network layers stochastic optimization algorithms used improve model accuracy inform network design\n",
            "Predicted Summary>>> framework links deep network layers stochastic optimization algorithms used improve model accuracy inform network design \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> defending physically realizable attacks image classification\n",
            "Predicted Summary>>> defending physically realizable attacks image classification \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> weakly supervised learning based clustering framework performs comparable fully supervised learning models exploiting unique class count\n",
            "Predicted Summary>>> weakly supervised learning based clustering framework performs comparable fully supervised learning models exploiting unique class count \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel meta learning model adaptively balances effect meta learning task specific learning also class specific learning within task\n",
            "Predicted Summary>>> novel meta learning model adaptively balances effect meta learning task specific learning also class specific learning within task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed system prevent impersonators facial disguises completing fraudulent transaction using pre trained dcnn\n",
            "Predicted Summary>>> proposed system prevent impersonators facial disguises completing fraudulent transaction using pre trained dcnn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present provable algorithm exactly recovering factors dictionary learning model\n",
            "Predicted Summary>>> present provable algorithm exactly recovering factors dictionary learning model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel architecture shot classification capable dealing uncertainty\n",
            "Predicted Summary>>> novel architecture shot classification capable dealing uncertainty \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method computing adversarially robust representations entirely unsupervised way\n",
            "Predicted Summary>>> propose method computing adversarially robust representations entirely unsupervised way \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> apply monte carlo tree search episode generation alpha zero\n",
            "Predicted Summary>>> apply monte carlo tree search episode generation alpha zero \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> sgd adam single spiked model tensor pca\n",
            "Predicted Summary>>> sgd adam single spiked model tensor pca \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> parametric manifold learning neural networks geometric framework\n",
            "Predicted Summary>>> parametric manifold learning neural networks geometric framework \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> bridge gap soft computing\n",
            "Predicted Summary>>> bridge gap soft computing \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose idea using norm successor representation exploration bonus reinforcement learning hard exploration atari games deep rl algorithm matches performance recent pseudo count based methods\n",
            "Predicted Summary>>> propose idea using norm successor representation exploration bonus reinforcement learning hard exploration atari games deep rl algorithm matches performance recent pseudo based methods methods \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show working memory input reservoir network makes local reward modulated hebbian rule perform well recursive least squares aka force\n",
            "Predicted Summary>>> show working memory input reservoir network makes local reward modulated hebbian rule perform well recursive least squares aka force \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce novel way represent graphs multi channel image like structures allows handled vanilla cnns\n",
            "Predicted Summary>>> introduce novel way represent graphs multi channel image like structures allows handled vanilla cnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn space motor primitives unannotated robot demonstrations show primitives semantically meaningful composed new robot tasks\n",
            "Predicted Summary>>> learn space motor primitives unannotated robot demonstrations show primitives semantically meaningful composed new robot tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> method accurate critic estimates reinforcement learning\n",
            "Predicted Summary>>> method accurate critic estimates reinforcement learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> meta learning methods used vision directly applied nlp perform worse nearest neighbors new classes better distributional signatures\n",
            "Predicted Summary>>> meta learning methods used vision directly applied nlp perform worse nearest neighbors new classes better distributional signatures signatures \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> framework links deep network layers stochastic optimization algorithms used improve model accuracy inform network design\n",
            "Predicted Summary>>> framework links deep network layers stochastic optimization algorithms used improve model accuracy inform network design \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> sgd adam single spiked model tensor pca\n",
            "Predicted Summary>>> sgd adam single spiked model tensor pca \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> first text adversarial defense method word level improved generic based attack method synonyms substitution based attacks\n",
            "Predicted Summary>>> first text adversarial defense method word level improved generic based attack method synonyms substitution based attacks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new method inferring model estimating entropy rate predicting continuous time discrete event processes\n",
            "Predicted Summary>>> new method inferring model estimating entropy rate predicting continuous time discrete event processes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> presents information theoretic training objective co training demonstrates power unsupervised learning phonetics\n",
            "Predicted Summary>>> presents information theoretic training objective co training demonstrates power unsupervised learning phonetics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> efficient lifelong learning algorithm provides better trade accuracy time memory complexity compared algorithms\n",
            "Predicted Summary>>> efficient lifelong learning algorithm provides better trade accuracy time memory complexity compared algorithms \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> models representation learning dynamic graphs latent hidden process bridging two observed processes topological evolution interactions dynamic graphs\n",
            "Predicted Summary>>> models representation learning dynamic graphs latent hidden process bridging two observed processes topological evolution interactions dynamic dynamic dynamic \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new way learning semantic program embedding\n",
            "Predicted Summary>>> new way learning semantic program embedding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> relaxing constraint shared hierarchies enables effective deep multitask learning\n",
            "Predicted Summary>>> relaxing constraint shared hierarchies enables effective deep multitask learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simple gan modification improves performance across many losses architectures regularization schemes datasets\n",
            "Predicted Summary>>> simple gan modification improves performance across many losses architectures regularization schemes datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose meta learning framework learns transferable policy weak supervision solve synthesis tasks different logical specifications grammars\n",
            "Predicted Summary>>> propose meta learning framework learns transferable policy weak supervision solve synthesis tasks different logical specifications grammars \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning hierarchical policies unsegmented demonstrations using directed information\n",
            "Predicted Summary>>> learning hierarchical policies unsegmented demonstrations using directed information \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> anticipation improves convergence deep reinforcement learning\n",
            "Predicted Summary>>> anticipation improves convergence deep reinforcement learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide necessary sufficient analytical forms critical points square loss functions various neural networks exploit analytical forms characterize landscape properties loss functions neural networks\n",
            "Predicted Summary>>> provide necessary sufficient analytical forms critical points square loss functions various neural networks exploit analytical forms landscape landscape loss properties neural networks networks networks networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper shows wasserstein distance objective enables training latent variable models discrete latents case variational autoencoder objective fails\n",
            "Predicted Summary>>> paper shows wasserstein distance objective enables training latent variable models discrete latents case variational autoencoder objective fails \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce method train models provable robustness wrt norms geq simultaneously\n",
            "Predicted Summary>>> introduce method train models provable robustness wrt norms geq simultaneously \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning imitate expert absence optimal actions learning dynamics model exploring environment\n",
            "Predicted Summary>>> learning imitate expert absence optimal actions learning dynamics model exploring environment \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study multi layer generalization magnitude based pruning\n",
            "Predicted Summary>>> study multi layer generalization magnitude based pruning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use self supervision domain align unsupervised domain adaptation\n",
            "Predicted Summary>>> use self supervision domain align unsupervised domain adaptation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce first hierarchical rl approach successfully learn level hierarchies parallel tasks continuous state action spaces\n",
            "Predicted Summary>>> introduce first hierarchical rl approach successfully learn level hierarchies parallel tasks continuous state action spaces \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extendable modular architecture proposed developing variety agent behaviors dqn\n",
            "Predicted Summary>>> extendable modular architecture proposed developing variety agent behaviors dqn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> strokenet novel architecture agent trained draw strokes differentiable simulation environment could effectively exploit power back propagation\n",
            "Predicted Summary>>> strokenet novel architecture agent trained draw strokes differentiable simulation environment could effectively exploit power back propagation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> end end trainable model compression method optimizing accuracy jointly expected model size\n",
            "Predicted Summary>>> end end trainable model compression method optimizing accuracy jointly expected model size \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn neural network uniformizes input distribution leads competitive indexing performance high dimensional space\n",
            "Predicted Summary>>> learn neural network uniformizes input distribution leads competitive indexing performance high dimensional space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> train accurate fully quantized networks using loss function maximizing full precision model accuracy minimizing difference full precision quantized networks\n",
            "Predicted Summary>>> train accurate fully quantized networks using loss function maximizing full precision model accuracy minimizing full full precision \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> automatic method converting music instruments styles\n",
            "Predicted Summary>>> automatic method converting music instruments styles \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> make transformer streamable monotonic attention\n",
            "Predicted Summary>>> make transformer streamable monotonic attention \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> address trade caused dependency classes domains improving domain adversarial nets\n",
            "Predicted Summary>>> address trade caused dependency classes domains improving domain adversarial nets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> minimum set exponentially distributed hashes useful collision probability generalizes jaccard index probability distributions\n",
            "Predicted Summary>>> minimum set exponentially distributed hashes useful collision probability generalizes jaccard index probability distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> empirical analysis explanation particle based gradient estimators approximate inference deep generative models\n",
            "Predicted Summary>>> empirical analysis explanation particle based gradient estimators approximate inference deep generative models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use neural networks trained image denoising plug play priors energy minimization algorithms image reconstruction problems provable convergence\n",
            "Predicted Summary>>> use neural networks trained image denoising plug play priors energy minimization algorithms image reconstruction problems provable convergence \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> latent optimisation improves adversarial training dynamics present theoretical analysis state art image generation imagenet\n",
            "Predicted Summary>>> latent optimisation improves adversarial training dynamics present theoretical analysis state art image generation imagenet \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> prove large class functions exists interval certified robust network approximating arbitrary precision\n",
            "Predicted Summary>>> prove large class functions exists interval certified robust network approximating arbitrary precision \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes fundamental theory optimal algorithms dnn training reduce training memory popular dnns\n",
            "Predicted Summary>>> paper proposes fundamental theory optimal algorithms dnn training reduce training memory popular dnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> stable domain adversarial training approach robust comprehensive domain adaptation\n",
            "Predicted Summary>>> stable domain adversarial training approach robust comprehensive domain adaptation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> metalearning unsupervised update rules neural networks improves performance potentially demonstrates neurons brain learn without access global labels\n",
            "Predicted Summary>>> metalearning unsupervised update rules neural networks improves performance potentially demonstrates neurons brain learn without access global labels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose structure generator gan consider objects relations explicitly generate images means composition\n",
            "Predicted Summary>>> propose structure generator gan consider objects relations explicitly generate images means composition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose matrix completion based task clustering algorithm deep multi task shot learning settings large numbers diverse tasks\n",
            "Predicted Summary>>> propose matrix completion based task clustering algorithm deep multi task shot learning settings large numbers diverse tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce type neural network structurally resistant adversarial attacks even trained unaugmented training sets resistance due stability network units wrt input perturbations\n",
            "Predicted Summary>>> introduce type neural network structurally resistant adversarial attacks even trained unaugmented training sets resistance due stability network units wrt input perturbations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show variants importance weighted autoencoders derived principled manner special cases adaptive importance sampling approaches like reweighted wake sleep algorithm\n",
            "Predicted Summary>>> show variants importance weighted autoencoders derived principled manner special cases adaptive importance sampling approaches like reweighted wake sleep \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gen rkm novel framework generative models using restricted kernel machines multi view generation uncorrelated feature learning\n",
            "Predicted Summary>>> gen rkm novel framework generative models using restricted kernel machines multi view generation uncorrelated feature learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> efficient differentiable ilp model learns first order logic rules explain data\n",
            "Predicted Summary>>> efficient differentiable ilp model learns first order logic rules explain data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> understand information stored latent space train gan style decoder constrained produce images vae encoder map region latent space\n",
            "Predicted Summary>>> understand information stored latent space train gan style decoder constrained produce images vae encoder map region latent space space space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze gradient propagation deep rnns analysis propose new multi layer deep rnn\n",
            "Predicted Summary>>> analyze gradient propagation deep rnns analysis propose new multi layer deep rnn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> demonstrate residual blocks viewed gauss newton steps propose new residual block exploits second order information\n",
            "Predicted Summary>>> demonstrate residual blocks viewed gauss newton steps propose new residual block exploits second order information \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> plan syntactic structural translation using codes\n",
            "Predicted Summary>>> plan syntactic structural translation using codes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> efficient video classification using frame based conditional gating module selecting dominant frames followed temporal modeling classifier\n",
            "Predicted Summary>>> efficient video classification using frame based conditional gating module selecting dominant frames followed temporal modeling classifier classifier \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate internal reasons observations diminishing effects well known hyperparameter optimization methods federated learning decentralized non iid data\n",
            "Predicted Summary>>> investigate internal reasons observations diminishing effects well known hyperparameter optimization methods federated learning decentralized non iid data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop new optimization approach vanilla relu based rnn enables long short term memory identification arbitrary nonlinear dynamical systems widely differing time scales\n",
            "Predicted Summary>>> develop new optimization approach vanilla relu based rnn enables long short term memory identification arbitrary nonlinear dynamical systems widely differing time scales scales \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> soft quantization approach learn pure fixed point representations deep neural networks\n",
            "Predicted Summary>>> soft quantization approach learn pure fixed point representations deep neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> address active learning batch setting noisy oracles use model uncertainty encode decision quality active learning algorithm acquisition\n",
            "Predicted Summary>>> address active learning batch setting noisy oracles use model uncertainty encode decision quality active learning algorithm acquisition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fix classifier neural networks without losing accuracy\n",
            "Predicted Summary>>> fix classifier neural networks without losing accuracy \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> addressing task heterogeneity problem meta learning introducing meta knowledge graph\n",
            "Predicted Summary>>> addressing task heterogeneity problem meta learning introducing meta knowledge graph \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup\n",
            "Predicted Summary>>> present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed method finding generalizable solution stable perturbations trainig data\n",
            "Predicted Summary>>> proposed method finding generalizable solution stable perturbations trainig data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose gated linear unit networks model performs similarly relu networks real data much easier analyze theoretically\n",
            "Predicted Summary>>> propose gated linear unit networks model performs similarly relu networks real data much easier analyze theoretically \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> represent entity based histogram contexts wasserstein need\n",
            "Predicted Summary>>> represent entity based histogram contexts wasserstein need \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present provable easily computable evaluation function estimates performance transferred representations one learning task another task transfer learning\n",
            "Predicted Summary>>> present provable easily computable evaluation function estimates performance transferred representations one learning task another task transfer learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper provides rigorous study variance reduced td learning characterizes advantage vanilla td learning\n",
            "Predicted Summary>>> paper provides rigorous study variance reduced td learning characterizes advantage vanilla td learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analysis bayesian hyperparameter inference gaussian process regression\n",
            "Predicted Summary>>> analysis bayesian hyperparameter inference gaussian process regression \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neural phrase based machine translation linear decoding time\n",
            "Predicted Summary>>> neural phrase based machine translation linear decoding time \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose additional training step called post training computes optimal weights last layer network\n",
            "Predicted Summary>>> propose additional training step called post training computes optimal weights last layer network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> advance state art model compression proposing atomic compression networks acns novel architecture constructed recursive repetition small set neurons\n",
            "Predicted Summary>>> advance state art model compression proposing atomic compression networks acns novel architecture constructed recursive repetition small set neurons \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> maml great many problems solve many problems result learn hyper parameters end end speed training inference set new sota shot learning\n",
            "Predicted Summary>>> maml great many problems solve many problems result learn hyper parameters end end speed training inference set new sota shot learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analysis deep convolutional networks terms associated arrangement hyperplanes\n",
            "Predicted Summary>>> analysis deep convolutional networks terms associated arrangement hyperplanes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposes effective coding scheme neural networks encodes random set weights variational distribution\n",
            "Predicted Summary>>> paper proposes effective coding scheme neural networks encodes random set weights variational distribution \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> lstms learn long range dependencies compositionally building shorter constituents course training\n",
            "Predicted Summary>>> lstms learn long range dependencies compositionally building shorter constituents course training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gradient clipping endow robustness label noise simple loss based variant\n",
            "Predicted Summary>>> gradient clipping endow robustness label noise simple loss based variant \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method efficient multi objective neural architecture search based lamarckian inheritance evolutionary algorithms\n",
            "Predicted Summary>>> propose method efficient multi objective neural architecture search based lamarckian inheritance evolutionary algorithms \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show using semi parametric prior estimations speed hpo significantly across datasets metrics\n",
            "Predicted Summary>>> show using semi parametric prior estimations speed hpo significantly across datasets metrics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present multi task benchmark analysis platform evaluating generalization natural language understanding systems\n",
            "Predicted Summary>>> present multi task benchmark analysis platform evaluating generalization natural language understanding systems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel high performing architecture end end named entity recognition relation extraction fast train\n",
            "Predicted Summary>>> novel high performing architecture end end named entity recognition relation extraction fast train \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> internal consistency constraints improve agents ability develop emergent protocols generalize across communicative roles\n",
            "Predicted Summary>>> internal consistency constraints improve agents ability develop emergent protocols generalize across communicative roles \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> shown resnet type cnns universal approximator expression ability worse fully connected neural networks fnns textit block sparse structure even size layer cnn fixed\n",
            "Predicted Summary>>> shown resnet type cnns universal approximator expression ability worse fully connected neural networks fnns textit block sparse structure even layer size cnn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> modular hierarchical approach learn policies exploring environments\n",
            "Predicted Summary>>> modular hierarchical approach learn policies exploring environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> convergence theory biased consistent gradient estimators stochastic optimization application graph convolutional networks\n",
            "Predicted Summary>>> convergence theory biased consistent gradient estimators stochastic optimization application graph convolutional networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> feature map compression method converts quantized activations binary vectors followed nonlinear dimensionality reduction layers embedded dnn\n",
            "Predicted Summary>>> feature map compression method converts quantized activations binary vectors followed nonlinear dimensionality reduction layers embedded \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> compare graph rnns graph convnets consider generic class graph convnets residuality\n",
            "Predicted Summary>>> compare graph rnns graph convnets consider generic class graph convnets residuality \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show contras popular wisdom exploding gradient problem solved limits depth mlps effectively trained show gradients explode resnet handles\n",
            "Predicted Summary>>> show contras popular wisdom exploding gradient problem solved limits depth mlps effectively trained show gradients explode resnet handles \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study graph generation problem propose powerful deep generative model capable generating arbitrary graphs\n",
            "Predicted Summary>>> study graph generation problem propose powerful deep generative model capable generating arbitrary graphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel way incorporate conditional image information discriminator gans using feature fusion used structured prediction tasks\n",
            "Predicted Summary>>> propose novel way incorporate conditional image information discriminator gans using feature fusion used structured prediction tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use simple biologically motivated modifications standard learning techniques achieve state art performance catastrophic forgetting benchmarks\n",
            "Predicted Summary>>> use simple biologically motivated modifications standard learning techniques achieve state art performance catastrophic forgetting benchmarks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present use secondary encoder decoder loss function help train summarizer\n",
            "Predicted Summary>>> present use secondary encoder decoder loss function help train summarizer \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> scalable general purpose factorization algorithm also helps circumvent cold start problem\n",
            "Predicted Summary>>> scalable general purpose factorization algorithm also helps circumvent cold start problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper hypothesize superficially perturbed data points merely map class map representation\n",
            "Predicted Summary>>> paper hypothesize superficially perturbed data points merely map class map representation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate framework discovery curating large collection predictions used construct agent representation partially observable domains\n",
            "Predicted Summary>>> investigate framework discovery curating large collection predictions used construct agent representation partially observable domains \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce novel way represent graphs multi channel image like structures allows handled vanilla cnns\n",
            "Predicted Summary>>> introduce novel way represent graphs multi channel image like structures allows handled vanilla cnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> apply gradient based meta learning graph domain introduce new graph specific transfer function bootstrap process\n",
            "Predicted Summary>>> apply gradient based meta learning graph domain introduce new graph specific transfer function bootstrap process \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study whether adaptive data augmentation knowledge distillation leveraged simultaneously synergistic manner better training student networks\n",
            "Predicted Summary>>> study whether adaptive data augmentation knowledge distillation leveraged simultaneously synergistic manner better training student networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gan based extreme image compression method using less half bits sota engineered codec preserving visual quality\n",
            "Predicted Summary>>> gan based extreme image compression method using less half bits sota engineered codec preserving visual quality \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> modern deep cnns invariant translations scalings realistic image transformations lack invariance related subsampling operation biases contained image datasets\n",
            "Predicted Summary>>> modern deep cnns invariant translations scalings realistic image transformations lack invariance related operation operation operation image image datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> using dsl grammar reinforcement learning improve synthesis programs complex control flow\n",
            "Predicted Summary>>> using dsl grammar reinforcement learning improve synthesis programs complex control flow \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> rederive wide class inference procedures global information bottleneck objective\n",
            "Predicted Summary>>> rederive wide class inference procedures global information bottleneck objective \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> stability scattering transform representations graph data deformations underlying graph support\n",
            "Predicted Summary>>> stability scattering transform representations graph data deformations underlying graph support \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> temporally consistent modality flexible unsupervised video video translation framework trained self supervised manner\n",
            "Predicted Summary>>> temporally consistent modality flexible unsupervised video video translation framework trained self supervised manner \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present scalable approximation wide range ebm objectives applications implicit vaes waes\n",
            "Predicted Summary>>> present scalable approximation wide range ebm objectives applications implicit vaes waes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new combination evolution strategy deep reinforcement learning takes best worlds\n",
            "Predicted Summary>>> propose new combination evolution strategy deep reinforcement learning takes best worlds \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new generative model discrete structured data proposed stochastic lazy attribute converts offline semantic check online guidance stochastic decoding effectively addresses constraints syntax semantics also achieves superior performance\n",
            "Predicted Summary>>> new generative model discrete structured data proposed stochastic lazy attribute converts offline semantic check online guidance stochastic decoding guidance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose train invertible neural network class perform class class continual learning\n",
            "Predicted Summary>>> propose train invertible neural network class perform class class continual learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> enhancing robustness pretrained transformer models lexical overlap bias extending input sentences training data corresponding predicate argument structures\n",
            "Predicted Summary>>> enhancing robustness pretrained transformer models lexical overlap bias extending input sentences training data corresponding predicate argument \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel architecture memory based attention mechanism multi agent communication\n",
            "Predicted Summary>>> novel architecture memory based attention mechanism multi agent communication \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed sesamebert generalized fine tuning method enables extraction global information among layers squeeze excitation enriches local information capturing neighboring contexts via gaussian blurring\n",
            "Predicted Summary>>> proposed sesamebert generalized fine tuning method enables extraction global information among layers squeeze excitation enriches enriches information capturing neighboring contexts via gaussian \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> prove activation functions satisfying conditions deep network gets wide lengths vectors hidden variables converge length map\n",
            "Predicted Summary>>> prove activation functions satisfying conditions deep network gets wide lengths vectors hidden variables converge length map \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper explores using wearable sensory augmenting technology facilitate first hand perspective taking like cat like whiskers\n",
            "Predicted Summary>>> paper explores using wearable sensory augmenting technology facilitate first hand perspective taking like cat like whiskers \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper hypothesize superficially perturbed data points merely map class map representation\n",
            "Predicted Summary>>> paper hypothesize superficially perturbed data points merely map class map representation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper presents deep learning model combines self organizing maps convolutional neural networks representation learning multi omics data\n",
            "Predicted Summary>>> paper presents deep learning model combines self organizing maps convolutional neural networks representation learning multi omics data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> demonstrate flow based generative models offer viable competitive approach generative modeling video\n",
            "Predicted Summary>>> demonstrate flow based generative models offer viable competitive approach generative modeling video \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> graph regularization forces spectral embedding focus largest clusters making representation less sensitive noise\n",
            "Predicted Summary>>> graph regularization forces spectral embedding focus largest clusters making representation less sensitive \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper studied pubn classification problem incorporate biased negative bn data negative data fully representative true underlying negative distribution positive unlabeled pu learning\n",
            "Predicted Summary>>> paper studied pubn classification problem incorporate biased negative bn data negative data fully representative true underlying negative distribution positive unlabeled pu \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improve saturating activations sigmoid tanh htanh etc binarized neural network bias initialization\n",
            "Predicted Summary>>> improve saturating activations sigmoid tanh htanh etc binarized neural network bias initialization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn high quality denoising using single instances corrupted images training data\n",
            "Predicted Summary>>> learn high quality denoising using single instances corrupted images training data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel matrix completion based algorithm model disease progression events\n",
            "Predicted Summary>>> novel matrix completion based algorithm model disease progression events \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work aiming boosting existing pruning mimic method\n",
            "Predicted Summary>>> work aiming boosting existing pruning mimic method \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce novel way represent graphs multi channel image like structures allows handled vanilla cnns\n",
            "Predicted Summary>>> introduce novel way represent graphs multi channel image like structures allows handled vanilla cnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> method learning image representations good disentangling factors variation obtaining faithful reconstructions\n",
            "Predicted Summary>>> method learning image representations good disentangling factors variation obtaining faithful reconstructions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel marginalized average attentional network weakly supervised temporal action localization\n",
            "Predicted Summary>>> novel marginalized average attentional network weakly supervised temporal action localization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> graphqa graph based method protein quality assessment improves state art hand engineered representation learning approaches\n",
            "Predicted Summary>>> graphqa graph based method protein quality assessment improves state art hand engineered representation learning approaches \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introducing notion optimal representation space provide theoretical argument experimental validation unsupervised model sentences perform well supervised similarity unsupervised transfer tasks\n",
            "Predicted Summary>>> introducing notion optimal representation space provide theoretical argument experimental validation unsupervised model sentences perform well supervised similarity similarity transfer unsupervised tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show adversarial robustness might come cost standard classification performance also yields unexpected benefits\n",
            "Predicted Summary>>> show adversarial robustness might come cost standard classification performance also yields unexpected benefits \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn high quality denoising using single instances corrupted images training data\n",
            "Predicted Summary>>> learn high quality denoising using single instances corrupted images training data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new stream adversarial training approach called robust local features adversarial training rlfat significantly improves adversarially robust generalization standard generalization\n",
            "Predicted Summary>>> propose new stream adversarial training approach called robust local features adversarial training rlfat significantly improves adversarially robust generalization generalization generalization generalization generalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn high dimensional constraints demonstrations sampling unsafe trajectories leveraging known constraint parameterization\n",
            "Predicted Summary>>> learn high dimensional constraints demonstrations sampling unsafe trajectories leveraging known constraint parameterization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel graph neural network architecture based non backtracking matrix defined edge adjacencies demonstrate effectiveness community detection tasks graphs\n",
            "Predicted Summary>>> propose novel graph neural network architecture based non backtracking matrix defined edge adjacencies demonstrate effectiveness community detection tasks graphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show individual units cnn representations learned nlp tasks selectively responsive natural language concepts\n",
            "Predicted Summary>>> show individual units cnn representations learned nlp tasks selectively responsive natural language concepts \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present new routing method capsule networks performs par resnet cifar cifar\n",
            "Predicted Summary>>> present new routing method capsule networks performs par resnet cifar \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose neural hyperlink predictor nhp nhp adapts graph convolutional networks link prediction hypergraphs\n",
            "Predicted Summary>>> propose neural hyperlink predictor nhp nhp adapts graph convolutional networks link prediction hypergraphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use ideas quantum computing proposed word embeddings utilize much fewer trainable parameters\n",
            "Predicted Summary>>> use ideas quantum computing proposed word embeddings utilize much fewer trainable parameters \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simple augmentation method overcomes robustness accuracy trade observed literature opens questions effect training distribution distribution generalization\n",
            "Predicted Summary>>> simple augmentation method overcomes robustness accuracy trade observed literature opens questions questions training distribution distribution generalization generalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present kg reinforcement learning agent builds dynamic knowledge graph exploring generates natural language using template based action space outperforming current agents wide set text based games\n",
            "Predicted Summary>>> present kg reinforcement learning agent builds dynamic knowledge graph exploring generates natural language using template based action space outperforming current agents wide set text based games \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new efficient algorithm construct adversarial examples means deformations rather additive perturbations\n",
            "Predicted Summary>>> propose new efficient algorithm construct adversarial examples means deformations rather additive perturbations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce three generic point cloud processing blocks improve accuracy memory consumption multiple state art networks thus allowing design deeper accurate networks\n",
            "Predicted Summary>>> introduce three generic point cloud processing blocks improve accuracy memory consumption multiple state art networks thus design design deeper networks networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> filter level sparsity emerges implicitly cnns trained adaptive gradient descent approaches due various phenomena extent sparsity inadvertently affected different seemingly unrelated hyperparameters\n",
            "Predicted Summary>>> filter level sparsity emerges implicitly cnns trained adaptive gradient descent approaches due various phenomena extent sparsity inadvertently affected different seemingly seemingly \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep multivariate mixture gaussians model bounding box regression occlusion\n",
            "Predicted Summary>>> deep multivariate mixture gaussians model bounding box regression occlusion \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> network depth increases outlier eigenvalues hessian residual connections mitigate\n",
            "Predicted Summary>>> network depth increases outlier eigenvalues hessian residual connections mitigate \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study introduce novel method relies svd discover number latent dimensions\n",
            "Predicted Summary>>> study introduce novel method relies svd discover number latent dimensions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use model free algorithms like dqn trpo solve short horizon problems model free iteratively policy value iteration fashion\n",
            "Predicted Summary>>> use model free algorithms like dqn trpo solve short horizon problems model free iteratively value iteration \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce online explanation consider cognitive requirement human understanding generated explanation agent\n",
            "Predicted Summary>>> introduce online explanation consider cognitive requirement human understanding generated explanation agent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> empirical evaluation generative adversarial networks\n",
            "Predicted Summary>>> empirical evaluation generative adversarial networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper provides multi stream end end approach learn unified embeddings query response pairs dialogue systems leveraging contextual syntactic semantic external information together\n",
            "Predicted Summary>>> paper provides multi stream end end approach learn unified embeddings query response pairs systems leveraging leveraging semantic semantic information external \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new loss function pca linear autoencoders provably yields ordered exact eigenvectors\n",
            "Predicted Summary>>> new loss function pca linear autoencoders provably yields ordered exact eigenvectors \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop new likelihood free parameter estimation method equivalent maximum likelihood conditions\n",
            "Predicted Summary>>> develop new likelihood free parameter estimation method equivalent maximum likelihood conditions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate convergence popular optimization algorithms like adam rmsprop propose new variants methods provably converge optimal solution convex settings\n",
            "Predicted Summary>>> investigate convergence popular optimization algorithms like adam rmsprop propose new variants methods provably converge optimal solution convex settings \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose four new ways collecting nli data help slightly pretraining data help reduce annotation artifacts\n",
            "Predicted Summary>>> propose four new ways collecting nli data help slightly pretraining data help reduce artifacts \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose set autoencoder model unsupervised representation learning sets elements\n",
            "Predicted Summary>>> propose set autoencoder model unsupervised representation learning sets elements \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use unrolled simulator end end differentiable model protein structure show sometimes hierarchically generalize unseen fold topologies\n",
            "Predicted Summary>>> use unrolled simulator end end differentiable model protein structure show sometimes hierarchically generalize unseen fold topologies \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present method interpreting black box models using instance wise backward selection identify minimal subsets features alone suffice justify particular decision made model\n",
            "Predicted Summary>>> present method interpreting black box models using instance wise backward selection identify minimal subsets features alone suffice justify particular decision decision made made \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep rl order invariant functions used conjunction standard memory modules improve gradient decay resilience noise\n",
            "Predicted Summary>>> deep rl order invariant functions used conjunction standard memory modules improve gradient decay resilience noise \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> structured latent variable approach adds discrete control states within standard autoregressive neural paradigm provide arbitrary grounding internal model decisions without sacrificing representational power neural models\n",
            "Predicted Summary>>> structured latent variable approach adds discrete control states within standard autoregressive neural paradigm provide arbitrary internal model internal internal without \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extend information bottleneck method unsupervised multiview setting show state art results standard datasets\n",
            "Predicted Summary>>> extend information bottleneck method unsupervised multiview setting show state art results standard datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new regularization technique based knowledge distillation\n",
            "Predicted Summary>>> propose new regularization technique based knowledge distillation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> accelerating cnn training pipeline accelerators stale weights\n",
            "Predicted Summary>>> accelerating cnn training pipeline accelerators stale weights \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new type end end trainable attention module applies global weight balances among layers utilizing co propagating rnn cnn\n",
            "Predicted Summary>>> propose new type end end trainable attention module applies global weight balances among utilizing co propagating rnn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> approach perform htn planning using external procedures evaluate predicates runtime semantic attachments\n",
            "Predicted Summary>>> approach perform htn planning using external procedures evaluate predicates runtime semantic attachments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present visual tool interactively explore latent space auto encoder peptide sequences attributes\n",
            "Predicted Summary>>> present visual tool interactively explore latent space auto encoder peptide sequences attributes \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose choco sgd decentralized sgd compressed communication non convex objectives show strong performance various deep learning applications device learning datacenter case\n",
            "Predicted Summary>>> propose choco sgd decentralized sgd compressed communication non convex objectives show strong performance various deep learning applications device learning datacenter case \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> bregman dilemma shown deep learning improvement margins parameterized models may result overfitting dynamics normalized margin distributions proposed predict generalization error identify dilemma\n",
            "Predicted Summary>>> bregman dilemma shown deep learning improvement margins parameterized models may result overfitting dynamics normalized margin distributions predict generalization error identify \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show make predictions using deep networks without training deep networks\n",
            "Predicted Summary>>> show make predictions using deep networks without training deep networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> loss surface neural networks disjoint union regions every local minimum global minimum corresponding region\n",
            "Predicted Summary>>> loss surface neural networks disjoint union regions every local minimum global minimum corresponding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> devise novel depthwise separable graph convolution dsgc generic spatial domain data highly compatible depthwise separable convolution\n",
            "Predicted Summary>>> devise novel depthwise separable graph convolution dsgc generic spatial domain data highly compatible depthwise separable convolution \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper focuses synthetic generation human mobility data urban areas using gans\n",
            "Predicted Summary>>> paper focuses synthetic generation human mobility data urban areas using gans \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> graph neural networks aggregation graph benefit continuous space underlying graph\n",
            "Predicted Summary>>> graph neural networks aggregation graph benefit continuous space underlying graph \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combine hard handcrafted constraints deep prior weak constraint perform seismic imaging reap information posterior distribution leveraging multiplicity data\n",
            "Predicted Summary>>> combine hard handcrafted constraints deep prior weak constraint perform seismic imaging reap information posterior distribution leveraging multiplicity data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop efficient methods train neural embedding models dot product structure reformulating objective function terms generalized gram matrices maintaining estimates matrices\n",
            "Predicted Summary>>> develop efficient methods train neural embedding models dot product structure reformulating objective function terms generalized gram matrices matrices estimates matrices \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel multi task learning framework extracts multi view dependency relationship automatically use guide knowledge transfer among different tasks\n",
            "Predicted Summary>>> propose novel multi task learning framework extracts multi view dependency relationship automatically use guide knowledge knowledge among tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simple text augmentation techniques significantly boost performance text classification tasks especially small datasets\n",
            "Predicted Summary>>> simple text augmentation techniques significantly boost performance text classification tasks especially small datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pointer network architecture ranking items learned click logs\n",
            "Predicted Summary>>> pointer network architecture ranking items learned click logs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> wasserstein autoencoder hyperbolic latent space\n",
            "Predicted Summary>>> wasserstein autoencoder hyperbolic latent space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fast truly scalable full matrix adagrad adam theory adaptive stochastic non convex optimization\n",
            "Predicted Summary>>> fast truly scalable full matrix adagrad adam theory adaptive stochastic non convex optimization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dnn encoder enhanced fm bilinear attention max pooling ctr\n",
            "Predicted Summary>>> dnn encoder enhanced fm bilinear attention max pooling ctr \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces partial grounding tackle problem arises full grounding process translation pddl input task ground representation like strips infeasible due memory time constraints\n",
            "Predicted Summary>>> paper introduces partial grounding tackle problem arises full grounding process translation input input task task representation like strips like infeasible memory due memory \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> finding correspondences domains performing matching mapping iterations\n",
            "Predicted Summary>>> finding correspondences domains performing matching mapping iterations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> reactor combines multiple algorithmic architectural contributions produce agent higher sample efficiency prioritized dueling dqn giving better run time performance\n",
            "Predicted Summary>>> reactor combines multiple algorithmic architectural contributions produce agent higher sample efficiency prioritized dueling dqn giving better run time performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper explores using wearable sensory augmenting technology facilitate first hand perspective taking like cat like whiskers\n",
            "Predicted Summary>>> paper explores using wearable sensory augmenting technology facilitate first hand perspective taking like cat like whiskers \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show contras popular wisdom exploding gradient problem solved limits depth mlps effectively trained show gradients explode resnet handles\n",
            "Predicted Summary>>> show contras popular wisdom exploding gradient problem solved limits depth mlps effectively trained show gradients explode resnet handles \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce attention mechanism improve feature extraction deep active learning al semi supervised setting\n",
            "Predicted Summary>>> introduce attention mechanism improve feature extraction deep active learning al semi supervised setting \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> closed form results deep learning layer decoupling limit applicable residual networks\n",
            "Predicted Summary>>> closed form results deep learning layer decoupling limit applicable residual networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> mmo inspired research game platform studying emergent behaviors large populations complex environment\n",
            "Predicted Summary>>> mmo inspired research game platform studying emergent behaviors large populations complex environment \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new pooling layer gnns learns pool nodes according features graph connectivity dowstream task objective\n",
            "Predicted Summary>>> new pooling layer gnns learns pool nodes according features graph connectivity dowstream task objective \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improving label efficiency multi task learning auditory data\n",
            "Predicted Summary>>> improving label efficiency multi task learning auditory data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present graph wavelet neural network gwnn novel graph convolutional neural network cnn leveraging graph wavelet transform address shortcoming previous spectral graph cnn methods depend graph fourier transform\n",
            "Predicted Summary>>> present graph wavelet neural network gwnn novel graph convolutional neural network cnn leveraging graph wavelet previous transform transform graph shortcoming graph spectral shortcoming graph \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep neural networks trained data augmentation require explicit regularization weight decay dropout exhibit greater adaptaibility changes architecture amount training data\n",
            "Predicted Summary>>> deep neural networks trained data augmentation require explicit regularization weight decay dropout exhibit greater adaptaibility changes architecture amount training training data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> reward estimation game videos\n",
            "Predicted Summary>>> reward estimation game videos \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new regularization technique based knowledge distillation\n",
            "Predicted Summary>>> propose new regularization technique based knowledge distillation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new agi architecture trans sapient performance high level overview omega agi architecture basis data science automation system submitted workshop\n",
            "Predicted Summary>>> new agi architecture trans sapient performance high level overview omega agi architecture basis data science automation system submitted workshop \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper propose generative method multisource domain adaptation based decomposition content style domain factors\n",
            "Predicted Summary>>> paper propose generative method multisource domain adaptation based decomposition content style domain factors \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose use separate exploration policy collect pre adaptation trajectories maml also show using self supervised objective inner loop leads stable training much better performance\n",
            "Predicted Summary>>> propose use separate exploration policy collect pre adaptation trajectories maml also show using self supervised objective inner loop leads stable training much better performance performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent\n",
            "Predicted Summary>>> new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised learning approach separating two structured signals superposition\n",
            "Predicted Summary>>> unsupervised learning approach separating two structured signals superposition \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> make transformer streamable monotonic attention\n",
            "Predicted Summary>>> make transformer streamable monotonic attention \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> check dnn models catastrophic forgetting using new evaluation scheme reflects typical application conditions surprising results\n",
            "Predicted Summary>>> check dnn models catastrophic forgetting using new evaluation scheme reflects typical application conditions surprising results \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> artificial neural networks trained gradient descent capable recapitulating realistic neural activity anatomical organization biological circuit\n",
            "Predicted Summary>>> artificial neural networks trained gradient descent capable recapitulating realistic neural activity anatomical organization biological circuit \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new certified adversarial training method crown ibp achieves state art robustness inf norm adversarial perturbations\n",
            "Predicted Summary>>> propose new certified adversarial training method crown ibp achieves state art robustness inf norm adversarial perturbations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method based adversarial training strategy learn discriminative features unbiased invariant confounder incorporating loss function encourages vanished correlation bias learned features\n",
            "Predicted Summary>>> propose method based adversarial training strategy learn discriminative features unbiased invariant confounder incorporating loss function encourages vanished correlation learned features \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new drl policy algorithm achieving state art performance\n",
            "Predicted Summary>>> propose new drl policy algorithm achieving state art performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> verify deterministic probabilistic properties neural networks using non convex relaxations visible transformations specified generative models\n",
            "Predicted Summary>>> verify deterministic probabilistic properties neural networks using non convex relaxations visible transformations specified generative models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop vaes encoder takes model parameter vector input rapid inference many models\n",
            "Predicted Summary>>> develop vaes encoder takes model parameter vector input rapid inference many models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed double neural framework solve large scale imperfect information game\n",
            "Predicted Summary>>> proposed double neural framework solve large scale imperfect information game \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> empirical analysis explanation particle based gradient estimators approximate inference deep generative models\n",
            "Predicted Summary>>> empirical analysis explanation particle based gradient estimators approximate inference deep generative models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> convert subgraphs structured images classify using deep learning transfer learning caffe achieve stunning results\n",
            "Predicted Summary>>> convert subgraphs structured images classify using deep learning transfer learning caffe achieve stunning results \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper describes three techniques allow non backtracking computationally limited scheduler consider small number alternative activities based resource availability\n",
            "Predicted Summary>>> paper describes three techniques allow non backtracking computationally limited scheduler consider small number alternative activities based resource availability \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method efficient multi objective neural architecture search based lamarckian inheritance evolutionary algorithms\n",
            "Predicted Summary>>> propose method efficient multi objective neural architecture search based lamarckian inheritance evolutionary algorithms \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop analytical method study bayesian inference finite width neural networks find renormalization group flow picture naturally emerges\n",
            "Predicted Summary>>> develop analytical method study bayesian inference finite width neural networks find renormalization group flow picture naturally emerges \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> used cvae type model structure learn directly generate slates whole pages recommendation systems\n",
            "Predicted Summary>>> used cvae type model structure learn directly generate slates whole pages recommendation systems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> address training gans discrete data formulating policy gradient generalizes across divergences\n",
            "Predicted Summary>>> address training gans discrete data formulating policy gradient generalizes across divergences \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze develop computationally efficient implementation jacobian regularization increases classification margins neural networks\n",
            "Predicted Summary>>> analyze develop computationally efficient implementation jacobian regularization increases classification margins neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> give algorithm learning two layer neural network symmetric input distribution\n",
            "Predicted Summary>>> give algorithm learning two layer neural network symmetric input distribution \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study graph generation problem propose powerful deep generative model capable generating arbitrary graphs\n",
            "Predicted Summary>>> study graph generation problem propose powerful deep generative model capable generating arbitrary graphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling suitable approach\n",
            "Predicted Summary>>> pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extension gans combining optimal transport primal form energy distance defined adversarially learned feature space\n",
            "Predicted Summary>>> extension gans combining optimal transport primal form energy distance defined adversarially learned feature space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> prove gradient descent robust label corruption despite parameterization rich dataset model\n",
            "Predicted Summary>>> prove gradient descent robust label corruption despite parameterization rich dataset model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop analytical method study bayesian inference finite width neural networks find renormalization group flow picture naturally emerges\n",
            "Predicted Summary>>> develop analytical method study bayesian inference finite width neural networks find renormalization group flow picture naturally emerges \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> artificial neural networks trained gradient descent capable recapitulating realistic neural activity anatomical organization biological circuit\n",
            "Predicted Summary>>> artificial neural networks trained gradient descent capable recapitulating realistic neural activity anatomical organization biological circuit \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present bayesian inference model infer contrastive explanations ltl specifications describing two sets plan traces differ\n",
            "Predicted Summary>>> present bayesian inference model infer contrastive explanations ltl specifications describing two sets plan traces differ \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel method calibrate knowledge graph embedding models without need negative examples\n",
            "Predicted Summary>>> propose novel method calibrate knowledge graph embedding models without need negative examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> derive new pac bayesian bound unbounded loss functions negative log likelihood\n",
            "Predicted Summary>>> derive new pac bayesian bound unbounded loss functions negative log likelihood \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn convert hand drawn sketch high level program\n",
            "Predicted Summary>>> learn convert hand drawn sketch high level program \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> interactively generating image incrementally growing scene graphs multiple steps using gans preserving contents image generated previous steps\n",
            "Predicted Summary>>> interactively generating image incrementally growing scene graphs multiple steps using gans preserving contents image generated previous steps \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new latent variable model learn latent embeddings high dimensional data\n",
            "Predicted Summary>>> propose new latent variable model learn latent embeddings high dimensional data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose cr nas reallocate engaged computation resources different resolution spatial position\n",
            "Predicted Summary>>> propose cr nas reallocate engaged computation resources different resolution spatial position \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present new routing method capsule networks performs par resnet cifar cifar\n",
            "Predicted Summary>>> present new routing method capsule networks performs par resnet cifar \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> first field show craft effective sparse kernel design three aspects composition performance efficiency\n",
            "Predicted Summary>>> first field show craft effective sparse kernel design three aspects composition performance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> limiting state information default policy improvement performance kl regularized rl framework agent default policy optimized together\n",
            "Predicted Summary>>> limiting state information default policy improvement performance kl regularized rl framework agent default policy optimized together \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simple text augmentation techniques significantly boost performance text classification tasks especially small datasets\n",
            "Predicted Summary>>> simple text augmentation techniques significantly boost performance text classification tasks especially small datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> implementing evaluating episodic memory rl\n",
            "Predicted Summary>>> implementing evaluating episodic memory rl \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> social agents learn talk natural language towards goal\n",
            "Predicted Summary>>> social agents learn talk natural language towards goal \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> dnn encoder enhanced fm bilinear attention max pooling ctr\n",
            "Predicted Summary>>> dnn encoder enhanced fm bilinear attention max pooling ctr \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> demostarte using clinical notes conjuntion icu instruments data improves perfomance icu management benchmark tasks\n",
            "Predicted Summary>>> demostarte using clinical notes conjuntion icu instruments data improves perfomance icu management benchmark tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present method adapting hyperparameters probabilistic models using optimal transport applications robotics\n",
            "Predicted Summary>>> present method adapting hyperparameters probabilistic models using optimal transport applications robotics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed implementation accelerate dnn data parallel training reducing communication bandwidth requirement\n",
            "Predicted Summary>>> proposed implementation accelerate dnn data parallel training reducing communication bandwidth requirement \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce continuous logic network cln novel neural architecture automatically learning loop invariants general smt formulas\n",
            "Predicted Summary>>> introduce continuous logic network cln novel neural architecture automatically learning loop invariants general smt formulas \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> make transformer streamable monotonic attention\n",
            "Predicted Summary>>> make transformer streamable monotonic attention \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new order decoder neural machine translation\n",
            "Predicted Summary>>> new order decoder neural machine translation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> train accurate fully quantized networks using loss function maximizing full precision model accuracy minimizing difference full precision quantized networks\n",
            "Predicted Summary>>> train accurate fully quantized networks using loss function maximizing full precision model accuracy minimizing full precision networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pruning vae proposed search disentangled variables intrinsic dimension\n",
            "Predicted Summary>>> pruning vae proposed search disentangled variables intrinsic dimension \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn neural embeddings graphs hyperbolic instead euclidean space\n",
            "Predicted Summary>>> learn neural embeddings graphs hyperbolic instead euclidean space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new stream adversarial training approach called robust local features adversarial training rlfat significantly improves adversarially robust generalization standard generalization\n",
            "Predicted Summary>>> propose new stream adversarial training approach called robust local features adversarial training rlfat significantly improves adversarially robust generalization generalization generalization generalization generalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose memory network model solve binary lp instances memory information perseved long term use\n",
            "Predicted Summary>>> propose memory network model solve binary lp instances memory information perseved long term use \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> instead learning parameters graphical model data learn inference network answer probabilistic queries\n",
            "Predicted Summary>>> instead learning parameters graphical model data learn inference network answer probabilistic queries \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present long time scale musical audio style transfer algorithm synthesizes audio time domain uses time frequency representations audio\n",
            "Predicted Summary>>> present long time scale musical audio style transfer algorithm synthesizes audio time domain uses time frequency representations audio \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce effective general framework incorporating conditioning information inference based generative models\n",
            "Predicted Summary>>> introduce effective general framework incorporating conditioning information inference based generative models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> querying black box neural network reveals lot information propose novel metamodels effectively extracting information black box\n",
            "Predicted Summary>>> querying black box neural network reveals lot information propose novel metamodels effectively extracting information black box \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose matrix completion based task clustering algorithm deep multi task shot learning settings large numbers diverse tasks\n",
            "Predicted Summary>>> propose matrix completion based task clustering algorithm deep multi task shot learning settings large numbers diverse tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide first theoretical analysis guaranteed recovery one hidden layer neural networks cross entropy loss classification problems\n",
            "Predicted Summary>>> provide first theoretical analysis guaranteed recovery one hidden layer neural networks cross entropy loss classification problems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose warped residual network using parallelizable warp operator forward backward propagation distant layers trains faster original residual neural network\n",
            "Predicted Summary>>> propose warped residual network using parallelizable warp operator forward backward propagation distant layers trains faster original residual network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> loopgan extends cycle length cyclegan enable unaligned sequential transformation two time steps\n",
            "Predicted Summary>>> loopgan extends cycle length cyclegan enable unaligned sequential transformation two time steps \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> even tradeoff infinite data limit adversarial training worse standard accuracy even convex problem\n",
            "Predicted Summary>>> even tradeoff infinite data limit adversarial training worse standard accuracy even convex problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> single episode policy transfer family environments related dynamics via optimized probing rapid inference latent variables immediate execution universal policy\n",
            "Predicted Summary>>> single episode policy transfer family environments related dynamics via optimized probing rapid inference latent variables immediate execution universal policy \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> comparison complex real valued multi layer perceptron respect number real valued parameters\n",
            "Predicted Summary>>> comparison complex real valued multi layer perceptron respect number real valued parameters \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> controlled study role environments respect properties emergent communication protocols\n",
            "Predicted Summary>>> controlled study role environments respect properties emergent communication protocols \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show learnability different neural architectures characterized directly computable measures data complexity\n",
            "Predicted Summary>>> show learnability different neural architectures characterized directly computable measures data complexity \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> preserving differential privacy adversarial learning provable robustness adversarial examples\n",
            "Predicted Summary>>> preserving differential privacy adversarial learning provable robustness adversarial examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose extension multi output learning continuum tasks using operator valued kernels\n",
            "Predicted Summary>>> propose extension multi output learning continuum tasks using operator valued kernels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> perform amortized variational inference latent gaussian process model achieve superior imputation performance multivariate time series missing data\n",
            "Predicted Summary>>> perform amortized variational inference latent gaussian process model achieve superior imputation performance multivariate time series missing data data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fix classifier neural networks without losing accuracy\n",
            "Predicted Summary>>> fix classifier neural networks without losing accuracy \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present new deep architecture varpsom extension time series data vartpsom achieve superior clustering performance compared current deep clustering methods static temporal data\n",
            "Predicted Summary>>> present new deep architecture varpsom extension time series data vartpsom achieve superior clustering performance compared current deep clustering methods static temporal data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analyze gradient propagation deep rnns analysis propose new multi layer deep rnn\n",
            "Predicted Summary>>> analyze gradient propagation deep rnns analysis propose new multi layer deep rnn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose reinforcement learning based variable swapping recomputation algorithm reduce memory cost\n",
            "Predicted Summary>>> propose reinforcement learning based variable swapping recomputation algorithm reduce memory cost \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel approach using mode connectivity loss landscapes mitigate adversarial effects repair tampered models evaluate adversarial robustness\n",
            "Predicted Summary>>> novel approach using mode connectivity loss landscapes mitigate adversarial effects repair tampered models evaluate adversarial robustness \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning preferences plan traces using active learning\n",
            "Predicted Summary>>> learning preferences plan traces using active learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop efficient multi scale approximate attributed network embedding procedures provable properties\n",
            "Predicted Summary>>> develop efficient multi scale approximate attributed network embedding procedures provable properties \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> multiflow network dynamic architecture domain adaptation learns potentially different computational graphs per domain map common representation inference performed domain agnostic fashion\n",
            "Predicted Summary>>> multiflow network dynamic architecture domain adaptation learns potentially different computational graphs per domain map common representation inference performed domain agnostic fashion \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> empirical study provides novel perspective shot learning fine tuning method shows comparable accuracy complex state art methods several classification tasks\n",
            "Predicted Summary>>> empirical study provides novel perspective shot learning fine tuning method shows comparable accuracy complex state art methods several classification tasks tasks tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> prove deep neural networks exponentially efficient shallow ones approximating sparse multivariate polynomials\n",
            "Predicted Summary>>> prove deep neural networks exponentially efficient shallow ones approximating sparse multivariate polynomials \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> fully parallelizable adversarial noise resistant metric learning algorithm theoretical guarantees\n",
            "Predicted Summary>>> fully parallelizable adversarial noise resistant metric learning algorithm theoretical guarantees \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deploy text classification sentiment analysis applications english chinese mw cnn accelerator chip device application scenarios\n",
            "Predicted Summary>>> deploy text classification sentiment analysis applications english chinese mw cnn accelerator chip device application scenarios \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> case study optimal deep learning model uavs\n",
            "Predicted Summary>>> case study optimal deep learning model uavs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed bayesian meta sampling method adapting model uncertainty meta learning\n",
            "Predicted Summary>>> proposed bayesian meta sampling method adapting model uncertainty meta learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show initialize recurrent architectures closed form solution linear autoencoder sequences show advantages approach compared orthogonal rnns\n",
            "Predicted Summary>>> show initialize recurrent architectures closed form solution linear autoencoder sequences show advantages approach compared orthogonal rnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study benefit sharing representation multi task reinforcement learning\n",
            "Predicted Summary>>> study benefit sharing representation multi task reinforcement learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose dynamic convolution method significantly accelerate inference time cnns maintaining accuracy\n",
            "Predicted Summary>>> propose dynamic convolution method significantly accelerate inference time cnns maintaining accuracy \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose meta learning framework learns transferable policy weak supervision solve synthesis tasks different logical specifications grammars\n",
            "Predicted Summary>>> propose meta learning framework learns transferable policy weak supervision solve synthesis tasks different logical specifications grammars \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose light weight enhancement attention neural architecture fusionnet achieve sota squad adversarial squad\n",
            "Predicted Summary>>> propose light weight enhancement attention neural architecture fusionnet achieve sota squad adversarial squad \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> exploit global linearity mixup trained models inference break locality adversarial perturbations\n",
            "Predicted Summary>>> exploit global linearity mixup trained models inference break locality adversarial perturbations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep semantic framework textual search engine document retrieval\n",
            "Predicted Summary>>> deep semantic framework textual search engine document retrieval \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> comparative study generative models continual learning scenarios\n",
            "Predicted Summary>>> comparative study generative models continual learning scenarios \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep architectures point clouds equivariant rotations well translations permutations\n",
            "Predicted Summary>>> deep architectures point clouds equivariant rotations well translations permutations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce simplicial transformer show architecture useful inductive bias logical reasoning context deep reinforcement learning\n",
            "Predicted Summary>>> introduce simplicial transformer show architecture useful inductive bias logical reasoning context deep reinforcement learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combining graph neural networks rnn graph generative model propose novel architecture able learn sequence evolving graphs predict graph topology evolution future timesteps\n",
            "Predicted Summary>>> combining graph neural networks rnn graph generative model propose novel architecture able learn sequence evolving graphs predict graph topology topology timesteps \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce statistical approach assessing neural network robustness provides informative notion robust network rather conventional binary assertion whether property violated\n",
            "Predicted Summary>>> introduce statistical approach assessing neural network robustness provides informative notion robust network rather conventional binary assertion whether property violated \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce class player games suited gradient based methods\n",
            "Predicted Summary>>> introduce class player games suited gradient based methods \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show knowledge transfer techniques improve accuracy low precision networks set new state art accuracy ternary bits precision\n",
            "Predicted Summary>>> show knowledge transfer techniques improve accuracy low precision networks set new state art accuracy ternary bits precision \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> demonstrate large pruned models large sparse outperform smaller dense small dense counterparts identical memory footprint\n",
            "Predicted Summary>>> demonstrate large pruned models large sparse outperform smaller dense small dense counterparts identical memory footprint \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> represent computer program using set simpler programs use representation improve program synthesis techniques\n",
            "Predicted Summary>>> represent computer program using set simpler programs use representation improve program synthesis techniques \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gans shown provide us new effective robust mean estimate agnostic contaminations statistical optimality practical tractability\n",
            "Predicted Summary>>> gans shown provide us new effective robust mean estimate agnostic contaminations statistical optimality practical tractability \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> progressively growing available action space great curriculum learning agents\n",
            "Predicted Summary>>> progressively growing available action space great curriculum learning agents \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study functioning autoencoders simple setting advise new strategies regularisation order obtain bettre generalisation latent interpolation mind image sythesis\n",
            "Predicted Summary>>> study functioning autoencoders simple setting advise new strategies regularisation order obtain bettre generalisation latent interpolation mind image sythesis \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> attribution sometimes misleading\n",
            "Predicted Summary>>> attribution sometimes misleading \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> data augmented relation extraction gpt\n",
            "Predicted Summary>>> data augmented relation extraction gpt \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> proposed unified generative adversarial networks gan framework learn noise aware knowledge graph embedding\n",
            "Predicted Summary>>> proposed unified generative adversarial networks gan framework learn noise aware knowledge graph embedding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present evidence lms capture common sense state art results winograd schema challenge commonsense knowledge mining\n",
            "Predicted Summary>>> present evidence lms capture common sense state art results winograd schema challenge commonsense knowledge mining \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> flow based autoregressive model molecular graph generation reaching state art results molecule generation properties optimization\n",
            "Predicted Summary>>> flow based autoregressive model molecular graph generation reaching state art results molecule generation properties optimization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> build artificial agents solve social dilemmas situations individuals face temptation increase payoffs cost total welfare\n",
            "Predicted Summary>>> build artificial agents solve social dilemmas situations individuals face temptation increase payoffs cost total welfare \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> large scale multi task architecture solves imagenet translation together shows transfer learning\n",
            "Predicted Summary>>> large scale multi task architecture solves imagenet translation together shows transfer learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> inference large transformers expensive due self attention multiple layers show simple decomposition technique yield faster low memory footprint model accurate original models\n",
            "Predicted Summary>>> inference large transformers expensive due self attention multiple layers show simple decomposition technique yield faster memory memory model model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> attribution sometimes misleading\n",
            "Predicted Summary>>> attribution sometimes misleading \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring roughly computational cost\n",
            "Predicted Summary>>> study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring roughly computational cost \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> generative modeling need adversarial training\n",
            "Predicted Summary>>> generative modeling need adversarial training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> training dnns interface black box functions intermediate labels using estimator sub network replaced black box training\n",
            "Predicted Summary>>> training dnns interface black box functions intermediate labels using estimator sub network replaced black box training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show transformer architecture neural gpu turing complete\n",
            "Predicted Summary>>> show transformer architecture neural gpu turing complete \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> semi supervised cross lingual document classification\n",
            "Predicted Summary>>> semi supervised cross lingual document classification \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose ae based gan alleviates mode collapse gans\n",
            "Predicted Summary>>> propose ae based gan alleviates mode collapse gans \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method stochastically optimize second order penalties show may apply training fairness aware classifiers\n",
            "Predicted Summary>>> propose method stochastically optimize second order penalties show may apply training fairness aware classifiers \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> decoding pixels still work representation learning images\n",
            "Predicted Summary>>> decoding pixels still work representation learning images \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study problem learning optimizing physical simulations via differentiable programming using proposed diffsim programming language compiler\n",
            "Predicted Summary>>> study problem learning optimizing physical simulations via differentiable programming using proposed diffsim programming language compiler \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose agile framework training agents perform instructions examples respective goal states\n",
            "Predicted Summary>>> propose agile framework training agents perform instructions examples respective goal states \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> method learning better representations acts regularizer despite significant additional computation cost achieves improvements strong baselines supervised semi supervised learning tasks\n",
            "Predicted Summary>>> method learning better representations acts regularizer despite significant additional computation cost achieves improvements strong baselines supervised supervised tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> cnn model pruning method using ista rescaling trick enforce sparsity scaling parameters batch normalization\n",
            "Predicted Summary>>> cnn model pruning method using ista rescaling trick enforce sparsity scaling parameters batch normalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> model based planning component improves rl based semantic parsing wikitablequestions\n",
            "Predicted Summary>>> model based planning component improves rl based semantic parsing wikitablequestions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper proposed novel algorithm gendice general stationary distribution correction estimation handle discounted average policy evaluation multiple behavior agnostic samples\n",
            "Predicted Summary>>> paper proposed novel algorithm gendice general stationary distribution correction estimation handle discounted average policy evaluation multiple behavior agnostic samples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce underparameterized nonconvolutional simple deep neural network without training effectively represent natural images solve image processing tasks like compression denoising competitively\n",
            "Predicted Summary>>> introduce underparameterized nonconvolutional simple deep neural network without training effectively represent natural images solve image processing tasks like compression denoising \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work enforces hamiltonian dynamics control learn system models embedded position velocity data exploits physically consistent dynamics synthesize model based control via energy shaping\n",
            "Predicted Summary>>> work enforces hamiltonian dynamics control learn system models embedded position velocity data exploits physically consistent dynamics model model based control energy \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose nearest neighbor overlap procedure quantifies similarity embedders task agnostic manner use compare sentence embedders\n",
            "Predicted Summary>>> propose nearest neighbor overlap procedure quantifies similarity embedders task agnostic manner use compare sentence embedders \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling suitable approach\n",
            "Predicted Summary>>> pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> inspired capsnet propose novel architecture graph embeddings basis node features extracted gnn\n",
            "Predicted Summary>>> inspired capsnet propose novel architecture graph embeddings basis node features extracted gnn \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel graph neural network architecture based non backtracking matrix defined edge adjacencies demonstrate effectiveness community detection tasks graphs\n",
            "Predicted Summary>>> propose novel graph neural network architecture based non backtracking matrix defined edge adjacencies demonstrate effectiveness community detection tasks graphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose scalable method approximate eigenvectors laplacian reinforcement learning context show learned representations improve performance rl agent\n",
            "Predicted Summary>>> propose scalable method approximate eigenvectors laplacian reinforcement learning context show learned representations improve performance agent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> better adversarial training learning map back data manifold autoencoders hidden states\n",
            "Predicted Summary>>> better adversarial training learning map back data manifold autoencoders hidden states \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new monte carlo tree search rollout algorithm relies width based search construct lookahead\n",
            "Predicted Summary>>> propose new monte carlo tree search rollout algorithm relies width based search construct lookahead \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> causally correct partial models generate whole observation remain causally correct stochastic environments\n",
            "Predicted Summary>>> causally correct partial models generate whole observation remain causally correct stochastic environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper aims provide empirical answer question whether well trained dialogue response model output malicious responses\n",
            "Predicted Summary>>> paper aims provide empirical answer question whether well trained dialogue response model output malicious responses \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method computing adversarially robust representations entirely unsupervised way\n",
            "Predicted Summary>>> propose method computing adversarially robust representations entirely unsupervised way \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn gans noisy distorted partial observations\n",
            "Predicted Summary>>> learn gans noisy distorted partial observations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> using dsl grammar reinforcement learning improve synthesis programs complex control flow\n",
            "Predicted Summary>>> using dsl grammar reinforcement learning improve synthesis programs complex control flow \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> describe neuro ai interface technique evaluate generative adversarial networks\n",
            "Predicted Summary>>> describe neuro ai interface technique evaluate generative adversarial networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper improves existing sample based evaluation gans contains insightful experiments\n",
            "Predicted Summary>>> paper improves existing sample based evaluation gans contains insightful experiments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> graph based recurrent retriever learns retrieve reasoning paths wikipedia graph outperforms recent state art hotpotqa points\n",
            "Predicted Summary>>> graph based recurrent retriever learns retrieve reasoning paths wikipedia graph outperforms recent state art hotpotqa points \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unsupervised method detect adversarial samples autoencoder activations reconstruction error space\n",
            "Predicted Summary>>> unsupervised method detect adversarial samples autoencoder activations reconstruction error space \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> derive nesterov method arises straightforward discretization ode different one su boyd candes prove acceleration stochastic case\n",
            "Predicted Summary>>> derive nesterov method arises straightforward discretization ode different one su boyd candes prove acceleration stochastic case \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> reinforcement learning based conversational search assistant provides contextual assistance subjective search like digital assets\n",
            "Predicted Summary>>> reinforcement learning based conversational search assistant provides contextual assistance subjective search like digital assets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> describe two end end autoencoding parsers semi supervised graph based dependency parsing\n",
            "Predicted Summary>>> describe two end end autoencoding parsers semi supervised graph based dependency parsing \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method make use multiple passages information open domain qa\n",
            "Predicted Summary>>> propose method make use multiple passages information open domain qa \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> finding shed lights preventing cancer progression\n",
            "Predicted Summary>>> finding shed lights preventing cancer progression \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper presents computational model efficient human postural control adaptation based hierarchical acquisition functions well known features\n",
            "Predicted Summary>>> paper presents computational model efficient human postural control adaptation based hierarchical acquisition functions well known features \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use deep learning techniques solve sparse signal representation recovery problem\n",
            "Predicted Summary>>> use deep learning techniques solve sparse signal representation recovery problem \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> prove deep neural networks exponentially efficient shallow ones approximating sparse multivariate polynomials\n",
            "Predicted Summary>>> prove deep neural networks exponentially efficient shallow ones approximating sparse multivariate polynomials \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use simple biologically motivated modifications standard learning techniques achieve state art performance catastrophic forgetting benchmarks\n",
            "Predicted Summary>>> use simple biologically motivated modifications standard learning techniques achieve state art performance catastrophic forgetting benchmarks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> parameters trained neural network permuted produce completely separate model different task enabling embedding trojan horse networks inside another network\n",
            "Predicted Summary>>> parameters trained neural network permuted produce completely separate model different task enabling embedding trojan horse networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> existing deep convolutional networks image classification tasks sensitive gabor noise patterns small structured changes input cause large changes output\n",
            "Predicted Summary>>> existing deep convolutional networks image classification tasks sensitive gabor noise patterns small structured changes input large changes large output output \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop efficient multi scale approximate attributed network embedding procedures provable properties\n",
            "Predicted Summary>>> develop efficient multi scale approximate attributed network embedding procedures provable properties \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep learning adaptation randomized least squares value iteration\n",
            "Predicted Summary>>> deep learning adaptation randomized least squares value iteration \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show working memory input reservoir network makes local reward modulated hebbian rule perform well recursive least squares aka force\n",
            "Predicted Summary>>> show working memory input reservoir network makes local reward modulated hebbian rule perform well recursive least squares aka force force \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simple rnn based meta learner achieves sota performance popular benchmarks\n",
            "Predicted Summary>>> simple rnn based meta learner achieves sota performance popular benchmarks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> describing semantic heuristics builds upon owl service description uses word sentence distance measures evaluate usefulness services given goal\n",
            "Predicted Summary>>> describing semantic heuristics builds upon owl service description uses word sentence distance measures evaluate usefulness services goal goal \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> solve specific sr issue low quality jpg images functional sub models\n",
            "Predicted Summary>>> solve specific sr issue low quality jpg images functional sub models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> consider tackling single agent rl problem distributing learners\n",
            "Predicted Summary>>> consider tackling single agent rl problem distributing learners \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate strong regularization fails propose method achieve strong regularization\n",
            "Predicted Summary>>> investigate strong regularization fails propose method achieve strong regularization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> extend wake sleep algorithm use learn learn structured models examples\n",
            "Predicted Summary>>> extend wake sleep algorithm use learn learn structured models examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel cnn structure video level representation learning surpassing recent cnns\n",
            "Predicted Summary>>> novel cnn structure video level representation learning surpassing recent cnns \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose dvd gan large video generative model state art several tasks produces highly complex videos trained large real world datasets\n",
            "Predicted Summary>>> propose dvd gan large video generative model state art several tasks produces highly complex videos trained large real world datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simplex based geometric method proposed cope shot learning problems\n",
            "Predicted Summary>>> simplex based geometric method proposed cope shot learning problems \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> see abstract revision paper identical except page supplementary material serve stand along technical report version paper\n",
            "Predicted Summary>>> see abstract revision paper identical except page supplementary material serve stand along technical report version paper \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel gram gauss newton method train neural networks inspired neural tangent kernel gauss newton method fast convergence speed theoretically experimentally\n",
            "Predicted Summary>>> novel gram gauss newton method train neural networks inspired neural tangent kernel gauss newton method fast convergence speed theoretically experimentally \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> recovery guarantee stochastic gradient descent random initialization learning two layer neural network two hidden nodes unit norm weights relu activation functions gaussian inputs\n",
            "Predicted Summary>>> recovery guarantee stochastic gradient descent random initialization learning two layer neural network two hidden nodes unit norm weights relu activation activation functions functions gaussian inputs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose flow mechanism end end architecture flowqa achieves sota two conversational qa datasets sequential instruction understanding task\n",
            "Predicted Summary>>> propose flow mechanism end end architecture flowqa achieves sota two conversational qa datasets sequential instruction understanding task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> constructively prove even slightest nonlinear activation functions introduce spurious local minima general datasets activation functions\n",
            "Predicted Summary>>> constructively prove even slightest nonlinear activation functions introduce spurious local minima general datasets activation functions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel meta learning model adaptively balances effect meta learning task specific learning also class specific learning within task\n",
            "Predicted Summary>>> novel meta learning model adaptively balances effect meta learning task specific learning also class specific learning within task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> investigate framework discovery curating large collection predictions used construct agent representation partially observable domains\n",
            "Predicted Summary>>> investigate framework discovery curating large collection predictions used construct agent representation partially observable domains \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> apply monte carlo tree search episode generation alpha zero\n",
            "Predicted Summary>>> apply monte carlo tree search episode generation alpha zero \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> isparse eliminates irrelevant insignificant network edges minimal impact network performance determining edge importance final network output\n",
            "Predicted Summary>>> isparse eliminates irrelevant insignificant network edges minimal impact network performance determining edge importance final network output \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study problem continuous control agents deep rl adversarial attacks proposed two step algorithm based learned model dynamics\n",
            "Predicted Summary>>> study problem continuous control agents deep rl adversarial attacks proposed two step algorithm based learned model dynamics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce extra data dependent gaussian prior objective augment current mle training designed capture prior knowledge ground truth data\n",
            "Predicted Summary>>> introduce extra data dependent gaussian prior objective augment current mle training designed capture prior knowledge ground truth data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> meta learning self proposed task distributions speed reinforcement learning without human specified task distributions\n",
            "Predicted Summary>>> meta learning self proposed task distributions speed reinforcement learning without human specified task distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combining auxiliary adversarial training interrogate help physical understanding\n",
            "Predicted Summary>>> combining auxiliary adversarial training interrogate help physical understanding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> general easy use framework improves adversarial robustness deep classification models embedding regularization\n",
            "Predicted Summary>>> general easy use framework improves adversarial robustness deep classification models embedding regularization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new framework preconditioner learning derive new forms preconditioners learning methods reveal relationship methods like rmsprop adam adagrad esgd kfac batch normalization etc\n",
            "Predicted Summary>>> propose new framework preconditioner learning derive new forms preconditioners learning methods reveal relationship methods like rmsprop adam adagrad esgd adagrad kfac batch \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces partial grounding tackle problem arises full grounding process translation pddl input task ground representation like strips infeasible due memory time constraints\n",
            "Predicted Summary>>> paper introduces partial grounding tackle problem arises full grounding process translation input input task task ground representation like infeasible infeasible memory due due memory \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> conditional entropy bottleneck information theoretic objective function learning optimal representations\n",
            "Predicted Summary>>> conditional entropy bottleneck information theoretic objective function learning optimal representations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present lsh softmax softmax approximation layer sub linear learning inference strong theoretical guarantees showcase applicability efficiency evaluating real world task language modeling\n",
            "Predicted Summary>>> present lsh softmax softmax approximation layer sub linear learning inference strong theoretical guarantees showcase applicability efficiency evaluating real task \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> effective regularization optimization strategies lstm based language models achieves sota ptb wt\n",
            "Predicted Summary>>> effective regularization optimization strategies lstm based language models achieves sota ptb wt \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose adversarial inverse reinforcement learning algorithm capable learning reward functions transfer new unseen environments\n",
            "Predicted Summary>>> propose adversarial inverse reinforcement learning algorithm capable learning reward functions transfer new unseen environments \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> theory connecting hessian solution generalization power model\n",
            "Predicted Summary>>> theory connecting hessian solution generalization power model \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> architecture enables cnn trained video sequences converging rapidly\n",
            "Predicted Summary>>> architecture enables cnn trained video sequences converging rapidly \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present rl agent minerva learns walk knowledge graph answer queries\n",
            "Predicted Summary>>> present rl agent minerva learns walk knowledge graph answer queries \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling suitable approach\n",
            "Predicted Summary>>> pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning optimal mapping deepnn distributions along theoretical guarantees\n",
            "Predicted Summary>>> learning optimal mapping deepnn distributions along theoretical guarantees \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present method adapting hyperparameters probabilistic models using optimal transport applications robotics\n",
            "Predicted Summary>>> present method adapting hyperparameters probabilistic models using optimal transport applications robotics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling suitable approach\n",
            "Predicted Summary>>> pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> graph neural network able automatically learn leverage dynamic interactive graph structure\n",
            "Predicted Summary>>> graph neural network able automatically learn leverage dynamic interactive graph structure \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show relatively simple black box adversarial attack scheme using bayesian optimization dimension upsampling preferable existing methods number available queries low\n",
            "Predicted Summary>>> show relatively simple black box adversarial attack scheme using bayesian optimization dimension upsampling preferable existing methods methods methods number low \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> improve existing dialogue systems responding people sharing personal stories incorporating emotion prediction representations also release new benchmark dataset empathetic dialogues\n",
            "Predicted Summary>>> improve existing dialogue systems responding people sharing personal stories incorporating emotion prediction representations also release new benchmark dataset empathetic dialogues \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> noise robust deep learning architecture\n",
            "Predicted Summary>>> noise robust deep learning architecture \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide pac bayes based generalization guarantee uncompressed deterministic deep networks generalizing noise resilience network training data test data\n",
            "Predicted Summary>>> provide pac bayes based generalization guarantee uncompressed deterministic deep networks generalizing noise resilience network training data test data \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper develops principled method continual learning deep models\n",
            "Predicted Summary>>> paper develops principled method continual learning deep models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> study representations phonology neural network models spoken language several variants analytical techniques\n",
            "Predicted Summary>>> study representations phonology neural network models spoken language several variants analytical techniques \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> variation network generative model able learn high level attributes without supervision used controlled input manipulation\n",
            "Predicted Summary>>> variation network generative model able learn high level attributes without supervision used controlled input manipulation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> work aiming boosting existing pruning mimic method\n",
            "Predicted Summary>>> work aiming boosting existing pruning mimic method \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present approach redesign environment uninterpretable agent behaviors minimized eliminated\n",
            "Predicted Summary>>> present approach redesign environment uninterpretable agent behaviors minimized eliminated \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new unsupervised machine translation model learn without using parallel corpora experimental results show impressive performance multiple corpora pairs languages\n",
            "Predicted Summary>>> propose new unsupervised machine translation model learn without using parallel corpora experimental results show impressive performance multiple corpora languages \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> hybrid model utilizing raw audio spectrogram information speech enhancement tasks\n",
            "Predicted Summary>>> hybrid model utilizing raw audio spectrogram information speech enhancement tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use machine learning generate synonyms large shopping taxonomies\n",
            "Predicted Summary>>> use machine learning generate synonyms large shopping taxonomies \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> state art complex text sql parsing combining hard soft relational reasoning schema question encoding\n",
            "Predicted Summary>>> state art complex text sql parsing combining hard soft relational reasoning schema question encoding \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show using semi parametric prior estimations speed hpo significantly across datasets metrics\n",
            "Predicted Summary>>> show using semi parametric prior estimations speed hpo significantly across datasets metrics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> ensemble method reinforcement learning weights functions based accumulated td errors\n",
            "Predicted Summary>>> ensemble method reinforcement learning weights functions based accumulated td errors \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new large batch training algorithm based layer wise adaptive rate scaling lars using lars scaled alexnet resnet batch\n",
            "Predicted Summary>>> new large batch training algorithm based layer wise adaptive rate scaling lars using lars scaled alexnet resnet batch \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use simple biologically motivated modifications standard learning techniques achieve state art performance catastrophic forgetting benchmarks\n",
            "Predicted Summary>>> use simple biologically motivated modifications standard learning techniques achieve state art performance catastrophic forgetting benchmarks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> training dnns interface black box functions intermediate labels using estimator sub network replaced black box training\n",
            "Predicted Summary>>> training dnns interface black box functions intermediate labels using estimator sub network replaced black box training \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces neuromodulation artificial neural networks\n",
            "Predicted Summary>>> paper introduces neuromodulation artificial neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new methods evaluating quantifying quality synthetic gan distributions perspective classification tasks\n",
            "Predicted Summary>>> propose new methods evaluating quantifying quality synthetic gan distributions perspective classification tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose model agnostic way leverage bert text generation achieve improvements transformer tasks datasets\n",
            "Predicted Summary>>> propose model agnostic way leverage bert text generation achieve improvements transformer tasks datasets \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper uses principles field calibration machine learning logits neural network defend adversarial attacks\n",
            "Predicted Summary>>> paper uses principles field calibration machine learning logits neural network defend adversarial attacks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> conventional memory networks generate many redundant latent vectors resulting overfitting need larger memories introduce memory dropout automatic technique encourages diversity latent space\n",
            "Predicted Summary>>> conventional memory networks generate many redundant latent vectors resulting overfitting need larger memories memory automatic automatic technique diversity latent \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose janossy pooling method learning deep permutation invariant functions designed exploit relationships within input sequence tractable inference strategies stochastic optimization procedure call pisgd\n",
            "Predicted Summary>>> propose janossy pooling method learning deep permutation invariant functions designed exploit relationships within input sequence tractable inference strategies stochastic optimization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> utilized within subjects study evaluate four paced breathing visuals common mobile apps understand effective providing breathing exercise guidance\n",
            "Predicted Summary>>> utilized within subjects study evaluate four paced breathing visuals common mobile apps understand effective providing breathing exercise guidance \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> strategy repair damaged neural networks\n",
            "Predicted Summary>>> strategy repair damaged neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose method efficient multi objective neural architecture search based lamarckian inheritance evolutionary algorithms\n",
            "Predicted Summary>>> propose method efficient multi objective neural architecture search based lamarckian inheritance evolutionary algorithms \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use gan based method scalably solve optimal transport\n",
            "Predicted Summary>>> use gan based method scalably solve optimal transport \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combine search reinforcement learning speed machine learning code\n",
            "Predicted Summary>>> combine search reinforcement learning speed machine learning code \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show neural encoding models trained capture signal spiking variability neural population data using gans\n",
            "Predicted Summary>>> show neural encoding models trained capture signal spiking variability neural population data using gans \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> explaining multivariate time series models finding important observations time using counterfactuals\n",
            "Predicted Summary>>> explaining multivariate time series models finding important observations time using counterfactuals \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> granularity controled multi domain multimodal image image translation method\n",
            "Predicted Summary>>> granularity controled multi domain multimodal image image translation method \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper provides rigorous study variance reduced td learning characterizes advantage vanilla td learning\n",
            "Predicted Summary>>> paper provides rigorous study variance reduced td learning characterizes advantage vanilla td learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> derive new pac bayesian bound unbounded loss functions negative log likelihood\n",
            "Predicted Summary>>> derive new pac bayesian bound unbounded loss functions negative log likelihood \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> comparison complex real valued multi layer perceptron respect number real valued parameters\n",
            "Predicted Summary>>> comparison complex real valued multi layer perceptron respect number real valued parameters \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new pretraining method establishes new state art results glue race squad benchmarks fewer parameters compared bert large\n",
            "Predicted Summary>>> new pretraining method establishes new state art results glue race squad benchmarks fewer parameters compared large \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> approximate determinantal point processes neural nets justify model theoretically empirically\n",
            "Predicted Summary>>> approximate determinantal point processes neural nets justify model theoretically empirically \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn efficient lossy image codec optimized facilitate reliable photo manipulation detection fractional cost payload quality even low bitrates\n",
            "Predicted Summary>>> learn efficient lossy image codec optimized facilitate reliable photo manipulation detection fractional cost payload quality even low \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> adapt family combinatorial games tunable difficulty optimal policy expressible linear network developing rich environment reinforcement learning showing contrasts performance supervised learning analyzing multiagent learning generalization\n",
            "Predicted Summary>>> adapt family combinatorial games tunable difficulty optimal policy expressible linear network developing rich environment reinforcement learning showing contrasts performance supervised learning analyzing multiagent learning generalization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper studies weakly supervised knowledge graph alignment adversarial training frameworks\n",
            "Predicted Summary>>> paper studies weakly supervised knowledge graph alignment adversarial training frameworks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> goal recognition approach based operator counting heuristics used account noise dataset\n",
            "Predicted Summary>>> goal recognition approach based operator counting heuristics used account noise dataset \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present rl agent minerva learns walk knowledge graph answer queries\n",
            "Predicted Summary>>> present rl agent minerva learns walk knowledge graph answer queries \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show ways train hierarchical video prediction model without needing pose labels\n",
            "Predicted Summary>>> show ways train hierarchical video prediction model without needing pose labels \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> comparative study generative models continual learning scenarios\n",
            "Predicted Summary>>> comparative study generative models continual learning scenarios \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep rl order invariant functions used conjunction standard memory modules improve gradient decay resilience noise\n",
            "Predicted Summary>>> deep rl order invariant functions used conjunction standard memory modules improve gradient decay resilience noise \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces cloudlstm new branch recurrent neural models tailored forecasting data streams generated geospatial point cloud sources\n",
            "Predicted Summary>>> paper introduces cloudlstm new branch recurrent neural models tailored forecasting data streams generated geospatial point cloud sources \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new order decoder neural machine translation\n",
            "Predicted Summary>>> new order decoder neural machine translation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce continual learning setup based language modelling explicit task segmentation signal given propose neural network model growing long term memory tackle\n",
            "Predicted Summary>>> introduce continual learning setup based language modelling explicit task segmentation signal given propose neural network model growing long term memory tackle \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> train accurate fully quantized networks using loss function maximizing full precision model accuracy minimizing difference full precision quantized networks\n",
            "Predicted Summary>>> train accurate fully quantized networks using loss function maximizing full precision model accuracy minimizing full precision precision \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce novel larger context language model simultaneously captures syntax semantics making capable generating highly interpretable sentences paragraphs\n",
            "Predicted Summary>>> introduce novel larger context language model simultaneously captures syntax semantics making capable generating highly interpretable sentences paragraphs \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> model agnostic regularization scheme neural network based conditional density estimation\n",
            "Predicted Summary>>> model agnostic regularization scheme neural network based conditional density estimation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learning preferences plan traces using active learning\n",
            "Predicted Summary>>> learning preferences plan traces using active learning \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper presents theoretical framework models data distribution explicitly deep locally connected relu network\n",
            "Predicted Summary>>> paper presents theoretical framework models data distribution explicitly deep locally connected relu network \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce gaussian process prior weights neural network explore ability model input dependent weights benefits various tasks including uncertainty estimation generalization low sample setting\n",
            "Predicted Summary>>> introduce gaussian process prior weights neural network explore ability model input dependent weights benefits various tasks including uncertainty estimation generalization low sample \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> unified min max optimization framework adversarial attack defense\n",
            "Predicted Summary>>> unified min max optimization framework adversarial attack defense \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose novel combination adversarial training provable defenses produces model state art accuracy certified robustness cifar\n",
            "Predicted Summary>>> propose novel combination adversarial training provable defenses produces model state art accuracy certified robustness cifar \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> neural architecture search series natural language understanding tasks design search space nlu tasks apply differentiable architecture search discover new models\n",
            "Predicted Summary>>> neural architecture search series natural language understanding tasks design search space nlu tasks apply differentiable architecture search discover \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show first successful use transformer generating music exhibits long term structure\n",
            "Predicted Summary>>> show first successful use transformer generating music exhibits long term structure \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper identifies classes problems adversarial examples inescapable derives fundamental bounds susceptibility classifier adversarial examples\n",
            "Predicted Summary>>> paper identifies classes problems adversarial examples inescapable derives fundamental bounds susceptibility classifier adversarial examples \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new class optimizers accelerated non convex optimization via nonlinear gradient transformation\n",
            "Predicted Summary>>> propose new class optimizers accelerated non convex optimization via nonlinear gradient transformation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose swap distributed algorithm large batch training neural networks\n",
            "Predicted Summary>>> propose swap distributed algorithm large batch training neural networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> solve tasks involving vision guided humanoid locomotion reusing locomotion behavior motion capture data\n",
            "Predicted Summary>>> solve tasks involving vision guided humanoid locomotion reusing locomotion behavior motion capture \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> train neural network agents develop language compositional properties raw pixel input\n",
            "Predicted Summary>>> train neural network agents develop language compositional properties raw pixel input \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose data augmentation approach meta learning prove valid\n",
            "Predicted Summary>>> propose data augmentation approach meta learning prove valid \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose extension batch normalization show first kind convergence analysis extension show numerical experiments better performance original batch normalizatin\n",
            "Predicted Summary>>> propose extension batch normalization show first kind convergence analysis extension show numerical experiments better performance original batch normalizatin \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> use mcts optimize bootstrapped policy continuous action spaces policy iteration setting\n",
            "Predicted Summary>>> use mcts optimize bootstrapped policy continuous action spaces policy iteration setting \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> provide efficient convergence rate gradient descent complete orthogonal dictionary learning objective based geometric analysis\n",
            "Predicted Summary>>> provide efficient convergence rate gradient descent complete orthogonal dictionary learning objective based geometric analysis \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose generative autoencoder learn expressive posterior conditional likelihood distributions using implicit distributions train model using new formulation elbo\n",
            "Predicted Summary>>> propose generative autoencoder learn expressive posterior conditional likelihood distributions using implicit distributions train model using new formulation elbo \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> propose new algorithm based optimal transport train cnn ssl fashion\n",
            "Predicted Summary>>> propose new algorithm based optimal transport train cnn ssl fashion \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> show pre training untrained neural network examples improve reconstruction results compressed sensing semantic recovery problems like colorization\n",
            "Predicted Summary>>> show pre training untrained neural network examples improve reconstruction results compressed sensing semantic recovery problems like colorization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> present simple modification alternating sgd method called prediction step improves stability adversarial networks\n",
            "Predicted Summary>>> present simple modification alternating sgd method called prediction step improves stability adversarial networks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new algorithm train deep neural networks tested optimization functions mnist\n",
            "Predicted Summary>>> new algorithm train deep neural networks tested optimization functions mnist \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> convolutional neural networks behave compositional nearest neighbors\n",
            "Predicted Summary>>> convolutional neural networks behave compositional nearest neighbors \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> develop general framework establish certified robustness ml models various classes adversarial perturbations\n",
            "Predicted Summary>>> develop general framework establish certified robustness ml models various classes adversarial perturbations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> explored visualization designs support chronic patients present review health data healthcare providers clinical visits\n",
            "Predicted Summary>>> explored visualization designs support chronic patients present review health data healthcare providers clinical visits \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> simulation real images translation video generation\n",
            "Predicted Summary>>> simulation real images translation video generation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> find deep models crucial maml work propose method enables effective meta learning smaller models\n",
            "Predicted Summary>>> find deep models crucial maml work propose method enables effective meta learning smaller models \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> learn space motor primitives unannotated robot demonstrations show primitives semantically meaningful composed new robot tasks\n",
            "Predicted Summary>>> learn space motor primitives unannotated robot demonstrations show primitives semantically meaningful composed new robot tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> using compressing tecniques encoding words possibility faster training cnn dimensionality reduction representation\n",
            "Predicted Summary>>> using compressing tecniques encoding words possibility faster training cnn dimensionality reduction representation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> meta learning self proposed task distributions speed reinforcement learning without human specified task distributions\n",
            "Predicted Summary>>> meta learning self proposed task distributions speed reinforcement learning without human specified task distributions \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduced novel gradient estimator using stein method compared methods learning implicit models approximate inference image generation\n",
            "Predicted Summary>>> introduced novel gradient estimator using stein method compared methods learning implicit models approximate inference image generation \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> novel marginalized average attentional network weakly supervised temporal action localization\n",
            "Predicted Summary>>> novel marginalized average attentional network weakly supervised temporal action localization \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> new methodology variational marginal inference permutations based sinkhorn algorithm applied probabilistic identification neurons\n",
            "Predicted Summary>>> new methodology variational marginal inference permutations based sinkhorn algorithm applied probabilistic identification neurons \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> high performance robotics simulation algorithm development framework\n",
            "Predicted Summary>>> high performance robotics simulation algorithm development framework \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> address training gans discrete data formulating policy gradient generalizes across divergences\n",
            "Predicted Summary>>> address training gans discrete data formulating policy gradient generalizes across divergences \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> deep net deep neural network cyber security use cases\n",
            "Predicted Summary>>> deep net deep neural network cyber security use cases \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> demostarte using clinical notes conjuntion icu instruments data improves perfomance icu management benchmark tasks\n",
            "Predicted Summary>>> demostarte using clinical notes conjuntion icu instruments data improves perfomance icu management benchmark tasks \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> paper introduces physics prior deep learning applies resulting network topology model based control\n",
            "Predicted Summary>>> paper introduces physics prior deep learning applies resulting network topology model based control \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> combine kernel method connectionist models show resulting deep architectures trained layer wise transparent learning dynamics\n",
            "Predicted Summary>>> combine kernel method connectionist models show resulting deep architectures trained layer wise transparent learning dynamics \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> analytical formulation equatorial standing wave phenomena application qbo enso\n",
            "Predicted Summary>>> analytical formulation equatorial standing wave phenomena application qbo enso \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> gaussian normalization performs least squares fit back propagation zero centers decorrelates partial derivatives normalized activations\n",
            "Predicted Summary>>> gaussian normalization performs least squares fit back propagation zero centers decorrelates partial derivatives normalized activations \n",
            "-----------------------------------------------------------------------\n",
            "original Summary>>> introduce gaussian process prior weights neural network explore ability model input dependent weights benefits various tasks including uncertainty estimation generalization low sample setting\n",
            "Predicted Summary>>> introduce gaussian process prior weights neural network explore ability model input dependent weights benefits various tasks including uncertainty estimation generalization low sample \n",
            "-----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_rogue(src_trg, pred_trg):\n",
        "\n",
        "  #cut off  token\n",
        "  pred_trg = pred_trg[:-6]\n",
        "\n",
        "\n",
        "  if (len(pred_trg) == 0):\n",
        "    rogue_score = 0.0\n",
        "  else:\n",
        "    s = rouge.get_scores(pred_trg, src_trg, avg= True)\n",
        "    rogue_score = s['rouge-1']['f']\n",
        "\n",
        "  return s"
      ],
      "metadata": {
        "id": "l98eHzhYg41A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attn_plot_threshold = 0.45\n",
        "\n",
        "def evaluateRandomlyprint_1(encoder, decoder, n=5):\n",
        "    text=list()\n",
        "    headline=list()\n",
        "    pred_headline=list()\n",
        "\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "\n",
        "        if(len(pair[0].split())>=150):\n",
        "          continue\n",
        "        else:\n",
        "          #if(i%1000==0):\n",
        "           # print(i*100/n,\"% complete\")\n",
        "\n",
        "          tokenized_input = nltk.word_tokenize(pair[0])\n",
        "          #print(len(tokenized_input))\n",
        "          output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "\n",
        "          output_sentence = ' '.join(output_words)\n",
        "\n",
        "          #get rogue f-score -- need to pass tokenized version here\n",
        "          #print(pair[0])\n",
        "          #print(output_sentence)\n",
        "          score = calculate_rogue(pair[0], output_sentence)\n",
        "          #if(score>0.25):\n",
        "          print(score)\n",
        "          print('pair: ', pair[0])\n",
        "          print('output sentence: ', output_sentence)\n",
        "          print('')\n",
        "    return(text,headline,pred_headline)\n"
      ],
      "metadata": {
        "id": "u60CC2X8g5X5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "\n",
        "evaluateRandomlyprint_1(encoder1, attn_decoder1,15000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BiFy-ymU5Jva",
        "outputId": "3efc4a3c-5c40-419d-af55-9d2f0017d174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "{'rouge-1': {'r': 0.11827956989247312, 'p': 0.7857142857142857, 'f': 0.2056074743610796}, 'rouge-2': {'r': 0.03278688524590164, 'p': 0.3076923076923077, 'f': 0.05925925751879293}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.42857142857142855, 'f': 0.11214953043584597}}\n",
            "pair:  one fundamental tasks understanding genomics problem predicting transcription factor binding sites tfbss hundreds transcription factors tfs labels genomic sequence based tfbs prediction challenging multi label classification task two major biological mechanisms tf binding sequence specific binding patterns genomes known motifs interactions among tfs known co binding effects paper propose novel deep architecture prototype matching network pmn mimic tf binding mechanisms pmn model automatically extracts prototypes motif like features tf novel prototype matching loss borrowing ideas shot matching models use notion support set prototypes lstm learn tfs interact bind genomic sequences reference tfbs dataset million genomic sequences pmn significantly outperforms baselines validates design choices empirically knowledge first deep learning architecture introduces prototype learning considers tf tf interactions large scale tfbs prediction proposed architecture accurate also models underlying biology\n",
            "output sentence:  combine matching network framework shot learning large scale multi label model genomic sequence classification \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.5, 'f': 0.20689654844233057}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.18181818181818182, 'f': 0.060606057828282954}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.3333333333333333, 'f': 0.13793103120095132}}\n",
            "pair:  well known neural networks universal approximators deeper networks tend practice powerful shallower ones shed light proving total number neurons required approximate natural classes multivariate polynomials variables grows linearly deep neural networks grows exponentially merely single hidden layer allowed also provide evidence number hidden layers increased neuron requirement grows exponentially suggesting minimum number layers required practical expressibility grows logarithmically\n",
            "output sentence:  prove deep neural networks exponentially efficient shallow ones approximating sparse multivariate polynomials \n",
            "\n",
            "{'rouge-1': {'r': 0.08771929824561403, 'p': 0.625, 'f': 0.15384615168757396}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07017543859649122, 'p': 0.5, 'f': 0.12307692091834321}}\n",
            "pair:  many models based variational autoencoder proposed achieve disentangled latent variables inference however current work focusing designing powerful disentangling regularizers given number dimensions latent representation initialization could severely uence disentanglement thus pruning mechanism introduced aiming automatically seeking intrinsic dimension data promoting disentangled representations proposed method validated mpi mnist advancing state art methods disentanglement reconstruction robustness code provided https github com weyshi fyp disentanglement\n",
            "output sentence:  pruning vae proposed search disentangled variables intrinsic dimension \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.6, 'f': 0.19354838439125913}, 'rouge-2': {'r': 0.009615384615384616, 'p': 0.05555555555555555, 'f': 0.016393440107498377}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.3333333333333333, 'f': 0.10752687901491507}}\n",
            "pair:  deep reinforcement learning rl applied tasks need visualize understand behavior learned agents saliency maps explain agent behavior highlighting features input state relevant agent taking action existing perturbation based approaches compute saliency often highlight regions input relevant action taken agent approach generates focused saliency maps balancing two aspects specificity relevance capture different desiderata saliency first captures impact perturbation relative expected reward action explained second downweights irrelevant features alter relative expected rewards actions action explained compare approach existing approaches agents trained play board games chess go atari games breakout pong space invaders show illustrative examples chess atari go human studies chess automated evaluation methods chess approach generates saliency maps interpretable humans existing approaches\n",
            "output sentence:  propose model agnostic approach explain behaviour black box deep rl rl play play board board highlighting relevant relevant features input \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.8, 'f': 0.19753086203322667}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.1, 'f': 0.02197802002173668}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.6, 'f': 0.14814814598384393}}\n",
            "pair:  autoencoders provide powerful framework learning compressed representations encoding information needed reconstruct data point latent code cases autoencoders interpolate decoding convex combination latent codes two datapoints autoencoder produce output semantically mixes characteristics datapoints paper propose regularization procedure encourages interpolated outputs appear realistic fooling critic network trained recover mixing coefficient interpolated data develop simple benchmark task quantitatively measure extent various autoencoders interpolate show regularizer dramatically improves interpolation setting also demonstrate empirically regularizer produces latent codes effective downstream tasks suggesting possible link interpolation abilities learning useful representations\n",
            "output sentence:  propose regularizer improves interpolation autoencoders show also improves learned representation downstream tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.09278350515463918, 'p': 0.47368421052631576, 'f': 0.15517241105380503}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.1111111111111111, 'f': 0.028571426330612427}, 'rouge-l': {'r': 0.07216494845360824, 'p': 0.3684210526315789, 'f': 0.12068965243311539}}\n",
            "pair:  adversarial feature learning afl one promising ways explicitly constrains neural networks learn desired representations example afl could help learn anonymized representations avoid privacy issues afl learn representations training networks deceive adversary predict sensitive information network therefore success afl heavily relies choice adversary paper proposes novel design adversary em multiple adversaries random subspaces mars instantiate concept em volunerableness proposed method motivated assumption deceiving adversary could fail give meaningful information adversary easily fooled adversary rely single classifier suffer issues contrast proposed method designed less vulnerable utilizing ensemble independent classifiers classifier tries predict sensitive variables different em subset representations empirical validations three user anonymization tasks show proposed method achieves state art performances three datasets without significantly harming utility data significant gives new implications designing adversary important improve performance afl\n",
            "output sentence:  paper improves quality recently proposed adversarial feature leaning afl approach incorporating explicit constrains representations introducing concept em vulnerableness adversary \n",
            "\n",
            "{'rouge-1': {'r': 0.19047619047619047, 'p': 0.631578947368421, 'f': 0.2926829232688876}, 'rouge-2': {'r': 0.06976744186046512, 'p': 0.3333333333333333, 'f': 0.11538461252218943}, 'rouge-l': {'r': 0.15873015873015872, 'p': 0.5263157894736842, 'f': 0.24390243546400955}}\n",
            "pair:  approaches generalized zero shot learning rely cross modal mapping image feature space class embedding space generating artificial image features however learning shared cross modal embedding aligning latent spaces modality specific autoencoders shown promising generalized zero shot learning following direction also take artificial feature generation one step propose model shared latent space image features class embeddings learned aligned variational autoencoders purpose generating latent features train softmax classifier evaluate learned latent features conventional benchmark datasets establish new state art generalized zero shot well shot learning moreover results imagenet various zero shot splits show latent features generalize well large scale settings\n",
            "output sentence:  use vaes learn shared latent space embedding image features attributes thereby achieve state art results generalized zero shot learning \n",
            "\n",
            "{'rouge-1': {'r': 0.12, 'p': 0.631578947368421, 'f': 0.20168066958548125}, 'rouge-2': {'r': 0.030534351145038167, 'p': 0.21052631578947367, 'f': 0.053333331120888984}, 'rouge-l': {'r': 0.09, 'p': 0.47368421052631576, 'f': 0.15126050151825438}}\n",
            "pair:  tremendous success deep neural networks motivated need better understand fundamental properties networks many theoretical results proposed shallow networks paper study important primitive understanding meaningful input space deep network span recovery let mathbf mathbb times innermost weight matrix arbitrary feed forward neural network mathbb mathbb written sigma mathbf network sigma mathbb mathbb goal recover row span mathbf given oracle access value show multi layered network relu activation functions partial recovery possible namely provably recover linearly independent vectors row span mathbf using poly non adaptive queries furthermore differentiable activation functions demonstrate textit full span recovery possible even output first passed sign thresholding function case algorithm adaptive empirically confirm full span recovery always possible unrealistically thin layers reasonably wide networks obtain full span recovery random networks networks trained mnist data furthermore demonstrate utility span recovery attack inducing neural networks misclassify data obfuscated controlled random noise sensical inputs\n",
            "output sentence:  provably recover span deep multi layered neural network latent structure empirically apply efficient span recovery algorithms attack networks obfuscating inputs \n",
            "\n",
            "{'rouge-1': {'r': 0.10975609756097561, 'p': 0.5625, 'f': 0.18367346665556022}, 'rouge-2': {'r': 0.042105263157894736, 'p': 0.26666666666666666, 'f': 0.0727272703719009}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.3125, 'f': 0.10204081359433577}}\n",
            "pair:  implicit models allow generation samples point wise evaluation probabilities omnipresent real world problems tackled machine learning hot topic current research examples include data simulators widely used engineering scientific research generative adversarial networks gans image synthesis hot press approximate inference techniques relying implicit distributions majority existing approaches learning implicit models rely approximating intractable distribution optimisation objective gradient based optimisation liable produce inaccurate updates thus poor models paper alleviates need approximations proposing emph stein gradient estimator directly estimates score function implicitly defined distribution efficacy proposed estimator empirically demonstrated examples include meta learning approximate inference entropy regularised gans provide improved sample diversity\n",
            "output sentence:  introduced novel gradient estimator using stein method compared methods learning implicit models approximate inference image generation \n",
            "\n",
            "{'rouge-1': {'r': 0.0784313725490196, 'p': 0.4444444444444444, 'f': 0.13333333078333337}, 'rouge-2': {'r': 0.016129032258064516, 'p': 0.125, 'f': 0.02857142654693892}, 'rouge-l': {'r': 0.0392156862745098, 'p': 0.2222222222222222, 'f': 0.06666666411666677}}\n",
            "pair:  present probabilistic framework session based recommendation latent variable user state updated user views items learn interests provide computational solutions using parameterization trick using bouchard bound softmax function explore employing variational auto encoder variational expectation maximization algorithm tightening variational bound finally show bouchard bound causes denominator softmax decompose sum enabling fast noisy gradients bound giving fully probabilistic algorithm reminiscent word vec fast online em algorithm\n",
            "output sentence:  fast variational approximations approximating user state learning product embeddings \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.6363636363636364, 'f': 0.16091953802087464}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.2727272727272727, 'f': 0.05769230580066575}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.6363636363636364, 'f': 0.16091953802087464}}\n",
            "pair:  relate minimax game generative adversarial networks gans finding saddle points lagrangian function convex optimization problem discriminator outputs distribution generator outputs play roles primal variables dual variables respectively formulation shows connection standard gan training process primal dual subgradient methods convex optimization inherent connection provide theoretical convergence proof training gans function space also inspires novel objective function training modified objective function forces distribution generator outputs updated along direction according primal dual subgradient methods toy example shows proposed method able resolve mode collapse case cannot avoided standard gan wasserstein gan experiments gaussian mixture synthetic data real world image datasets demonstrate performance proposed method generating diverse samples\n",
            "output sentence:  propose primal dual subgradient method training gans method effectively alleviates mode collapse \n",
            "\n",
            "{'rouge-1': {'r': 0.2542372881355932, 'p': 0.9375, 'f': 0.39999999664355557}, 'rouge-2': {'r': 0.1875, 'p': 0.9375, 'f': 0.31249999722222227}, 'rouge-l': {'r': 0.2542372881355932, 'p': 0.9375, 'f': 0.39999999664355557}}\n",
            "pair:  propose method incrementally learn embedding space domain network architectures enable careful selection architectures evaluation compressed architecture search given teacher network search compressed network architecture using bayesian optimization bo kernel function defined proposed embedding space select architectures evaluation demonstrate search algorithm significantly outperform various baseline methods random search reinforcement learning ashok et al compressed architectures found method also better state art manually designed compact architecture shufflenet zhang et al also demonstrate learned embedding space transferred new settings architecture search larger teacher network teacher network different architecture family without training\n",
            "output sentence:  propose method incrementally learn embedding space domain network architectures enable careful selection architectures evaluation compressed architecture search \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.375, 'f': 0.1538461505851414}, 'rouge-2': {'r': 0.027777777777777776, 'p': 0.125, 'f': 0.045454542479339034}, 'rouge-l': {'r': 0.08064516129032258, 'p': 0.3125, 'f': 0.1282051249441158}}\n",
            "pair:  paper investigates strategies defend adversarial example attacks image classification systems transforming inputs feeding system specifically study applying image transformations bit depth reduction jpeg compression total variance minimization image quilting feeding image convolutional network classifier experiments imagenet show total variance minimization image quilting effective defenses practice particular network trained transformed images strength defenses lies non differentiable nature inherent randomness makes difficult adversary circumvent defenses best defense eliminates strong gray box strong black box attacks variety major attack methods\n",
            "output sentence:  apply model agnostic defense strategy adversarial examples achieve white box accuracy black box accuracy major attack algorithms algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.08974358974358974, 'p': 0.6363636363636364, 'f': 0.15730336862012376}, 'rouge-2': {'r': 0.02197802197802198, 'p': 0.2, 'f': 0.03960395861190088}, 'rouge-l': {'r': 0.05128205128205128, 'p': 0.36363636363636365, 'f': 0.08988763828304512}}\n",
            "pair:  distributed machine learning ml systems store copy model parameters locally machine minimize network communication practice order reduce synchronization waiting time copies model necessarily updated lock step become stale despite much development large scale ml effect staleness learning efficiency inconclusive mainly challenging control monitor staleness complex distributed environments work study convergence behaviors wide array ml models algorithms delayed updates extensive experiments reveal rich diversity effects staleness convergence ml algorithms offer insights seemingly contradictory reports literature empirical findings also inspire new convergence analysis sgd non convex optimization staleness matching best known convergence rate sqrt\n",
            "output sentence:  empirical theoretical study effects staleness non synchronous execution machine learning algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.1348314606741573, 'p': 0.8, 'f': 0.23076922830066568}, 'rouge-2': {'r': 0.037383177570093455, 'p': 0.2857142857142857, 'f': 0.06611570043303058}, 'rouge-l': {'r': 0.0898876404494382, 'p': 0.5333333333333333, 'f': 0.15384615137758878}}\n",
            "pair:  growing number learning methods actually differentiable games whose players optimise multiple interdependent objectives parallel gans intrinsic curiosity multi agent rl opponent shaping powerful approach improve learning dynamics games accounting player influence others updates learning opponent learning awareness lola recent algorithm exploits response leads cooperation settings like iterated prisoner dilemma although experimentally successful show lola agents exhibit arrogant behaviour directly odds convergence fact remarkably algorithms theoretical guarantees applying across player non convex games paper present stable opponent shaping sos new method interpolates lola stable variant named lookahead prove lookahead converges locally equilibria avoids strict saddles differentiable games sos inherits essential guarantees also shaping learning opponents consistently either matching outperforming lola experimentally\n",
            "output sentence:  opponent shaping powerful approach multi agent learning prevent convergence sos algorithm fixes strong guarantees differentiable games \n",
            "\n",
            "{'rouge-1': {'r': 0.12162162162162163, 'p': 0.75, 'f': 0.2093023231800974}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.09090909090909091, 'f': 0.01904761717188227}, 'rouge-l': {'r': 0.08108108108108109, 'p': 0.5, 'f': 0.13953488131963226}}\n",
            "pair:  recovering sparse conditional independence graphs data fundamental problem machine learning wide applications popular formulation problem ell regularized maximum likelihood estimation many convex optimization algorithms designed solve formulation recover graph structure recently surge interest learn algorithms directly based data case learn map empirical covariance sparse precision matrix however challenging task case since symmetric positive definiteness spd sparsity matrix easy enforce learned algorithms direct mapping data precision matrix may contain many parameters propose deep learning architecture glad uses alternating minimization algorithm model inductive bias learns model parameters via supervised learning show glad learns compact effective model recovering sparse graphs data\n",
            "output sentence:  data driven learning algorithm based unrolling alternating minimization optimization sparse graph recovery \n",
            "\n",
            "{'rouge-1': {'r': 0.10204081632653061, 'p': 0.9090909090909091, 'f': 0.18348623671744804}, 'rouge-2': {'r': 0.04065040650406504, 'p': 0.5, 'f': 0.07518796853411727}, 'rouge-l': {'r': 0.04081632653061224, 'p': 0.36363636363636365, 'f': 0.07339449359818201}}\n",
            "pair:  ai systems garner widespread public acceptance must develop methods capable explaining decisions black box models neural networks work identify two issues current explanatory methods first show two prevalent perspectives explanations feature additivity feature selection lead fundamentally different instance wise explanations literature explainers different perspectives currently directly compared despite distinct explanation goals second issue current post hoc explainers thoroughly validated simple models linear regression applied real world neural networks explainers commonly evaluated assumption learned models behave reasonably however neural networks often rely unreasonable correlations even producing correct decisions introduce verification framework explanatory methods feature selection perspective framework based non trivial neural network architecture trained real world task able provide guarantees inner workings validate efficacy evaluation showing failure modes current explainers aim framework provide publicly available shelf evaluation feature selection perspective explanations needed\n",
            "output sentence:  evaluation framework based real world neural network post hoc explanatory methods \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.45454545454545453, 'f': 0.10416666463758685}, 'rouge-2': {'r': 0.018867924528301886, 'p': 0.2, 'f': 0.03448275704518438}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.45454545454545453, 'f': 0.10416666463758685}}\n",
            "pair:  backpropagation algorithm popular algorithm training neural networks nowadays however suffers forward locking backward locking update locking problems especially neural network large layers distributed across multiple devices existing solutions either handle one locking problem lead severe accuracy loss memory inefficiency moreover none consider straggler problem among devices paper propose textbf layer wise staleness novel efficient training algorithm textbf diversely stale parameters dsp address challenges without loss accuracy memory issue also analyze convergence dsp two popular gradient based methods prove guaranteed converge critical points non convex problems finally extensive experimental results training deep convolutional neural networks demonstrate proposed dsp algorithm achieve significant training speedup stronger robustness better generalization compared methods\n",
            "output sentence:  propose diversely stale parameters break lockings backpropoagation algorithm train cnn parallel \n",
            "\n",
            "{'rouge-1': {'r': 0.2597402597402597, 'p': 0.9090909090909091, 'f': 0.40404040058361396}, 'rouge-2': {'r': 0.17204301075268819, 'p': 0.7272727272727273, 'f': 0.2782608664710775}, 'rouge-l': {'r': 0.24675324675324675, 'p': 0.8636363636363636, 'f': 0.38383838038159374}}\n",
            "pair:  activity populations sensory neurons carries stimulus information temporal spatial dimensions poses question compactly represent information population codes carry across dimensions developed analytical method factorize large number retinal ganglion cells spike trains robust low dimensional representation captures efficiently spatial temporal information particular extended previously used single trial space time tensor decomposition based non negative matrix factorization efficiently discount pre stimulus baseline activity data recorded retinal ganglion cells strong pre stimulus baseline showed situations stimulus elicits strong change firing rate extensions yield boost stimulus decoding performance results thus suggest taking account baseline important finding compact information rich representation neural activity\n",
            "output sentence:  extended single trial space time tensor decomposition based non negative matrix factorization efficiently discount pre stimulus baseline activity improves improves improves performance decoding performance \n",
            "\n",
            "{'rouge-1': {'r': 0.13793103448275862, 'p': 0.6666666666666666, 'f': 0.2285714257306123}, 'rouge-2': {'r': 0.09836065573770492, 'p': 0.5454545454545454, 'f': 0.16666666407793213}, 'rouge-l': {'r': 0.1206896551724138, 'p': 0.5833333333333334, 'f': 0.1999999971591837}}\n",
            "pair:  study problem fitting task specific learning rate schedules perspective hyperparameter optimization allows us explicitly search schedules achieve good generalization describe structure gradient validation error learning rates hypergradient based introduce novel online algorithm method adaptively interpolates two recently proposed techniques franceschi et al baydin et al featuring increased stability faster convergence show empirically proposed technique compares favorably baselines related methodsin terms final test accuracy\n",
            "output sentence:  marthe new method fit task specific learning rate schedules perspective hyperparameter optimization \n",
            "\n",
            "{'rouge-1': {'r': 0.15584415584415584, 'p': 0.9230769230769231, 'f': 0.26666666419506174}, 'rouge-2': {'r': 0.10101010101010101, 'p': 0.8333333333333334, 'f': 0.1801801782517653}, 'rouge-l': {'r': 0.15584415584415584, 'p': 0.9230769230769231, 'f': 0.26666666419506174}}\n",
            "pair:  paper introduces network architecture solve structure motion sfm problem via feature metric bundle adjustment ba explicitly enforces multi view geometry constraints form feature metric error whole pipeline differentiable network learn suitable features make ba problem tractable furthermore work introduces novel depth parameterization recover dense per pixel depth network first generates several basis depth maps according input image optimizes final depth linear combination basis depth maps via feature metric ba basis depth maps generator also learned via end end training whole system nicely combines domain knowledge hard coded multi view geometry constraints deep learning feature learning basis depth maps learning address challenging dense sfm problem experiments large scale real data prove success proposed method\n",
            "output sentence:  paper introduces network architecture solve structure motion sfm problem via feature bundle adjustment ba \n",
            "\n",
            "{'rouge-1': {'r': 0.12643678160919541, 'p': 0.6470588235294118, 'f': 0.2115384588036243}, 'rouge-2': {'r': 0.02912621359223301, 'p': 0.17647058823529413, 'f': 0.049999997568055676}, 'rouge-l': {'r': 0.09195402298850575, 'p': 0.47058823529411764, 'f': 0.1538461511113166}}\n",
            "pair:  reading comprehension challenging task especially executed across longer across multiple evidence documents answer likely reoccur existing neural architectures typically scale entire evidence hence resort selecting single passage document either via truncation means carefully searching answer within passage however cases strategy suboptimal since focusing specific passage becomes difficult leverage multiple mentions answer throughout document work take different approach constructing lightweight models combined cascade find answer submodel consists feed forward networks equipped attention mechanism making trivially parallelizable show approach scale approximately order magnitude larger evidence documents aggregate information multiple mentions answer candidate across document empirically approach achieves state art performance wikipedia web domains triviaqa dataset outperforming complex recurrent architectures\n",
            "output sentence:  propose neural cascades simple trivially parallelizable approach reading consisting feed feed forward nets attention achieves state performance triviaqa \n",
            "\n",
            "{'rouge-1': {'r': 0.1320754716981132, 'p': 0.5833333333333334, 'f': 0.21538461237396456}, 'rouge-2': {'r': 0.0410958904109589, 'p': 0.25, 'f': 0.07058823286920424}, 'rouge-l': {'r': 0.09433962264150944, 'p': 0.4166666666666667, 'f': 0.15384615083550301}}\n",
            "pair:  flexibly efficiently reason temporal sequences abstract representations compactly represent important information sequence needed one way constructing representations focusing important events sequence paper propose model learns discover key events keyframes well represent sequence terms using hierarchical keyframe inpainter keyin model first generates keyframes temporal placement inpaints sequences keyframes propose fully differentiable formulation efficiently learning keyframe placement show keyin finds informative keyframes several datasets diverse dynamics evaluated planning task keyin outperforms recent proposals learning hierarchical representations\n",
            "output sentence:  propose model learns discover informative frames future video sequence represent video via keyframes \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6, 'f': 0.2068965488703924}, 'rouge-2': {'r': 0.02040816326530612, 'p': 0.1111111111111111, 'f': 0.034482755998811135}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.4, 'f': 0.13793103162901313}}\n",
            "pair:  score matching provides effective approach learning flexible unnormalized models scalability limited need evaluate second order derivative paper connect general family learning objectives including score matching wassersteingradient flows connection enables us design scalable approximation theseobjectives form similar single step contrastive divergence present applications training implicit variational wasserstein auto encoders manifold valued priors\n",
            "output sentence:  present scalable approximation wide range ebm objectives applications implicit vaes waes \n",
            "\n",
            "{'rouge-1': {'r': 0.056179775280898875, 'p': 0.625, 'f': 0.10309278199171008}, 'rouge-2': {'r': 0.008, 'p': 0.14285714285714285, 'f': 0.015151514147153418}, 'rouge-l': {'r': 0.056179775280898875, 'p': 0.625, 'f': 0.10309278199171008}}\n",
            "pair:  reinforcement learning agents need explore unknown environments solve tasks given bayes optimal solution exploration intractable complex environments several exploration methods proposed approximations remains unclear underlying objective optimized existing exploration methods altered incorporate prior knowledge task moreover unclear acquire single exploration strategy useful solving multiple downstream tasks address shortcomings learning single exploration policy quickly solve suite downstream tasks multi task setting amortizing cost learning explore recast exploration problem state marginal matching smm aim learn policy state marginal distribution matches given target state distribution incorporate prior knowledge task optimize objective reducing two player zero sum game state density model parametric policy theoretical analysis approach suggests prior exploration methods learn policy distribution matching acquire replay buffer performs distribution matching observation potentially explains prior methods success single task settings simulated real world tasks demonstrate algorithm explores faster adapts quickly prior methods\n",
            "output sentence:  view exploration rl problem matching marginal distribution states \n",
            "\n",
            "{'rouge-1': {'r': 0.09278350515463918, 'p': 0.6428571428571429, 'f': 0.16216215995779565}, 'rouge-2': {'r': 0.025210084033613446, 'p': 0.23076923076923078, 'f': 0.04545454367883386}, 'rouge-l': {'r': 0.05154639175257732, 'p': 0.35714285714285715, 'f': 0.09009008788572362}}\n",
            "pair:  deep learning based models speech enhancement mainly focused estimating magnitude spectrogram reusing phase noisy speech reconstruction due difficulty estimating phase clean speech improve speech enhancement performance tackle phase estimation problem three ways first propose deep complex net advanced net structured model incorporating well defined complex valued building blocks deal complex valued spectrograms second propose polar coordinate wise complex valued masking method reflect distribution complex ideal ratio masks third define novel loss function weighted source distortion ratio wsdr loss designed directly correlate quantitative evaluation measure model evaluated mixture voice bank corpus demand database widely used many deep learning models speech enhancement ablation experiments conducted mixed dataset showing three proposed approaches empirically valid experimental results show proposed method achieves state art performance metrics outperforming previous approaches large margin\n",
            "output sentence:  paper proposes novel complex masking method speech enhancement along loss function efficient phase estimation \n",
            "\n",
            "{'rouge-1': {'r': 0.10227272727272728, 'p': 0.8181818181818182, 'f': 0.1818181798428732}, 'rouge-2': {'r': 0.046296296296296294, 'p': 0.5, 'f': 0.08474576116058606}, 'rouge-l': {'r': 0.07954545454545454, 'p': 0.6363636363636364, 'f': 0.14141413943883277}}\n",
            "pair:  allocation computation resources backbone crucial issue object detection however classification allocation pattern usually adopted directly object detector proved sub optimal order reallocate engaged computation resources efficient way present cr nas computation reallocation neural architecture search learn computation reallocation strategies across different feature resolution spatial position diectly target detection dataset two level reallocation space proposed stage spatial reallocation novel hierarchical search procedure adopted cope complex search space apply cr nas multiple backbones achieve consistent improvements cr resnet cr mobilenetv outperforms baseline coco ap respectively without additional computation budget models discovered cr nas equiped powerful detection neck head easily transferred dataset pascal voc vision tasks instance segmentation cr nas used plugin improve performance various networks demanding\n",
            "output sentence:  propose cr nas reallocate engaged computation resources different resolution spatial position \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.6666666666666666, 'f': 0.21621621349890432}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.09090909090909091, 'f': 0.02380952153344693}, 'rouge-l': {'r': 0.08064516129032258, 'p': 0.4166666666666667, 'f': 0.13513513241782327}}\n",
            "pair:  training generative adversarial networks gans notoriously challenging propose study architectural modification self modulation improves gan performance across different data sets architectures losses regularizers hyperparameter settings intuitively self modulation allows intermediate feature maps generator change function input noise vector reminiscent conditioning techniques requires labeled data large scale empirical study observe relative decrease fid furthermore else equal adding modification generator leads improved performance studied settings self modulation simple architectural change requires additional parameter tuning suggests applied readily gan\n",
            "output sentence:  simple gan modification improves performance across many losses architectures regularization schemes datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.631578947368421, 'f': 0.21428571146843114}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.2, 'f': 0.06153845893491135}, 'rouge-l': {'r': 0.08602150537634409, 'p': 0.42105263157894735, 'f': 0.14285714003985975}}\n",
            "pair:  recent literature suggests averaged word vectors followed simple post processing outperform many deep learning methods semantic textual similarity tasks furthermore averaged word vectors trained supervised large corpora paraphrases achieve state art results standard sts benchmarks inspired insights push limits word embeddings even propose novel fuzzy bag words fbow representation text contains words vocabulary simultaneously different degrees membership derived similarities word vectors show max pooled word vectors special case fuzzy bow compared via fuzzy jaccard index rather cosine similarity finally propose dynamax completely unsupervised non parametric similarity measure dynamically extracts max pools good features depending sentence pair method efficient easy implement yet outperforms current baselines sts tasks large margin even competitive supervised word vectors trained directly optimise cosine similarity\n",
            "output sentence:  max pooled word vectors fuzzy jaccard set similarity extremely competitive baseline semantic similarity propose simple dynamic variant performs even even better \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.45454545454545453, 'f': 0.12658227608396094}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.2, 'f': 0.04651162585181188}, 'rouge-l': {'r': 0.04411764705882353, 'p': 0.2727272727272727, 'f': 0.07594936469155592}}\n",
            "pair:  present network embedding algorithms capture information node local distribution node attributes around observed random walks following approach similar skip gram observations neighborhoods different sizes either pooled ae encoded distinctly multi scale approach musae capturing attribute neighborhood relationships multiple scales useful diverse range applications including latent feature identification across disconnected networks similar attributes prove theoretically matrices node feature pointwise mutual information implicitly factorized embeddings experiments show algorithms robust computationally efficient outperform comparable models social web citation network datasets\n",
            "output sentence:  develop efficient multi scale approximate attributed network embedding procedures provable properties \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.5333333333333333, 'f': 0.23880596667409226}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.07142857142857142, 'f': 0.02666666363022257}, 'rouge-l': {'r': 0.057692307692307696, 'p': 0.2, 'f': 0.08955223533080878}}\n",
            "pair:  generative networks promising models specifying visual transformations unfortunately certification generative models challenging one needs capture sufficient non convexity produce precise bounds output existing verification methods either fail scale generative networks capture enough non convexity work present new verifier called approxline certify non trivial properties generative networks approxline performs deterministic probabilistic abstract interpretation captures infinite sets outputs generative networks show approxline verify interesting interpolations network latent space\n",
            "output sentence:  verify deterministic probabilistic properties neural networks using non convex relaxations visible transformations specified generative models \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.5882352941176471, 'f': 0.19417475452540298}, 'rouge-2': {'r': 0.05309734513274336, 'p': 0.3157894736842105, 'f': 0.09090908844467409}, 'rouge-l': {'r': 0.09302325581395349, 'p': 0.47058823529411764, 'f': 0.1553398030690923}}\n",
            "pair:  variety cooperative multi agent control problems require agents achieve individual goals contributing collective success multi goal multi agent setting poses difficulties recent algorithms primarily target settings single global reward due two new challenges efficient exploration learning individual goal attainment cooperation others success credit assignment interactions actions goals different agents address challenges restructure problem novel two stage curriculum single agent goal attainment learned prior learning multi agent cooperation derive new multi goal multi agent policy gradient credit function localized credit assignment use function augmentation scheme bridge value policy functions across curriculum complete architecture called cm learns significantly faster direct adaptations existing algorithms three challenging multi goal multi agent problems cooperative navigation difficult formations negotiating multi vehicle lane changes sumo traffic simulator strategic cooperation checkers environment\n",
            "output sentence:  modular method fully cooperative multi goal multi agent reinforcement learning based curriculum learning efficient exploration credit assignment action goal interactions \n",
            "\n",
            "{'rouge-1': {'r': 0.05454545454545454, 'p': 0.3333333333333333, 'f': 0.09374999758300787}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03636363636363636, 'p': 0.2222222222222222, 'f': 0.06249999758300791}}\n",
            "pair:  present soseleto source selection target optimization new method exploiting source dataset solve classification problem target dataset soseleto based following simple intuition source examples informative others target problem capture intuition source samples given weights weights solved jointly source target classification problems via bilevel optimization scheme target therefore gets choose source samples informative classification task furthermore bilevel nature optimization acts kind regularization target mitigating overfitting soseleto may applied classic transfer learning well problem training datasets noisy labels show state art results problems\n",
            "output sentence:  learning limited training data exploiting helpful instances rich data source \n",
            "\n",
            "{'rouge-1': {'r': 0.2361111111111111, 'p': 1.0, 'f': 0.3820224688195935}, 'rouge-2': {'r': 0.1348314606741573, 'p': 0.75, 'f': 0.22857142598820862}, 'rouge-l': {'r': 0.19444444444444445, 'p': 0.8235294117647058, 'f': 0.31460673848251486}}\n",
            "pair:  deep learning yielded state art performance many natural language processing tasks including named entity recognition ner however typically requires large amounts labeled data work demonstrate amount labeled training data drastically reduced deep learning combined active learning active learning sample efficient computationally expensive since requires iterative retraining speed introduce lightweight architecture ner viz cnn cnn lstm model consisting convolutional character word encoders long short term memory lstm tag decoder model achieves nearly state art performance standard datasets task computationally much efficient best performing models carry incremental active learning training process able nearly match state art performance original training data\n",
            "output sentence:  introduce lightweight architecture named entity recognition carry incremental active learning able match state art performance original training data \n",
            "\n",
            "{'rouge-1': {'r': 0.05970149253731343, 'p': 0.2857142857142857, 'f': 0.0987654292394453}, 'rouge-2': {'r': 0.02531645569620253, 'p': 0.14285714285714285, 'f': 0.043010750130651094}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.2857142857142857, 'f': 0.0987654292394453}}\n",
            "pair:  many deployed learned models black boxes given input returns output internal information model architecture optimisation procedure training data disclosed explicitly might contain proprietary information make system vulnerable work shows attributes neural networks exposed sequence queries multiple implications one hand work exposes vulnerability black box neural networks different types attacks show revealed internal information helps generate effective adversarial examples black box model hand technique used better protection private content automatic recognition models using adversarial examples paper suggests actually hard draw line white box black box models\n",
            "output sentence:  querying black box neural network reveals lot information propose novel metamodels effectively extracting information black box \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.625, 'f': 0.2197802168820191}, 'rouge-2': {'r': 0.052083333333333336, 'p': 0.3125, 'f': 0.08928571183673477}, 'rouge-l': {'r': 0.10666666666666667, 'p': 0.5, 'f': 0.17582417292597516}}\n",
            "pair:  many approaches causal discovery limited inability discriminate markov equivalent graphs given observational data formulate causal discovery marginal likelihood based bayesian model selection problem adopt parameterization based notion independence causal mechanisms renders markov equivalent graphs distinguishable complement empirical bayesian approach setting priors actual underlying causal graph assigned higher marginal likelihood alternatives adopting bayesian approach also allows straightforward modeling unobserved confounding variables provide variational algorithm approximate marginal likelihood since desirable feat renders computation marginal likelihood intractable believe bayesian approach causal discovery allows rich methodology bayesian inference used various difficult aspects problem provides unifying framework causal discovery research demonstrate promising results experiments conducted real data supporting modeling approach inference methodology\n",
            "output sentence:  cast causal structure discovery bayesian model selection way allows us discriminate markov equivalent graphs identify unique causal graph \n",
            "\n",
            "{'rouge-1': {'r': 0.0297029702970297, 'p': 0.3333333333333333, 'f': 0.05454545304297525}, 'rouge-2': {'r': 0.0072992700729927005, 'p': 0.125, 'f': 0.013793102405707571}, 'rouge-l': {'r': 0.019801980198019802, 'p': 0.2222222222222222, 'f': 0.03636363486115709}}\n",
            "pair:  many applications desirable extract relevant information complex input data involves making decision input features relevant information bottleneck method formalizes information theoretic optimization problem maintaining optimal tradeoff compression throwing away irrelevant input information predicting target many problem settings including reinforcement learning problems consider work might prefer compress part input typically case standard conditioning input state observation privileged input might correspond goal task output costly planning algorithm communication another agent cases might prefer compress privileged input either achieve better generalization respect goals minimize access costly information case communication practical implementations information bottleneck based variational inference require access privileged input order compute bottleneck variable although perform compression compression operation needs unrestricted lossless access work propose variational bandwidth bottleneck decides example estimated value privileged information seeing based standard input accordingly chooses stochastically whether access privileged input formulate tractable approximation framework demonstrate series reinforcement learning experiments improve generalization reduce access computationally costly information\n",
            "output sentence:  training agents adaptive computation based information bottleneck promote generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.06944444444444445, 'p': 0.625, 'f': 0.12499999820000002}, 'rouge-2': {'r': 0.011627906976744186, 'p': 0.14285714285714285, 'f': 0.021505374952017663}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  paper presents novel two step approach fundamental problem learning optimal map one distribution another first learn optimal transport ot plan thought one many map two distributions end propose stochastic dual approach regularized ot show empirically scales better recent related approach amount samples large second estimate monge map deep neural network learned approximating barycentric projection previously obtained ot plan parameterization allows generalization mapping outside support input measure prove two theoretical stability results regularized ot show estimations converge ot monge map underlying continuous measures showcase proposed approach two applications domain adaptation generative modeling\n",
            "output sentence:  learning optimal mapping deepnn distributions along theoretical guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.1282051282051282, 'p': 0.625, 'f': 0.2127659546220009}, 'rouge-2': {'r': 0.04878048780487805, 'p': 0.2857142857142857, 'f': 0.08333333084201397}, 'rouge-l': {'r': 0.1282051282051282, 'p': 0.625, 'f': 0.2127659546220009}}\n",
            "pair:  introduce neural architecture perform amortized approximate bayesian inference latent random permutations two sets objects method involves approximating permanents matrices pairwise probabilities using recent ideas functions defined sets sampled permutation comes probability estimate quantity unavailable mcmc approaches illustrate method sets points mnist images\n",
            "output sentence:  novel neural architecture efficient amortized inference latent permutations \n",
            "\n",
            "{'rouge-1': {'r': 0.027777777777777776, 'p': 0.2222222222222222, 'f': 0.04938271407407415}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.027777777777777776, 'p': 0.2222222222222222, 'f': 0.04938271407407415}}\n",
            "pair:  keyword spotting wakeword detection essential feature hands free operation modern voice controlled devices devices becoming ubiquitous users might want choose personalized custom wakeword work present donut ctc based algorithm online query example keyword spotting enables custom wakeword detection algorithm works recording small number training examples user generating set label sequence hypotheses training examples detecting wakeword aggregating scores hypotheses given new audio recording method combines generalization interpretability ctc based keyword spotting user adaptation convenience conventional query example system donut low computational requirements well suited learning inference embedded systems without requiring private user data uploaded cloud\n",
            "output sentence:  propose interpretable model detecting user chosen wakewords learns user examples \n",
            "\n",
            "{'rouge-1': {'r': 0.1625, 'p': 0.6842105263157895, 'f': 0.2626262595245384}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.3157894736842105, 'f': 0.09917355107164819}, 'rouge-l': {'r': 0.1125, 'p': 0.47368421052631576, 'f': 0.18181817871645756}}\n",
            "pair:  deep generative models achieved remarkable progress recent years despite progress quantitative evaluation comparison generative models remains one important challenges one popular metrics evaluating generative models log likelihood direct computation log likelihood intractable recently shown log likelihood interesting generative models variational autoencoders vae generative adversarial networks gan efficiently estimated using annealed importance sampling ais work argue log likelihood metric cannot represent different performance characteristics generative models propose use rate distortion curves evaluate compare deep generative models show approximate entire rate distortion curve using one single run ais roughly computational cost single log likelihood estimate evaluate lossy compression rates different deep generative models vaes gans variants adversarial autoencoders aae mnist cifar arrive number insights obtainable log likelihoods alone\n",
            "output sentence:  study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring roughly computational \n",
            "\n",
            "{'rouge-1': {'r': 0.07058823529411765, 'p': 0.5, 'f': 0.12371133803804871}, 'rouge-2': {'r': 0.019230769230769232, 'p': 0.18181818181818182, 'f': 0.034782606965595556}, 'rouge-l': {'r': 0.07058823529411765, 'p': 0.5, 'f': 0.12371133803804871}}\n",
            "pair:  approaches continual learning aim successfully learn set related tasks arrive online manner recently several frameworks developed enable deep learning deployed learning scenario key modelling decision extent architecture shared across tasks one hand separately modelling task avoids catastrophic forgetting support transfer learning leads large models hand rigidly specifying shared component task specific part enables task transfer limits model size vulnerable catastrophic forgetting restricts form task transfer occur ideally network adaptively identify parts network share data driven way introduce approach called continual learning adaptive weights claw based probabilistic modelling variational inference experiments show claw achieves state art performance six benchmarks terms overall continual learning performance measured classification accuracy terms addressing catastrophic forgetting\n",
            "output sentence:  continual learning framework learns automatically adapt architecture based proposed variational inference algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.12345679012345678, 'p': 0.6666666666666666, 'f': 0.20833333069661458}, 'rouge-2': {'r': 0.06363636363636363, 'p': 0.4666666666666667, 'f': 0.11199999788800002}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.6, 'f': 0.1874999973632813}}\n",
            "pair:  training neural networks verifiable robustness guarantees challenging several existing approaches utilize linear relaxation based neural network output bounds perturbation slow training factor hundreds depending underlying network architectures meanwhile interval bound propagation ibp based training efficient significantly outperforms linear relaxation based methods many tasks yet may suffer stability issues since bounds much looser especially beginning training paper propose new certified adversarial training method crown ibp combining fast ibp bounds forward bounding pass tight linear relaxation based bound crown backward bounding pass crown ibp computationally efficient consistently outperforms ibp baselines training verifiably robust neural networks conduct large scale experiments mnist cifar datasets outperform previous linear relaxation bound propagation based certified defenses inf robustness notably achieve verified test error mnist epsilon cifar epsilon\n",
            "output sentence:  propose new certified adversarial training method crown ibp achieves state art robustness inf norm adversarial perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.18823529411764706, 'p': 0.9411764705882353, 'f': 0.3137254874183007}, 'rouge-2': {'r': 0.14545454545454545, 'p': 0.8888888888888888, 'f': 0.24999999758300784}, 'rouge-l': {'r': 0.18823529411764706, 'p': 0.9411764705882353, 'f': 0.3137254874183007}}\n",
            "pair:  multi hop text based question answering current challenge machine comprehension task requires sequentially integrate facts multiple passages answer complex natural language questions paper propose novel architecture called latent question reformulation network lqr net multi hop parallel attentive network designed question answering tasks require reasoning capabilities lqr net composed association textbf reading modules textbf reformulation modules purpose reading module produce question aware representation document document representation reformulation module extracts essential elements calculate updated representation question updated question passed following hop evaluate architecture hotpotqa question answering dataset designed assess multi hop reasoning capabilities model achieves competitive results public leaderboard outperforms best current textit published models terms exact match em score finally show analysis sequential reformulations provide interpretable reasoning paths\n",
            "output sentence:  paper propose latent question reformulation network lqr net multi hop parallel attentive network designed question answering tasks require reasoning \n",
            "\n",
            "{'rouge-1': {'r': 0.05102040816326531, 'p': 0.625, 'f': 0.09433962124599504}, 'rouge-2': {'r': 0.02654867256637168, 'p': 0.375, 'f': 0.04958677562461584}, 'rouge-l': {'r': 0.04081632653061224, 'p': 0.5, 'f': 0.07547169671769313}}\n",
            "pair:  work present novel upper bound target error address problem unsupervised domain adaptation recent studies reveal deep neural network learn transferable features generalize well novel tasks furthermore ben david et al provide upper bound target error transferring knowledge summarized minimizing source error distance marginal distributions simultaneously however common methods based theory usually ignore joint error samples different classes might mixed together matching marginal distribution case matter minimize marginal discrepancy target error bounded due increasing joint error address problem propose general upper bound taking joint error account undesirable case properly penalized addition utilize constrained hypothesis space formalize tighter bound well novel cross margin discrepancy measure dissimilarity hypotheses alleviates instability adversarial learning extensive empirical evidence shows proposal outperforms related approaches image classification error rates standard domain adaptation benchmarks\n",
            "output sentence:  joint error matters unsupervised domain adaptation especially domain shift huge \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.75, 'f': 0.1935483848491155}, 'rouge-2': {'r': 0.07547169811320754, 'p': 0.6153846153846154, 'f': 0.13445377956641483}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.75, 'f': 0.1935483848491155}}\n",
            "pair:  deep neural networks dnns typically enough capacity fit random data brute force even conventional data dependent regularizations focusing geometry features imposed find reason inconsistency enforced geometry standard softmax cross entropy loss resolve propose new framework data dependent dnn regularization geometrically regularized self validating neural networks grsvnet training geometry enforced one batch features simultaneously validated separate batch using validation loss consistent geometry study particular case grsvnet orthogonal low rank embedding ole grsvnet capable producing highly discriminative features residing orthogonal low rank subspaces numerical experiments show ole grsvnet outperforms dnns conventional regularization trained real data importantly unlike conventional dnns ole grsvnet refuses memorize random data random labels suggesting learns intrinsic patterns reducing memorizing capacity baseline dnn\n",
            "output sentence:  propose new framework data dependent dnn regularization prevent dnns overfitting random data random labels \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.6, 'f': 0.11650485261570365}, 'rouge-2': {'r': 0.008547008547008548, 'p': 0.1111111111111111, 'f': 0.015873014546485376}, 'rouge-l': {'r': 0.043010752688172046, 'p': 0.4, 'f': 0.07766990115939301}}\n",
            "pair:  knowledge extraction techniques used convert neural networks symbolic descriptions objective producing comprehensible learning models central challenge find explanation comprehensible original model still representing model faithfully distributed nature deep networks led many believe hidden features neural network cannot explained logical descriptions simple enough understood humans decompositional knowledge extraction abandoned favour methods paper examine question systematically proposing knowledge extraction method using textit rules allows us map complexity accuracy landscape rules describing hidden features convolutional neural network cnn experiments reported paper show shape landscape reveals optimal trade comprehensibility accuracy showing latent variable optimal textit rule describe behaviour find rules optimal tradeoff first final layer high degree explainability whereas rules optimal tradeoff second third layer less explainable results shed light feasibility rule extraction deep networks point value decompositional knowledge extraction method explainability\n",
            "output sentence:  systematically examines well explain hidden features deep network terms logical rules \n",
            "\n",
            "{'rouge-1': {'r': 0.14457831325301204, 'p': 0.6, 'f': 0.2330097056084457}, 'rouge-2': {'r': 0.06363636363636363, 'p': 0.2916666666666667, 'f': 0.10447760899977729}, 'rouge-l': {'r': 0.10843373493975904, 'p': 0.45, 'f': 0.1747572784239797}}\n",
            "pair:  decades research neural code underlying spatial navigation revealed diverse set neural response properties entorhinal cortex ec mammalian brain contains rich set spatial correlates including grid cells encode space using tessellating patterns however mechanisms functional significance spatial representations remain largely mysterious new way understand neural representations trained recurrent neural networks rnns perform navigation tasks arenas based velocity inputs surprisingly find grid like spatial response patterns emerge trained networks along units exhibit spatial correlates including border cells band like cells different functional types neurons observed experimentally order emergence grid like border cells also consistent observations developmental studies together results suggest grid cells border cells others observed ec may natural solution representing space efficiently given predominant recurrent connections neural circuits\n",
            "output sentence:  knowledge first study show neural representations space including grid like cells border cells observed brain could training recurrent neural neural network tasks tasks neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.8571428571428571, 'f': 0.23529411527873895}, 'rouge-2': {'r': 0.08181818181818182, 'p': 0.6428571428571429, 'f': 0.14516128831945893}, 'rouge-l': {'r': 0.11363636363636363, 'p': 0.7142857142857143, 'f': 0.19607842900422914}}\n",
            "pair:  existing black box attacks deep neural networks dnns far largely focused transferability adversarial instance generated locally trained model transfer attack learning models paper propose novel gradient estimation black box attacks adversaries query access target model class probabilities rely transferability also propose strategies decouple number queries required generate adversarial sample dimensionality input iterative variant attack achieves close adversarial success rates targeted untargeted attacks dnns carry extensive experiments thorough comparative evaluation black box attacks show proposed gradient estimation attacks outperform transferability based black box attacks tested mnist cifar datasets achieving adversarial success rates similar well known state art white box attacks also apply gradient estimation attacks successfully real world content moderation classi er hosted clarifai furthermore evaluate black box attacks state art defenses show gradient estimation attacks effective even defenses\n",
            "output sentence:  query based black box attacks deep neural networks adversarial success rates matching white box attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.8, 'f': 0.22222221983024693}, 'rouge-2': {'r': 0.06756756756756757, 'p': 0.5555555555555556, 'f': 0.1204819257773262}, 'rouge-l': {'r': 0.12903225806451613, 'p': 0.8, 'f': 0.22222221983024693}}\n",
            "pair:  present neural rendering architecture helps variational autoencoders vaes learn disentangled representations instead deconvolutional network typically used decoder vaes tile broadcast latent vector across space concatenate fixed coordinate channels apply fully convolutional network stride provides architectural prior dissociating positional non positional features latent space yet without providing explicit supervision effect show architecture term spatial broadcast decoder improves disentangling reconstruction accuracy generalization held regions data space show spatial broadcast decoder complementary state art sota disentangling techniques incorporated improves performance\n",
            "output sentence:  introduce neural rendering architecture helps vaes learn disentangled latent representations \n",
            "\n",
            "{'rouge-1': {'r': 0.08695652173913043, 'p': 0.5454545454545454, 'f': 0.14999999762812505}, 'rouge-2': {'r': 0.0375, 'p': 0.3, 'f': 0.06666666469135808}, 'rouge-l': {'r': 0.07246376811594203, 'p': 0.45454545454545453, 'f': 0.12499999762812504}}\n",
            "pair:  much recent research devoted video prediction generation mostly short scale time horizons hierarchical video prediction method villegas et al example state art method long term video prediction however method limited applicability practical settings requires ground truth pose poses joints human training time paper presents long term hierarchical video prediction model restriction show network learns higher level structure pose equivalent hidden variables works better cases ground truth pose fully capture information needed predict next frame method gives sharper results video prediction methods require ground truth pose efficiency shown humans robot pushing datasets\n",
            "output sentence:  show ways train hierarchical video prediction model without needing pose labels \n",
            "\n",
            "{'rouge-1': {'r': 0.06779661016949153, 'p': 0.3076923076923077, 'f': 0.11111110815200625}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.08333333333333333, 'f': 0.025974023342891145}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.23076923076923078, 'f': 0.0833333303742285}}\n",
            "pair:  paper fosters idea deep learning methods sided classical visual odometry pipelines improve accuracy produce uncertainty models estimations show biases inherent visual odom etry process faithfully learnt compensated learning ar chitecture associated probabilistic loss function jointly estimate full covariance matrix residual errors defining heteroscedastic error model experiments autonomous driving image sequences micro aerial vehicles camera acquisitions assess possibility concurrently improve visual odome try estimate error associated outputs\n",
            "output sentence:  paper discusses different methods pairing vo deep learning proposes simultaneous prediction corrections uncertainty \n",
            "\n",
            "{'rouge-1': {'r': 0.09836065573770492, 'p': 0.6666666666666666, 'f': 0.1714285691877551}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}, 'rouge-l': {'r': 0.09836065573770492, 'p': 0.6666666666666666, 'f': 0.1714285691877551}}\n",
            "pair:  users tremendous potential aid construction maintenance knowledges bases kbs contribution feedback identifies incorrect missing entity attributes relations however new data added kb kb entities constructed running entity resolution er change rendering intended targets user feedback unknown problem term identity uncertainty work present framework integrating user feedback kbs presence identity uncertainty approach based user feedback participate alongside mentions er propose specific representation user feedback feedback mentions introduce new online algorithm integrating mentions existing kb experiments demonstrate proposed approach outperforms baselines experimental conditions\n",
            "output sentence:  paper develops framework integrating user feedback identity uncertainty knowledge bases \n",
            "\n",
            "{'rouge-1': {'r': 0.07462686567164178, 'p': 0.7142857142857143, 'f': 0.13513513342220598}, 'rouge-2': {'r': 0.012987012987012988, 'p': 0.16666666666666666, 'f': 0.024096384200900062}, 'rouge-l': {'r': 0.04477611940298507, 'p': 0.42857142857142855, 'f': 0.08108107936815197}}\n",
            "pair:  misspecified settings posterior distribution bayesian statistics may lead inconsistent estimates fix issue suggested replace likelihood pseudo likelihood exponential loss function enjoying suitable robustness properties paper build pseudo likelihood based maximum mean discrepancy defined via embedding probability distributions reproducing kernel hilbert space show mmd bayes posterior consistent robust model misspecification posterior obtained way might intractable also prove reasonable variational approximations posterior enjoy properties provide details stochastic gradient algorithm compute variational approximations numerical simulations indeed suggest estimator robust misspecification ones based likelihood\n",
            "output sentence:  robust bayesian estimation via maximum mean discrepancy \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.75, 'f': 0.13953488203353168}, 'rouge-2': {'r': 0.02040816326530612, 'p': 0.2857142857142857, 'f': 0.03809523685079368}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.625, 'f': 0.11627906808004328}}\n",
            "pair:  cloze test widely adopted language exams evaluate students language proficiency paper propose first large scale human designed cloze test dataset cloth questions used middle school high school language exams missing blanks carefully created teachers candidate choices purposely designed confusing cloth requires deeper language understanding wider attention span previous automatically generated cloze datasets show humans outperform dedicated designed baseline models significant margin even model trained sufficiently large external data investigate source performance gap trace model deficiencies distinct properties cloth identify limited ability comprehending long term context key bottleneck addition find human designed data leads larger gap model performance human performance compared automatically generated data\n",
            "output sentence:  cloze test dataset designed teachers assess language proficiency \n",
            "\n",
            "{'rouge-1': {'r': 0.12244897959183673, 'p': 0.8, 'f': 0.21238937822852222}, 'rouge-2': {'r': 0.05970149253731343, 'p': 0.47058823529411764, 'f': 0.10596026290250432}, 'rouge-l': {'r': 0.12244897959183673, 'p': 0.8, 'f': 0.21238937822852222}}\n",
            "pair:  representation learning graph structured data received significant attention recently due ubiquitous applicability however advancements made static graph settings efforts jointly learning dynamic graph dynamic graph still infant stage two fundamental questions arise learning dynamic graphs elegantly model dynamical processes graphs ii leverage model effectively encode evolving graph information low dimensional representations present dyrep novel modeling framework dynamic graphs posits representation learning latent mediation process bridging two observed processes namely dynamics network realized topological evolution dynamics network realized activities nodes concretely propose two time scale deep temporal point process model captures interleaved dynamics observed processes model parameterized temporal attentive representation network encodes temporally evolving structural information node representations turn drives nonlinear evolution observed graph dynamics unified framework trained using efficient unsupervised procedure capability generalize unseen nodes demonstrate dyrep outperforms state art baselines dynamic link prediction time prediction tasks present extensive qualitative insights framework\n",
            "output sentence:  models representation learning dynamic graphs latent hidden process bridging two observed processes topological evolution dynamic evolution dynamic dynamic interactions \n",
            "\n",
            "{'rouge-1': {'r': 0.17647058823529413, 'p': 0.9, 'f': 0.29508196447191626}, 'rouge-2': {'r': 0.1267605633802817, 'p': 0.9, 'f': 0.22222222005791806}, 'rouge-l': {'r': 0.17647058823529413, 'p': 0.9, 'f': 0.29508196447191626}}\n",
            "pair:  investigate variant variational autoencoders superstructure discrete latent variables top latent features general superstructure tree structure multiple super latent variables automatically learned data one latent variable superstructure model reduces one assumes latent features generated gaussian mixture model call model latent tree variational autoencoder ltvae whereas previous deep learning methods clustering produce one partition data ltvae produces multiple partitions data given one super latent variable desirable high dimensional data usually many different natural facets meaningfully partitioned multiple ways\n",
            "output sentence:  investigate variant variational autoencoders superstructure discrete latent variables top latent features \n",
            "\n",
            "{'rouge-1': {'r': 0.042105263157894736, 'p': 0.5, 'f': 0.0776699014798756}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.031578947368421054, 'p': 0.375, 'f': 0.05825242575172028}}\n",
            "pair:  make deep neural networks feasible resource constrained environments mobile devices beneficial quantize models using low precision weights one common technique quantizing neural networks straight gradient method enables back propagation quantization mapping despite empirical success little understood straight gradient method works building upon novel observation straight gradient method fact identical well known nesterov dual averaging algorithm quantization constrained optimization problem propose principled alternative approach called proxquant formulates quantized network training regularized learning problem instead optimizes via prox gradient method proxquant back propagation underlying full precision vector applies efficient prox operator stochastic gradient steps encourage quantizedness quantizing resnets lstms proxquant outperforms state art results binary quantization par state art multi bit quantization binary quantization analysis shows theoretically experimentally proxquant stable straight gradient method binaryconnect challenging indispensability straight gradient method providing powerful alternative\n",
            "output sentence:  principled framework model quantization using proximal gradient method \n",
            "\n",
            "{'rouge-1': {'r': 0.08139534883720931, 'p': 0.7, 'f': 0.1458333314670139}, 'rouge-2': {'r': 0.028037383177570093, 'p': 0.3333333333333333, 'f': 0.051724136499702776}, 'rouge-l': {'r': 0.06976744186046512, 'p': 0.6, 'f': 0.12499999813368057}}\n",
            "pair:  machine learning models including traditional models neural networks easily fooled adversarial examples generated natural examples small perturbations poses critical challenge machine learning security impedes wide application machine learning many important domains computer vision malware detection unfortunately even state art defense approaches adversarial training defensive distillation still suffer major limitations circumvented unique angle propose investigate two important research questions paper adversarial examples distinguishable natural examples adversarial examples generated different methods distinguishable two questions concern distinguishability adversarial examples answering potentially lead simple yet effective approach termed defensive distinction paper formulation multi label classification protecting adversarial examples design perform experiments using mnist dataset investigate two questions obtain highly positive results demonstrating strong distinguishability adversarial examples recommend unique defensive distinction approach seriously considered complement defense approaches\n",
            "output sentence:  propose defensive distinction protection approach demonstrate strong distinguishability adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.0641025641025641, 'p': 0.7142857142857143, 'f': 0.11764705731211073}, 'rouge-2': {'r': 0.028846153846153848, 'p': 0.5, 'f': 0.0545454535140496}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.7142857142857143, 'f': 0.11764705731211073}}\n",
            "pair:  neural embeddings used great success natural language processing nlp provide compact representations encapsulate word similarity attain state art performance range linguistic tasks success neural embeddings prompted significant amounts research applications domains language one domain graph structured data embeddings vertices learned encapsulate vertex similarity improve performance tasks including edge prediction vertex labelling nlp graph based tasks embeddings high dimensional euclidean spaces learned however recent work shown appropriate isometric space embedding complex networks flat euclidean space negatively curved hyperbolic space present new concept exploits recent insights propose learning neural embeddings graphs hyperbolic space provide experimental evidence hyperbolic embeddings significantly outperform euclidean embeddings vertex classification tasks several real world public datasets\n",
            "output sentence:  learn neural embeddings graphs hyperbolic instead euclidean space \n",
            "\n",
            "{'rouge-1': {'r': 0.05084745762711865, 'p': 0.375, 'f': 0.08955223670305197}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.375, 'f': 0.08955223670305197}}\n",
            "pair:  deep learning models rely expressive high dimensional representations achieve good performance tasks classification however high dimensionality representations makes difficult interpret prone fitting propose simple intuitive scalable dimension reduction framework takes account soft probabilistic interpretation standard deep models classification applying framework visualization representations accurately reflect inter class distances standard visualization techniques sne show experimentally framework improves generalization performance unseen categories zero shot learning also provide finite sample error upper bound guarantee method\n",
            "output sentence:  dimensionality reduction cases examples represented soft probability distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.4166666666666667, 'f': 0.15873015564625853}, 'rouge-2': {'r': 0.01818181818181818, 'p': 0.09090909090909091, 'f': 0.03030302752525278}, 'rouge-l': {'r': 0.09803921568627451, 'p': 0.4166666666666667, 'f': 0.15873015564625853}}\n",
            "pair:  deep neural networks proven powerful tool many recognition classification tasks stability properties still well understood past image classifiers shown vulnerable called adversarial attacks created additively perturbing correctly classified image paper propose adef algorithm construct different kind adversarial attack created iteratively applying small deformations image found gradient descent step demonstrate results mnist convolutional neural networks imagenet inception resnet\n",
            "output sentence:  propose new efficient algorithm construct adversarial examples means deformations rather additive perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.75, 'f': 0.2142857118367347}, 'rouge-2': {'r': 0.020618556701030927, 'p': 0.18181818181818182, 'f': 0.03703703520747608}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.3333333333333333, 'f': 0.09523809278911571}}\n",
            "pair:  many large text collections exhibit graph structures either inherent content encoded metadata individual documents example graphs extracted document collections co author networks citation networks named entity cooccurrence networks furthermore social networks extracted email corpora tweets social media comes visualising large corpora either textual content network graph used paper propose incorporate text graph visualise semantic information encoded documents content also relationships expressed inherent network structure end introduce novel algorithm based multi objective optimisation jointly position embedded documents graph nodes two dimensional landscape illustrate effectiveness approach real world datasets show capture semantics large document collections better visualisations based either content network information\n",
            "output sentence:  dimensionality reduction algorithm visualise text network information example email corpus co authorships \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.6, 'f': 0.15584415358407824}, 'rouge-2': {'r': 0.0379746835443038, 'p': 0.3333333333333333, 'f': 0.06818181634555791}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.4, 'f': 0.10389610163602636}}\n",
            "pair:  backpropagation driving today artificial neural networks however despite extensive research remains unclear brain implements algorithm among neuroscientists reinforcement learning rl algorithms often seen realistic alternative however convergence rate learning scales poorly number involved neurons propose hybrid learning approach neuron uses rl type strategy learn approximate gradients backpropagation would provide show approach learns approximate gradient match performance gradient based learning fully connected convolutional networks learning feedback weights provides biologically plausible mechanism achieving good performance without need precise pre specified learning rules\n",
            "output sentence:  perturbations used learn feedback weights large fully connected convolutional networks \n",
            "\n",
            "{'rouge-1': {'r': 0.1595744680851064, 'p': 1.0, 'f': 0.2752293554246276}, 'rouge-2': {'r': 0.07751937984496124, 'p': 0.6666666666666666, 'f': 0.13888888702256946}, 'rouge-l': {'r': 0.10638297872340426, 'p': 0.6666666666666666, 'f': 0.1834862361585725}}\n",
            "pair:  investigate task clustering deep learning based multi task shot learning settings large numbers diverse tasks method measures task similarities using cross task transfer performance matrix although matrix provides us critical information regarding similarities tasks uncertain task pairs ones extremely asymmetric transfer scores may collectively mislead clustering algorithms output inaccurate task partition moreover number tasks large generating full transfer performance matrix time consuming overcome limitations propose novel task clustering algorithm estimate similarity matrix based theory matrix completion proposed algorithm work partially observed similarity matrices based sampled task pairs reliable scores ensuring efficiency robustness theoretical analysis shows mild assumptions reconstructed matrix perfectly matches underlying true similarity matrix overwhelming probability final task partition computed applying efficient spectral clustering algorithm recovered matrix results show new task clustering method discover task clusters benefit multi task learning shot learning setups sentiment classification dialog intent classification tasks\n",
            "output sentence:  propose matrix completion based task clustering algorithm deep multi task shot learning settings large numbers diverse tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.1794871794871795, 'p': 0.6363636363636364, 'f': 0.279999996568}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.23809523809523808, 'f': 0.09433961946422224}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.5909090909090909, 'f': 0.25999999656800005}}\n",
            "pair:  vanilla rnn relu activation simple structure amenable systematic dynamical systems analysis interpretation suffer exploding vs vanishing gradients problem recent attempts retain simplicity alleviating gradient problem based proper initialization schemes orthogonality unitary constraints rnn recurrency matrix however comes limitations expressive power regards dynamical systems phenomena like chaos multi stability instead suggest regularization scheme pushes part rnn latent subspace toward line attractor configuration enables long short term memory arbitrarily slow time scales show approach excels number benchmarks like sequential mnist multiplication problems enables reconstruction dynamical systems harbor widely different time scales\n",
            "output sentence:  develop new optimization approach vanilla relu based rnn enables long short term memory identification arbitrary nonlinear dynamical systems widely differing time scales \n",
            "\n",
            "{'rouge-1': {'r': 0.0594059405940594, 'p': 0.6, 'f': 0.10810810646863081}, 'rouge-2': {'r': 0.022388059701492536, 'p': 0.3333333333333333, 'f': 0.04195804077852221}, 'rouge-l': {'r': 0.04950495049504951, 'p': 0.5, 'f': 0.0900900884506128}}\n",
            "pair:  concepts unitary evolution matrices associative memory boosted field recurrent neural networks rnn state art performance variety sequential tasks however rnn still limited capacity manipulate long term memory bypass weakness successful applications rnn use external techniques attention mechanisms paper propose novel rnn model unifies state art approaches rotational unit memory rum core rum rotational operation naturally unitary matrix providing architectures power learn long term dependencies overcoming vanishing exploding gradients problem moreover rotational unit also serves associative memory evaluate model synthetic memorization question answering language modeling tasks rum learns copying memory task completely improves state art result recall task rum performance babi question answering task comparable models attention mechanism also improve state art result bits per character bpc loss character level penn treebank ptb task signify applications rum real world sequential data universality construction core rnn establishes rum promising approach language modeling speech recognition machine translation\n",
            "output sentence:  novel rnn model outperforms significantly current frontier models variety sequential tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.07142857142857142, 'p': 0.5555555555555556, 'f': 0.12658227646210546}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.125, 'f': 0.020618555187586464}, 'rouge-l': {'r': 0.05714285714285714, 'p': 0.4444444444444444, 'f': 0.10126582076590292}}\n",
            "pair:  two major paradigms white box adversarial attacks attempt impose input perturbations first paradigm called fix perturbation attack crafts adversarial samples within given perturbation level second paradigm called zero confidence attack finds smallest perturbation needed cause misclassification also known margin input feature former paradigm well resolved latter existing zero confidence attacks either introduce significant approximation errors time consuming therefore propose marginattack zero confidence attack framework able compute margin improved accuracy efficiency experiments show marginattack able compute smaller margin state art zero confidence attacks matches state art fix perturbation attacks addition runs significantly faster carlini wagner attack currently accurate zero confidence attack algorithm\n",
            "output sentence:  paper introduces marginattack stronger faster zero confidence adversarial attack \n",
            "\n",
            "{'rouge-1': {'r': 0.08888888888888889, 'p': 0.8, 'f': 0.1599999982}, 'rouge-2': {'r': 0.025, 'p': 0.3, 'f': 0.04615384473372785}, 'rouge-l': {'r': 0.07777777777777778, 'p': 0.7, 'f': 0.13999999820000003}}\n",
            "pair:  modern applications autonomous vehicles video surveillance generate massive amounts image data work propose novel image outlier detection approach iod short leverages cutting edge image classifier discover outliers without using labeled outlier observe although intuitively confidence convolutional neural network cnn image belongs particular class could serve outlierness measure image directly applying confidence detect outlier work well cnn often high confidence outlier image belong target class due generalization ability ensures high accuracy classification solve issue propose deep neural forest based approach harmonizes contradictory requirements accurately classifying images correctly detecting outlier images experiments using several benchmark image datasets including mnist cifar cifar svhn demonstrate effectiveness iod approach outlier detection capturing outliers generated injecting one image dataset another still preserving classification accuracy multi class classification problem\n",
            "output sentence:  novel approach detects outliers image data preserving classification accuracy image classification \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.6, 'f': 0.12903225614521913}, 'rouge-2': {'r': 0.019801980198019802, 'p': 0.2222222222222222, 'f': 0.03636363486115709}, 'rouge-l': {'r': 0.060240963855421686, 'p': 0.5, 'f': 0.10752687980113311}}\n",
            "pair:  data breaches involve information accessed unauthorized parties research concerns user perception data breaches especially issues relating accountability preliminary study indicated many people weak understanding issues felt somehow responsible speculated impression might stem organizational communication strategies therefore compared texts organizations external sources news media suggested organizations use well known crisis communication methods reduce reputational damage strategies align repositioning narrative elements involved story conducted quantitative study asking participants rate either organizational texts news texts breaches findings study line document analysis suggest organizational communication affects users perception victimization attitudes data protection accountability study suggests software design legal implications supporting users protect develop better mental models security breaches\n",
            "output sentence:  paper tested communication strategies influence users mental models data breach \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.16666666666666666, 'f': 0.03960395830212735}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.46153846153846156, 'f': 0.14117646799723185}}\n",
            "pair:  present meta learning approach adaptive text speech tts data training learn multi speaker model using shared conditional wavenet core independent learned embeddings speaker aim training produce neural network fixed weights deployed tts system instead aim produce network requires data deployment time rapidly adapt new speakers introduce benchmark three strategies learning speaker embedding keeping wavenet core fixed ii fine tuning entire architecture stochastic gradient descent iii predicting speaker embedding trained neural network encoder experiments show approaches successful adapting multi speaker neural network new speakers obtaining state art results sample naturalness voice similarity merely minutes audio data new speakers\n",
            "output sentence:  sample efficient algorithms adapt text speech model new voice style state art performance \n",
            "\n",
            "{'rouge-1': {'r': 0.02857142857142857, 'p': 0.2857142857142857, 'f': 0.051948050295159434}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02857142857142857, 'p': 0.2857142857142857, 'f': 0.051948050295159434}}\n",
            "pair:  paper present method adversarial decomposition text representation method used decompose representation input sentence several independent vectors vector responsible specific aspect input sentence evaluate proposed method two case studies conversion different social registers diachronic language change show proposed method capable fine grained con trolled change aspects input sentence example model capable learning continuous rather categorical representation style sentence line reality language use model uses adversarial motivational training includes special motivational loss acts opposite discriminator encourages better decomposition finally evaluate obtained meaning embeddings downstream task para phrase detection show significantly better embeddings regular autoencoder\n",
            "output sentence:  method learns separate representations meaning form sentence \n",
            "\n",
            "{'rouge-1': {'r': 0.24, 'p': 0.6666666666666666, 'f': 0.3529411725778547}, 'rouge-2': {'r': 0.06557377049180328, 'p': 0.23529411764705882, 'f': 0.1025640991551612}, 'rouge-l': {'r': 0.14, 'p': 0.3888888888888889, 'f': 0.20588234904844294}}\n",
            "pair:  building recent successes distributed training rl agents paper investigate training rnn based rl agents distributed prioritized experience replay study effects parameter lag resulting representational drift recurrent state staleness empirically derive improved training strategy using single network architecture fixed set hyper parameters resulting agent recurrent replay distributed dqn quadruples previous state art atari matches state art dmlab first agent exceed human level performance atari games\n",
            "output sentence:  investigation combining recurrent neural networks experience replay leading state art agent atari dmlab using single set hyper parameters \n",
            "\n",
            "{'rouge-1': {'r': 0.26865671641791045, 'p': 0.9473684210526315, 'f': 0.4186046477203894}, 'rouge-2': {'r': 0.1839080459770115, 'p': 0.8421052631578947, 'f': 0.30188678951050196}, 'rouge-l': {'r': 0.26865671641791045, 'p': 0.9473684210526315, 'f': 0.4186046477203894}}\n",
            "pair:  statistical inference methods fundamentally important machine learning state art inference algorithms variants markov chain monte carlo mcmc variational inference vi however methods struggle limitations practice mcmc methods computationally demanding vi methods may large bias work aim improve upon mcmc vi novel hybrid method based idea reducing simulation bias finite length mcmc chains using gradient based optimisation proposed method generate low biased samples increasing length mcmc simulation optimising mcmc hyper parameters offers attractive balance approximation bias computational efficiency show method produces promising results popular benchmarks compared recent hybrid methods mcmc vi\n",
            "output sentence:  work aim improve upon mcmc vi novel hybrid method based idea reducing simulation bias finite length using using using gradient gradient \n",
            "\n",
            "{'rouge-1': {'r': 0.2361111111111111, 'p': 1.0, 'f': 0.3820224688195935}, 'rouge-2': {'r': 0.1348314606741573, 'p': 0.75, 'f': 0.22857142598820862}, 'rouge-l': {'r': 0.19444444444444445, 'p': 0.8235294117647058, 'f': 0.31460673848251486}}\n",
            "pair:  deep learning yielded state art performance many natural language processing tasks including named entity recognition ner however typically requires large amounts labeled data work demonstrate amount labeled training data drastically reduced deep learning combined active learning active learning sample efficient computationally expensive since requires iterative retraining speed introduce lightweight architecture ner viz cnn cnn lstm model consisting convolutional character word encoders long short term memory lstm tag decoder model achieves nearly state art performance standard datasets task computationally much efficient best performing models carry incremental active learning training process able nearly match state art performance original training data\n",
            "output sentence:  introduce lightweight architecture named entity recognition carry incremental active learning able match state art performance original training data \n",
            "\n",
            "{'rouge-1': {'r': 0.1038961038961039, 'p': 0.6153846153846154, 'f': 0.1777777753061729}, 'rouge-2': {'r': 0.061224489795918366, 'p': 0.46153846153846156, 'f': 0.10810810604009419}, 'rouge-l': {'r': 0.1038961038961039, 'p': 0.6153846153846154, 'f': 0.1777777753061729}}\n",
            "pair:  propose novel framework generate clean video frames single motion blurred image broad range literature focuses recovering single image blurred image work tackle challenging task video restoration blurred image formulate video restoration single blurred image inverse problem setting clean image sequence respective motion latent factors blurred image observation framework based encoder decoder structure spatial transformer network modules restore video sequence underlying motion end end manner design loss function regularizers complementary properties stabilize training analyze variant models proposed network effectiveness transferability network highlighted large set experiments two different types datasets camera rotation blurs generated panorama scenes dynamic motion blurs high speed videos code models publicly available\n",
            "output sentence:  present novel unified architecture restores video frames single motion blurred image end end manner \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.5882352941176471, 'f': 0.25974025629954467}, 'rouge-2': {'r': 0.06172839506172839, 'p': 0.29411764705882354, 'f': 0.10204081345897552}, 'rouge-l': {'r': 0.1, 'p': 0.35294117647058826, 'f': 0.1558441524034408}}\n",
            "pair:  log linear models models widely used machine learning particular ubiquitous deep learning architectures form softmax exact inference learning requires linear time done approximately sub linear time strong concentrations guarantees work present lsh softmax method perform sub linear learning inference softmax layer deep learning setting method relies popular locality sensitive hashing build well concentrated gradient estimator using nearest neighbors uniform samples also present inference scheme sub linear time lsh softmax using gumbel distribution language modeling show recurrent neural networks trained lsh softmax perform par computing exact softmax requiring sub linear computations\n",
            "output sentence:  present lsh softmax softmax approximation layer sub linear learning inference strong theoretical guarantees showcase applicability efficiency evaluating real task \n",
            "\n",
            "{'rouge-1': {'r': 0.1864406779661017, 'p': 0.6111111111111112, 'f': 0.28571428213189415}, 'rouge-2': {'r': 0.1, 'p': 0.3888888888888889, 'f': 0.15909090583677693}, 'rouge-l': {'r': 0.1864406779661017, 'p': 0.6111111111111112, 'f': 0.28571428213189415}}\n",
            "pair:  goal survival clustering map subjects users social network patients medical study clusters ranging low risk high risk existing survival methods assume presence clear textit end life signals introduce artificially using pre defined timeout paper forego assumption introduce loss function differentiates empirical lifetime distributions clusters using modified kuiper statistic learn deep neural network optimizing loss performs soft clustering users survival groups apply method social network dataset subjects show significant improvement index compared alternatives\n",
            "output sentence:  goal survival clustering map subjects clusters without end life signals challenging task address task propose new loss function modifying \n",
            "\n",
            "{'rouge-1': {'r': 0.06593406593406594, 'p': 0.5454545454545454, 'f': 0.11764705689926953}, 'rouge-2': {'r': 0.015748031496062992, 'p': 0.2, 'f': 0.029197078938675538}, 'rouge-l': {'r': 0.04395604395604396, 'p': 0.36363636363636365, 'f': 0.07843137062475976}}\n",
            "pair:  myriad kinds segmentation ultimately right segmentation given scene eye annotator standard approaches require large amounts labeled data learn one particular kind segmentation first step towards relieving annotation burden propose problem guided segmentation given varying amounts pixel wise labels segment unannotated pixels propagating supervision locally within image non locally across images propose guided networks extract latent task representation guidance variable amounts classes categories instances etc pixel supervision optimize architecture end end fast accurate data efficient segmentation meta learning span shot many shot learning regimes examine guidance little one pixel per concept much images compare full gradient optimization extremes explore generalization analyze guidance bridge different levels supervision segment classes union instances segmentor concentrates different amounts supervision different types classes efficient latent representation non locally propagates supervision across images updated quickly cumulatively given supervision\n",
            "output sentence:  propose meta learning approach guiding visual segmentation tasks varying amounts supervision \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5714285714285714, 'f': 0.18604650890210925}, 'rouge-2': {'r': 0.024390243902439025, 'p': 0.15384615384615385, 'f': 0.042105260795568006}, 'rouge-l': {'r': 0.06944444444444445, 'p': 0.35714285714285715, 'f': 0.1162790670416442}}\n",
            "pair:  recent literature demonstrated promising results training generative adversarial networks employing set discriminators opposed traditional game involving one generator single adversary methods perform single objective optimization simple consolidation losses average work revisit multiple discriminator approach framing simultaneous minimization losses provided different models multi objective optimization problem specifically evaluate performance multiple gradient descent hypervolume maximization algorithm number different datasets moreover argue previously proposed methods hypervolume maximization seen variations multiple gradient descent update direction computation done efficiently results indicate hypervolume maximization presents better compromise sample quality diversity computational cost previous methods\n",
            "output sentence:  introduce hypervolume maximization training gans multiple discriminators showing performance improvements terms sample quality diversity \n",
            "\n",
            "{'rouge-1': {'r': 0.15, 'p': 0.6521739130434783, 'f': 0.24390243598387207}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.34782608695652173, 'f': 0.11188810918871346}, 'rouge-l': {'r': 0.13, 'p': 0.5652173913043478, 'f': 0.21138211078062003}}\n",
            "pair:  recently neural network based forward dynamics models proposed attempt learn dynamics physical systems deterministic way near term motion predicted accurately long term predictions suffer accumulating input prediction errors lead plausible different trajectories diverge ground truth system predicts distributions future physical states long time horizons based uncertainty thus promising solution work introduce novel robust monte carlo sampling based graph convolutional dropout method allows us sample multiple plausible trajectories initial state given neural network based forward dynamics predictor introducing new shape preservation loss training dynamics model recurrently stabilize long term predictions show model long term forward dynamics prediction errors complicated physical interactions rigid deformable objects various shapes significantly lower existing strong baselines lastly demonstrate generating multiple trajectories monte carlo dropout method used train model free reinforcement learning agents faster better solutions simple manipulation tasks\n",
            "output sentence:  propose stochastic differentiable forward dynamics predictor able sample multiple physically plausible trajectories initial input state show used train model free policies efficiently policies efficiently \n",
            "\n",
            "{'rouge-1': {'r': 0.2647058823529412, 'p': 0.8181818181818182, 'f': 0.3999999963061729}, 'rouge-2': {'r': 0.2, 'p': 0.7, 'f': 0.311111107654321}, 'rouge-l': {'r': 0.2647058823529412, 'p': 0.8181818181818182, 'f': 0.3999999963061729}}\n",
            "pair:  paper presents system immersive visualization non euclidean spaces using real time ray tracing exploits capabilities new generation gpu based nvidia turing architecture order develop new methods intuitive exploration landscapes featuring non trivial geometry topology virtual reality\n",
            "output sentence:  immersive visualization classical non euclidean spaces using real time ray tracing \n",
            "\n",
            "{'rouge-1': {'r': 0.1125, 'p': 0.6923076923076923, 'f': 0.1935483846918719}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.3333333333333333, 'f': 0.06779660834243038}, 'rouge-l': {'r': 0.1, 'p': 0.6153846153846154, 'f': 0.1720430083477859}}\n",
            "pair:  present domain adaptation method transferring neural representations label rich source domains unlabeled target domains recent adversarial methods proposed task learn align features across domains fooling special domain classifier network however drawback approach domain classifier simply labels generated features domain without considering boundaries classes means ambiguous target features generated near class boundaries reducing target classification accuracy propose novel approach adversarial dropout regularization adr encourages generator output discriminative features target domain key idea replace traditional domain critic critic detects non discriminative features using dropout classifier network generator learns avoid areas feature space thus creates better features apply adr approach problem unsupervised domain adaptation image classification semantic segmentation tasks demonstrate significant improvements state art\n",
            "output sentence:  present new adversarial method adapting neural representations based critic detects non discriminative features \n",
            "\n",
            "{'rouge-1': {'r': 0.10465116279069768, 'p': 0.9, 'f': 0.1874999981336806}, 'rouge-2': {'r': 0.08247422680412371, 'p': 0.8888888888888888, 'f': 0.1509433946724813}, 'rouge-l': {'r': 0.10465116279069768, 'p': 0.9, 'f': 0.1874999981336806}}\n",
            "pair:  propose robust bayesian deep learning algorithm infer complex posteriors latent variables inspired dropout popular tool regularization model ensemble assign sparse priors weights deep neural networks dnn order achieve automatic dropout avoid fitting alternatively sampling posterior distribution stochastic gradient markov chain monte carlo sg mcmc optimizing latent variables via stochastic approximation sa trajectory target weights proved converge true posterior distribution conditioned optimal latent variables ensures stronger regularization fitted parameter space accurate uncertainty quantification decisive variables simulations large small regressions showcase robustness method applied models latent variables additionally application convolutional neural networks cnn leads state art performance mnist fashion mnist datasets improved resistance adversarial attacks\n",
            "output sentence:  robust bayesian deep learning algorithm infer complex posteriors latent variables \n",
            "\n",
            "{'rouge-1': {'r': 0.07777777777777778, 'p': 0.7, 'f': 0.13999999820000003}, 'rouge-2': {'r': 0.02912621359223301, 'p': 0.3333333333333333, 'f': 0.05357142709343116}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  neural networks widely used natural language processing yet despite empirical successes behaviour brittle sensitive small input changes sensitive deletions large fractions input text paper aims tackle sensitivity context natural language inference ensuring models become confident predictions arbitrary subsets words input text deleted develop novel technique formal verification specification models based popular decomposable attention mechanism employing efficient yet effective interval bound propagation ibp approach using method efficiently prove given model whether particular sample free sensitivity problem compare different training methods address sensitivity compare metrics measure experiments snli mnli datasets observe ibp training leads significantly improved verified accuracy snli test set verify samples substantial improvement using standard training\n",
            "output sentence:  formal verification specification model prediction undersensitivity using interval bound propagation \n",
            "\n",
            "{'rouge-1': {'r': 0.12371134020618557, 'p': 0.631578947368421, 'f': 0.2068965489848395}, 'rouge-2': {'r': 0.008, 'p': 0.05555555555555555, 'f': 0.013986011785417725}, 'rouge-l': {'r': 0.061855670103092786, 'p': 0.3157894736842105, 'f': 0.10344827312277059}}\n",
            "pair:  work propose goal driven collaborative task contains language vision action virtual environment core components specifically develop collaborative image drawing game two agents called codraw game grounded virtual world contains movable clip art objects game involves two players teller drawer teller sees abstract scene containing multiple clip art pieces semantically meaningful configuration drawer tries reconstruct scene empty canvas using available clip art pieces two players communicate via two way communication using natural language collect codraw dataset dialogs consisting messages exchanged human agents define protocols metrics evaluate effectiveness learned agents testbed highlighting need novel crosstalk condition pairs agents trained independently disjoint subsets training data evaluation present models task including simple effective baselines neural network approaches trained using combination imitation learning goal driven training models benchmarked using fully automated evaluation playing game live human agents\n",
            "output sentence:  introduce dataset models training evaluation protocols collaborative drawing task allows studying goal driven perceptually actionably grounded language generation understanding \n",
            "\n",
            "{'rouge-1': {'r': 0.047619047619047616, 'p': 0.8, 'f': 0.08988763938896605}, 'rouge-2': {'r': 0.010101010101010102, 'p': 0.25, 'f': 0.01941747498161941}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.8, 'f': 0.08988763938896605}}\n",
            "pair:  neural language models nlms generative model distribution grammatical sentences trained huge corpus nlms pushing limit modeling accuracy besides also applied supervised learning tasks decode text automatic speech recognition asr scoring best list nlm select grammatically correct candidate among list significantly reduce word char error rate however generative nature nlm may guarantee discrimination good bad task specific sense sentences resulting suboptimal performance work proposes approach adapt generative nlm discriminative one different commonly used maximum likelihood objective proposed method aims enlarging margin good bad sentences trained end end widely applied tasks involve scoring decoded text significant gains observed asr statistical machine translation smt tasks\n",
            "output sentence:  enhance language model supervised learning task \n",
            "\n",
            "{'rouge-1': {'r': 0.08433734939759036, 'p': 0.7777777777777778, 'f': 0.1521739112783554}, 'rouge-2': {'r': 0.009615384615384616, 'p': 0.125, 'f': 0.017857141530612346}, 'rouge-l': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}}\n",
            "pair:  transfer learning fine tuning pre trained neural network extremely large dataset imagenet significantly accelerate training accuracy frequently bottlenecked limited dataset size new target task solve problem regularization methods constraining outer layer weights target network using starting point references spar studied paper propose novel regularized transfer learning framework delta namely deep learning transfer using feature map attention instead constraining weights neural network delta aims preserve outer layer outputs target network specifically addition minimizing empirical loss delta intends align outer layer outputs two networks constraining subset feature maps precisely selected attention learned supervised learning manner evaluate delta state art algorithms including sp experiment results show proposed method outperforms baselines higher accuracy new tasks\n",
            "output sentence:  improving deep transfer learning regularization using attention based feature maps \n",
            "\n",
            "{'rouge-1': {'r': 0.16363636363636364, 'p': 0.9, 'f': 0.2769230743195267}, 'rouge-2': {'r': 0.11594202898550725, 'p': 0.8888888888888888, 'f': 0.20512820308678503}, 'rouge-l': {'r': 0.16363636363636364, 'p': 0.9, 'f': 0.2769230743195267}}\n",
            "pair:  adversaries neural networks drawn much attention since first debut existing methods aim deceiving image classification models misclassification crafting attacks specific object instances object setection tasks focus creating universal adversaries fool object detectors hide objects detectors adversaries examine universal three ways specific specific object instances image independent transfer different unknown models achieve propose two novel techniques improve transferability adversaries textit piling textit monochromatization techniques prove simplify patterns generated adversaries ultimately result higher transferability\n",
            "output sentence:  focus creating universal adversaries fool object detectors hide objects detectors \n",
            "\n",
            "{'rouge-1': {'r': 0.07777777777777778, 'p': 0.6363636363636364, 'f': 0.13861385944515245}, 'rouge-2': {'r': 0.018018018018018018, 'p': 0.2, 'f': 0.03305784972337962}, 'rouge-l': {'r': 0.044444444444444446, 'p': 0.36363636363636365, 'f': 0.07920791885109307}}\n",
            "pair:  recurrent neural networks rnns widely used models sequence data like feedforward networks become common build deep rnns stack multiple recurrent layers obtain higher level abstractions data however works handful layers unlike feedforward networks stacking recurrent units lstm cells usually hurts model performance reason vanishing exploding gradients training investigate training multi layer rnns examine magnitude gradients propagate network show depending structure basic recurrent unit gradients systematically attenuated amplified increasing depth tend vanish respectively explode based analysis design new type gated cell better preserves gradient magnitude therefore makes possible train deeper rnns experimentally validate design five different sequence modelling tasks three different datasets proposed stackable recurrent star cell allows substantially deeper recurrent architectures improved performance\n",
            "output sentence:  analyze gradient propagation deep rnns analysis propose new multi layer deep rnn \n",
            "\n",
            "{'rouge-1': {'r': 0.08888888888888889, 'p': 0.5, 'f': 0.15094339366322537}, 'rouge-2': {'r': 0.008771929824561403, 'p': 0.06666666666666667, 'f': 0.015503873913827569}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.375, 'f': 0.11320754460662165}}\n",
            "pair:  fine grained entity recognition fger task detecting classifying entity mentions large set types spanning diverse domains biomedical finance sports observe type set spans several domains detection entity mention becomes limitation supervised learning models primary reason lack dataset entity boundaries properly annotated covering large spectrum entity types work directly addresses issue propose heuristics allied distant supervision hands framework automatically construct quality dataset suitable fger task hands framework exploits high interlink among wikipedia freebase pipelined manner reducing annotation errors introduced naively using distant supervision approach using hands framework create two datasets one suitable building fger systems recognizing entity types based figer type hierarchy another entity types based typenet hierarchy extensive empirical experimentation warrants quality generated datasets along also provide manually annotated dataset benchmarking fger systems\n",
            "output sentence:  initiate push towards building er systems recognize thousands types providing method automatically construct suitable datasets based type \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.625, 'f': 0.14492753418189458}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03278688524590164, 'p': 0.25, 'f': 0.0579710124427642}}\n",
            "pair:  current literature machine learning holds unaligned self interested agents learn use emergent communication channel introduce new sender receiver game study emergent communication spectrum partially competitive scenarios put special care evaluation find communication indeed emerge partially competitive scenarios discover three things tied improving first selfish communication proportional cooperation naturally occurs situations cooperative competitive second stability performance improved using lola foerster et al especially competitive scenarios third discrete protocols lend better learning cooperative communication continuous ones\n",
            "output sentence:  manage emerge communication selfish agents contrary current view ml \n",
            "\n",
            "{'rouge-1': {'r': 0.14545454545454545, 'p': 0.6666666666666666, 'f': 0.23880596720873248}, 'rouge-2': {'r': 0.0821917808219178, 'p': 0.46153846153846156, 'f': 0.13953488115467824}, 'rouge-l': {'r': 0.10909090909090909, 'p': 0.5, 'f': 0.17910447467141904}}\n",
            "pair:  verification planning domain models crucial ensure safety integrity correctness planning based automated systems task usually performed using model checking techniques however directly applying model checkers verify planning domain models result false positives counterexamples unreachable sound planner using domain verification planning task paper discuss downside unconstrained planning domain model verification propose fail safe practice designing planning domain models inherently guarantee safety produced plans case undetected errors domain models addition demonstrate model checkers well state trajectory constraints planning techniques used verify planning domain models unreachable counterexamples returned\n",
            "output sentence:  constrain planning domain model verification planning goals avoid unreachable counterexamples false positives verification outcomes \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 0.875, 'f': 0.20895522177767878}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.625, 'f': 0.12048192596893599}, 'rouge-l': {'r': 0.11864406779661017, 'p': 0.875, 'f': 0.20895522177767878}}\n",
            "pair:  bayesian learning model parameters neural networks important scenarios estimates well calibrated uncertainty important paper propose bayesian quantized networks bqns quantized neural networks qnns learn posterior distribution discrete parameters provide set efficient algorithms learning prediction bqns without need sample parameters activations allows differentiable learning quantized models also reduces variance gradients estimation evaluate bqns mnist fashion mnist kmnist classification datasets compared bootstrap ensemble qnns qnn demonstrate bqns achieve lower predictive errors better calibrated uncertainties qnn less negative log likelihood\n",
            "output sentence:  propose bayesian quantized networks learn posterior distribution quantized parameters \n",
            "\n",
            "{'rouge-1': {'r': 0.07291666666666667, 'p': 0.875, 'f': 0.1346153831952663}, 'rouge-2': {'r': 0.040983606557377046, 'p': 0.7142857142857143, 'f': 0.0775193788185806}, 'rouge-l': {'r': 0.0625, 'p': 0.75, 'f': 0.11538461396449705}}\n",
            "pair:  exploration learning representations one main challenges deep reinforcement learning drl faces today learned representation dependant observed data exploration strategy crucial role popular dqn algorithm improved significantly capabilities reinforcement learning rl algorithms learn state representations raw data yet uses naive exploration strategy statistically inefficient randomized least squares value iteration rlsvi algorithm osband et al hand explores generalizes efficiently via linearly parameterized value functions however based hand designed state representation requires prior engineering work every environment paper propose deep learning adaptation rlsvi rather using hand design state representation use state representation learned directly data dqn agent representation optimized learning process key component suggested method likelihood matching mechanism adapts changing representations demonstrate importance various properties algorithm toy problem show method outperforms dqn five atari benchmarks reaching competitive results rainbow algorithm\n",
            "output sentence:  deep learning adaptation randomized least squares value iteration \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.7, 'f': 0.16279069561925366}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.2222222222222222, 'f': 0.03883494986143847}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.7, 'f': 0.16279069561925366}}\n",
            "pair:  gradient clipping widely used technique training deep networks generally motivated optimisation lens informally controls dynamics iterates thus enhancing rate convergence local minimum intuition made precise line recent works show suitable clipping yield significantly faster convergence vanilla gradient descent paper propose new lens studying gradient clipping namely robustness informally one expects clipping provide robustness noise since one overly trust single sample surprisingly prove common problem label noise classification standard gradient clipping general provide robustness hand show simple variant gradient clipping provably robust corresponds suitably modifying underlying loss function yields simple noise robust alternative standard cross entropy loss performs well empirically\n",
            "output sentence:  gradient clipping endow robustness label noise simple loss based variant \n",
            "\n",
            "{'rouge-1': {'r': 0.12, 'p': 0.6666666666666666, 'f': 0.20338982792301064}, 'rouge-2': {'r': 0.04838709677419355, 'p': 0.375, 'f': 0.08571428368979596}, 'rouge-l': {'r': 0.08, 'p': 0.4444444444444444, 'f': 0.13559321775351915}}\n",
            "pair:  present hybrid framework leverages trade temporal frequency precision audio representations improve performance speech enhancement task first show conventional approaches using specific representations raw audio spectrograms effective targeting different types noise integrating approaches model learn multi scale multi domain features effectively removing noise existing different regions time frequency space complementary way experimental results show proposed hybrid model yields better performance robustness using model individually\n",
            "output sentence:  hybrid model utilizing raw audio spectrogram information speech enhancement tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.1346153846153846, 'p': 0.875, 'f': 0.23333333102222226}, 'rouge-2': {'r': 0.0967741935483871, 'p': 0.8571428571428571, 'f': 0.17391304165511445}, 'rouge-l': {'r': 0.1346153846153846, 'p': 0.875, 'f': 0.23333333102222226}}\n",
            "pair:  generative adversarial networks gans one popular tools learning complex high dimensional distributions however generalization properties gans well understood paper analyze generalization gans practical settings show discriminators trained discrete datasets original gan loss poor generalization capability approximate theoretically optimal discriminator propose zero centered gradient penalty improving generalization discriminator pushing toward optimal discriminator penalty guarantees generalization convergence gans experiments synthetic large scale datasets verify theoretical analysis\n",
            "output sentence:  propose zero centered gradient penalty improving generalization stability gans \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.2222222222222222, 'f': 0.1052631542797785}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.034482758620689655, 'p': 0.1111111111111111, 'f': 0.05263157533241022}}\n",
            "pair:  argue symmetry important consideration addressing problem systematicity investigate two forms symmetry relevant symbolic processes implement approach terms convolution show used achieve effective generalisation three toy problems rule learning composition grammar learning\n",
            "output sentence:  use convolution make neural networks behave like symbolic systems \n",
            "\n",
            "{'rouge-1': {'r': 0.14736842105263157, 'p': 0.875, 'f': 0.25225224978492006}, 'rouge-2': {'r': 0.03418803418803419, 'p': 0.23529411764705882, 'f': 0.05970149032189807}, 'rouge-l': {'r': 0.08421052631578947, 'p': 0.5, 'f': 0.14414414167681197}}\n",
            "pair:  deep reinforcement learning algorithms proven successful variety domains however tasks sparse rewards remain challenging state space large goal oriented tasks among typical problems domain reward received final goal accomplished work propose potential solution problems introduction experience based tendency reward mechanism provides agent additional hints based discriminative learning past experiences automated reverse curriculum mechanism provides dense additional learning signals states lead success also allows agent retain tendency reward instead whole histories experience multi phase curriculum learning extensively study advantages method standard sparse reward domains like maze super mario bros show method performs efficiently robustly prior approaches tasks long time horizons large state space addition demonstrate using optional keyframe scheme small quantity key states approach solve difficult robot manipulation challenges directly perception sparse rewards\n",
            "output sentence:  propose tendency rl efficiently solve goal oriented tasks large state space using automated curriculum discriminative discriminative shaping shaping shaping shaping shaping shaping shaping shaping robot \n",
            "\n",
            "{'rouge-1': {'r': 0.05333333333333334, 'p': 0.5714285714285714, 'f': 0.09756097404818563}, 'rouge-2': {'r': 0.009523809523809525, 'p': 0.16666666666666666, 'f': 0.018018016995373812}, 'rouge-l': {'r': 0.05333333333333334, 'p': 0.5714285714285714, 'f': 0.09756097404818563}}\n",
            "pair:  collecting high quality large scale datasets typically requires significant resources aim present work improve label efficiency large neural networks operating audio data multitask learning self supervised tasks unlabeled data end trained end end audio feature extractor based wavenet feeds simple yet versatile task specific neural networks describe three self supervised learning tasks operate large unlabeled audio corpus demonstrate scenario limited labeled training data one significantly improve performance supervised classification task simultaneously training additional self supervised tasks show one improve performance diverse sound events classification task nearly jointly trained three distinct self supervised tasks improvement scales number additional auxiliary tasks well amount unsupervised data also show incorporating data augmentation multitask setting leads even gains performance\n",
            "output sentence:  improving label efficiency multi task learning auditory data \n",
            "\n",
            "{'rouge-1': {'r': 0.17307692307692307, 'p': 0.6, 'f': 0.26865671294274895}, 'rouge-2': {'r': 0.09836065573770492, 'p': 0.42857142857142855, 'f': 0.15999999696355563}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.26666666666666666, 'f': 0.11940298159946547}}\n",
            "pair:  analyze joint probability distribution lengths vectors hidden variables different layers fully connected deep network weights biases chosen randomly according gaussian distributions input binary valued show activation function satisfies minimal set assumptions satisfied activation functions know used practice width network gets large length process converges probability length map determined simple function variances random weights biases activation function also show convergence may fail activation functions violate assumptions\n",
            "output sentence:  prove activation functions satisfying conditions deep network gets wide lengths vectors hidden variables converge length map \n",
            "\n",
            "{'rouge-1': {'r': 0.1702127659574468, 'p': 0.5714285714285714, 'f': 0.2622950784305294}, 'rouge-2': {'r': 0.1, 'p': 0.38461538461538464, 'f': 0.15873015545477456}, 'rouge-l': {'r': 0.14893617021276595, 'p': 0.5, 'f': 0.2295081931846278}}\n",
            "pair:  address problem teaching rnn approximate list processing algorithms given small number input output training examples approach generalize idea parametricity programming language theory formulate semantic property distinguishes common algorithms arbitrary non algorithmic functions characterization leads naturally learned data augmentation scheme encourages rnns learn algorithmic behavior enables small sample learning variety list processing tasks\n",
            "output sentence:  learned data augmentation instills algorithm favoring inductive biases let rnns learn list processing algorithms fewer \n",
            "\n",
            "{'rouge-1': {'r': 0.0379746835443038, 'p': 0.42857142857142855, 'f': 0.06976744036506224}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02531645569620253, 'p': 0.2857142857142857, 'f': 0.04651162641157387}}\n",
            "pair:  rise graph structured data social networks regulatory networks citation graphs functional brain networks combination resounding success deep learning various applications brought interest generalizing deep learning models non euclidean domains paper introduce new spectral domain convolutional architecture deep learning graphs core ingredient model new class parametric rational complex functions cayley polynomials allowing efficiently compute spectral filters graphs specialize frequency bands interest model generates rich spectral filters localized space scales linearly size input data sparsely connected graphs handle different constructions laplacian operators extensive experimental results show superior performance approach spectral image classification community detection vertex classification matrix completion tasks\n",
            "output sentence:  spectral graph convolutional neural network spectral zoom properties \n",
            "\n",
            "{'rouge-1': {'r': 0.08108108108108109, 'p': 0.5454545454545454, 'f': 0.14117646833494812}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.2, 'f': 0.04166666480034731}, 'rouge-l': {'r': 0.04054054054054054, 'p': 0.2727272727272727, 'f': 0.07058823304083052}}\n",
            "pair:  many biological learning systems mushroom body hippocampus cerebellum built sparsely connected networks neurons new understanding networks study function spaces induced sparse random features characterize functions may may learned network inputs per neuron found equivalent additive model order whereas degree distribution network combines additive terms different orders identify three specific advantages sparsity additive function approximation powerful inductive bias limits curse dimensionality sparse networks stable outlier noise inputs sparse random features scalable thus even simple brain architectures powerful function approximators finally hope work helps popularize kernel theories networks among computational neuroscientists\n",
            "output sentence:  advocate random features theory biological neural networks focusing sparsely connected networks \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.7777777777777778, 'f': 0.1647058804595156}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.25, 'f': 0.04123711188861734}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.4444444444444444, 'f': 0.09411764516539796}}\n",
            "pair:  carbon footprint natural language processing nlp research increasing recent years due reliance large inefficient neural network implementations distillation network compression technique attempts impart knowledge large model smaller one use teacher student distillation improve efficiency biaffine dependency parser obtains state art performance respect accuracy parsing speed dozat manning distilling original model trainable parameters observe average decrease point uas las across number diverse universal dependency treebanks faster baseline model cpu gpu inference time also observe small increase performance compressing treebanks finally distillation attain parser faster also accurate fastest modern parser penn treebank\n",
            "output sentence:  increase efficiency neural network dependency parsers teacher student distillation \n",
            "\n",
            "{'rouge-1': {'r': 0.027777777777777776, 'p': 0.2222222222222222, 'f': 0.04938271407407415}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.027777777777777776, 'p': 0.2222222222222222, 'f': 0.04938271407407415}}\n",
            "pair:  keyword spotting wakeword detection essential feature hands free operation modern voice controlled devices devices becoming ubiquitous users might want choose personalized custom wakeword work present donut ctc based algorithm online query example keyword spotting enables custom wakeword detection algorithm works recording small number training examples user generating set label sequence hypotheses training examples detecting wakeword aggregating scores hypotheses given new audio recording method combines generalization interpretability ctc based keyword spotting user adaptation convenience conventional query example system donut low computational requirements well suited learning inference embedded systems without requiring private user data uploaded cloud\n",
            "output sentence:  propose interpretable model detecting user chosen wakewords learns user examples \n",
            "\n",
            "{'rouge-1': {'r': 0.11235955056179775, 'p': 0.7142857142857143, 'f': 0.19417475493260442}, 'rouge-2': {'r': 0.05504587155963303, 'p': 0.375, 'f': 0.09599999776768006}, 'rouge-l': {'r': 0.10112359550561797, 'p': 0.6428571428571429, 'f': 0.17475727920444906}}\n",
            "pair:  neural architecture search nas made rapid progress incomputervision wherebynewstate artresultshave beenachievedinaseriesoftaskswithautomaticallysearched neural network nn architectures contrast nas made comparable advances natural language understanding nlu corresponding encoder aggregator meta architecture typical neural networks models nlu tasks gong et al de ne search space splittingitinto twoparts encodersearchspace andaggregator search space encoder search space contains basic operations convolutions rnns multi head attention sparse variants star transformers dynamic routing included aggregator search space along max avg pooling self attention pooling search algorithm ful lled via darts differentiable neural architecture search framework progressively reduce search space every epochs reduces search time resource costs experiments benchmark data sets show new neural networks generate achieve performances comparable state art models involve language model pre training\n",
            "output sentence:  neural architecture search series natural language understanding tasks design search space nlu tasks apply differentiable architecture search discover \n",
            "\n",
            "{'rouge-1': {'r': 0.14444444444444443, 'p': 0.9285714285714286, 'f': 0.24999999767011838}, 'rouge-2': {'r': 0.10619469026548672, 'p': 0.9230769230769231, 'f': 0.19047618862559842}, 'rouge-l': {'r': 0.14444444444444443, 'p': 0.9285714285714286, 'f': 0.24999999767011838}}\n",
            "pair:  propose new form autoencoding model incorporates best properties variational autoencoders vae generative adversarial networks gan known gan produce realistic samples vae suffer mode collapsing problem model optimizes jeffreys divergence model distribution true data distribution show takes best properties vae gan objectives consists two parts one parts optimized using standard adversarial training second one objective vae model however straightforward way substituting vae loss work well use explicit likelihood gaussian laplace limited flexibility high dimensions unnatural modelling images space pixels tackle problem propose novel approach train vae model implicit likelihood adversarially trained discriminator extensive set experiments cifar tinyimagent datasets show model achieves state art generation reconstruction quality demonstrate balance mode seeking mode covering behaviour model adjusting weight objective\n",
            "output sentence:  propose new form autoencoding model incorporates best properties variational autoencoders vae generative adversarial networks gan \n",
            "\n",
            "{'rouge-1': {'r': 0.21428571428571427, 'p': 0.8823529411764706, 'f': 0.34482758306249184}, 'rouge-2': {'r': 0.10465116279069768, 'p': 0.5294117647058824, 'f': 0.17475727879724767}, 'rouge-l': {'r': 0.21428571428571427, 'p': 0.8823529411764706, 'f': 0.34482758306249184}}\n",
            "pair:  propose procedures evaluating strengthening contextual embedding alignment show useful analyzing improving multilingual bert particular proposed alignment procedure bert exhibits significantly improved zero shot performance xnli compared base model remarkably matching pseudo fully supervised translate train models bulgarian greek measure degree alignment introduce contextual version word retrieval show correlates well downstream zero shot transfer using word retrieval task also analyze bert find exhibits systematic deficiencies worse alignment open class parts speech word pairs written different scripts corrected alignment procedure results support contextual alignment useful concept understanding large multilingual pre trained models\n",
            "output sentence:  propose procedures evaluating strengthening contextual embedding alignment show improve multilingual bert zero shot xnli transfer provide useful useful model \n",
            "\n",
            "{'rouge-1': {'r': 0.16494845360824742, 'p': 0.7619047619047619, 'f': 0.2711864377520828}, 'rouge-2': {'r': 0.046296296296296294, 'p': 0.25, 'f': 0.07812499736328134}, 'rouge-l': {'r': 0.1134020618556701, 'p': 0.5238095238095238, 'f': 0.18644067504021836}}\n",
            "pair:  sequence prediction models learned example sequences variety training algorithms maximum likelihood learning simple efficient yet suffer compounding error test time reinforcement learning policy gradient addresses issue prohibitively poor exploration efficiency rich set algorithms data noising raml softmax policy gradient also developed different perspectives paper present formalism entropy regularized policy optimization show apparently distinct algorithms including mle reformulated special instances formulation difference characterized reward function two weight hyperparameters unifying interpretation enables us systematically compare algorithms side side gain new insights trade offs algorithm design new perspective also leads improved approach dynamically interpolates among family algorithms learns model scheduled way experiments machine translation text summarization game imitation learning demonstrate superiority proposed approach\n",
            "output sentence:  entropy regularized policy optimization formalism subsumes set sequence prediction learning algorithms new interpolation algorithm improved results text generation game imitation learning \n",
            "\n",
            "{'rouge-1': {'r': 0.09574468085106383, 'p': 0.6, 'f': 0.16513761230536153}, 'rouge-2': {'r': 0.036036036036036036, 'p': 0.26666666666666666, 'f': 0.06349206139455789}, 'rouge-l': {'r': 0.09574468085106383, 'p': 0.6, 'f': 0.16513761230536153}}\n",
            "pair:  boltzmann distribution natural model many systems brains materials biomolecules often limited utility fitting data monte carlo algorithms unable simulate available time gap expressive capabilities sampling practicalities energy based models exemplified protein folding problem since energy landscapes underlie contemporary knowledge protein biophysics computer simulations challenged fold smallest proteins first principles work aim bridge gap expressive capacity energy functions practical capabilities simulators using unrolled monte carlo simulation model data compose neural energy function novel efficient simulator based langevin dynamics build end end differentiable model atomic protein structure given amino acid sequence information introduce techniques stabilizing backpropagation long roll outs demonstrate model capacity make multimodal predictions cases generalize unobserved protein fold types trained large corpus protein structures\n",
            "output sentence:  use unrolled simulator end end differentiable model protein structure show sometimes hierarchically generalize unseen fold topologies \n",
            "\n",
            "{'rouge-1': {'r': 0.13414634146341464, 'p': 0.8461538461538461, 'f': 0.23157894500609422}, 'rouge-2': {'r': 0.09090909090909091, 'p': 0.8333333333333334, 'f': 0.16393442445579146}, 'rouge-l': {'r': 0.13414634146341464, 'p': 0.8461538461538461, 'f': 0.23157894500609422}}\n",
            "pair:  multiagent systems mass agent makes individual decisions contribute globally system evolution learning mass difficult since agent selection actions must take place presence co learning agents moreover environmental stochasticity uncertainties increase exponentially increase number agents previous works borrow various multiagent coordination mechanisms deep learning architecture facilitate multiagent coordination however none explicitly consider action semantics agents different actions different influences agents paper propose novel network architecture named action semantics network asn explicitly represents action semantics agents asn characterizes different actions influence agents using neural networks based action semantics asn easily combined existing deep reinforcement learning drl algorithms boost performance experimental results starcraft ii micromanagement neural mmo show asn significantly improves performance state art drl approaches compared several network architectures\n",
            "output sentence:  proposed asn characterizes different actions influence agents using neural networks based action semantics \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.4, 'f': 0.13793103162901313}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.2, 'f': 0.06153845893491135}, 'rouge-l': {'r': 0.0625, 'p': 0.3, 'f': 0.1034482730083235}}\n",
            "pair:  propose method automatically compute importance features every observation time series simulating counterfactual trajectories given previous observations define importance observation change model output caused replacing observation generated one method applied arbitrarily complex time series models compare generated feature importance existing methods like sensitivity analyses feature occlusion explanation baselines show approach generates precise explanations less sensitive noise input signals\n",
            "output sentence:  explaining multivariate time series models finding important observations time using counterfactuals \n",
            "\n",
            "{'rouge-1': {'r': 0.13793103448275862, 'p': 0.6666666666666666, 'f': 0.2285714257306123}, 'rouge-2': {'r': 0.04854368932038835, 'p': 0.2777777777777778, 'f': 0.08264462556655974}, 'rouge-l': {'r': 0.10344827586206896, 'p': 0.5, 'f': 0.17142856858775513}}\n",
            "pair:  reading comprehension challenging task especially executed across longer across multiple evidence documents answer likely reoccur existing neural architectures typically scale entire evidence hence resort selecting single passage document either via truncation means carefully searching answer within passage however cases strategy suboptimal since focusing specific passage becomes difficult leverage multiple mentions answer throughout document work take different approach constructing lightweight models combined cascade find answer submodel consists feed forward networks equipped attention mechanism making trivially parallelizable show approach scale approximately order magnitude larger evidence documents aggregate information multiple mentions answer candidate across document empirically approach achieves state art performance wikipedia web domains triviaqa dataset outperforming complex recurrent architectures\n",
            "output sentence:  propose neural cascades simple trivially parallelizable approach reading consisting consisting feed forward nets attention achieves state art performance triviaqa \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 0.7692307692307693, 'f': 0.31746031418493326}, 'rouge-2': {'r': 0.14754098360655737, 'p': 0.75, 'f': 0.24657533971852133}, 'rouge-l': {'r': 0.2, 'p': 0.7692307692307693, 'f': 0.31746031418493326}}\n",
            "pair:  present optimal transport gan ot gan variant generative adversarial nets minimizing new metric measuring distance generator distribution data distribution metric call mini batch energy distance combines optimal transport primal form energy distance defined adversarially learned feature space resulting highly discriminative distance function unbiased mini batch gradients experimentally show ot gan highly stable trained large mini batches present state art results several popular benchmark problems image generation\n",
            "output sentence:  extension gans combining optimal transport primal form energy distance defined adversarially learned feature space \n",
            "\n",
            "{'rouge-1': {'r': 0.039603960396039604, 'p': 0.5, 'f': 0.07339449405268918}, 'rouge-2': {'r': 0.008264462809917356, 'p': 0.14285714285714285, 'f': 0.015624998966064523}, 'rouge-l': {'r': 0.039603960396039604, 'p': 0.5, 'f': 0.07339449405268918}}\n",
            "pair:  paper studies undesired phenomena sensitivity representations learned deep networks semantically irrelevant changes data identify cause shortcoming classical variational auto encoder vae objective evidence lower bound elbo show elbo fails control behaviour encoder support empirical data distribution behaviour vae lead extreme errors learned representation key hurdle effective use representations data efficient learning transfer address problem propose augment data specifications enforce insensitivity representation respect families transformations incorporate specifications propose regularization method based selection mechanism creates fictive data point explicitly perturbing observed true data point certain choices parameters formulation naturally leads minimization entropy regularized wasserstein distance representations illustrate approach standard datasets experimentally show significant improvements downstream adversarial accuracy achieved learning robust representations completely unsupervised manner without reference particular downstream task without costly supervised adversarial training procedure\n",
            "output sentence:  propose method computing adversarially robust representations entirely unsupervised way \n",
            "\n",
            "{'rouge-1': {'r': 0.05084745762711865, 'p': 0.5, 'f': 0.0923076906319527}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.3333333333333333, 'f': 0.06153845986272194}}\n",
            "pair:  work offers new method domain translation semantic label maps computer graphic cg simulation edge map images photo realistic im ages train generative adversarial network gan conditional way generate photo realistic version given cg scene existing architectures gans still lack photo realism capabilities needed train dnns computer vision tasks address issue embedding edge maps training adversarial mode also offer extension model uses gan architecture create visually appealing temporally coherent videos\n",
            "output sentence:  simulation real images translation video generation \n",
            "\n",
            "{'rouge-1': {'r': 0.029411764705882353, 'p': 0.16666666666666666, 'f': 0.04999999745000013}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.029411764705882353, 'p': 0.16666666666666666, 'f': 0.04999999745000013}}\n",
            "pair:  restricted boltzmann machine rbm learns probabilistic distribution input samples numerous uses like dimensionality reduction classification generative modeling conventional rbms accept vectorized data dismisses potentially important structural information original tensor multi way input matrix variate tensor variate rbms named mvrbm tvrbm proposed restrictive construction work presents matrix product operator rbm mporbm utilizes tensor network generalization mv tvrbm preserves input formats visible hidden layers results higher expressive power novel training algorithm integrating contrastive divergence alternating optimization procedure also developed\n",
            "output sentence:  propose general tensor based rbm model compress model greatly keep strong model expression capacity \n",
            "\n",
            "{'rouge-1': {'r': 0.1016949152542373, 'p': 0.6666666666666666, 'f': 0.17647058593858134}, 'rouge-2': {'r': 0.046153846153846156, 'p': 0.3333333333333333, 'f': 0.08108107894448509}, 'rouge-l': {'r': 0.06779661016949153, 'p': 0.4444444444444444, 'f': 0.11764705652681663}}\n",
            "pair:  grasping object precisely stacking another difficult task traditional robotic control hand engineered approaches examine problem simulation provide techniques aimed solving via deep reinforcement learning introduce two straightforward extensions deep deterministic policy gradient algorithm ddpg make significantly data efficient scalable results show making extensive use policy data replay possible find high performance control policies results hint may soon feasible train successful stacking policies collecting interactions real robots\n",
            "output sentence:  data efficient deep reinforcement learning used learning precise stacking policies \n",
            "\n",
            "{'rouge-1': {'r': 0.08490566037735849, 'p': 0.6923076923076923, 'f': 0.15126050225549045}, 'rouge-2': {'r': 0.024, 'p': 0.25, 'f': 0.04379561883957595}, 'rouge-l': {'r': 0.05660377358490566, 'p': 0.46153846153846156, 'f': 0.10084033418826356}}\n",
            "pair:  discovering causal structure among set variables fundamental problem many empirical sciences traditional score based casual discovery methods rely various local heuristics search directed acyclic graph dag according predefined score function methods greedy equivalence search may attractive results infinite samples certain model assumptions less satisfactory practice due finite data possible violation assumptions motivated recent advances neural combinatorial optimization propose use reinforcement learning rl search dag best scoring encoder decoder model takes observable data input generates graph adjacency matrices used compute rewards reward incorporates predefined score function two penalty terms enforcing acyclicity contrast typical rl applications goal learn policy use rl search strategy final output would graph among graphs generated training achieves best reward conduct experiments synthetic real datasets show proposed approach improved search ability also allows flexible score function acyclicity constraint\n",
            "output sentence:  apply reinforcement learning score based causal discovery achieve promising results synthetic real datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.12345679012345678, 'p': 0.6666666666666666, 'f': 0.20833333069661458}, 'rouge-2': {'r': 0.06363636363636363, 'p': 0.4666666666666667, 'f': 0.11199999788800002}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.6, 'f': 0.1874999973632813}}\n",
            "pair:  training neural networks verifiable robustness guarantees challenging several existing approaches utilize linear relaxation based neural network output bounds perturbation slow training factor hundreds depending underlying network architectures meanwhile interval bound propagation ibp based training efficient significantly outperforms linear relaxation based methods many tasks yet may suffer stability issues since bounds much looser especially beginning training paper propose new certified adversarial training method crown ibp combining fast ibp bounds forward bounding pass tight linear relaxation based bound crown backward bounding pass crown ibp computationally efficient consistently outperforms ibp baselines training verifiably robust neural networks conduct large scale experiments mnist cifar datasets outperform previous linear relaxation bound propagation based certified defenses inf robustness notably achieve verified test error mnist epsilon cifar epsilon\n",
            "output sentence:  propose new certified adversarial training method crown ibp achieves state art robustness inf norm adversarial perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.5, 'f': 0.17142856858775513}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.11764705882352941, 'f': 0.03100774964966065}, 'rouge-l': {'r': 0.08045977011494253, 'p': 0.3888888888888889, 'f': 0.13333333049251705}}\n",
            "pair:  paper study implicit regularization gradient descent algorithm homogeneous neural networks including fully connected convolutional neural networks relu leakyrelu activations particular study gradient descent gradient flow gradient descent infinitesimal step size optimizing logistic loss cross entropy loss homogeneous model possibly non smooth show training loss decreases certain threshold define smoothed version normalized margin increases time also formulate natural constrained optimization problem related margin maximization prove normalized margin smoothed version converge objective value kkt point optimization problem results generalize previous results logistic regression one layer multi layer linear networks provide quantitative convergence results weaker assumptions previous results homogeneous smooth neural networks conduct several experiments justify theoretical finding mnist cifar datasets finally margin closely related robustness discuss potential benefits training longer improving robustness model\n",
            "output sentence:  study implicit bias gradient descent prove minimal set assumptions parameter direction homogeneous models converges kkt points natural margin \n",
            "\n",
            "{'rouge-1': {'r': 0.017391304347826087, 'p': 0.2857142857142857, 'f': 0.03278688416420321}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.017391304347826087, 'p': 0.2857142857142857, 'f': 0.03278688416420321}}\n",
            "pair:  historically pursuit efficient inference one driving forces hind research new deep learning architectures building blocks recent examples include squeeze excitation module hu et al depthwise separable convolutions xception chollet inverted bottleneck mobilenet sandler et al notably cases resulting building blocks enabled higher efficiency also higher accuracy found wide adoption field work expand arsenal efficient building blocks neural network architectures instead combining standard primitives convolution advocate replacement dense primitives sparse counterparts idea using sparsity decrease parameter count new mozer smolensky conventional wisdom reduction theoretical flops translate real world efficiency gains aim correct misconception introducing family efficient sparse kernels several hardware platforms plan open source benefit community equipped efficient implementation sparse primitives show sparse versions mobilenet mobilenet architectures substantially outperform strong dense baselines efficiency accuracy curve snapdragon sparse networks outperform dense equivalents equivalent approximately one entire generation improvement hope findings facilitate wider adoption sparsity tool creating efficient accurate deep learning architectures\n",
            "output sentence:  sparse mobilenets faster dense ones appropriate kernels \n",
            "\n",
            "{'rouge-1': {'r': 0.2054794520547945, 'p': 0.8333333333333334, 'f': 0.3296703264967999}, 'rouge-2': {'r': 0.1797752808988764, 'p': 0.8421052631578947, 'f': 0.29629629339677643}, 'rouge-l': {'r': 0.2054794520547945, 'p': 0.8333333333333334, 'f': 0.3296703264967999}}\n",
            "pair:  capability reliably detecting distribution samples one key factors deploying good classifier test distribution always match training distribution real world applications work propose deep generative classifier effective detect distribution samples well classify distribution samples integrating concept gaussian discriminant analysis deep neural networks unlike discriminative softmax classifier focuses decision boundary partitioning latent space multiple regions generative classifier aims explicitly model class conditional distributions separable gaussian distributions thereby define confidence score distance test sample center distribution empirical evaluation multi class images tabular data demonstrate generative classifier achieves best performances distinguishing distribution samples also generalized well various types deep neural networks\n",
            "output sentence:  paper proposes deep generative classifier effective detect distribution samples well classify distribution samples integrating concept gaussian discriminant analysis deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.20270270270270271, 'p': 0.9375, 'f': 0.3333333304098766}, 'rouge-2': {'r': 0.16666666666666666, 'p': 0.9375, 'f': 0.2830188653613386}, 'rouge-l': {'r': 0.20270270270270271, 'p': 0.9375, 'f': 0.3333333304098766}}\n",
            "pair:  deep reinforcement learning algorithms learn complex behavioral skills real world application methods requires considerable amount experience collected agent practical settings robotics involves repeatedly attempting task resetting environment attempt however tasks easily automatically reversible practice learning process requires considerable human intervention work propose autonomous method safe efficient reinforcement learning simultaneously learns forward backward policy backward policy resetting environment subsequent attempt learning value function backward policy automatically determine forward policy enter non reversible state providing uncertainty aware safety aborts experiments illustrate proper use backward policy greatly reduce number manual resets required learn task reduce number unsafe actions lead non reversible states\n",
            "output sentence:  propose autonomous method safe efficient reinforcement learning simultaneously learns forward backward policy backward policy resetting environment subsequent attempt \n",
            "\n",
            "{'rouge-1': {'r': 0.06741573033707865, 'p': 0.4, 'f': 0.11538461291605034}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.13333333333333333, 'f': 0.03149606090892196}, 'rouge-l': {'r': 0.056179775280898875, 'p': 0.3333333333333333, 'f': 0.09615384368528113}}\n",
            "pair:  prohibitive energy cost running high performance convolutional neural networks cnns limiting deployment resource constrained platforms including mobile wearable devices propose cnn energy aware dynamic routing called energynet achieves adaptive complexity inference based inputs leading overall reduction run time energy cost without noticeably losing even improving accuracy achieved proposing energy loss captures computational data movement costs combine accuracy oriented loss learn dynamic routing policy skipping certain layers networks optimizes hybrid loss empirical results demonstrate compared baseline cnns energynetcan trim energy cost inference cifar tiny imagenet testing sets respectively maintaining testing accuracies encouraging observe energy awareness might serve training regularization even improve prediction accuracy models achieve higher top testing accuracy baseline cifar saving energy higher top testing accuracy tiny imagenet saving energy respectively\n",
            "output sentence:  paper proposes new cnn model combines energy cost dynamic routing strategy enable adaptive energy efficient inference \n",
            "\n",
            "{'rouge-1': {'r': 0.08235294117647059, 'p': 0.6363636363636364, 'f': 0.1458333313042535}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.2, 'f': 0.036697246039895715}, 'rouge-l': {'r': 0.07058823529411765, 'p': 0.5454545454545454, 'f': 0.12499999797092017}}\n",
            "pair:  deep learning demonstrated abilities learn complex structures restricted available data recently consensus networks cns proposed alleviate data sparsity utilizing features multiple modalities limited size labeled data paper extend cn transductive consensus networks tcns suitable semi supervised learning tcns different modalities input compressed latent representations encourage become indistinguishable iterative adversarial training understand tcns two mechanisms consensus classification put forward three variants ablation studies mechanisms investigate tcn models treat latent representations probability distributions measure similarities negative relative jensen shannon divergences show consensus state beneficial classification desires stable imperfect similarity representations overall tcns outperform align best benchmark algorithms given labeled samples bank marketing dementiabank datasets\n",
            "output sentence:  tcn multimodal semi supervised learning ablation study mechanisms interpretations latent representations \n",
            "\n",
            "{'rouge-1': {'r': 0.08247422680412371, 'p': 0.8888888888888888, 'f': 0.1509433946724813}, 'rouge-2': {'r': 0.03508771929824561, 'p': 0.5, 'f': 0.06557376926632627}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.4444444444444444, 'f': 0.07547169655927378}}\n",
            "pair:  ever increasing demand resultant reduced quality services focus shifted towards easing network congestion enable efficient flow systems like traffic supply chains electrical grids step direction imagine traditional heuristics based training systems approach incapable modelling involved dynamics one apply multi agent reinforcement learning marl problems considering vertex network agent marl based models assume agents independent many real world tasks agents need behave group rather collection individuals paper propose framework induces cooperation coordination amongst agents connected via underlying network using emergent communication marl based setup formulate problem general network setting demonstrate utility communication networks help case study traffic systems furthermore study emergent communication protocol show formation distinct communities grounded vocabulary best knowledge work studies emergent language networked marl setting\n",
            "output sentence:  framework studying emergent communication networked multi agent reinforcement learning setup \n",
            "\n",
            "{'rouge-1': {'r': 0.06756756756756757, 'p': 0.5555555555555556, 'f': 0.1204819257773262}, 'rouge-2': {'r': 0.01020408163265306, 'p': 0.125, 'f': 0.01886792313278757}, 'rouge-l': {'r': 0.06756756756756757, 'p': 0.5555555555555556, 'f': 0.1204819257773262}}\n",
            "pair:  lexical ambiguity presence two meanings single word inherent challenging problem machine translation systems even though use recurrent neural networks attention mechanisms expected solve problem machine translation systems always able correctly translate lexically ambiguous sentences work attempt resolve problem lexical ambiguity english japanese neural machine translation systems combining pretrained bidirectional encoder representations transformer bert language model produce contextualized word embeddings transformer translation model state art architecture machine translation task two proposed architectures shown effective translating ambiguous sentences vanilla transformer model google translate system furthermore one proposed models transformer bert achieves higher bleu score compared vanilla transformer model terms general translation concrete proof use contextualized word embeddings bert solve problem lexical ambiguity also boost translation quality general\n",
            "output sentence:  paper solves lexical ambiguity problem caused homonym neural translation bert \n",
            "\n",
            "{'rouge-1': {'r': 0.14457831325301204, 'p': 0.631578947368421, 'f': 0.23529411461553248}, 'rouge-2': {'r': 0.06363636363636363, 'p': 0.3333333333333333, 'f': 0.10687022631548283}, 'rouge-l': {'r': 0.10843373493975904, 'p': 0.47368421052631576, 'f': 0.17647058520376782}}\n",
            "pair:  decades research neural code underlying spatial navigation revealed diverse set neural response properties entorhinal cortex ec mammalian brain contains rich set spatial correlates including grid cells encode space using tessellating patterns however mechanisms functional significance spatial representations remain largely mysterious new way understand neural representations trained recurrent neural networks rnns perform navigation tasks arenas based velocity inputs surprisingly find grid like spatial response patterns emerge trained networks along units exhibit spatial correlates including border cells band like cells different functional types neurons observed experimentally order emergence grid like border cells also consistent observations developmental studies together results suggest grid cells border cells others observed ec may natural solution representing space efficiently given predominant recurrent connections neural circuits\n",
            "output sentence:  knowledge first study show neural representations space including grid like cells border cells observed brain could training recurrent recurrent neural network tasks tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.0875, 'p': 0.5833333333333334, 'f': 0.15217391077504727}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.36363636363636365, 'f': 0.06611570082644631}, 'rouge-l': {'r': 0.075, 'p': 0.5, 'f': 0.13043478034026468}}\n",
            "pair:  work tackles problem characterizing understanding decision boundaries neural networks piece wise linear non linearity activations use tropical geometry new development area algebraic geometry provide characterization decision boundaries simple neural network form affine relu affine specifically show decision boundaries subset tropical hypersurface intimately related polytope formed convex hull two zonotopes generators zonotopes precise functions neural network parameters utilize geometric characterization shed light new perspective three tasks propose new tropical perspective lottery ticket hypothesis see effect different initializations tropical geometric representation decision boundaries also leverage characterization new set tropical regularizers deal directly decision boundaries network investigate use regularizers neural network pruning removing network parameters contribute tropical geometric representation decision boundaries generating adversarial input attacks input perturbations explicitly perturbing decision boundaries geometry change network prediction input\n",
            "output sentence:  tropical geometry leveraged represent decision boundaries neural networks bring light interesting insights \n",
            "\n",
            "{'rouge-1': {'r': 0.10126582278481013, 'p': 0.38095238095238093, 'f': 0.15999999668200007}, 'rouge-2': {'r': 0.020618556701030927, 'p': 0.1, 'f': 0.034188031353641846}, 'rouge-l': {'r': 0.08860759493670886, 'p': 0.3333333333333333, 'f': 0.13999999668200008}}\n",
            "pair:  important detect anomalous inputs deploying machine learning systems use larger complex inputs deep learning magnifies difficulty distinguishing anomalous distribution examples time diverse image text data available enormous quantities propose leveraging data improve deep anomaly detection training anomaly detectors auxiliary dataset outliers approach call outlier exposure oe enables anomaly detectors generalize detect unseen anomalies extensive experiments natural language processing small large scale vision tasks find outlier exposure significantly improves detection performance also observe cutting edge generative models trained cifar may assign higher likelihoods svhn images cifar images use oe mitigate issue also analyze flexibility robustness outlier exposure identify characteristics auxiliary dataset improve performance\n",
            "output sentence:  oe teaches anomaly detectors learn heuristics detecting unseen anomalies experiments classification density estimation calibration nlp vision settings tune test distribution samples \n",
            "\n",
            "{'rouge-1': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}, 'rouge-2': {'r': 0.015873015873015872, 'p': 0.2, 'f': 0.029411763343425667}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  propose neural clustering model jointly learns latent features cluster unlike similar methods model require predefined number clusters using supervised approach agglomerate latent features towards randomly sampled targets within space whilst progressively removing targets left targets represent cluster centroids show behavior model across different modalities apply model text image data competitive results mnist finally also provide results baseline models fashion mnist newsgroups dataset twitter dataset create\n",
            "output sentence:  neural clustering without needing number clusters \n",
            "\n",
            "{'rouge-1': {'r': 0.08064516129032258, 'p': 0.4166666666666667, 'f': 0.13513513241782327}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.3333333333333333, 'f': 0.10810810539079627}}\n",
            "pair:  present multitask soft option learning msol hierarchical multi task framework based planning inference msol extends concept options using separate variational posteriors task regularized shared prior learned soft options temporally extended allowing higher level master policy train faster new tasks making decisions lower frequency additionally msol allows fine tuning soft options new tasks without unlearning previously useful behavior avoids problems local minima multitask training demonstrate empirically msol significantly outperforms hierarchical flat transfer learning baselines challenging multi task environments\n",
            "output sentence:  hierarchical rl introduce notion soft adaptable option show helps learning multitask settings \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.75, 'f': 0.17142856940408163}, 'rouge-2': {'r': 0.024, 'p': 0.2727272727272727, 'f': 0.04411764557201562}, 'rouge-l': {'r': 0.0967741935483871, 'p': 0.75, 'f': 0.17142856940408163}}\n",
            "pair:  based observation exists dramatic drop singular values fully connected layers single feature map convolutional layer dimension concatenated feature vector almost equals summation dimension feature map propose singular value decomposition svd based approach estimate dimension deep manifolds typical convolutional neural network vgg choose three categories imagenet namely persian cat container ship volcano determine local dimension deep manifolds deep layers tangent space target image several augmentation methods found gaussian noise method closer intrinsic dimension adding random noise image moving arbitrary dimension rank feature matrix augmented images increase close local dimension manifold also estimate dimension deep manifold based tangent space maxpooling layers results show dimensions different categories close decline quickly along convolutional layers fully connected layers furthermore show dimensions decline quickly inside conv layer work provides new insights intrinsic structure deep neural networks helps unveiling inner organization black box deep neural networks\n",
            "output sentence:  propose svd based method explore local dimension activation manifold deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.16, 'p': 0.6666666666666666, 'f': 0.2580645130072841}, 'rouge-2': {'r': 0.10344827586206896, 'p': 0.42857142857142855, 'f': 0.16666666353395063}, 'rouge-l': {'r': 0.08, 'p': 0.3333333333333333, 'f': 0.12903225494276802}}\n",
            "pair:  propose support guided adversarial imitation learning sail generic imitation learning framework unifies support estimation expert policy family adversarial imitation learning ail algorithms sail addresses two important challenges ail including implicit reward bias potential training instability also show sail least efficient standard ail extensive evaluation demonstrate proposed method effectively handles reward bias achieves better performance training stability baseline methods wide range benchmark control tasks\n",
            "output sentence:  unify support estimation family adversarial imitation learning algorithms support guided adversarial imitation learning robust stable imitation learning framework \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.75, 'f': 0.20338982816432058}, 'rouge-2': {'r': 0.0625, 'p': 0.5, 'f': 0.1111111091358025}, 'rouge-l': {'r': 0.11764705882352941, 'p': 0.75, 'f': 0.20338982816432058}}\n",
            "pair:  paper aim develop novel mechanism preserve differential privacy dp adversarial learning deep neural networks provable robustness adversarial examples leverage sequential composition theory dp establish new connection dp preservation provable robustness address trade among model utility privacy loss robustness design original differentially private adversarial objective function based post processing property dp tighten sensitivity model end end theoretical analysis thorough evaluations show mechanism notably improves robustness dp deep neural networks\n",
            "output sentence:  preserving differential privacy adversarial learning provable robustness adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.18055555555555555, 'p': 0.8666666666666667, 'f': 0.29885057185889813}, 'rouge-2': {'r': 0.15, 'p': 0.75, 'f': 0.24999999722222221}, 'rouge-l': {'r': 0.18055555555555555, 'p': 0.8666666666666667, 'f': 0.29885057185889813}}\n",
            "pair:  distinct commonality hmms rnns learn hidden representations sequential data addition noted backward computation baum welch algorithm hmms special case back propagation algorithm used neural networks eisner observations suggest despite many apparent differences hmms special case rnns paper investigate series architectural transformations hmms rnns theoretical derivations empirical hybridization answer question particular investigate three key design factors independence assumptions hidden states observation placement softmax use non linearity order pin empirical effects present comprehensive empirical study provide insights interplay expressivity interpretability respect language modeling parts speech induction\n",
            "output sentence:  hmms special case rnns investigate series architectural transformations hmms rnns theoretical derivations empirical hybridization provide new insights \n",
            "\n",
            "{'rouge-1': {'r': 0.19607843137254902, 'p': 0.8333333333333334, 'f': 0.31746031437641725}, 'rouge-2': {'r': 0.06451612903225806, 'p': 0.36363636363636365, 'f': 0.10958903853631081}, 'rouge-l': {'r': 0.17647058823529413, 'p': 0.75, 'f': 0.2857142826303855}}\n",
            "pair:  federated learning involves jointly learning massively distributed partitions data generated remote devices naively minimizing aggregate loss function network may disproportionately advantage disadvantage devices work propose fair federated learning ffl novel optimization objective inspired resource allocation strategies wireless networks encourages fair accuracy distribution across devices federated networks solve ffl devise scalable method fedavg run federated networks validate improved fairness flexibility ffl efficiency fedavg simulations federated datasets\n",
            "output sentence:  propose novel optimization objective encourages fairness heterogeneous federated networks develop scalable method solve \n",
            "\n",
            "{'rouge-1': {'r': 0.14942528735632185, 'p': 0.9285714285714286, 'f': 0.25742574018625636}, 'rouge-2': {'r': 0.11206896551724138, 'p': 0.9285714285714286, 'f': 0.19999999807810656}, 'rouge-l': {'r': 0.14942528735632185, 'p': 0.9285714285714286, 'f': 0.25742574018625636}}\n",
            "pair:  model free deep reinforcement learning rl algorithms demonstrated range challenging decision making control tasks however methods typically suffer two major challenges high sample complexity brittle convergence properties necessitate meticulous hyperparameter tuning challenges severely limit applicability methods complex real world domains paper propose soft actor critic policy actor critic deep rl algorithm based maximum entropy reinforcement learning framework framework actor aims maximize expected reward also maximizing entropy succeed task acting randomly possible prior deep rl methods based framework formulated either policy learning policy policy gradient methods combining policy updates stable stochastic actor critic formulation method achieves state art performance range continuous control benchmark tasks outperforming prior policy policy methods furthermore demonstrate contrast policy algorithms approach stable achieving similar performance across different random seeds\n",
            "output sentence:  propose soft actor critic policy actor critic deep rl algorithm based maximum entropy reinforcement learning framework \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5294117647058824, 'f': 0.18367346652019997}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.23529411764705882, 'f': 0.07079645762080046}, 'rouge-l': {'r': 0.08641975308641975, 'p': 0.4117647058823529, 'f': 0.14285713998958774}}\n",
            "pair:  semi supervised learning jointly learning labeled unlabeled samples active research topic due key role relaxing human annotation constraints context image classification recent advances learn unlabeled samples mainly focused consistency regularization methods encourage invariant predictions different perturbations unlabeled samples conversely propose learn unlabeled data generating soft pseudo labels using network predictions show naive pseudo labeling overfits incorrect pseudo labels due called confirmation bias demonstrate mixup augmentation setting minimum number labeled samples per mini batch effective regularization techniques reducing proposed approach achieves state art results cifar mini imagenet despite much simpler state art results demonstrate pseudo labeling outperform consistency regularization methods opposite supposed previous work code made available\n",
            "output sentence:  pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling \n",
            "\n",
            "{'rouge-1': {'r': 0.12121212121212122, 'p': 0.5333333333333333, 'f': 0.19753086117969826}, 'rouge-2': {'r': 0.0273972602739726, 'p': 0.14285714285714285, 'f': 0.0459770087937642}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.26666666666666666, 'f': 0.09876542908093287}}\n",
            "pair:  propose tackle time series regression problem computing temporal evolution probability density function provide probabilistic forecast recurrent neural network rnn based model employed learn nonlinear operator temporal evolution probability density function use softmax layer numerical discretization smooth probability density functions transforms function approximation problem classification task explicit implicit regularization strategies introduced impose smoothness condition estimated probability distribution monte carlo procedure compute temporal evolution distribution multiple step forecast presented evaluation proposed algorithm three synthetic two real data sets shows advantage compared baselines\n",
            "output sentence:  proposed rnn based algorithm estimate predictive distribution one multi step forecasts time series prediction problems \n",
            "\n",
            "{'rouge-1': {'r': 0.14942528735632185, 'p': 0.7647058823529411, 'f': 0.24999999726516273}, 'rouge-2': {'r': 0.024793388429752067, 'p': 0.1875, 'f': 0.04379561837498012}, 'rouge-l': {'r': 0.08045977011494253, 'p': 0.4117647058823529, 'f': 0.1346153818805474}}\n",
            "pair:  ability overparameterized deep networks generalize well linked fact stochastic gradient descent sgd finds solutions lie flat wide minima training loss minima output network resilient small random noise added parameters far observation used provide generalization guarantees neural networks whose parameters either textit stochastic textit compressed work present general pac bayesian framework leverages observation provide bound original network learned network deterministic uncompressed enables us key novelty approach framework allows us show training data interactions weight matrices satisfy certain conditions imply wide training loss minimum conditions em generalize interactions matrices test data thereby implying wide test loss minimum apply general framework setup assume pre activation values network small although assume training data setup provide generalization guarantee original deterministic uncompressed network scale product spectral norms weight matrices guarantee would possible prior approaches\n",
            "output sentence:  provide pac bayes based generalization guarantee uncompressed deterministic deep networks generalizing noise resilience network training data test data \n",
            "\n",
            "{'rouge-1': {'r': 0.2028985507246377, 'p': 0.7368421052631579, 'f': 0.3181818147959711}, 'rouge-2': {'r': 0.12987012987012986, 'p': 0.47619047619047616, 'f': 0.20408162928571433}, 'rouge-l': {'r': 0.18840579710144928, 'p': 0.6842105263157895, 'f': 0.29545454206869837}}\n",
            "pair:  knowledge regarding function proteins necessary gives clear picture biological processes nevertheless many protein sequences found added databases lacks functional annotation laboratory experiments take considerable amount time annotation sequences arises need use computational techniques classify proteins based functions work collected data swiss prot containing proteins grouped families pass recurrent neural network rnn long short term memory lstm gated recurrent unit gru model compare applying trigram deep neural network shallow neural network dataset approach could achieve maximum around accuracy classification protein families\n",
            "output sentence:  proteins amino acid sequences machine learning deep learning recurrent neural network rnn long short term memory lstm gated recurrent gru neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.24444444444444444, 'p': 0.6470588235294118, 'f': 0.3548387056971904}, 'rouge-2': {'r': 0.12, 'p': 0.375, 'f': 0.1818181781450873}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.5882352941176471, 'f': 0.32258064118106144}}\n",
            "pair:  paper presents method autonomously find periodicities signal based idea using fourier transform autocorrelation function presented vlachos et al showing interesting results method perform well noisy signals signals multiple periodicities thus method adds several new extra steps hints clustering filtering detrending fix issues experimental results show proposed method outperforms state art algorithms\n",
            "output sentence:  paper presents method autonomously find multiple periodicities signal using fft acf add three news steps clustering filtering \n",
            "\n",
            "{'rouge-1': {'r': 0.10416666666666667, 'p': 0.3333333333333333, 'f': 0.15873015510204092}, 'rouge-2': {'r': 0.017543859649122806, 'p': 0.07142857142857142, 'f': 0.028169010918468917}, 'rouge-l': {'r': 0.0625, 'p': 0.2, 'f': 0.09523809160997747}}\n",
            "pair:  pruning neural networks wiring length efficiency considered three techniques proposed experimentally tested distance based regularization nested rank pruning layer layer bipartite matching first two algorithms used training pruning phases respectively third used arranging neurons phase experiments show distance based regularization weight based pruning tends perform best without layer layer bipartite matching results suggest techniques may useful creating neural networks implementation widely deployed specialized circuits\n",
            "output sentence:  three new algorithms ablation studies prune neural network optimize wiring length opposed number remaining weights \n",
            "\n",
            "{'rouge-1': {'r': 0.14754098360655737, 'p': 0.6923076923076923, 'f': 0.24324324034696862}, 'rouge-2': {'r': 0.057971014492753624, 'p': 0.3333333333333333, 'f': 0.09876542957476002}, 'rouge-l': {'r': 0.09836065573770492, 'p': 0.46153846153846156, 'f': 0.16216215926588756}}\n",
            "pair:  neural network models shown excellent fluency performance applied abstractive summarization many approaches neural abstractive summarization involve introduction significant inductive bias pointer generator architectures coverage partially extractive procedures designed mimic human summarization show possible attain competitive performance instead directly viewing summarization language modeling introduce simple procedure built upon pre trained decoder transformers obtain competitive rouge scores using language modeling loss alone beam search decoding time optimization instead rely efficient nucleus sampling greedy decoding\n",
            "output sentence:  introduce simple procedure repurpose pre trained transformer based language models perform abstractive summarization well \n",
            "\n",
            "{'rouge-1': {'r': 0.12698412698412698, 'p': 0.8, 'f': 0.21917807982735973}, 'rouge-2': {'r': 0.04938271604938271, 'p': 0.4, 'f': 0.08791208595580248}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.6, 'f': 0.16438355927941453}}\n",
            "pair:  introduce mtlab new algorithm learning multiple related tasks strong theoretical guarantees key idea perform learning sequentially data tasks without interruptions restarts task boundaries predictors individual tasks derived process additional online batch conversion step learning across task boundaries mtlab achieves sublinear regret true risks number tasks lifelong learning setting leads improved generalization bound converges total number samples across observed tasks instead number examples per tasks number tasks independently time widely applicable handle finite sets tasks common multi task learning well stochastic task sequences studied lifelong learning\n",
            "output sentence:  new algorithm online multi task learning learns without restarts task borders \n",
            "\n",
            "{'rouge-1': {'r': 0.14754098360655737, 'p': 0.9, 'f': 0.2535211243404087}, 'rouge-2': {'r': 0.075, 'p': 0.6666666666666666, 'f': 0.13483145885620504}, 'rouge-l': {'r': 0.14754098360655737, 'p': 0.9, 'f': 0.2535211243404087}}\n",
            "pair:  give new algorithm learning two layer neural network general class input distributions assuming ground truth two layer network sigma wx xi weight matrices xi represents noise number neurons hidden layer larger input output algorithm guaranteed recover parameters ground truth network requirement input symmetric still allows highly complicated structured input algorithm based method moments framework extends several results tensor decompositions use spectral algorithms avoid complicated non convex optimization learning neural networks experiments show algorithm robustly learn ground truth neural network small number samples many symmetric input distributions\n",
            "output sentence:  give algorithm learning two layer neural network symmetric input distribution \n",
            "\n",
            "{'rouge-1': {'r': 0.23684210526315788, 'p': 0.6428571428571429, 'f': 0.34615384221893497}, 'rouge-2': {'r': 0.09302325581395349, 'p': 0.3076923076923077, 'f': 0.14285713929209193}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.5714285714285714, 'f': 0.3076923037573965}}\n",
            "pair:  deepa deep learning framework explores parallelism parallelizable dimensions accelerate training process convolutional neural networks deepa optimizes parallelism granularity individual layer network present elimination based algorithm finds optimal parallelism configuration every layer evaluation shows deepa achieves speedup compared state art deep learning frameworks reduces data transfers\n",
            "output sentence:  best knowledge deepa first deep learning framework controls optimizes parallelism cnns parallelizable dimensions granularity layer \n",
            "\n",
            "{'rouge-1': {'r': 0.11428571428571428, 'p': 1.0, 'f': 0.205128203287311}, 'rouge-2': {'r': 0.06060606060606061, 'p': 0.8571428571428571, 'f': 0.11320754593627626}, 'rouge-l': {'r': 0.1, 'p': 0.875, 'f': 0.17948717764628536}}\n",
            "pair:  state art generative model factorized action variational autoencoder favae presented learning disentangled interpretable representations sequential data via information bottleneck without supervision purpose disentangled representation learning obtain interpretable transferable representations data focused disentangled representation sequential data wide range potential applications disentanglement representation extended sequential data video speech stock price data sequential data characterized dynamic factors static factors dynamic factors time dependent static factors independent time previous works succeed disentangling static factors dynamic factors explicitly modeling priors latent variables distinguish static dynamic factors however model disentangle representations dynamic factors disentangling picking throwing robotic tasks paper propose new model disentangle multiple dynamic factors since method require modeling priors capable disentangling dynamic factors experiments show favae extract disentangled dynamic factors\n",
            "output sentence:  propose new model disentangle multiple dynamic factors sequential data \n",
            "\n",
            "{'rouge-1': {'r': 0.11956521739130435, 'p': 0.8461538461538461, 'f': 0.20952380735419504}, 'rouge-2': {'r': 0.04504504504504504, 'p': 0.4166666666666667, 'f': 0.0813008112472735}, 'rouge-l': {'r': 0.09782608695652174, 'p': 0.6923076923076923, 'f': 0.17142856925895694}}\n",
            "pair:  hierarchical reinforcement learning methods offer powerful means planning flexible behavior complicated domains however learning appropriate hierarchical decomposition domain subtasks remains substantial challenge present novel algorithm subtask discovery based recently introduced multitask linearly solvable markov decision process mlmdp framework mlmdp perform never seen tasks representing linear combination previously learned basis set tasks setting subtask discovery problem naturally posed finding optimal low rank approximation set tasks agent face domain use non negative matrix factorization discover minimal basis set tasks show technique learns intuitive decompositions variety domains method several qualitatively desirable features limited learning subtasks single goal states instead learning distributed patterns preferred states learns qualitatively different hierarchical decompositions domain depending ensemble tasks agent face may straightforwardly iterated obtain deeper hierarchical decompositions\n",
            "output sentence:  present novel algorithm hierarchical subtask discovery leverages multitask linear markov decision process framework \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.2857142857142857, 'f': 0.08888888626172847}, 'rouge-2': {'r': 0.010101010101010102, 'p': 0.07692307692307693, 'f': 0.017857140805166056}, 'rouge-l': {'r': 0.039473684210526314, 'p': 0.21428571428571427, 'f': 0.06666666403950629}}\n",
            "pair:  generally intelligent learner generalize complex tasks previously encountered two common paradigms machine learning either training separate learner per task training single learner tasks difficulty generalization leverage compositional structure task distribution paper introduces compositional problem graph broadly applicable formalism relate tasks different complexity terms problems shared subproblems propose compositional generalization problem measuring readily old knowledge reused hence built upon first step tackling compositional generalization introduce compositional recursive learner domain general framework learning algorithmic procedures composing representation transformations producing learner reasons computation execute making analogies previously seen problems show symbolic high dimensional domain compositional approach generalize complex problems learner previously encountered whereas baselines explicitly compositional\n",
            "output sentence:  explore problem compositional generalization propose means endowing neural network architectures ability compose solve problems \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.45454545454545453, 'f': 0.17857142541454082}, 'rouge-2': {'r': 0.032520325203252036, 'p': 0.16666666666666666, 'f': 0.054421765975288215}, 'rouge-l': {'r': 0.08888888888888889, 'p': 0.36363636363636365, 'f': 0.14285713970025515}}\n",
            "pair:  contextualized word representations elmo bert become de facto starting point incorporating pretrained representations downstream nlp tasks settings contextual representations largely made obsolete static embedding predecessors word vec glove however static embeddings advantages straightforward understand faster use additionally embedding analysis methods static embeddings far diverse mature available dynamic counterparts work introduce simple methods generating static lookup table embeddings existing pretrained contextual representations demonstrate outperform word vec glove embeddings variety word similarity word relatedness tasks results also reveal insights may useful subsequent downstream tasks using embeddings original contextual models demonstrate increased potential analysis applying existing approaches estimating social bias word embeddings analysis constitutes comprehensive study social bias contextual word representations via proxy distilled embeddings reveals number inconsistencies current techniques quantifying social bias word embeddings publicly release code distilled word embeddings support reproducible research broader nlp community\n",
            "output sentence:  procedure distilling contextual models static embeddings apply method popular models demonstrate clear gains representation quality wrt wrt word vec analysis potential improved potential social potential \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.5333333333333333, 'f': 0.1684210499722992}, 'rouge-2': {'r': 0.031578947368421054, 'p': 0.16666666666666666, 'f': 0.053097342454381836}, 'rouge-l': {'r': 0.0875, 'p': 0.4666666666666667, 'f': 0.14736841839335182}}\n",
            "pair:  conditional generative adversarial networks cgan led large improvements task conditional image generation lies heart computer vision major focus far performance improvement little effort making cgan robust noise regression generator might lead arbitrarily large errors output makes cgan unreliable real world applications work introduce novel conditional gan model called rocgan leverages structure target space model address issue model augments generator unsupervised pathway promotes outputs generator span target manifold even presence intense noise prove rocgan share similar theoretical properties gan experimentally verify model outperforms existing state art cgan architectures large margin variety domains including images natural scenes faces\n",
            "output sentence:  introduce new type conditional gan aims leverage structure target space generator augment generator new new pathway learn target structure \n",
            "\n",
            "{'rouge-1': {'r': 0.23333333333333334, 'p': 0.9333333333333333, 'f': 0.37333333013333336}, 'rouge-2': {'r': 0.14925373134328357, 'p': 0.7142857142857143, 'f': 0.24691357738759337}, 'rouge-l': {'r': 0.21666666666666667, 'p': 0.8666666666666667, 'f': 0.3466666634666667}}\n",
            "pair:  reinforce used train models structured prediction settings directly optimize test time objective however common case sampling one prediction per datapoint input data inefficient show drawing multiple samples predictions per datapoint learn significantly less data freely obtain reinforce baseline reduce variance additionally derive reinforce estimator baseline based sampling without replacement combined recent technique sample sequences without replacement using stochastic beam search improves training procedure sequence model predicts solution travelling salesman problem\n",
            "output sentence:  show drawing multiple samples predictions per input datapoint learn less data freely obtain reinforce baseline \n",
            "\n",
            "{'rouge-1': {'r': 0.13953488372093023, 'p': 0.5454545454545454, 'f': 0.22222221897805214}, 'rouge-2': {'r': 0.07407407407407407, 'p': 0.36363636363636365, 'f': 0.12307692026508882}, 'rouge-l': {'r': 0.13953488372093023, 'p': 0.5454545454545454, 'f': 0.22222221897805214}}\n",
            "pair:  challenging train multi task neural networks outperform even match single task counterparts help address propose using knowledge distillation single task models teach multi task model enhance training teacher annealing novel method gradually transitions model distillation supervised learning helping multi task model surpass single task teachers evaluate approach multi task fine tuning bert glue benchmark method consistently improves standard single task multi task training\n",
            "output sentence:  distilling single task models multi task model improves natural language understanding performance \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.6666666666666666, 'f': 0.24999999695312503}, 'rouge-2': {'r': 0.07936507936507936, 'p': 0.4166666666666667, 'f': 0.13333333064533337}, 'rouge-l': {'r': 0.1346153846153846, 'p': 0.5833333333333334, 'f': 0.21874999695312503}}\n",
            "pair:  although word analogy problems become standard tool evaluating word vectors little known word vectors good solving problems paper attempt understanding subject developing simple highly accurate generative approach solve word analogy problem case terms involved problem nouns results demonstrate ambiguities associated learning relationship word pair role training dataset determining relationship gets highlighted furthermore results show ability model accurately solve word analogy problem may indicative model ability learn relationship word pair way human\n",
            "output sentence:  simple generative approach solve word analogy problem yields insights word relationships problems estimating \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.5263157894736842, 'f': 0.22727272388688022}, 'rouge-2': {'r': 0.0379746835443038, 'p': 0.15789473684210525, 'f': 0.0612244866701376}, 'rouge-l': {'r': 0.07246376811594203, 'p': 0.2631578947368421, 'f': 0.11363636025051664}}\n",
            "pair:  modern neural networks highly overparameterized capacity substantially overfit training data nevertheless networks often generalize well practice also observed trained networks often compressed much smaller representations purpose paper connect two empirical observations main technical result generalization bound compressed networks based compressed size combined shelf compression algorithms leads state art generalization guarantees particular provide first non vacuous generalization guarantees realistic architectures applied imagenet classification problem additionally show compressibility models tend overfit limited empirical results show increase overfitting increases number bits required describe trained network\n",
            "output sentence:  obtain non vacuous generalization bounds imagenet scale deep neural networks combining original pac bayes bound shelf neural network compression method \n",
            "\n",
            "{'rouge-1': {'r': 0.07017543859649122, 'p': 0.4444444444444444, 'f': 0.12121211885674935}, 'rouge-2': {'r': 0.0125, 'p': 0.125, 'f': 0.022727271074380287}, 'rouge-l': {'r': 0.07017543859649122, 'p': 0.4444444444444444, 'f': 0.12121211885674935}}\n",
            "pair:  word alignments useful tasks like statistical neural machine translation nmt annotation projection statistical word aligners perform well methods extract alignments jointly translations nmt however approaches require parallel training data quality decreases less training data available propose word alignment methods require little parallel data key idea leverage multilingual word embeddings static contextualized word alignment multilingual embeddings created monolingual data without relying parallel data dictionaries find traditional statistical aligners outperformed contextualized embeddings even scenarios abundant parallel data example set parallel sentences contextualized embeddings achieve word alignment higher absolute eflomal\n",
            "output sentence:  use representations trained without parallel data creating word alignments \n",
            "\n",
            "{'rouge-1': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.25, 'f': 0.037383176186566565}, 'rouge-l': {'r': 0.039473684210526314, 'p': 0.3333333333333333, 'f': 0.07058823340069209}}\n",
            "pair:  goal generative models model underlying data distribution sample based dataset intuition accurate model principle also include sample based dataset part induced probability distribution investigate look fully trained generative models using generative adversarial networks gan framework analyze resulting generator ability memorize dataset show size initial latent space paramount allow accurate reconstruction training data gives us link compression theory autoencoders ae used lower bound reconstruction capabilities generative model observe similar results perception distortion tradeoff blau michaeli given small latent space ae produces low quality gan produces high quality outputs perceptual viewpoint contrast distortion error smaller ae increasing dimensionality latent space distortion decreases models perceptual quality increases ae\n",
            "output sentence:  analyze impact latent space fully trained generators pseudo inverting \n",
            "\n",
            "{'rouge-1': {'r': 0.07766990291262135, 'p': 0.4, 'f': 0.13008129808976143}, 'rouge-2': {'r': 0.023809523809523808, 'p': 0.15789473684210525, 'f': 0.04137930806753877}, 'rouge-l': {'r': 0.05825242718446602, 'p': 0.3, 'f': 0.09756097288650943}}\n",
            "pair:  deep neural networks particular convolutional neural networks become highly effective tools compressing images solving inverse problems including denoising inpainting reconstruction noisy measurements success attributed part ability represent generate natural images well contrary classical tools wavelets image generating deep neural networks large number parameters typically multiple output dimension need trained large datasets paper propose untrained simple image model called deep decoder deep neural network generate natural images weight parameters deep decoder simple architecture convolutions fewer weight parameters output dimensionality underparameterization enables deep decoder compress images concise set network weights show par wavelet based thresholding underparameterization provides barrier overfitting allowing deep decoder state art performance denoising deep decoder simple sense layer identical structure consists one upsampling unit pixel wise linear combination channels relu activation channelwise normalization simplicity makes network amenable theoretical analysis sheds light aspects neural networks enable form effective signal representations\n",
            "output sentence:  introduce underparameterized nonconvolutional simple deep neural network without training effectively represent natural images solve image processing tasks like compression denoising \n",
            "\n",
            "{'rouge-1': {'r': 0.10294117647058823, 'p': 0.7, 'f': 0.17948717725180804}, 'rouge-2': {'r': 0.03896103896103896, 'p': 0.3333333333333333, 'f': 0.06976743998647923}, 'rouge-l': {'r': 0.10294117647058823, 'p': 0.7, 'f': 0.17948717725180804}}\n",
            "pair:  low bit width integer weights activations important efficient inference especially respect lower power consumption propose apply monte carlo methods importance sampling sparsify quantize pre trained neural networks without retraining obtain sparse low bit width integer representations approximate full precision weights activations precision sparsity complexity easily configurable amount sampling performed approach called monte carlo quantization mcq linear time space resulting quantized sparse networks show minimal accuracy loss compared original full precision networks method either outperforms achieves results competitive methods require additional training variety challenging tasks\n",
            "output sentence:  monte carlo methods quantizing pre trained models without additional training \n",
            "\n",
            "{'rouge-1': {'r': 0.11494252873563218, 'p': 0.7142857142857143, 'f': 0.19801979959219684}, 'rouge-2': {'r': 0.024193548387096774, 'p': 0.21428571428571427, 'f': 0.043478259046418895}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.42857142857142855, 'f': 0.11881187880011769}}\n",
            "pair:  deep neural networks work well approximating complicated functions provided data trained gradient descent methods time vast amount existing functions programmatically solve different tasks precise manner eliminating need training many cases possible decompose task series functions may prefer use neural network learn functionality others preferred method would use existing black box functions propose method end end training base neural network integrates calls existing black box functions approximating black box functionality differentiable neural network way drives base network comply black box function interface end end optimization process inference time replace differentiable estimator external black box non differentiable counterpart base network output matches input arguments black box function using estimate replace paradigm train neural network end end compute input black box functionality eliminating need intermediate labels show leveraging existing precise black box function inference integrated model generalizes better fully differentiable model learns efficiently compared rl based methods\n",
            "output sentence:  training dnns interface black box functions intermediate labels using estimator sub network replaced black box training \n",
            "\n",
            "{'rouge-1': {'r': 0.16393442622950818, 'p': 0.9090909090909091, 'f': 0.27777777518904323}, 'rouge-2': {'r': 0.10294117647058823, 'p': 0.7, 'f': 0.17948717725180804}, 'rouge-l': {'r': 0.16393442622950818, 'p': 0.9090909090909091, 'f': 0.27777777518904323}}\n",
            "pair:  paper presents two methods disentangle interpret contextual effects encoded pre trained deep neural network unlike convolutional studies visualize image appearances corresponding network output neural activation global perspective research aims clarify certain input unit dimension collaborates units dimensions constitute inference patterns neural network thus contribute network output analysis local contextual effects certain input units special values real applications particular used methods explain gaming strategy alphago zero model experiments method successfully disentangled rationale move game\n",
            "output sentence:  paper presents methods disentangle interpret contextual effects encoded deep neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.09782608695652174, 'p': 0.8181818181818182, 'f': 0.17475727964558396}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.5, 'f': 0.08333333180555559}, 'rouge-l': {'r': 0.07608695652173914, 'p': 0.6363636363636364, 'f': 0.1359223281892733}}\n",
            "pair:  deep neural networks dnns dominate current research machine learning due massive gpu parallelization dnn training longer bottleneck large models many parameters high computational effort lead common benchmark tables contrast embedded devices limited capability result model size inference time must significantly reduced dnns achieve suitable performance embedded devices propose soft quantization approach train dnns evaluated using pure fixed point arithmetic exploiting bit shift mechanism derive fixed point quantization constraints important components including batch normalization relu compared floating point arithmetic fixed point calculations significantly reduce computational effort whereas low bit representations immediately decrease memory costs evaluate approach different architectures common benchmark data sets compare recent quantization approaches achieve new state art performance using bit fixed point models error rate cifar\n",
            "output sentence:  soft quantization approach learn pure fixed point representations deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.10294117647058823, 'p': 0.7777777777777778, 'f': 0.18181817975375275}, 'rouge-2': {'r': 0.011111111111111112, 'p': 0.125, 'f': 0.020408161765930976}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.5555555555555556, 'f': 0.1298701278057008}}\n",
            "pair:  interpretability ai agent behavior utmost importance effective human ai interaction end increasing interest characterizing generating interpretable behavior agent alternative approach guarantee agent generates interpretable behavior would design agent environment uninterpretable behaviors either prohibitively expensive unavailable agent date work umbrella goal plan recognition design exploring notion environment redesign specific instances interpretable behavior position paper scope landscape interpretable behavior environment redesign different flavors specifically focus three specific types interpretable behaviors explicability legibility predictability present general framework problem environment design instantiated achieve three interpretable behaviors also discuss specific instantiations framework correspond prior works environment design identify exciting opportunities future work\n",
            "output sentence:  present approach redesign environment uninterpretable agent behaviors minimized eliminated \n",
            "\n",
            "{'rouge-1': {'r': 0.010869565217391304, 'p': 0.2, 'f': 0.02061855572324375}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.010869565217391304, 'p': 0.2, 'f': 0.02061855572324375}}\n",
            "pair:  coalition operations essential responding increasing number world wide incidents require large scale humanitarian assistance many nations non governmental organizations regularly coordinate address problems cooperation often impeded limits information able share paper consider use advanced cryptographic technique called secure multi party computation enable coalition members achieve joint objectives still meeting privacy requirements particular focus multi nation aid delivery scheduling task involves coordinating various aid provider nations deliver relief materials occurrence natural disaster even use secure multi party computation technology information private data leak describe emerging field quantitative information flow used help data owners understand extent private data might become vulnerable result possible actual scheduling operations enable automated adjustments scheduling process ensure privacy requirements\n",
            "output sentence:  privacy thought way resources planning \n",
            "\n",
            "{'rouge-1': {'r': 0.11428571428571428, 'p': 0.6153846153846154, 'f': 0.19277108169545654}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.3333333333333333, 'f': 0.07999999788800007}, 'rouge-l': {'r': 0.08571428571428572, 'p': 0.46153846153846156, 'f': 0.14457831061111923}}\n",
            "pair:  markov logic networks mlns elegantly combine logic rules probabilistic graphical models used address many knowledge graph problems however inference mln computationally intensive making industrial scale application mln difficult recent years graph neural networks gnns emerged efficient effective tools large scale graph problems nevertheless gnns explicitly incorporate prior logic rules models may require many labeled examples target task paper explore combination mlns gnns use graph neural networks variational inference mln propose gnn variant named expressgnn strikes nice balance representation power simplicity model extensive experiments several benchmark datasets demonstrate expressgnn leads effective efficient probabilistic logic reasoning\n",
            "output sentence:  employ graph neural networks variational em framework efficient inference learning markov logic networks \n",
            "\n",
            "{'rouge-1': {'r': 0.20270270270270271, 'p': 0.8333333333333334, 'f': 0.32608695337429117}, 'rouge-2': {'r': 0.08888888888888889, 'p': 0.4444444444444444, 'f': 0.1481481453703704}, 'rouge-l': {'r': 0.1891891891891892, 'p': 0.7777777777777778, 'f': 0.30434782293950857}}\n",
            "pair:  present trellis networks new architecture sequence modeling one hand trellis network temporal convolutional network special structure characterized weight tying across depth direct injection input deep layers hand show truncated recurrent networks equivalent trellis networks special sparsity structure weight matrices thus trellis networks general weight matrices generalize truncated recurrent networks leverage connections design high performing trellis networks absorb structural algorithmic elements recurrent convolutional models experiments demonstrate trellis networks outperform current state art methods variety challenging benchmarks including word level language modeling character level language modeling tasks stress tests designed evaluate long term memory retention code available https github com locuslab trellisnet\n",
            "output sentence:  trellis networks new sequence modeling architecture bridges recurrent convolutional models sets new state art word character level language modeling \n",
            "\n",
            "{'rouge-1': {'r': 0.13793103448275862, 'p': 0.6666666666666666, 'f': 0.2285714257306123}, 'rouge-2': {'r': 0.09836065573770492, 'p': 0.5454545454545454, 'f': 0.16666666407793213}, 'rouge-l': {'r': 0.1206896551724138, 'p': 0.5833333333333334, 'f': 0.1999999971591837}}\n",
            "pair:  study problem fitting task specific learning rate schedules perspective hyperparameter optimization allows us explicitly search schedules achieve good generalization describe structure gradient validation error learning rates hypergradient based introduce novel online algorithm method adaptively interpolates two recently proposed techniques franceschi et al baydin et al featuring increased stability faster convergence show empirically proposed technique compares favorably baselines related methodsin terms final test accuracy\n",
            "output sentence:  marthe new method fit task specific learning rate schedules perspective hyperparameter optimization \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.8888888888888888, 'f': 0.19999999800312504}, 'rouge-2': {'r': 0.08080808080808081, 'p': 0.8888888888888888, 'f': 0.1481481466203704}, 'rouge-l': {'r': 0.11267605633802817, 'p': 0.8888888888888888, 'f': 0.19999999800312504}}\n",
            "pair:  paper study new graph learning problem learning count subgraph isomorphisms although learning based approach inexact able generalize count large patterns data graphs polynomial time compared exponential time original np complete problem different traditional graph learning problems node classification link prediction subgraph isomorphism counting requires global inference oversee whole graph tackle problem propose dynamic intermedium attention memory network diamnet augments different representation learning architectures iteratively attends pattern target data graphs memorize different subgraph isomorphisms global counting develop small graphs subgraph isomorphisms large graphs subgraph isomorphisms sets evaluate different models experimental results show learning based subgraph isomorphism counting help reduce time complexity acceptable accuracy diamnet improve existing representation learning models global problem\n",
            "output sentence:  paper study new graph learning problem learning count subgraph isomorphisms \n",
            "\n",
            "{'rouge-1': {'r': 0.26, 'p': 0.9285714285714286, 'f': 0.4062499965820313}, 'rouge-2': {'r': 0.140625, 'p': 0.6428571428571429, 'f': 0.23076922782380016}, 'rouge-l': {'r': 0.22, 'p': 0.7857142857142857, 'f': 0.34374999658203126}}\n",
            "pair:  catastrophic forgetting neural networks one well known problems continual learning previous attempts addressing problem focus preventing important weights changing methods often require task boundaries learn effectively support backward transfer learning paper propose meta learning algorithm learns reconstruct gradients old tasks current parameters combines reconstructed gradients current gradient enable continual learning backward transfer learning current task previous tasks experiments standard continual learning benchmarks show algorithm effectively prevent catastrophic forgetting supports backward transfer learning\n",
            "output sentence:  propose meta learning algorithm continual learning effectively prevent catastrophic forgetting problem support backward transfer learning \n",
            "\n",
            "{'rouge-1': {'r': 0.05084745762711865, 'p': 0.3333333333333333, 'f': 0.08823529182093431}, 'rouge-2': {'r': 0.014084507042253521, 'p': 0.125, 'f': 0.025316453875981543}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.2222222222222222, 'f': 0.05882352711505199}}\n",
            "pair:  ability algorithms evolve learn compositional communication protocols traditionally studied language evolution literature use emergent communication tasks scale research using contemporary deep learning methods training reinforcement learning neural network agents referential communication games extend previous work agents trained symbolic environments developing agents able learn raw pixel data challenging realistic input representation find degree structure found input data affects nature emerged protocols thereby corroborate hypothesis structured compositional language likely emerge agents perceive world structured\n",
            "output sentence:  controlled study role environments respect properties emergent communication protocols \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.5333333333333333, 'f': 0.23880596667409226}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.07142857142857142, 'f': 0.02666666363022257}, 'rouge-l': {'r': 0.057692307692307696, 'p': 0.2, 'f': 0.08955223533080878}}\n",
            "pair:  generative networks promising models specifying visual transformations unfortunately certification generative models challenging one needs capture sufficient non convexity produce precise bounds output existing verification methods either fail scale generative networks capture enough non convexity work present new verifier called approxline certify non trivial properties generative networks approxline performs deterministic probabilistic abstract interpretation captures infinite sets outputs generative networks show approxline verify interesting interpolations network latent space\n",
            "output sentence:  verify deterministic probabilistic properties neural networks using non convex relaxations visible transformations specified generative models \n",
            "\n",
            "{'rouge-1': {'r': 0.07954545454545454, 'p': 0.875, 'f': 0.14583333180555555}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.7142857142857143, 'f': 0.07874015643871288}, 'rouge-l': {'r': 0.07954545454545454, 'p': 0.875, 'f': 0.14583333180555555}}\n",
            "pair:  emerging topic face recognition designing margin based loss functions increase feature margin different classes enhanced discriminability recently absorbing idea mining based strategies adopted emphasize misclassified samples achieve promising results however entire training process prior methods either explicitly emphasize sample based importance renders hard samples fully exploited explicitly emphasize effects semi hard hard samples even early training stage may lead convergence issues work propose novel adaptive curriculum learning loss curricularface embeds idea curriculum learning loss function achieve novel training strategy deep face recognition mainly addresses easy samples early training stage hard ones later stage specifically curricularface adaptively adjusts relative importance easy hard samples different training stages stage different samples assigned different importance according corresponding difficultness extensive experimental results popular benchmarks demonstrate superiority curricularface state art competitors code available upon publication\n",
            "output sentence:  novel adaptive curriculum learning loss deep face recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.5, 'f': 0.1555555529283951}, 'rouge-2': {'r': 0.010526315789473684, 'p': 0.07142857142857142, 'f': 0.0183486216143425}, 'rouge-l': {'r': 0.07894736842105263, 'p': 0.42857142857142855, 'f': 0.1333333307061729}}\n",
            "pair:  provide novel perspective forward pass block layers deep network particular show forward pass standard dropout layer followed linear layer non linear activation equivalent optimizing convex objective single iteration tau nice proximal stochastic gradient method show replacing standard bernoulli dropout additive dropout equivalent optimizing convex objective variance reduced proximal method expressing fully connected convolutional layers special cases high order tensor product unify underlying convex optimization problem tensor setting derive formula lipschitz constant used determine optimal step size proximal methods conduct experiments standard convolutional networks applied cifar cifar datasets show replacing block layers multiple iterations corresponding solver step size set via consistently improves classification accuracy\n",
            "output sentence:  framework links deep network layers stochastic optimization algorithms used improve model accuracy inform network design \n",
            "\n",
            "{'rouge-1': {'r': 0.047619047619047616, 'p': 0.375, 'f': 0.08450704025391792}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.375, 'f': 0.08450704025391792}}\n",
            "pair:  noise injection fundamental tool data augmentation yet widely accepted procedure incorporate learning frameworks study analyzes effects adding applying different noise models varying magnitudes convolutional neural network cnn architectures noise models distributed different density functions given common magnitude levels via structural similarity ssim metric order create appropriate ground comparison basic results conforming common notions machine learning also introduces novel heuristics recommendations noise injection new approaches provide better understanding optimal learning procedures image classification\n",
            "output sentence:  ideal methodology inject noise input data cnn training \n",
            "\n",
            "{'rouge-1': {'r': 0.34146341463414637, 'p': 0.9333333333333333, 'f': 0.49999999607780615}, 'rouge-2': {'r': 0.28, 'p': 0.875, 'f': 0.42424242056932976}, 'rouge-l': {'r': 0.34146341463414637, 'p': 0.9333333333333333, 'f': 0.49999999607780615}}\n",
            "pair:  work presents method active anomaly detection built upon existing deep learning solutions unsupervised anomaly detection show prior needs assumed anomalies order performance guarantees unsupervised anomaly detection argue active anomaly detection practice cost unsupervised anomaly detection possibility much better results solve problem present new layer attached deep learning model designed unsupervised anomaly detection transform active method presenting results synthetic real anomaly detection datasets\n",
            "output sentence:  method active anomaly detection present new layer attached deep learning model designed unsupervised anomaly detection transform active method \n",
            "\n",
            "{'rouge-1': {'r': 0.02586206896551724, 'p': 0.5, 'f': 0.049180326933620004}, 'rouge-2': {'r': 0.007042253521126761, 'p': 0.16666666666666666, 'f': 0.013513512735573458}, 'rouge-l': {'r': 0.02586206896551724, 'p': 0.5, 'f': 0.049180326933620004}}\n",
            "pair:  machine learning workloads often expensive train taking weeks converge current generation frameworks relies custom back ends order achieve efficiency making impractical train models less common hardware back ends exist knossos builds recent work avoids need hand written libraries instead compiles machine learning models much way one would compile kinds software order make resulting code efficient knossos complier directly optimises abstract syntax tree program however contrast traditional compilers employ hand written optimisation passes take rewriting approach driven star search algorithm learn value function evaluates future potential cost reduction taking various rewriting actions program show knossos automatically learned optimisations past compliers implement hand furthermore demonstrate knossos achieve wall time reduction compared hand tuned compiler suite machine learning programs including basic linear algebra convolutional networks knossos compiler minimal dependencies used architecture supports cpp toolchain since cost model proposed algorithm optimises tailored particular hardware architecture proposed approach potentially applied variety hardware\n",
            "output sentence:  combine search reinforcement learning speed machine learning code \n",
            "\n",
            "{'rouge-1': {'r': 0.0963855421686747, 'p': 0.6153846153846154, 'f': 0.16666666432508684}, 'rouge-2': {'r': 0.01834862385321101, 'p': 0.16666666666666666, 'f': 0.033057849452906324}, 'rouge-l': {'r': 0.08433734939759036, 'p': 0.5384615384615384, 'f': 0.14583333099175347}}\n",
            "pair:  model distillation aims distill knowledge complex model simpler one paper consider alternative formulation called dataset distillation keep model fixed instead attempt distill knowledge large training dataset small one idea synthesize small number data points need come correct data distribution given learning algorithm training data approximate model trained original data example show possible compress mnist training images synthetic distilled images one per class achieve close original performance given fixed network initialization evaluate method various initialization settings experiments multiple datasets mnist cifar pascal voc cub demonstrate ad vantage approach compared alternative methods finally include real world application dataset distillation continual learning setting show storing distilled images episodic memory previous tasks alleviate forgetting effectively real images\n",
            "output sentence:  propose distill large dataset small set synthetic data train networks close original performance \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.5454545454545454, 'f': 0.1153846134929734}, 'rouge-2': {'r': 0.017699115044247787, 'p': 0.2, 'f': 0.03252032370943228}, 'rouge-l': {'r': 0.043010752688172046, 'p': 0.36363636363636365, 'f': 0.07692307503143496}}\n",
            "pair:  deep neuroevolution deep reinforcement learning deep rl algorithms two popular approaches policy search former widely applicable rather stable suffers low sample efficiency contrast latter sample efficient sample efficient variants also rather unstable highly sensitive hyper parameter setting far families methods mostly compared competing tools however emerging approach consists combining get best worlds two previously existing combinations use either ad hoc evolutionary algorithm goal exploration process together deep deterministic policy gradient ddpg algorithm sample efficient policy deep rl algorithm paper propose different combination scheme using simple cross entropy method cem twin delayed deep deterministic policy gradient td another policy deep rl algorithm improves ddpg evaluate resulting method cem rl set benchmarks classically used deep rl show cem rl benefits several advantages competitors offers satisfactory trade performance sample efficiency\n",
            "output sentence:  propose new combination evolution strategy deep reinforcement learning takes best worlds \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5294117647058824, 'f': 0.18367346652019997}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.23529411764705882, 'f': 0.07079645762080046}, 'rouge-l': {'r': 0.08641975308641975, 'p': 0.4117647058823529, 'f': 0.14285713998958774}}\n",
            "pair:  semi supervised learning jointly learning labeled unlabeled samples active research topic due key role relaxing human annotation constraints context image classification recent advances learn unlabeled samples mainly focused consistency regularization methods encourage invariant predictions different perturbations unlabeled samples conversely propose learn unlabeled data generating soft pseudo labels using network predictions show naive pseudo labeling overfits incorrect pseudo labels due called confirmation bias demonstrate mixup augmentation setting minimum number labeled samples per mini batch effective regularization techniques reducing proposed approach achieves state art results cifar mini imagenet despite much simpler state art results demonstrate pseudo labeling outperform consistency regularization methods opposite supposed previous work code made available\n",
            "output sentence:  pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.6153846153846154, 'f': 0.21333333046755557}, 'rouge-2': {'r': 0.04938271604938271, 'p': 0.3333333333333333, 'f': 0.08602150312868545}, 'rouge-l': {'r': 0.11290322580645161, 'p': 0.5384615384615384, 'f': 0.18666666380088892}}\n",
            "pair:  unsupervised representation learning holds promise exploiting large amount available unlabeled data learn general representations promising technique unsupervised learning framework variational auto encoders vaes however unsupervised representations learned vaes significantly outperformed learned supervising recognition hypothesis learn useful representations recognition model needs encouraged learn repeating consistent patterns data drawing inspiration mid level representation discovery work propose patchvae reasons images patch level key contribution bottleneck formulation vae framework encourages mid level style representations experiments demonstrate representations learned method perform much better recognition tasks compared learned vanilla vaes\n",
            "output sentence:  patch based bottleneck formulation vae framework learns unsupervised representations better suited visual recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.04672897196261682, 'p': 0.45454545454545453, 'f': 0.08474576102125829}, 'rouge-2': {'r': 0.008130081300813009, 'p': 0.1, 'f': 0.015037592594267754}, 'rouge-l': {'r': 0.028037383177570093, 'p': 0.2727272727272727, 'f': 0.050847455936512555}}\n",
            "pair:  choice tuning optimizer affects speed ultimately performance deep learning significant past recent research area yet perhaps surprisingly generally agreed upon protocol quantitative reproducible evaluation optimization strategies deep learning suggest routines benchmarks stochastic optimization special focus unique aspects deep learning stochasticity tunability generalization primary contribution present deepobs python package deep learning optimization benchmarks package addresses key challenges quantitative assessment stochastic optimizers automates steps benchmarking library includes wide extensible set ready use realistic optimization problems training residual networks image classification imagenet character level language prediction models well popular classics like mnist cifar package also provides realistic baseline results popular optimizers test problems ensuring fair comparison competition benchmarking new optimizers without run costly experiments comes output back ends directly produce latex code inclusion academic publications supports tensorflow available open source\n",
            "output sentence:  provide software package drastically simplifies automates improves evaluation deep learning optimizers \n",
            "\n",
            "{'rouge-1': {'r': 0.10526315789473684, 'p': 0.6153846153846154, 'f': 0.1797752784042419}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.25, 'f': 0.057142855118367426}, 'rouge-l': {'r': 0.06578947368421052, 'p': 0.38461538461538464, 'f': 0.11235954806716329}}\n",
            "pair:  character level language modeling essential challenging task natural language processing prior works focused identifying long term dependencies characters built deeper wider networks better performance however models require substantial computational resources hinders usability character level language models applications limited resources paper propose lightweight model called group transformer reduces resource requirements transformer promising method modeling sequence long term dependencies specifically proposed method partitions linear operations reduce number parameters computational cost result group transformer uses parameters compared best performing lstm based model providing better performance two benchmark tasks enwik text compared transformers comparable number parameters time complexity proposed model shows better performance implementation code available\n",
            "output sentence:  paper proposes novel lightweight transformer character level language modeling utilizing group wise operations \n",
            "\n",
            "{'rouge-1': {'r': 0.13559322033898305, 'p': 0.7272727272727273, 'f': 0.228571425922449}, 'rouge-2': {'r': 0.08064516129032258, 'p': 0.45454545454545453, 'f': 0.13698629881028337}, 'rouge-l': {'r': 0.13559322033898305, 'p': 0.7272727272727273, 'f': 0.228571425922449}}\n",
            "pair:  propose fusion discriminator single unified framework incorporating conditional information generative adversarial network gan variety distinct structured prediction tasks including image synthesis semantic segmentation depth estimation much like commonly used convolutional neural network conditional markov random field cnn crf models proposed method able enforce higher order consistency model without limited specific class potentials method conceptually simple flexible experimental results demonstrate improvement several diverse structured prediction tasks\n",
            "output sentence:  propose fusion discriminator novel architecture incorporating conditional information discriminator gans structured prediction tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.10256410256410256, 'p': 0.8888888888888888, 'f': 0.1839080441220769}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.5, 'f': 0.07017543729147431}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.7777777777777778, 'f': 0.16091953837495046}}\n",
            "pair:  paper present general framework distilling expectations respect bayesian posterior distribution deep neural network significantly extending prior work method known bayesian dark knowledge generalized framework applies case classification models takes input architecture teacher network general posterior expectation interest architecture student network distillation method performs online compression selected posterior expectation using iteratively generated monte carlo samples parameter posterior teacher model consider problem optimizing student model architecture respect accuracy speed storage trade present experimental results investigating multiple data sets distillation targets teacher model architectures approaches searching student model architectures establish key result distilling student model architecture matches teacher done bayesian dark knowledge lead sub optimal performance lastly show student architecture search methods identify student models significantly improved performance\n",
            "output sentence:  general framework distilling bayesian posterior expectations deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.12790697674418605, 'p': 0.6875, 'f': 0.21568627186466746}, 'rouge-2': {'r': 0.0673076923076923, 'p': 0.4375, 'f': 0.11666666435555559}, 'rouge-l': {'r': 0.11627906976744186, 'p': 0.625, 'f': 0.19607842872741257}}\n",
            "pair:  learning learn meta learning leverages data driven inductive bias increase efficiency learning novel task approach encounters difficulty transfer mutually beneficial instance tasks sufficiently dissimilar change time use connection gradient based meta learning hierarchical bayes propose mixture hierarchical bayesian models parameters arbitrary function approximator neural network generalizing model agnostic meta learning maml algorithm present stochastic expectation maximization procedure jointly estimate parameter initializations gradient descent well latent assignment tasks initializations approach better captures diversity training tasks opposed consolidating inductive biases single set hyperparameters experiments demonstrate better generalization standard miniimagenet benchmark shot classification derive novel scalable non parametric variant method captures evolution task distribution time demonstrated set shot regression tasks\n",
            "output sentence:  use connection gradient based meta learning hierarchical bayes learn mixture meta learners appropriate heterogeneous evolving task distribution \n",
            "\n",
            "{'rouge-1': {'r': 0.038461538461538464, 'p': 0.6, 'f': 0.07228915549426625}, 'rouge-2': {'r': 0.011494252873563218, 'p': 0.25, 'f': 0.02197802113754381}, 'rouge-l': {'r': 0.02564102564102564, 'p': 0.4, 'f': 0.04819276995209757}}\n",
            "pair:  paper addresses problem representing system belief using multi variate normal distributions mnd underlying model based deep neural network dnn major challenge dnns computational complexity needed obtain model uncertainty using mnds achieve scalable method propose novel approach expresses parameter posterior sparse information form inference algorithm based novel laplace approximation scheme involves diagonal correction kronecker factored eigenbasis makes inversion information matrix intractable operation required full bayesian analysis devise low rank approximation eigenbasis memory efficient sampling scheme provide theoretical analysis empirical evaluation various benchmark data sets showing superiority approach existing methods\n",
            "output sentence:  approximate inference algorithm deep learning \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.6666666666666666, 'f': 0.23529411474048442}, 'rouge-2': {'r': 0.04040404040404041, 'p': 0.23529411764705882, 'f': 0.06896551473989308}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.4444444444444444, 'f': 0.15686274219146487}}\n",
            "pair:  hierarchical reinforcement learning promising approach long horizon decision making problems sparse rewards unfortunately methods still decouple lower level skill acquisition process training higher level controls skills new task treating skills fixed lead significant sub optimality transfer setting work propose novel algorithm discover set skills continuously adapt along higher level even training new task main contributions two fold first derive new hierarchical policy gradient well unbiased latent dependent baseline introduce hierarchical proximal policy optimization hippo policy method efficiently train levels hierarchy simultaneously second propose method training time abstractions improves robustness obtained skills environment changes code results available sites google com view hippo rl\n",
            "output sentence:  propose hippo stable hierarchical reinforcement learning algorithm train several levels hierarchy simultaneously giving good performance skill discovery adaptation \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.8181818181818182, 'f': 0.224999997628125}, 'rouge-2': {'r': 0.0898876404494382, 'p': 0.7272727272727273, 'f': 0.15999999804200002}, 'rouge-l': {'r': 0.13043478260869565, 'p': 0.8181818181818182, 'f': 0.224999997628125}}\n",
            "pair:  describe two end end autoencoding models semi supervised graph based dependency parsing first model local autoencoding parser lap encoding input using continuous latent variables sequential manner second model global autoencoding parser gap encoding input dependency trees latent variables exact inference models consist two parts encoder enhanced deep neural networks dnn utilize contextual information encode input latent variables decoder generative model able reconstruct input lap gap admit unified structure different loss functions labeled unlabeled data shared parameters conducted experiments wsj ud dependency parsing data sets showing models exploit unlabeled data boost performance given limited amount labeled data\n",
            "output sentence:  describe two end end autoencoding parsers semi supervised graph based dependency parsing \n",
            "\n",
            "{'rouge-1': {'r': 0.09782608695652174, 'p': 0.5294117647058824, 'f': 0.1651376120461241}, 'rouge-2': {'r': 0.007936507936507936, 'p': 0.0625, 'f': 0.014084505042650552}, 'rouge-l': {'r': 0.05434782608695652, 'p': 0.29411764705882354, 'f': 0.09174311663328011}}\n",
            "pair:  ensembles multiple neural networks trained individually predictions averaged shown widely successful improving accuracy predictive uncertainty single neural networks however ensemble cost training testing increases linearly number networks paper propose batchensemble ensemble method whose computational memory costs significantly lower typical ensembles batchensemble achieves defining weight matrix hadamard product shared weight among ensemble members rank one matrix per member unlike ensembles batchensemble parallelizable across devices one device trains one member also parallelizable within device multiple ensemble members updated simultaneously given mini batch across cifar cifar wmt en de en fr translation contextual bandits tasks batchensemble yields competitive accuracy uncertainties typical ensembles speedup test time memory reduction ensemble size also apply batchensemble lifelong learning split cifar batchensemble yields comparable performance progressive neural networks much lower computational memory costs show batchensemble easily scale lifelong learning split imagenet involves sequential learning tasks\n",
            "output sentence:  introduced batchensemble efficient method ensembling lifelong learning used improve accuracy uncertainty neural network like typical ensemble methods \n",
            "\n",
            "{'rouge-1': {'r': 0.18333333333333332, 'p': 0.9166666666666666, 'f': 0.30555555277777785}, 'rouge-2': {'r': 0.13043478260869565, 'p': 0.8181818181818182, 'f': 0.224999997628125}, 'rouge-l': {'r': 0.18333333333333332, 'p': 0.9166666666666666, 'f': 0.30555555277777785}}\n",
            "pair:  present graph neural network assisted monte carlo tree search approach classical traveling salesman problem tsp adopt greedy algorithm framework construct optimal solution tsp adding nodes successively graph neural network gnn trained capture local global graph structure give prior probability selecting vertex every step prior probability provides heuristics mcts mcts output improved probability selecting successive vertex feedback information fusing prior scouting procedure experimental results tsp nodes demonstrate proposed method obtains shorter tours learning based methods\n",
            "output sentence:  graph neural network assisted monte carlo tree search approach traveling salesman problem \n",
            "\n",
            "{'rouge-1': {'r': 0.09876543209876543, 'p': 0.6666666666666666, 'f': 0.1720430085050295}, 'rouge-2': {'r': 0.03125, 'p': 0.2727272727272727, 'f': 0.05607476451043766}, 'rouge-l': {'r': 0.08641975308641975, 'p': 0.5833333333333334, 'f': 0.15053763216094346}}\n",
            "pair:  class labels empirically shown useful improving sample quality generative adversarial nets gans paper mathematically study properties current variants gans make use class label information class aware gradient cross entropy decomposition reveal class labels associated losses influence gan training based propose activation maximization generative adversarial networks gan advanced solution comprehensive experiments conducted validate analysis evaluate effectiveness solution gan outperforms strong baselines achieves state art inception score cifar addition demonstrate inception imagenet classifier inception score mainly tracks diversity generator however reliable evidence reflect true sample quality thus propose new metric called score provide accurate estimation sample quality proposed model also outperforms baseline methods new metric\n",
            "output sentence:  understand class labels help gan training propose new evaluation metric generative models \n",
            "\n",
            "{'rouge-1': {'r': 0.08064516129032258, 'p': 0.5555555555555556, 'f': 0.14084506820868875}, 'rouge-2': {'r': 0.013333333333333334, 'p': 0.1, 'f': 0.023529409688581502}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.3333333333333333, 'f': 0.08450704003967473}}\n",
            "pair:  describe simple general neural network weight compression approach network parameters weights biases represented latent space amounting reparameterization space equipped learned probability model used impose entropy penalty parameter representation training compress representation using simple arithmetic coder training classification accuracy model compressibility maximized jointly bitrate accuracy trade specified hyperparameter evaluate method mnist cifar imagenet classification benchmarks using six distinct model architectures results show state art model compression achieved scalable general way without requiring complex procedures multi stage training\n",
            "output sentence:  end end trainable model compression method optimizing accuracy jointly expected model size \n",
            "\n",
            "{'rouge-1': {'r': 0.07216494845360824, 'p': 0.6363636363636364, 'f': 0.1296296278000686}, 'rouge-2': {'r': 0.04032258064516129, 'p': 0.38461538461538464, 'f': 0.07299269901220101}, 'rouge-l': {'r': 0.07216494845360824, 'p': 0.6363636363636364, 'f': 0.1296296278000686}}\n",
            "pair:  adversarial learning methods proposed wide range applications training adversarial models notoriously unstable effectively balancing performance generator discriminator critical since discriminator achieves high accuracy produce relatively uninformative gradients work propose simple general technique constrain information flow discriminator means information bottleneck enforcing constraint mutual information observations discriminator internal representation effectively modulate discriminator accuracy maintain useful informative gradients demonstrate proposed variational discriminator bottleneck vdb leads significant improvements across three distinct application areas adversarial learning algorithms primary evaluation studies applicability vdb imitation learning dynamic continuous control skills running show method learn skills directly raw video demonstrations substantially outperforming prior adversarial imitation learning methods vdb also combined adversarial inverse reinforcement learning learn parsimonious reward functions transferred optimized new settings finally demonstrate vdb train gans effectively image generation improving upon number prior stabilization methods\n",
            "output sentence:  regularizing adversarial learning information bottleneck applied imitation learning inverse reinforcement learning generative adversarial networks \n",
            "\n",
            "{'rouge-1': {'r': 0.12962962962962962, 'p': 0.5833333333333334, 'f': 0.21212120914600552}, 'rouge-2': {'r': 0.046153846153846156, 'p': 0.2727272727272727, 'f': 0.07894736594529093}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.5, 'f': 0.18181817884297521}}\n",
            "pair:  uncertainty estimation ensembling methods go hand hand uncertainty estimation one main benchmarks assessment ensembling performance time deep learning ensembles provided state art results uncertainty estimation work focus domain uncertainty image classification explore standards quantification point pitfalls existing metrics avoiding pitfalls perform broad study different ensembling techniques provide insight broad comparison introduce deep ensemble equivalent dee show many sophisticated ensembling techniques equivalent ensemble independently trained networks terms test log likelihood\n",
            "output sentence:  highlight problems common metrics domain uncertainty perform broad study modern ensembling techniques \n",
            "\n",
            "{'rouge-1': {'r': 0.09876543209876543, 'p': 0.6666666666666666, 'f': 0.1720430085050295}, 'rouge-2': {'r': 0.03125, 'p': 0.2727272727272727, 'f': 0.05607476451043766}, 'rouge-l': {'r': 0.08641975308641975, 'p': 0.5833333333333334, 'f': 0.15053763216094346}}\n",
            "pair:  class labels empirically shown useful improving sample quality generative adversarial nets gans paper mathematically study properties current variants gans make use class label information class aware gradient cross entropy decomposition reveal class labels associated losses influence gan training based propose activation maximization generative adversarial networks gan advanced solution comprehensive experiments conducted validate analysis evaluate effectiveness solution gan outperforms strong baselines achieves state art inception score cifar addition demonstrate inception imagenet classifier inception score mainly tracks diversity generator however reliable evidence reflect true sample quality thus propose new metric called score provide accurate estimation sample quality proposed model also outperforms baseline methods new metric\n",
            "output sentence:  understand class labels help gan training propose new evaluation metric generative models \n",
            "\n",
            "{'rouge-1': {'r': 0.06666666666666667, 'p': 0.875, 'f': 0.12389380399404809}, 'rouge-2': {'r': 0.047244094488188976, 'p': 0.8571428571428571, 'f': 0.0895522378157719}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.875, 'f': 0.12389380399404809}}\n",
            "pair:  deep neural networks trained wide range datasets demonstrate impressive transferability deep features appear general applicable many datasets tasks property prevalent use real world applications neural network pretrained large datasets imagenet significantly boost generalization accelerate training fine tuned smaller target dataset despite pervasiveness effort devoted uncovering reason transferability deep feature representations paper tries understand transferability perspectives improved generalization optimization feasibility transferability demonstrate transferred models tend find flatter minima since weight matrices stay close original flat region pretrained parameters transferred similar target dataset transferred representations make loss landscape favorable improved lipschitzness accelerates stabilizes training substantially improvement largely attributes fact principal component gradient suppressed pretrained parameters thus stabilizing magnitude gradient back propagation feasibility transferability related similarity input label surprising discovery feasibility also impacted training stages transferability first increases training declines provide theoretical analysis verify observations\n",
            "output sentence:  understand transferability perspectives improved generalization optimization feasibility transferability \n",
            "\n",
            "{'rouge-1': {'r': 0.1038961038961039, 'p': 0.6666666666666666, 'f': 0.17977527856583767}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.09090909090909091, 'f': 0.019999998042000193}, 'rouge-l': {'r': 0.07792207792207792, 'p': 0.5, 'f': 0.13483145834111857}}\n",
            "pair:  reinforcement learning rl typically defines discount factor part markov decision process discount factor values future rewards exponential scheme leads theoretical convergence guarantees bellman equation however evidence psychology economics neuroscience suggests humans animals instead hyperbolic time preferences extend earlier work kurth nelson redish propose efficient deep reinforcement learning agent acts via hyperbolic discounting non exponential discount mechanisms demonstrate simple approach approximates hyperbolic discount functions still using familiar temporal difference learning techniques rl additionally independent hyperbolic discounting make surprising discovery simultaneously learning value functions multiple time horizons effective auxiliary task often improves state art methods\n",
            "output sentence:  deep rl agent learns hyperbolic non exponential values new multi horizon auxiliary task \n",
            "\n",
            "{'rouge-1': {'r': 0.11290322580645161, 'p': 0.7777777777777778, 'f': 0.19718309637770284}, 'rouge-2': {'r': 0.07692307692307693, 'p': 0.75, 'f': 0.13953488203353168}, 'rouge-l': {'r': 0.11290322580645161, 'p': 0.7777777777777778, 'f': 0.19718309637770284}}\n",
            "pair:  state art sound event classification relies neural networks learn associations class labels audio recordings within dataset datasets typically define ontology create structure relates sound classes abstract super classes hence ontology serves source domain knowledge representation sounds however ontology information rarely considered specially explored model neural network architectures propose two ontology based neural network architectures sound event classification defined framework design simple network architectures preserve ontological structure networks trained evaluated using two common sound event classification datasets results show improvement classification performance demonstrating benefits including ontological information\n",
            "output sentence:  present ontology based neural network architectures sound event classification \n",
            "\n",
            "{'rouge-1': {'r': 0.07777777777777778, 'p': 0.7, 'f': 0.13999999820000003}, 'rouge-2': {'r': 0.02912621359223301, 'p': 0.3333333333333333, 'f': 0.05357142709343116}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  neural networks widely used natural language processing yet despite empirical successes behaviour brittle sensitive small input changes sensitive deletions large fractions input text paper aims tackle sensitivity context natural language inference ensuring models become confident predictions arbitrary subsets words input text deleted develop novel technique formal verification specification models based popular decomposable attention mechanism employing efficient yet effective interval bound propagation ibp approach using method efficiently prove given model whether particular sample free sensitivity problem compare different training methods address sensitivity compare metrics measure experiments snli mnli datasets observe ibp training leads significantly improved verified accuracy snli test set verify samples substantial improvement using standard training\n",
            "output sentence:  formal verification specification model prediction undersensitivity using interval bound propagation \n",
            "\n",
            "{'rouge-1': {'r': 0.11392405063291139, 'p': 0.5625, 'f': 0.1894736814094183}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.25, 'f': 0.07142856897959192}, 'rouge-l': {'r': 0.10126582278481013, 'p': 0.5, 'f': 0.16842104983047096}}\n",
            "pair:  model based reinforcement learning agent interleaves model learning planning two components inextricably intertwined model able provide sensible long term prediction executed planer would exploit model flaws yield catastrophic failures paper focuses building model reasons long term future demonstrates use efficient planning exploration end build latent variable autoregressive model leveraging recent ideas variational inference argue forcing latent variables carry future information auxiliary task substantially improves long term predictions moreover planning latent space planner solution ensured within regions model valid exploration strategy devised searching unlikely trajectories model methods achieves higher reward faster compared baselines variety tasks environments imitation learning model based reinforcement learning settings\n",
            "output sentence:  incorporating model latent variables encode future content improves long term prediction accuracy critical better planning model based rl \n",
            "\n",
            "{'rouge-1': {'r': 0.0851063829787234, 'p': 0.5714285714285714, 'f': 0.1481481458916324}, 'rouge-2': {'r': 0.031007751937984496, 'p': 0.3076923076923077, 'f': 0.05633802650565369}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.5714285714285714, 'f': 0.1481481458916324}}\n",
            "pair:  many differences convolutional networks ventral visual streams primates example standard convolutional networks lack recurrent lateral connections cell dynamics etc however feedforward architectures somewhat similar ventral stream warrant detailed comparison recent study found feedforward architecture visual cortex could closely approximated convolutional network resulting architecture differed widely used deep networks several ways study also found somewhat surprisingly training ventral stream network object recognition resulted poor performance paper examines performance network detail particular made number changes ventral stream based architecture make like densenet tested performance step chose densenet high brainscore cortex like architectural features large degrees long skip connections changes made cortex like network like densenet improved performance work needed better understand results one possibility details ventral stream architecture may ill suited feedforward computation simple processing units backpropagation could suggest differences way high performance deep networks brain approach core object recognition\n",
            "output sentence:  approximation primate ventral stream convolutional network performs poorly object recognition multiple architectural features contribute \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.4117647058823529, 'f': 0.17499999665312507}, 'rouge-2': {'r': 0.02702702702702703, 'p': 0.11764705882352941, 'f': 0.04395604091776377}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.35294117647058826, 'f': 0.14999999665312505}}\n",
            "pair:  generative adversarial networks gans form generative modeling approach known producing appealing samples notably difficult train one common way tackle issue propose new formulations gan objective yet surprisingly studies looked optimization methods designed adversarial training work cast gan optimization problems general variational inequality framework tapping mathematical programming literature counter common misconceptions difficulties saddle point optimization propose extend methods designed variational inequalities training gans apply averaging extrapolation computationally cheaper variant call extrapolation past stochastic gradient method sgd adam\n",
            "output sentence:  cast gans variational inequality framework import techniques literature optimize gans better give algorithmic extensions empirically test performance training gans \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03508771929824561, 'p': 0.3333333333333333, 'f': 0.06349206176870753}}\n",
            "pair:  memorization data deep neural networks become subject significant research interest paper link memorization images deep convolutional autoencoders downsampling strided convolution analyze mechanism simpler setting train linear convolutional autoencoders show linear combinations training data stored eigenvectors linear operator corresponding network downsampling used hand networks without downsampling memorize training data provide evidence effect happens nonlinear networks moreover downsampling nonlinear networks causes model memorize linear combinations images individual training images since convolutional autoencoder components building blocks deep convolutional networks envision findings shed light important phenomenon memorization parameterized deep networks\n",
            "output sentence:  identify downsampling mechansim memorization convolutional autoencoders \n",
            "\n",
            "{'rouge-1': {'r': 0.06593406593406594, 'p': 0.6666666666666666, 'f': 0.11999999836200001}, 'rouge-2': {'r': 0.023622047244094488, 'p': 0.375, 'f': 0.04444444332949248}, 'rouge-l': {'r': 0.054945054945054944, 'p': 0.5555555555555556, 'f': 0.09999999836200002}}\n",
            "pair:  spiking neural networks investigated biologically plausible models neural computation also potentially efficient type neural network convolutional spiking neural networks demonstrated achieve near state art performance one solution proposed convert gated recurrent neural networks far recurrent neural networks form networks gating memory cells central state art solutions problem domains involve sequence recognition generation design analog gated lstm cell neurons substituted efficient stochastic spiking neurons adaptive spiking neurons implement adaptive form sigma delta coding convert internally computed analog activation values spike trains neurons approximate effective activation function resembles sigmoid show analog neurons activation functions used create analog lstm cell networks cells trained standard backpropagation train lstm networks noisy noiseless version original sequence prediction task hochreiter schmidhuber also noisy noiseless version classical working memory reinforcement learning task maze substituting analog neurons corresponding adaptive spiking neurons show almost resulting spiking neural network equivalents correctly compute original tasks\n",
            "output sentence:  demonstrate gated recurrent asynchronous spiking neural network corresponds lstm unit \n",
            "\n",
            "{'rouge-1': {'r': 0.038461538461538464, 'p': 0.6, 'f': 0.07228915549426625}, 'rouge-2': {'r': 0.011494252873563218, 'p': 0.25, 'f': 0.02197802113754381}, 'rouge-l': {'r': 0.02564102564102564, 'p': 0.4, 'f': 0.04819276995209757}}\n",
            "pair:  paper addresses problem representing system belief using multi variate normal distributions mnd underlying model based deep neural network dnn major challenge dnns computational complexity needed obtain model uncertainty using mnds achieve scalable method propose novel approach expresses parameter posterior sparse information form inference algorithm based novel laplace approximation scheme involves diagonal correction kronecker factored eigenbasis makes inversion information matrix intractable operation required full bayesian analysis devise low rank approximation eigenbasis memory efficient sampling scheme provide theoretical analysis empirical evaluation various benchmark data sets showing superiority approach existing methods\n",
            "output sentence:  approximate inference algorithm deep learning \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.6666666666666666, 'f': 0.13333333153333335}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.2, 'f': 0.03030302890266306}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  hypernetworks meta neural networks generate weights main neural network end end differentiable manner despite extensive applications ranging multi task learning bayesian deep learning problem optimizing hypernetworks studied date observe classical weight initialization methods like glorot bengio et al applied directly hypernet fail produce weights mainnet correct scale develop principled techniques weight initialization hypernets show lead stable mainnet weights lower training loss faster convergence\n",
            "output sentence:  first principled weight initialization method hypernetworks \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.7, 'f': 0.16279069561925366}, 'rouge-2': {'r': 0.009523809523809525, 'p': 0.1, 'f': 0.017391302759924534}, 'rouge-l': {'r': 0.07894736842105263, 'p': 0.6, 'f': 0.13953488166576528}}\n",
            "pair:  imitation learning demonstrations usually relies learning policy trajectories optimal states actions however real life expert demonstrations often action information missing state trajectories available present model based imitation learning method learn environment specific optimal actions expert state trajectories proposed method starts model free reinforcement learning algorithm heuristic reward signal sample environment dynamics used train state transition probability subsequently learn optimal actions expert state trajectories supervised learning back propagating error gradients modeled environment dynamics experimental evaluations show proposed method successfully achieves performance similar state action trajectory based traditional imitation learning methods even absence action information much fewer iterations compared conventional model free reinforcement learning methods also demonstrate method learn act video demonstrations expert agent simple games learn achieve desired performance less number iterations\n",
            "output sentence:  learning imitate expert absence optimal actions learning dynamics model exploring environment \n",
            "\n",
            "{'rouge-1': {'r': 0.13513513513513514, 'p': 0.5882352941176471, 'f': 0.21978021674193937}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.25, 'f': 0.07692307431952672}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.4117647058823529, 'f': 0.15384615080787348}}\n",
            "pair:  simultaneously capture syntax semantics text corpus propose new larger context language model extracts recurrent hierarchical semantic structure via dynamic deep topic model guide natural language generation moving beyond conventional language model ignores long range word dependencies sentence order proposed model captures intra sentence word dependencies also temporal transitions sentences inter sentence topic dependences inference develop hybrid stochastic gradient mcmc recurrent autoencoding variational bayes experimental results variety real world text corpora demonstrate proposed model outperforms state art larger context language models also learns interpretable recurrent multilayer topics generates diverse sentences paragraphs syntactically correct semantically coherent\n",
            "output sentence:  introduce novel larger context language model simultaneously captures syntax semantics making capable generating highly interpretable sentences paragraphs \n",
            "\n",
            "{'rouge-1': {'r': 0.12941176470588237, 'p': 0.6470588235294118, 'f': 0.21568627173202617}, 'rouge-2': {'r': 0.04424778761061947, 'p': 0.3125, 'f': 0.07751937767201497}, 'rouge-l': {'r': 0.09411764705882353, 'p': 0.47058823529411764, 'f': 0.15686274232026148}}\n",
            "pair:  work address semi supervised classification graph data categories unlabeled nodes inferred labeled nodes well graph structures recent works often solve problem advanced graph convolution conventional supervised manner performance could heavily affected labeled data scarce propose graph inference learning gil framework boost performance node classification learning inference node labels graph topology bridge connection two nodes formally define structure relation encapsulating node attributes node paths local topological structures together make inference conveniently deduced one node another node learning inference process introduce meta optimization structure relations training nodes validation nodes learnt graph inference capability better self adapted test nodes comprehensive evaluations four benchmark datasets including cora citeseer pubmed nell demonstrate superiority gil compared state art methods semi supervised node classification task\n",
            "output sentence:  propose novel graph inference learning framework building structure relations infer unknown node labels labeled nodes end end way \n",
            "\n",
            "{'rouge-1': {'r': 0.10204081632653061, 'p': 0.8333333333333334, 'f': 0.1818181798743802}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.5454545454545454, 'f': 0.08391608249596559}, 'rouge-l': {'r': 0.10204081632653061, 'p': 0.8333333333333334, 'f': 0.1818181798743802}}\n",
            "pair:  increasing demand deploy convolutional neural networks cnns mobile platforms sparse kernel approach proposed could save parameters standard convolution maintaining accuracy however despite great potential prior research pointed craft sparse kernel design potential effective design prior works adopt simple combinations existing sparse kernels group convolution meanwhile due large design space also impossible try combinations existing sparse kernels paper first field consider craft effective sparse kernel design eliminating large design space specifically present sparse kernel scheme illustrate reduce space three aspects first terms composition remove designs composed repeated layers second remove designs large accuracy degradation find unified property named emph information field behind various sparse kernel designs could directly indicate final accuracy last remove designs two cases better parameter efficiency could achieved additionally provide detailed efficiency analysis final designs scheme experimental results validate idea scheme showing scheme able find designs efficient using parameters computation similar higher accuracy\n",
            "output sentence:  first field show craft effective sparse kernel design three aspects composition performance \n",
            "\n",
            "{'rouge-1': {'r': 0.15714285714285714, 'p': 0.7333333333333333, 'f': 0.2588235265051903}, 'rouge-2': {'r': 0.043010752688172046, 'p': 0.26666666666666666, 'f': 0.07407407168209884}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.6666666666666666, 'f': 0.23529411474048442}}\n",
            "pair:  introduce learning based algorithm low rank decomposition problem given times matrix parameter compute rank matrix minimizes approximation loss algorithm uses training set input matrices order optimize performance specifically efficient approximate algorithms computing low rank approximations proceed computing projection sa sparse random times sketching matrix performing singular value decomposition sa show replace random matrix learned matrix sparsity reduce error experiments show multiple types data sets learned sketch matrix substantially reduce approximation loss compared random matrix sometimes one order magnitude also study mixed matrices rows trained remaining ones random show matrices still offer improved performance retaining worst case guarantees\n",
            "output sentence:  learning based algorithms improve upon performance classical algorithms low rank approximation problem retaining worst case guarantee \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.5555555555555556, 'f': 0.24096385202496737}, 'rouge-2': {'r': 0.05555555555555555, 'p': 0.2777777777777778, 'f': 0.09259258981481489}, 'rouge-l': {'r': 0.12307692307692308, 'p': 0.4444444444444444, 'f': 0.19277108094063003}}\n",
            "pair:  demonstrate possibility call sparse learning accelerated training deep neural networks maintain sparse weights throughout training achieving dense performance levels accomplish developing sparse momentum algorithm uses exponentially smoothed gradients momentum identify layers weights reduce error efficiently sparse momentum redistributes pruned weights across layers according mean momentum magnitude layer within layer sparse momentum grows weights according momentum magnitude zero valued weights demonstrate state art sparse performance mnist cifar imagenet decreasing mean error relative compared sparse algorithms furthermore show sparse momentum reliably reproduces dense performance levels providing faster training analysis ablations show benefits momentum redistribution growth increase depth size network\n",
            "output sentence:  redistributing growing weights according momentum magnitude enables training sparse networks random initializations reach dense performance levels weights accelerating training \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.5, 'f': 0.13333333102222228}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05128205128205128, 'p': 0.3333333333333333, 'f': 0.08888888657777784}}\n",
            "pair:  recent research proposed lottery ticket hypothesis suggesting deep neural network exist trainable sub networks performing equally better original model commensurate training steps discovery insightful finding proper sub networks requires iterative training pruning high cost incurred limits applications lottery ticket hypothesis show exists subset aforementioned sub networks converge significantly faster training process thus mitigate cost issue conduct extensive experiments show sub networks consistently exist across various model structures restrictive setting hyperparameters carefully selected learning rate pruning ratio model capacity practical application findings demonstrate sub networks help cutting total time adversarial training standard approach improve robustness cifar achieve state art robustness\n",
            "output sentence:  show possibility pruning find small sub network significantly higher convergence rate full model \n",
            "\n",
            "{'rouge-1': {'r': 0.05357142857142857, 'p': 0.6, 'f': 0.09836065423273314}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.25, 'f': 0.028571427493877595}, 'rouge-l': {'r': 0.05357142857142857, 'p': 0.6, 'f': 0.09836065423273314}}\n",
            "pair:  momentum based methods conjunction stochastic gradient descent widely used training machine learning models little theoretical understanding generalization error methods practice momentum parameter often chosen heuristic fashion little theoretical guidance work use framework algorithmic stability provide upper bound generalization error class strongly convex loss functions mild technical assumptions bound decays zero inversely size training set increases momentum parameter increased also develop upper bound expected true risk terms number training steps size training set momentum parameter\n",
            "output sentence:  stochastic gradient method momentum generalizes \n",
            "\n",
            "{'rouge-1': {'r': 0.0989010989010989, 'p': 0.9, 'f': 0.17821781999803943}, 'rouge-2': {'r': 0.07142857142857142, 'p': 0.8888888888888888, 'f': 0.13223140358172258}, 'rouge-l': {'r': 0.0989010989010989, 'p': 0.9, 'f': 0.17821781999803943}}\n",
            "pair:  fine tuning pre trained imagenet models become de facto standard various computer vision tasks current practices fine tuning typically involve selecting ad hoc choice hyper parameters keeping fixed values normally used training scratch paper examines several common practices setting hyper parameters fine tuning findings based extensive empirical evaluation fine tuning various transfer learning benchmarks prior works thoroughly investigated learning rate batch size momentum fine tuning relatively unexplored parameter find picking right value momentum critical fine tuning performance connect previous theoretical findings optimal hyper parameters fine tuning particular effective learning rate dataset dependent also sensitive similarity source domain target domain contrast hyper parameters training scratch reference based regularization keeps models close initial model necessarily apply dissimilar datasets findings challenge common practices fine tuning encourages deep learning practitioners rethink hyper parameters fine tuning\n",
            "output sentence:  paper examines several common practices setting hyper parameters fine tuning \n",
            "\n",
            "{'rouge-1': {'r': 0.10638297872340426, 'p': 1.0, 'f': 0.19230769056952665}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.5555555555555556, 'f': 0.08403361204717184}, 'rouge-l': {'r': 0.09574468085106383, 'p': 0.9, 'f': 0.1730769213387574}}\n",
            "pair:  partially observable markov decision processes pomdps widely used framework model decision making uncertainty environment stochastic outcome conventional pomdp models observations agent receives originate fixed known distribution however variety real world scenarios agent active role perception selecting observations receive due combinatorial nature selection process computationally intractable integrate perception decision planning decision prevent expansion action space propose greedy strategy observation selection aims minimize uncertainty state develop novel point based value iteration algorithm incorporates greedy strategy achieve near optimal uncertainty reduction sampled belief points turn enables solver efficiently approximate reachable subspace belief simplex essentially separating computations related perception planning lastly implement proposed solver demonstrate performance computational advantage range robotic scenarios robot simultaneously performs active perception planning\n",
            "output sentence:  develop point based value iteration solver pomdps active perception planning tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.17475728155339806, 'p': 0.8571428571428571, 'f': 0.29032257783168575}, 'rouge-2': {'r': 0.06557377049180328, 'p': 0.4, 'f': 0.1126760539178735}, 'rouge-l': {'r': 0.1262135922330097, 'p': 0.6190476190476191, 'f': 0.20967741654136318}}\n",
            "pair:  state art computer vision models shown vulnerable small adversarial perturbations input words images data distribution correctly classified model close visually similar misclassified image despite substantial research interest cause phenomenon still poorly understood remains unsolved hypothesize counter intuitive behavior naturally occurring result high dimensional geometry data manifold first step towards exploring hypothesis study simple synthetic dataset classifying two concentric high dimensional spheres dataset show fundamental tradeoff amount test error average distance nearest error particular prove model misclassifies small constant fraction sphere vulnerable adversarial perturbations size sqrt surprisingly train several different architectures dataset error sets naturally approach theoretical bound result theory vulnerability neural networks small adversarial perturbations logical consequence amount test error observed hope theoretical analysis simple case point way forward explore geometry complex real world data sets leads adversarial examples\n",
            "output sentence:  hypothesize vulnerability image models small adversarial perturbation naturally occurring result high dimensional geometry data manifold explore theoretically prove hypothesis simple synthetic \n",
            "\n",
            "{'rouge-1': {'r': 0.05405405405405406, 'p': 0.5, 'f': 0.09756097384889949}, 'rouge-2': {'r': 0.010526315789473684, 'p': 0.125, 'f': 0.019417474295409663}, 'rouge-l': {'r': 0.02702702702702703, 'p': 0.25, 'f': 0.048780486044021486}}\n",
            "pair:  deep neural networks shown outstanding results wide range applications learning limited number examples still challenging task despite difficulties shot learning metric learning techniques showed potential neural networks task methods perform well provide satisfactory results work idea metric learning extended support vector machines svm working mechanism well known generalization capabilities small dataset furthermore paper presents end end learning framework training adaptive kernel svms eliminates problem choosing correct kernel good features svms next one shot learning problem redefined audio signals model tested vision task using omniglot dataset speech task using timit dataset well actually algorithm using omniglot dataset improved accuracy one shot classification task shot classification task\n",
            "output sentence:  proposed method end end neural svm optimized shot learning \n",
            "\n",
            "{'rouge-1': {'r': 0.06172839506172839, 'p': 0.625, 'f': 0.1123595489256407}, 'rouge-2': {'r': 0.010101010101010102, 'p': 0.14285714285714285, 'f': 0.018867923294766904}, 'rouge-l': {'r': 0.04938271604938271, 'p': 0.5, 'f': 0.08988763881328117}}\n",
            "pair:  lifelong learning poses considerable challenges terms effectiveness minimizing prediction errors tasks overall computational tractability real time performance paper addresses continuous lifelong multitask learning jointly estimating inter task relations textit output kernel per task model parameters round assuming data arrives streaming fashion propose novel algorithm called textit online output kernel learning algorithm ookla lifelong learning setting avoid memory explosion propose robust budget limited versions proposed algorithm efficiently utilize relationship tasks bound total number representative examples support set addition propose two stage budgeted scheme efficiently tackling task specific budget constraints lifelong learning empirical results three datasets indicate superior auc performance ookla budget limited cousins strong baselines\n",
            "output sentence:  novel approach online lifelong learning using output kernels \n",
            "\n",
            "{'rouge-1': {'r': 0.12244897959183673, 'p': 0.75, 'f': 0.2105263133764235}, 'rouge-2': {'r': 0.05970149253731343, 'p': 0.47058823529411764, 'f': 0.10596026290250432}, 'rouge-l': {'r': 0.12244897959183673, 'p': 0.75, 'f': 0.2105263133764235}}\n",
            "pair:  representation learning graph structured data received significant attention recently due ubiquitous applicability however advancements made static graph settings efforts jointly learning dynamic graph dynamic graph still infant stage two fundamental questions arise learning dynamic graphs elegantly model dynamical processes graphs ii leverage model effectively encode evolving graph information low dimensional representations present dyrep novel modeling framework dynamic graphs posits representation learning latent mediation process bridging two observed processes namely dynamics network realized topological evolution dynamics network realized activities nodes concretely propose two time scale deep temporal point process model captures interleaved dynamics observed processes model parameterized temporal attentive representation network encodes temporally evolving structural information node representations turn drives nonlinear evolution observed graph dynamics unified framework trained using efficient unsupervised procedure capability generalize unseen nodes demonstrate dyrep outperforms state art baselines dynamic link prediction time prediction tasks present extensive qualitative insights framework\n",
            "output sentence:  models representation learning dynamic graphs latent hidden process bridging two observed processes topological evolution dynamic dynamic graphs interactions dynamic \n",
            "\n",
            "{'rouge-1': {'r': 0.06349206349206349, 'p': 0.5, 'f': 0.11267605433842494}, 'rouge-2': {'r': 0.025974025974025976, 'p': 0.2857142857142857, 'f': 0.04761904609126989}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.375, 'f': 0.08450704025391792}}\n",
            "pair:  show beneficial express metropolis accept reject decisions terms comparison uniform value update uniform value non reversibly part markov chain state rather sampling independently iteration provides small improvement random walk metropolis langevin updates high dimensions produces larger improvement using langevin updates persistent momentum giving performance comparable hamiltonian monte carlo hmc long trajectories significance variables updated methods since hmc used updates done trajectories whereas done often langevin updates seen bayesian neural network model connection weights updated persistent langevin hmc hyperparameters updated gibbs sampling\n",
            "output sentence:  non reversible way making accept reject decisions beneficial \n",
            "\n",
            "{'rouge-1': {'r': 0.1320754716981132, 'p': 0.5833333333333334, 'f': 0.21538461237396456}, 'rouge-2': {'r': 0.06060606060606061, 'p': 0.3333333333333333, 'f': 0.10256409996055234}, 'rouge-l': {'r': 0.11320754716981132, 'p': 0.5, 'f': 0.18461538160473379}}\n",
            "pair:  present eda easy data augmentation techniques boosting performance text classification tasks eda consists four simple powerful operations synonym replacement random insertion random swap random deletion five text classification tasks show eda improves performance convolutional recurrent neural networks eda demonstrates particularly strong results smaller datasets average across five datasets training eda using available training set achieved accuracy normal training available data also performed extensive ablation studies suggest parameters practical use\n",
            "output sentence:  simple text augmentation techniques significantly boost performance text classification tasks especially small datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.07766990291262135, 'p': 0.4, 'f': 0.13008129808976143}, 'rouge-2': {'r': 0.023809523809523808, 'p': 0.15789473684210525, 'f': 0.04137930806753877}, 'rouge-l': {'r': 0.05825242718446602, 'p': 0.3, 'f': 0.09756097288650943}}\n",
            "pair:  deep neural networks particular convolutional neural networks become highly effective tools compressing images solving inverse problems including denoising inpainting reconstruction noisy measurements success attributed part ability represent generate natural images well contrary classical tools wavelets image generating deep neural networks large number parameters typically multiple output dimension need trained large datasets paper propose untrained simple image model called deep decoder deep neural network generate natural images weight parameters deep decoder simple architecture convolutions fewer weight parameters output dimensionality underparameterization enables deep decoder compress images concise set network weights show par wavelet based thresholding underparameterization provides barrier overfitting allowing deep decoder state art performance denoising deep decoder simple sense layer identical structure consists one upsampling unit pixel wise linear combination channels relu activation channelwise normalization simplicity makes network amenable theoretical analysis sheds light aspects neural networks enable form effective signal representations\n",
            "output sentence:  introduce underparameterized nonconvolutional simple deep neural network without training effectively represent natural images solve image processing tasks like compression denoising \n",
            "\n",
            "{'rouge-1': {'r': 0.10204081632653061, 'p': 0.38461538461538464, 'f': 0.16129031926638923}, 'rouge-2': {'r': 0.03389830508474576, 'p': 0.15384615384615385, 'f': 0.05555555259645077}, 'rouge-l': {'r': 0.061224489795918366, 'p': 0.23076923076923078, 'f': 0.09677419023413122}}\n",
            "pair:  propose novel subgraph image representation classification network fragments target parent networks graph image representation based image embeddings adjacency matrices use image representation two modes first input machine learning algorithm second input pure transfer learner conclusions multiple datasets deep learning using structured image features performs best compared graph kernel classical features based methods pure transfer learning works effectively minimum interference user robust small data\n",
            "output sentence:  convert subgraphs structured images classify using deep learning transfer learning caffe achieve stunning results \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.6666666666666666, 'f': 0.19672130895995704}, 'rouge-2': {'r': 0.03389830508474576, 'p': 0.25, 'f': 0.05970149043439526}, 'rouge-l': {'r': 0.09615384615384616, 'p': 0.5555555555555556, 'f': 0.1639344237140554}}\n",
            "pair:  propose approach sequence modeling based autoregressive normalizing flows autoregressive transform acting across time serves moving reference frame modeling higher level dynamics technique provides simple general purpose method improving sequence modeling connections existing classical techniques demonstrate proposed approach standalone models well part larger sequential latent variable models results presented three benchmark video datasets flow based dynamics improve log likelihood performance baseline models\n",
            "output sentence:  show autoregressive flows used improve sequential latent variable models \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.42857142857142855, 'f': 0.14999999711250006}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.42857142857142855, 'f': 0.14999999711250006}}\n",
            "pair:  introduce largest among publicly available dataset cyrillic handwritten text recognition first dataset cyrillic text wild recognition well suggest method recognizing cyrillic handwritten text text wild based approach develop system reduce document processing time one largest mathematical competitions ukraine days amount used paper ton\n",
            "output sentence:  introduce several datasets cyrillic ocr method recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.03508771929824561, 'p': 0.2, 'f': 0.05970148999777244}, 'rouge-2': {'r': 0.014705882352941176, 'p': 0.1111111111111111, 'f': 0.02597402390959706}, 'rouge-l': {'r': 0.03508771929824561, 'p': 0.2, 'f': 0.05970148999777244}}\n",
            "pair:  nowadays deep learning one main topics almost every field helped get amazing results great number tasks main problem kind learning consequently neural networks defined deep resource intensive need specialized hardware perform computation reasonable time unfortunately sufficient make deep learning usable real life many tasks mandatory much possible real time needed optimize many components code algorithms numeric accuracy hardware make efficient usable optimizations help us produce incredibly accurate fast learning models\n",
            "output sentence:  embedded architecture deep learning optimized devices face detection emotion recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.5555555555555556, 'f': 0.14285714061632654}, 'rouge-2': {'r': 0.028985507246376812, 'p': 0.25, 'f': 0.051948050086017945}, 'rouge-l': {'r': 0.08196721311475409, 'p': 0.5555555555555556, 'f': 0.14285714061632654}}\n",
            "pair:  size complexity models datasets grow need communication efficient variants stochastic gradient descent deployed clusters perform model fitting parallel alistarh et al describe two variants data parallel sgd quantize encode gradients lessen communication costs first variant qsgd provide strong theoretical guarantees second variant call qsgdinf demonstrate impressive empirical gains distributed training large neural networks building work propose alternative scheme quantizing gradients show yields stronger theoretical guarantees exist qsgd matching empirical performance qsgdinf\n",
            "output sentence:  nuqsgd closes gap theoretical guarantees qsgd empirical performance qsgdinf \n",
            "\n",
            "{'rouge-1': {'r': 0.10227272727272728, 'p': 0.5, 'f': 0.1698113179352083}, 'rouge-2': {'r': 0.02727272727272727, 'p': 0.17647058823529413, 'f': 0.04724409216938445}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.3333333333333333, 'f': 0.11320754435030267}}\n",
            "pair:  recent research intensively revealed vulnerability deep neural networks especially convolutional neural networks cnns task image recognition creating adversarial samples slightly differ legitimate samples vulnerability indicates powerful models sensitive specific perturbations cannot filter adversarial perturbations work propose quantization based method enables cnn filter adversarial perturbations effectively notably different prior work input quantization apply quantization intermediate layers cnn approach naturally aligned clustering coarse grained semantic information learned cnn furthermore compensate loss information inevitably caused quantization propose multi head quantization project data points different sub spaces perform quantization within sub space enclose design quantization layer named layer results obtained mnist fashion mnsit datasets demonstrate adding one layer cnn could significantly improve robustness white box black box attacks\n",
            "output sentence:  propose quantization based method regularizes cnn learned representations automatically aligned trainable concept matrix hence effectively filtering adversarial perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.3333333333333333, 'f': 0.0869565194706995}, 'rouge-2': {'r': 0.014285714285714285, 'p': 0.125, 'f': 0.025641023800131623}, 'rouge-l': {'r': 0.05, 'p': 0.3333333333333333, 'f': 0.0869565194706995}}\n",
            "pair:  deep neural networks recently demonstrated vulnerable backdoor attacks specifically altering small set training examples adversary able install backdoor used inference fully control model behavior attack powerful crucially relies adversary able introduce arbitrary often clearly mislabeled inputs training set thus detected even fairly rudimentary data filtering paper introduce new approach executing backdoor attacks utilizing adversarial examples gan generated data key feature resulting poisoned inputs appear consistent label thus seem benign even upon human inspection\n",
            "output sentence:  show successfully perform backdoor attacks without changing training labels \n",
            "\n",
            "{'rouge-1': {'r': 0.109375, 'p': 0.5384615384615384, 'f': 0.18181817901163774}, 'rouge-2': {'r': 0.02666666666666667, 'p': 0.16666666666666666, 'f': 0.045977009116131715}, 'rouge-l': {'r': 0.078125, 'p': 0.38461538461538464, 'f': 0.12987012706358583}}\n",
            "pair:  consider problem using variational latent variable models data compression models produce compressed binary sequence universal data representation digital world latent representation needs subjected entropy coding range coding entropy coding technique optimal fail catastrophically computation prior differs even slightly sending receiving side unfortunately common scenario floating point math used sender receiver operate different hardware software platforms numerical round often platform dependent propose using integer networks universal solution problem demonstrate enable reliable cross platform encoding decoding images using variational models\n",
            "output sentence:  train variational models quantized networks computational determinism enables using cross platform data compression \n",
            "\n",
            "{'rouge-1': {'r': 0.10185185185185185, 'p': 0.5789473684210527, 'f': 0.17322834391220784}, 'rouge-2': {'r': 0.05555555555555555, 'p': 0.3684210526315789, 'f': 0.09655172186064215}, 'rouge-l': {'r': 0.07407407407407407, 'p': 0.42105263157894735, 'f': 0.1259842494240189}}\n",
            "pair:  sepsis life threatening complication infection leading cause mortality hospitals early detection sepsis improves patient outcomes little consensus exact treatment guidelines treating septic patients remains open problem work present new deep reinforcement learning method use learn optimal personalized treatment policies septic patients model patient continuous valued physiological time series using multi output gaussian processes probabilistic model easily handles missing values irregularly spaced observation times maintaining estimates uncertainty gaussian process directly tied deep recurrent network learns clinically interpretable treatment policies models learned together end end evaluate approach heterogeneous dataset septic spanning months university health system find learned policy could reduce patient mortality much overall baseline mortality rate algorithm could used make treatment recommendations physicians part decision support tool framework readily applies reinforcement learning problems rely sparsely sampled frequently missing multivariate time series data\n",
            "output sentence:  combine multi output gaussian processes deep recurrent networks learn optimal treatments sepsis show improved performance standard deep reinforcement learning methods \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.625, 'f': 0.13157894548476456}, 'rouge-2': {'r': 0.03260869565217391, 'p': 0.42857142857142855, 'f': 0.060606059291909015}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.625, 'f': 0.13157894548476456}}\n",
            "pair:  effectively capturing graph node sequences form vector embeddings critical many applications achieve first learning vector embeddings single graph nodes ii composing compactly represent node sequences specifically propose sense semantically enhanced node sequence embedding single nodes skip gram based novel embedding mechanism single graph nodes co learns graph structure well textual descriptions demonstrate sense vectors increase accuracy multi label classification tasks link prediction tasks variety scenarios using real datasets based sense next propose generic sense compute composite vectors represent sequence nodes preserving node order important prove approach efficient embedding node sequences experiments real data confirm high accuracy node order decoding\n",
            "output sentence:  node sequence embedding mechanism captures graph text properties \n",
            "\n",
            "{'rouge-1': {'r': 0.0684931506849315, 'p': 0.625, 'f': 0.12345678834324036}, 'rouge-2': {'r': 0.036585365853658534, 'p': 0.375, 'f': 0.06666666504691361}, 'rouge-l': {'r': 0.0684931506849315, 'p': 0.625, 'f': 0.12345678834324036}}\n",
            "pair:  temporal point processes dominant paradigm modeling sequences events happening irregular intervals standard way learning models estimating conditional intensity function however parameterizing intensity function usually incurs several trade offs show overcome limitations intensity based approaches directly modeling conditional distribution inter event times draw literature normalizing flows design models flexible efficient additionally propose simple mixture model matches flexibility flow based models also permits sampling computing moments closed form proposed models achieve state art performance standard prediction tasks suitable novel applications learning sequence embeddings imputing missing data\n",
            "output sentence:  learn temporal point processes modeling conditional density conditional intensity \n",
            "\n",
            "{'rouge-1': {'r': 0.08235294117647059, 'p': 0.6363636363636364, 'f': 0.1458333313042535}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.2, 'f': 0.036697246039895715}, 'rouge-l': {'r': 0.07058823529411765, 'p': 0.5454545454545454, 'f': 0.12499999797092017}}\n",
            "pair:  deep learning demonstrated abilities learn complex structures restricted available data recently consensus networks cns proposed alleviate data sparsity utilizing features multiple modalities limited size labeled data paper extend cn transductive consensus networks tcns suitable semi supervised learning tcns different modalities input compressed latent representations encourage become indistinguishable iterative adversarial training understand tcns two mechanisms consensus classification put forward three variants ablation studies mechanisms investigate tcn models treat latent representations probability distributions measure similarities negative relative jensen shannon divergences show consensus state beneficial classification desires stable imperfect similarity representations overall tcns outperform align best benchmark algorithms given labeled samples bank marketing dementiabank datasets\n",
            "output sentence:  tcn multimodal semi supervised learning ablation study mechanisms interpretations latent representations \n",
            "\n",
            "{'rouge-1': {'r': 0.06666666666666667, 'p': 0.2, 'f': 0.09999999625000015}, 'rouge-2': {'r': 0.014084507042253521, 'p': 0.045454545454545456, 'f': 0.021505372732108365}, 'rouge-l': {'r': 0.05, 'p': 0.15, 'f': 0.07499999625000019}}\n",
            "pair:  current classical planners successful finding non optimal plans even large planning instances planners rely preprocessing stage computes grounded representation task whenever grounded task big generated whenever preprocess fails instance cannot even tackled actual planner address issue introduce partial grounding approach grounds projection task complete grounding feasible propose guiding mechanism given domain identifies parts task relevant find plan using shelf machine learning methods empirical evaluation attests approach capable solving planning instances big fully grounded\n",
            "output sentence:  paper introduces partial grounding tackle problem arises full grounding process translation pddl input task input ground representation like infeasible memory due due memory \n",
            "\n",
            "{'rouge-1': {'r': 0.16981132075471697, 'p': 0.8181818181818182, 'f': 0.28124999715332033}, 'rouge-2': {'r': 0.07246376811594203, 'p': 0.45454545454545453, 'f': 0.12499999762812504}, 'rouge-l': {'r': 0.1320754716981132, 'p': 0.6363636363636364, 'f': 0.21874999715332033}}\n",
            "pair:  large deep neural networks powerful exhibit undesirable behaviors memorization sensitivity adversarial examples work propose mixup simple learning principle alleviate issues essence mixup trains neural network convex combinations pairs examples labels mixup regularizes neural network favor simple linear behavior training examples experiments imagenet cifar cifar google commands uci datasets show mixup improves generalization state art neural network architectures also find mixup reduces memorization corrupt labels increases robustness adversarial examples stabilizes training generative adversarial networks\n",
            "output sentence:  training convex combinations random training examples labels improves generalization deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.049019607843137254, 'p': 0.5, 'f': 0.08928571265943878}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.029411764705882353, 'p': 0.3, 'f': 0.053571426945153114}}\n",
            "pair:  knowledge bases kb automatically manually constructed often incomplete many valid facts inferred kb synthesizing existing information popular approach kb completion infer new relations combinatory reasoning information found along paths connecting pair entities given enormous size kbs exponential number paths previous path based models considered problem predicting missing relation given two entities evaluating truth proposed triple additionally methods traditionally used random paths fixed entity pairs recently learned pick paths propose new algorithm minerva addresses much difficult practical task answering questions relation known one entity since random walks impractical setting unknown destination combinatorially many paths start node present neural reinforcement learning approach learns navigate graph conditioned input query find predictive paths comprehensive evaluation seven knowledge base datasets found minerva competitive many current state art methods\n",
            "output sentence:  present rl agent minerva learns walk knowledge graph answer queries \n",
            "\n",
            "{'rouge-1': {'r': 0.09836065573770492, 'p': 0.6, 'f': 0.16901408208688753}, 'rouge-2': {'r': 0.04225352112676056, 'p': 0.3333333333333333, 'f': 0.07499999800312505}, 'rouge-l': {'r': 0.09836065573770492, 'p': 0.6, 'f': 0.16901408208688753}}\n",
            "pair:  introduce new rigorously formulated pac bayes shot meta learning algorithm implicitly learns model prior distribution interest proposed method extends pac bayes framework single task setting shot meta learning setting upper bound generalisation errors unseen tasks also propose generative based approach model shared prior task specific posterior expressively compared usual diagonal gaussian assumption show models trained proposed meta learning algorithm well calibrated accurate state art calibration classification results mini imagenet benchmark competitive results multi modal task distribution regression\n",
            "output sentence:  bayesian meta learning using pac bayes framework implicit prior distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.16393442622950818, 'p': 0.5263157894736842, 'f': 0.24999999637812503}, 'rouge-2': {'r': 0.06756756756756757, 'p': 0.2631578947368421, 'f': 0.10752687846918729}, 'rouge-l': {'r': 0.09836065573770492, 'p': 0.3157894736842105, 'f': 0.14999999637812508}}\n",
            "pair:  study cross entropy method cem non convex optimization continuous parameterized objective function introduce differentiable variant dcem enables us differentiate output cem respect objective function parameters machine learning setting brings cem inside end end learning pipeline cases otherwise impossible show applications synthetic energy based structured prediction task non convex continuous control control setting show simulated cheetah walker tasks embed optimal action sequences dcem use policy optimization fine tune components controller step towards combining model based model free rl\n",
            "output sentence:  dcem learns latent domains optimization problems helps bridge gap model based model free rl create differentiable controller fine tune parts ppo \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.5294117647058824, 'f': 0.21686746662215128}, 'rouge-2': {'r': 0.012195121951219513, 'p': 0.05, 'f': 0.01960783998462181}, 'rouge-l': {'r': 0.10606060606060606, 'p': 0.4117647058823529, 'f': 0.16867469553781395}}\n",
            "pair:  study two types preconditioners preconditioned stochastic gradient descent sgd methods unified framework call first one newton type due close relationship newton method second one fisher type preconditioner closely related inverse fisher information matrix preconditioners derived one framework efficiently estimated matrix lie groups designated user using natural relative gradient descent minimizing certain preconditioner estimation criteria many existing preconditioners methods rmsprop adam kfac equilibrated sgd batch normalization etc special cases closely related either newton type fisher type ones experimental results relatively large scale machine learning problems reported performance study\n",
            "output sentence:  propose new framework preconditioner learning derive new forms preconditioners learning methods reveal relationship methods like rmsprop adam adagrad esgd kfac kfac batch \n",
            "\n",
            "{'rouge-1': {'r': 0.10843373493975904, 'p': 0.5294117647058824, 'f': 0.17999999717800003}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.23529411764705882, 'f': 0.06611570006420335}, 'rouge-l': {'r': 0.08433734939759036, 'p': 0.4117647058823529, 'f': 0.13999999717800005}}\n",
            "pair:  intrinsically motivated goal exploration algorithms enable machines discover repertoires policies produce diversity effects complex environments exploration algorithms shown allow real world robots acquire skills tool use high dimensional continuous state action spaces however far assumed self generated goals sampled specifically engineered feature space limiting autonomy work propose approach using deep representation learning algorithms learn adequate goal space developmental stage approach first perceptual learning stage deep learning algorithms use passive raw sensor observations world changes learn corresponding latent space goal exploration happens second stage sampling goals latent space present experiments simulated robot arm interacting object show exploration algorithms using learned representations closely match even sometimes improve performance obtained using engineered representations\n",
            "output sentence:  propose novel intrinsically motivated goal exploration architecture unsupervised learning goal space representations evaluate various implementations enable discovery diversity \n",
            "\n",
            "{'rouge-1': {'r': 0.038461538461538464, 'p': 0.21428571428571427, 'f': 0.06521738872400767}, 'rouge-2': {'r': 0.01098901098901099, 'p': 0.07692307692307693, 'f': 0.019230767043269485}, 'rouge-l': {'r': 0.038461538461538464, 'p': 0.21428571428571427, 'f': 0.06521738872400767}}\n",
            "pair:  current trade depth computational cost makes difficult adopt deep neural networks many industrial applications especially computing power limited inspired idea deeper embeddings needed discriminate difficult samples large number samples well discriminated via much shallower embeddings study introduce concept decision gates gate modules trained decide whether sample needs projected deeper embedding early prediction made gate thus enabling computation dynamic representations different depths proposed gate modules integrated deep neural network reduces average computational cost deep neural networks maintaining modeling accuracy experimental results show leveraging proposed gate modules led speed flops reduction resnet speed sim flops reduction densenet trained cifar dataset drop accuracy\n",
            "output sentence:  paper introduces new dynamic feature representation approach provide efficient way inference deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.05434782608695652, 'p': 0.625, 'f': 0.09999999852800001}, 'rouge-2': {'r': 0.007874015748031496, 'p': 0.14285714285714285, 'f': 0.014925372144130162}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.5, 'f': 0.07999999852800002}}\n",
            "pair:  graph neural networks gnns received tremendous attention recently due power handling graph data different downstream tasks across different application domains key gnn graph convolutional filters recently various kinds filters designed however still lacks depth analysis whether exists best filter perform best graph data graph properties influence optimal choice graph filter design appropriate filter adaptive graph data paper focus addressing three questions first propose novel assessment tool evaluate effectiveness graph convolutional filters given graph using assessment tool find single filter silver bullet perform best possible graphs addition different graph structure properties influence optimal graph convolutional filter design choice based findings develop adaptive filter graph neural network afgnn simple powerful model adaptively learn task specific filter given graph leverages graph filter assessment regularization learns combine set base filters experiments synthetic real world benchmark datasets demonstrate proposed model indeed learn appropriate filter perform well graph tasks\n",
            "output sentence:  propose assessment framework analyze learn graph convolutional filter \n",
            "\n",
            "{'rouge-1': {'r': 0.030612244897959183, 'p': 0.3, 'f': 0.05555555387517152}, 'rouge-2': {'r': 0.008695652173913044, 'p': 0.1, 'f': 0.015999998528000135}, 'rouge-l': {'r': 0.02040816326530612, 'p': 0.2, 'f': 0.03703703535665302}}\n",
            "pair:  recent advances recurrent neural nets rnns shown much promise many applications natural language processing tasks sentiment analysis customer reviews recurrent neural net model parses entire review forming decision argue reading entire input always necessary practice since lot reviews often easy classify decision formed reading crucial sentences words provided text paper present approach fast reading text classification inspired several well known human reading techniques approach implements intelligent recurrent agent evaluates importance current snippet order decide whether make prediction skip texts read part sentence agent uses rnn module encode information past current tokens applies policy module form decisions end end training algorithm based policy gradient train test agent several text classification datasets achieve higher efficiency better accuracy compared previous approaches\n",
            "output sentence:  develop end end trainable approach skimming rereading early stopping applicable classification tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.0707070707070707, 'p': 0.7777777777777778, 'f': 0.12962962810185186}, 'rouge-2': {'r': 0.015748031496062992, 'p': 0.2222222222222222, 'f': 0.029411763469939498}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.6666666666666666, 'f': 0.11111110958333334}}\n",
            "pair:  consider problem weakly supervised structured prediction sp reinforcement learning rl example given database table question perform sequence computation actions table generates response receives binary success failure reward line research successful leveraging rl directly optimizes desired metrics sp tasks example accuracy question answering bleu score machine translation however different common rl settings environment dynamics deterministic sp fully utilized model freerl methods usually applied since sp models usually full access environment dynamics propose apply model based rl methods rely planning primary model component demonstrate effectiveness planning based sp neural program planner npp given set candidate programs pretrained search policy decides program promising considering information generated executing programs evaluate npp weakly supervised program synthesis natural language semantic parsing stacked learning planning module based pretrained search policies wikitablequestions benchmark npp achieves new state art accuracy\n",
            "output sentence:  model based planning component improves rl based semantic parsing wikitablequestions \n",
            "\n",
            "{'rouge-1': {'r': 0.08, 'p': 0.75, 'f': 0.14457831151110467}, 'rouge-2': {'r': 0.03296703296703297, 'p': 0.42857142857142855, 'f': 0.061224488469387786}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.625, 'f': 0.12048192596893599}}\n",
            "pair:  present deep generative model named monge amp ere flow builds continuous time gradient flow arising monge amp ere equation optimal transport theory generative map latent space data space follows dynamical system learnable potential function guides compressible fluid flow towards target density distribution training model amounts solving optimal control problem monge amp ere flow tractable likelihoods supports efficient sampling inference one easily impose symmetry constraints generative model designing suitable scalar potential functions apply approach unsupervised density estimation mnist dataset variational calculation two dimensional ising model critical point approach brings insights techniques monge amp ere equation optimal transport fluid dynamics reversible flow based generative models\n",
            "output sentence:  gradient flow based dynamical system invertible generative modeling \n",
            "\n",
            "{'rouge-1': {'r': 0.08064516129032258, 'p': 0.8333333333333334, 'f': 0.14705882192041522}, 'rouge-2': {'r': 0.04054054054054054, 'p': 0.6, 'f': 0.07594936590290019}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.6666666666666666, 'f': 0.11764705721453288}}\n",
            "pair:  recent work exhibited surprising cross lingual abilities multilingual bert bert surprising since trained without cross lingual objective aligned data work provide comprehensive study contribution different components bert cross lingual ability study impact linguistic properties languages architecture model learning objectives experimental study done context three typologically different languages spanish hindi russian using two conceptually different nlp tasks textual entailment named entity recognition among key conclusions fact lexical overlap languages plays negligible role cross lingual success depth network important part\n",
            "output sentence:  cross lingual ability multilingual bert empirical study \n",
            "\n",
            "{'rouge-1': {'r': 0.07608695652173914, 'p': 0.7, 'f': 0.13725490019223377}, 'rouge-2': {'r': 0.031496062992125984, 'p': 0.4444444444444444, 'f': 0.05882352817582182}, 'rouge-l': {'r': 0.05434782608695652, 'p': 0.5, 'f': 0.09803921391772397}}\n",
            "pair:  backpropagation bp algorithm often thought biologically implausible brain one main reasons bp requires symmetric weight matrices feedforward feedback pathways address weight transport problem grossberg two biologically plausible algorithms proposed liao et al lillicrap et al relax bp weight symmetry requirements demonstrate comparable learning capabilities bp small datasets however recent study bartunov et al finds although feedback alignment fa variants target propagation tp perform well mnist cifar perform significantly worse bp imagenet additionally evaluate sign symmetry ss algorithm liao et al differs bp fa feedback feedforward weights share magnitudes share signs examined performance sign symmetry feedback alignment imagenet ms coco datasets using different network architectures resnet alexnet imagenet retinanet ms coco surprisingly networks trained sign symmetry attain classification performance approaching bp trained networks results complement study bartunov et al establish new benchmark future biologically plausible learning algorithms difficult datasets complex architectures\n",
            "output sentence:  biologically plausible learning algorithms particularly sign symmetry work well imagenet \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.8571428571428571, 'f': 0.23529411527873895}, 'rouge-2': {'r': 0.0784313725490196, 'p': 0.6666666666666666, 'f': 0.14035087530932594}, 'rouge-l': {'r': 0.11363636363636363, 'p': 0.7142857142857143, 'f': 0.19607842900422914}}\n",
            "pair:  two main families reinforcement learning algorithms learning policy gradients recently proven equivalent using softmax relaxation one part entropic regularization relate result well known convex duality shannon entropy softmax function result also known donsker varadhan formula provides short proof equivalence interpret duality use ideas convex analysis prove new policy inequality relative soft learning\n",
            "output sentence:  short proof equivalence soft learning policy gradients \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.5, 'f': 0.11428571226122453}, 'rouge-2': {'r': 0.014492753623188406, 'p': 0.14285714285714285, 'f': 0.026315787801246642}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.375, 'f': 0.08571428368979596}}\n",
            "pair:  intuitively image classification profit using spatial information recent work however suggests might overrated standard cnns paper pushing envelope aim investigate reliance necessity spatial information propose analyze three methods namely shuffle conv gap fc conv destroy spatial information training testing phases extensively evaluate methods several object recognition datasets cifar small imagenet imagenet wide range cnn architectures vgg resnet resnet mobilenet squeezenet interestingly consistently observe spatial information completely deleted significant number layers small performance drops\n",
            "output sentence:  spatial information last layers necessary good classification accuracy \n",
            "\n",
            "{'rouge-1': {'r': 0.17073170731707318, 'p': 0.8235294117647058, 'f': 0.28282827998367516}, 'rouge-2': {'r': 0.10476190476190476, 'p': 0.6111111111111112, 'f': 0.17886178611937345}, 'rouge-l': {'r': 0.17073170731707318, 'p': 0.8235294117647058, 'f': 0.28282827998367516}}\n",
            "pair:  effective performance neural networks depends critically effective tuning optimization hyperparameters especially learning rates schedules thereof present amortized proximal optimization apo takes perspective optimization step approximately minimize proximal objective similar ones used motivate natural gradient trust region policy optimization optimization hyperparameters adapted best minimize proximal objective one weight update show idealized version apo oracle minimizes proximal objective exactly achieves global convergence stationary point locally second order convergence global optimum neural networks apo incurs minimal computational overhead experiment using apo adapt variety optimization hyperparameters online training including possibly layer specific learning rates damping coefficients gradient variance exponents variety network architectures optimization algorithms including sgd rmsprop fac show minimal tuning apo performs competitively carefully tuned optimizers\n",
            "output sentence:  introduce amortized proximal optimization apo method adapt variety optimization hyperparameters online training including learning rates damping gradient gradient variance \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.75, 'f': 0.11538461396449705}, 'rouge-2': {'r': 0.022900763358778626, 'p': 0.375, 'f': 0.043165466541069335}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}}\n",
            "pair:  unpaired image image translation among category domains achieved remarkable success past decades recent studies mainly focus two challenges one thing translation inherently multimodal due variations domain specific information domain house cat multiple fine grained subcategories another existing multimodal approaches limitations handling two domains independently build one model every pair domains address problems propose hierarchical image image translation hit method jointly formulates multimodal multi domain problem semantic hierarchy structure control uncertainty multimodal specifically regard domain specific variations result multi granularity property domains one control granularity multimodal translation dividing domain large variations multiple subdomains capture local fine grained variations assumption gaussian prior variations domains modeled common space translations done among multiple domains within one model learn complicated space propose leverage inclusion relation among domains constrain distributions parent children nested experiments several datasets validate promising results competitive performance state arts\n",
            "output sentence:  granularity controled multi domain multimodal image image translation method \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.7692307692307693, 'f': 0.24390243635633554}, 'rouge-2': {'r': 0.0449438202247191, 'p': 0.3333333333333333, 'f': 0.0792079186981669}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.6153846153846154, 'f': 0.19512194855145748}}\n",
            "pair:  graphs fundamental data structures required model many important real world data knowledge graphs physical social interactions molecules proteins paper study problem learning generative models graphs dataset graphs interest learning models used generate samples similar properties ones dataset models useful lot applications drug discovery knowledge graph construction task learning generative models graphs however unique challenges particular handle symmetries graphs ordering elements generation process important issues propose generic graph neural net based model capable generating arbitrary graph study performance graph generation tasks compared baselines exploit domain knowledge discuss potential issues open problems generative models going forward\n",
            "output sentence:  study graph generation problem propose powerful deep generative model capable generating arbitrary graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.1891891891891892, 'p': 0.5, 'f': 0.27450979993848523}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.23076923076923078, 'f': 0.10344827238406672}, 'rouge-l': {'r': 0.13513513513513514, 'p': 0.35714285714285715, 'f': 0.19607842738946568}}\n",
            "pair:  saliency maps often used suggest explanations behavior deep rein forcement learning rl agents however explanations derived saliency maps often unfalsifiable highly subjective introduce empirical approach grounded counterfactual reasoning test hypotheses generated saliency maps show explanations suggested saliency maps often supported experiments experiments suggest saliency maps best viewed exploratory tool rather explanatory tool\n",
            "output sentence:  proposing new counterfactual based methodology evaluate hypotheses generated saliency maps deep rl agent behavior \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.75, 'f': 0.13483145903800026}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.037037037037037035, 'p': 0.375, 'f': 0.06741572870092163}}\n",
            "pair:  separating mixed distributions long standing challenge machine learning signal processing applications include single channel multi speaker separation cocktail party problem singing voice separation separating reflections images current methods either rely making strong assumptions source distributions sparsity low rank repetitiveness rely training samples source mixture work tackle scenario extracting unobserved distribution additively mixed signal observed arbitrary distribution introduce new method neural egg separation iterative method learns separate known distribution progressively finer estimates unknown distribution settings neural egg separation initialization sensitive therefore introduce glo masking ensures good initialization extensive experiments show method outperforms current methods use level supervision often achieves similar performance full supervision\n",
            "output sentence:  iterative neural method extracting signals observed mixed signals \n",
            "\n",
            "{'rouge-1': {'r': 0.12643678160919541, 'p': 0.6111111111111112, 'f': 0.20952380668299328}, 'rouge-2': {'r': 0.0625, 'p': 0.3888888888888889, 'f': 0.10769230530650893}, 'rouge-l': {'r': 0.12643678160919541, 'p': 0.6111111111111112, 'f': 0.20952380668299328}}\n",
            "pair:  goal multi label learning mll associate given instance relevant labels set concepts previous works mll mainly focused setting concept set assumed fixed many real world applications require introducing new concepts set meet new demands one common need refine original coarse concepts split finer grained ones refinement process typically begins limited labeled data finer grained concepts address need propose special weakly supervised mll problem focuses situation limited fine grained supervision also leverages hierarchical relationship coarse concepts fine grained ones problem reduced multi label version negative unlabeled learning problem using hierarchical relationship tackle reduced problem meta learning approach learns assign pseudo labels unlabeled entries experimental results demonstrate proposed method able assign accurate pseudo labels turn achieves superior classification performance compared existing methods\n",
            "output sentence:  propose special weakly supervised multi label learning problem along newly tailored algorithm learns underlying classifier learning assign pseudo labels \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.8181818181818182, 'f': 0.20224718884484283}, 'rouge-2': {'r': 0.04854368932038835, 'p': 0.5, 'f': 0.08849557360795679}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.6363636363636364, 'f': 0.15730336862012376}}\n",
            "pair:  widely recognized adversarial examples easily crafted fool deep networks mainly root locally non linear behavior nearby input examples applying mixup training provides effective mechanism improve generalization performance model robustness adversarial perturbations introduces globally linear behavior training examples however previous work mixup trained models passively defend adversarial attacks inference directly classifying inputs induced global linearity well exploited namely since locality adversarial perturbations would efficient actively break locality via globality model predictions inspired simple geometric intuition develop inference principle named mixup inference mi mixup trained models mi mixups input random clean samples shrink transfer equivalent perturbation input adversarial experiments cifar cifar demonstrate mi improve adversarial robustness models trained mixup variants\n",
            "output sentence:  exploit global linearity mixup trained models inference break locality adversarial perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.2222222222222222, 'p': 0.9411764705882353, 'f': 0.35955055870723396}, 'rouge-2': {'r': 0.1590909090909091, 'p': 0.8235294117647058, 'f': 0.2666666639528345}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.9411764705882353, 'f': 0.35955055870723396}}\n",
            "pair:  propose warped residual network warpnet using parallelizable warp operator forward backward propagation distant layers trains faster original residual neural network apply perturbation theory residual networks decouple interactions residual units resulting warp operator first order approximation output multiple layers first order perturbation theory exhibits properties binomial path lengths exponential gradient scaling found experimentally veit et al demonstrate extensive performance study proposed network achieves comparable predictive performance original residual network number parameters achieving significant speed total training time warpnet performs model parallelism residual network training weights distributed different gpus offers speed capability train larger networks compared original residual networks\n",
            "output sentence:  propose warped residual network using parallelizable warp operator forward backward propagation distant layers trains faster residual neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.08235294117647059, 'p': 0.5833333333333334, 'f': 0.14432989473907962}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.18181818181818182, 'f': 0.036363634563636456}, 'rouge-l': {'r': 0.047058823529411764, 'p': 0.3333333333333333, 'f': 0.08247422463598689}}\n",
            "pair:  sequence generation models recurrent networks trained diverse set learning algorithms example maximum likelihood learning simple efficient yet suffers exposure bias problem reinforcement learning like policy gradient addresses problem prohibitively poor exploration efficiency variety algorithms raml spg data noising also developed different perspectives paper establishes formal connection algorithms present generalized entropy regularized policy optimization formulation show apparently divergent algorithms reformulated special instances framework difference configurations reward function couple hyperparameters unified interpretation offers systematic view varying properties exploration learning efficiency besides based framework present new algorithm dynamically interpolates among existing algorithms improved learning experiments machine translation text summarization demonstrate superiority proposed algorithm\n",
            "output sentence:  unified perspective various learning algorithms sequence generation mle rl raml data noising etc \n",
            "\n",
            "{'rouge-1': {'r': 0.2361111111111111, 'p': 0.85, 'f': 0.3695652139886579}, 'rouge-2': {'r': 0.1348314606741573, 'p': 0.6, 'f': 0.22018348324215137}, 'rouge-l': {'r': 0.20833333333333334, 'p': 0.75, 'f': 0.32608695311909264}}\n",
            "pair:  present new method black box adversarial attack unlike previous methods combined transfer based scored based methods using gradient initialization surrogate white box model new method tries learn low dimensional embedding using pretrained model performs efficient search within embedding space attack unknown target network method produces adversarial perturbations high level semantic patterns easily transferable show approach greatly improve query efficiency black box adversarial attack across different target network architectures evaluate approach mnist imagenet google cloud vision api resulting significant reduction number queries also attack adversarially defended networks cifar imagenet method reduces number queries also improves attack success rate\n",
            "output sentence:  present new method combines transfer based scored black box adversarial attack improving success rate query efficiency black box adversarial attack across different network architectures \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.6666666666666666, 'f': 0.19672130895995704}, 'rouge-2': {'r': 0.03389830508474576, 'p': 0.25, 'f': 0.05970149043439526}, 'rouge-l': {'r': 0.09615384615384616, 'p': 0.5555555555555556, 'f': 0.1639344237140554}}\n",
            "pair:  propose approach sequence modeling based autoregressive normalizing flows autoregressive transform acting across time serves moving reference frame modeling higher level dynamics technique provides simple general purpose method improving sequence modeling connections existing classical techniques demonstrate proposed approach standalone models well part larger sequential latent variable models results presented three benchmark video datasets flow based dynamics improve log likelihood performance baseline models\n",
            "output sentence:  show autoregressive flows used improve sequential latent variable models \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.6666666666666666, 'f': 0.11764705721453288}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.2, 'f': 0.02564102444115719}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.5, 'f': 0.08823529250865055}}\n",
            "pair:  state art sequence sequence models large scale tasks perform fixed number computations input sequence regardless whether easy hard process paper train transformer models make output predictions different stages network investigate different ways predict much computation required particular sequence unlike dynamic computation universal transformers applies set layers iteratively apply different layers every step adjust amount computation well model capacity iwslt german english translation approach matches accuracy well tuned baseline transformer using less quarter decoder layers\n",
            "output sentence:  sequence model dynamically adjusts amount computation input \n",
            "\n",
            "{'rouge-1': {'r': 0.12121212121212122, 'p': 0.5333333333333333, 'f': 0.19753086117969826}, 'rouge-2': {'r': 0.0273972602739726, 'p': 0.14285714285714285, 'f': 0.0459770087937642}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.26666666666666666, 'f': 0.09876542908093287}}\n",
            "pair:  propose tackle time series regression problem computing temporal evolution probability density function provide probabilistic forecast recurrent neural network rnn based model employed learn nonlinear operator temporal evolution probability density function use softmax layer numerical discretization smooth probability density functions transforms function approximation problem classification task explicit implicit regularization strategies introduced impose smoothness condition estimated probability distribution monte carlo procedure compute temporal evolution distribution multiple step forecast presented evaluation proposed algorithm three synthetic two real data sets shows advantage compared baselines\n",
            "output sentence:  proposed rnn based algorithm estimate predictive distribution one multi step forecasts time series prediction problems \n",
            "\n",
            "{'rouge-1': {'r': 0.05813953488372093, 'p': 0.4166666666666667, 'f': 0.10204081417742612}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.15384615384615385, 'f': 0.029629627889163346}, 'rouge-l': {'r': 0.05813953488372093, 'p': 0.4166666666666667, 'f': 0.10204081417742612}}\n",
            "pair:  meta learning algorithms learn acquire new tasks quickly past experience context reinforcement learning meta learning algorithms acquire reinforcement learning procedures solve new problems efficiently utilizing experience prior tasks performance meta learning algorithms depends tasks available meta training way supervised learning generalizes best test points drawn distribution training points meta learning methods generalize best tasks distribution meta training tasks effect meta reinforcement learning offloads design burden algorithm design task design automate process task design well devise meta learning algorithm truly automated work take step direction proposing family unsupervised meta learning algorithms reinforcement learning motivate describe general recipe unsupervised meta reinforcement learning present instantiation approach conceptual theoretical contributions consist formulating unsupervised meta reinforcement learning problem describing task proposals based mutual information principle used train optimal meta learners experimental results indicate unsupervised meta reinforcement learning effectively acquires accelerated reinforcement learning procedures without need manual task design significantly exceeds performance learning scratch\n",
            "output sentence:  meta learning self proposed task distributions speed reinforcement learning without human specified task distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.15476190476190477, 'p': 0.6190476190476191, 'f': 0.24761904441904764}, 'rouge-2': {'r': 0.05357142857142857, 'p': 0.2857142857142857, 'f': 0.09022556125049473}, 'rouge-l': {'r': 0.10714285714285714, 'p': 0.42857142857142855, 'f': 0.17142856822857147}}\n",
            "pair:  model free reinforcement learning rl proven powerful general tool learning complex behaviors however sample efficiency often impractically large solving challenging real world problems even policy algorithms learning limiting factor classic model free rl learning signal consists scalar rewards ignoring much rich information contained state transition tuples model based rl uses information training predictive model often achieve asymptotic performance model free rl due model bias introduce temporal difference models tdms family goal conditioned value functions trained model free learning used model based control tdms combine benefits model free model based rl leverage rich information state transitions learn efficiently still attaining asymptotic performance exceeds direct model based rl methods experimental results show range continuous control tasks tdms provide substantial improvement efficiency compared state art model based model free methods\n",
            "output sentence:  show special goal condition value function trained model free methods used within model based control resulting substantially better sample efficiency performance performance \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 0.9444444444444444, 'f': 0.33009708449429737}, 'rouge-2': {'r': 0.15151515151515152, 'p': 0.8823529411764706, 'f': 0.25862068715368614}, 'rouge-l': {'r': 0.16470588235294117, 'p': 0.7777777777777778, 'f': 0.27184465730983126}}\n",
            "pair:  many practical robot locomotion tasks require agents use control policies parameterized goals popular deep reinforcement learning approaches direction involve learning goal conditioned policies value functions inverse dynamics models idms idms map agent current state desired goal required actions show key achieving good performance idms lies learning information shared equivalent experiences generalized unseen scenarios design training process guides learning latent representations encode shared information using limited number environment interactions agent able efficiently navigate arbitrary points goal space demonstrate effectiveness approach high dimensional locomotion environments mujoco ant pybullet humanoid pybullet minitaur provide quantitative qualitative results show method clearly outperforms competing baseline approaches\n",
            "output sentence:  show key achieving good performance idms lies learning latent representations encode information shared equivalent experiences generalized unseen scenarios \n",
            "\n",
            "{'rouge-1': {'r': 0.11320754716981132, 'p': 0.46153846153846156, 'f': 0.18181817865472916}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.25, 'f': 0.07692307431952672}, 'rouge-l': {'r': 0.11320754716981132, 'p': 0.46153846153846156, 'f': 0.18181817865472916}}\n",
            "pair:  autonomy adaptation machines requires able measure errors consider advantages limitations approach machine measure error regression task machine measure error regression sub components ground truth correct predictions compressed sensing approach applied error signal regressors recover precision error without ground truth allows regressors strongly correlated long many related solutions however unique property ground truth inference solutions adding ell minimization condition recover correct solution settings error correction possible briefly discuss similarity mathematics ground truth inference regressors classifiers\n",
            "output sentence:  non parametric method measure error moments regressors without ground truth used biased regressors \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.3125, 'f': 0.14084506693116455}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07272727272727272, 'p': 0.25, 'f': 0.1126760528466575}}\n",
            "pair:  solving tasks reinforcement learning easy feat goal agent maximize accumulated reward often learns exploit loopholes misspecifications reward signal resulting unwanted behavior constraints may solve issue closed form solution general constraints work present novel multi timescale approach constrained policy optimization called reward constrained policy optimization rcpo uses alternative penalty signal guide policy towards constraint satisfying one prove convergence approach provide empirical evidence ability train constraint satisfying policies\n",
            "output sentence:  complex constraints easy estimate gradient use discounted penalty guiding signal prove certain assumptions converges feasible solution \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.16666666666666666, 'f': 0.03960395830212735}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.46153846153846156, 'f': 0.14117646799723185}}\n",
            "pair:  present meta learning approach adaptive text speech tts data training learn multi speaker model using shared conditional wavenet core independent learned embeddings speaker aim training produce neural network fixed weights deployed tts system instead aim produce network requires data deployment time rapidly adapt new speakers introduce benchmark three strategies learning speaker embedding keeping wavenet core fixed ii fine tuning entire architecture stochastic gradient descent iii predicting speaker embedding trained neural network encoder experiments show approaches successful adapting multi speaker neural network new speakers obtaining state art results sample naturalness voice similarity merely minutes audio data new speakers\n",
            "output sentence:  sample efficient algorithms adapt text speech model new voice style state art performance \n",
            "\n",
            "{'rouge-1': {'r': 0.07575757575757576, 'p': 0.45454545454545453, 'f': 0.1298701274211503}, 'rouge-2': {'r': 0.02666666666666667, 'p': 0.2, 'f': 0.04705882145328729}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.36363636363636365, 'f': 0.10389610144712437}}\n",
            "pair:  unintended consequence feature sharing model fitting correlated tasks within dataset termed negative transfer paper revisit problem negative transfer multitask setting find corrosive effects applicable wide range linear non linear models including neural networks first study effects negative transfer principled way show previously proposed counter measures insufficient particularly trainable features propose adversarial training approach mitigate effects negative transfer viewing problem domain adaptation setting finally empirical results attribute prediction multi task awa cub datasets validate need correcting negative sharing end end manner\n",
            "output sentence:  look negative transfer domain adaptation point view derive adversarial learning algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.12631578947368421, 'p': 0.75, 'f': 0.21621621374888406}, 'rouge-2': {'r': 0.05309734513274336, 'p': 0.4, 'f': 0.09374999793090825}, 'rouge-l': {'r': 0.11578947368421053, 'p': 0.6875, 'f': 0.198198195730866}}\n",
            "pair:  recent pretrained transformer based language models set state art performances various nlp datasets however despite great progress suffer various structural syntactic biases work investigate lexical overlap bias model classifies two sentences high lexical overlap entailing regardless underlying meaning improve robustness enrich input sentences training data automatically detected predicate argument structures enhanced representation allows transformer based models learn different attention patterns focusing recognizing major semantically syntactically important parts sentences evaluate solution tasks natural language inference grounded commonsense inference using bert roberta xlnet models evaluate models understanding syntactic variations antonym relations named entities presence lexical overlap results show incorporation predicate argument structures fine tuning considerably improves robustness pp discriminating different named entities incurs additional cost test time require changing model training procedure\n",
            "output sentence:  enhancing robustness pretrained transformer models lexical overlap bias extending input sentences training data corresponding predicate structures \n",
            "\n",
            "{'rouge-1': {'r': 0.21568627450980393, 'p': 0.9166666666666666, 'f': 0.349206346122449}, 'rouge-2': {'r': 0.14285714285714285, 'p': 0.9090909090909091, 'f': 0.2469135778997104}, 'rouge-l': {'r': 0.21568627450980393, 'p': 0.9166666666666666, 'f': 0.349206346122449}}\n",
            "pair:  propose rapp new methodology novelty detection utilizing hidden space activation values obtained deep autoencoder precisely rapp compares input autoencoder reconstruction input space also hidden spaces show feed reconstructed input autoencoder activated values hidden space equivalent corresponding reconstruction hidden space given original input order aggregate hidden space activation values propose two metrics enhance novelty detection performance extensive experiments using diverse datasets validate rapp improves novelty detection performances autoencoder based approaches besides show rapp outperforms recent novelty detection methods evaluated popular benchmarks\n",
            "output sentence:  new methodology novelty detection utilizing hidden space activation values obtained deep autoencoder \n",
            "\n",
            "{'rouge-1': {'r': 0.08823529411764706, 'p': 0.75, 'f': 0.15789473495844877}, 'rouge-2': {'r': 0.016666666666666666, 'p': 0.18181818181818182, 'f': 0.030534349606666355}, 'rouge-l': {'r': 0.06862745098039216, 'p': 0.5833333333333334, 'f': 0.12280701566020316}}\n",
            "pair:  reinforcement learning continues drive machine intelligence beyond conventional boundary unsubstantial practices sparse reward environment severely limit applications broader range advanced fields motivated demand effective deep reinforcement learning algorithm accommodates sparse reward environment paper presents hindsight trust region policy optimization htrpo method efficiently utilizes interactions sparse reward conditions optimize policies within trust region meantime maintains learning stability firstly theoretically adapt trpo objective function form expected return policy distribution hindsight data generated alternative goals apply monte carlo importance sampling estimate kl divergence two policies taking hindsight data input condition distributions sufficiently close kl divergence approximated another divergence approximation results decrease variance alleviates instability policy update experimental results discrete continuous benchmark tasks demonstrate htrpo converges significantly faster previous policy gradient methods achieves effective performances high data efficiency training policies sparse reward environments\n",
            "output sentence:  paper proposes advanced policy optimization method hindsight experience sparse reward reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.08641975308641975, 'p': 0.7, 'f': 0.1538461518898684}, 'rouge-2': {'r': 0.03260869565217391, 'p': 0.3333333333333333, 'f': 0.0594059389706892}, 'rouge-l': {'r': 0.08641975308641975, 'p': 0.7, 'f': 0.1538461518898684}}\n",
            "pair:  explore use vector quantized variational autoencoder vq vae models large scale image generation end scale enhance autoregressive priors used vq vae generate synthetic samples much higher coherence fidelity possible use simple feed forward encoder decoder networks thus model attractive candidate applications encoding decoding speed critical additionally allows us sample autoregressively compressed latent space order magnitude faster sampling pixel space especially large images demonstrate multi scale hierarchical organization vq vae augmented powerful priors latent codes able generate samples quality rivals state art generative adversarial networks multifaceted datasets imagenet suffering gan known shortcomings mode collapse lack diversity\n",
            "output sentence:  scale enhance vq vae powerful priors generate near realistic images \n",
            "\n",
            "{'rouge-1': {'r': 0.10679611650485436, 'p': 0.6470588235294118, 'f': 0.18333333090138892}, 'rouge-2': {'r': 0.03731343283582089, 'p': 0.2631578947368421, 'f': 0.06535947494895132}, 'rouge-l': {'r': 0.08737864077669903, 'p': 0.5294117647058824, 'f': 0.1499999975680556}}\n",
            "pair:  like humans deep networks learn better samples organized introduced meaningful order curriculum conventional approaches curriculum learning emphasize difficulty samples core incremental strategy forces networks learn small subsets data introducing pre computation overheads work propose learning incremental labels adaptive compensation lilac introduces novel approach curriculum learning lilac emphasizes incrementally learning labels instead incrementally learning difficult samples works two distinct phases first incremental label introduction phase unmask ground truth labels fixed increments training improve starting point networks learn adaptive compensation phase compensate failed predictions adaptively altering target vector smoother distribution evaluate lilac closest comparable methods batch curriculum learning label smoothing across three standard image benchmarks cifar cifar stl show method outperforms batch learning higher mean recognition accuracy well lower standard deviation performance consistently across benchmarks extend lilac state art performance across cifar using simple data augmentation exhibiting label order invariance among important properties\n",
            "output sentence:  novel approach curriculum learning incrementally learning labels adaptively smoothing labels mis classified samples boost average performance decreases decreases deviation deviation \n",
            "\n",
            "{'rouge-1': {'r': 0.0392156862745098, 'p': 0.25, 'f': 0.06779660782533764}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0196078431372549, 'p': 0.125, 'f': 0.03389830274059195}}\n",
            "pair:  propose end end trainable attention module convolutional neural network cnn architectures built image classification module takes input feature vector maps form intermediate representations input image different stages cnn pipeline outputs matrix scores map standard cnn architectures modified incorporation module trained constraint convex combination intermediate feature vectors parametrised score matrices must alone used classification incentivised amplify relevant suppress irrelevant misleading scores thus assume role attention values experimental observations provide clear evidence effect learned attention maps neatly highlight regions interest suppressing background clutter consequently proposed function able bootstrap standard cnn architectures task image classification demonstrating superior generalisation unseen benchmark datasets binarised attention maps outperform cnn based attention maps traditional saliency maps top object proposals weakly supervised segmentation demonstrated object discovery dataset also demonstrate improved robustness fast gradient sign method adversarial attack\n",
            "output sentence:  paper proposes method forcing cnns leverage spatial attention learning object centric representations perform better various respects \n",
            "\n",
            "{'rouge-1': {'r': 0.14814814814814814, 'p': 0.4444444444444444, 'f': 0.22222221847222226}, 'rouge-2': {'r': 0.06451612903225806, 'p': 0.25, 'f': 0.10256409930309017}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.3333333333333333, 'f': 0.16666666291666676}}\n",
            "pair:  classic papers zellner demonstrated bayesian inference could derived solution information theoretic functional derive generalized form functional variational lower bound predictive information bottleneck objective generalized functional encompasses modern inference procedures suggests novel ones\n",
            "output sentence:  rederive wide class inference procedures global information bottleneck objective \n",
            "\n",
            "{'rouge-1': {'r': 0.0891089108910891, 'p': 0.47368421052631576, 'f': 0.14999999733472225}, 'rouge-2': {'r': 0.017391304347826087, 'p': 0.1111111111111111, 'f': 0.030075185629487435}, 'rouge-l': {'r': 0.039603960396039604, 'p': 0.21052631578947367, 'f': 0.066666664001389}}\n",
            "pair:  analysis histopathology slides critical step many diagnoses particular oncology defines gold standard case digital histopathological analysis highly trained pathologists must review vast whole slide images extreme digital resolution pixels across multiple zoom levels order locate abnormal regions cells cases single cells millions application deep learning problem hampered small sample sizes typical datasets contain hundred samples also generation ground truth localized annotations training interpretable classification segmentation models propose method disease available training even without pixel level annotations able demonstrate performance comparable models trained strong annotations camelyon lymph node metastases detection challenge accomplish use pre trained deep convolutional networks feature embedding well learning via top instances negative evidence multiple instance learning technique fromatp field semantic segmentation object detection\n",
            "output sentence:  propose weakly supervised learning method classification localization cancers extremely high resolution histopathology whole slide images using image wide labels \n",
            "\n",
            "{'rouge-1': {'r': 0.11650485436893204, 'p': 0.6666666666666666, 'f': 0.19834710490540264}, 'rouge-2': {'r': 0.015267175572519083, 'p': 0.10526315789473684, 'f': 0.02666666445422241}, 'rouge-l': {'r': 0.05825242718446602, 'p': 0.3333333333333333, 'f': 0.09917355118639443}}\n",
            "pair:  combining deep model free reinforcement learning line planning promising approach building successes deep rl line planning look ahead trees proven successful environments transition models known priori however complex environments transition models need learned data deficiencies learned models limited utility planning address challenges propose treeqn differentiable recursive tree structured model serves drop replacement value function network deep rl discrete actions treeqn dynamically constructs tree recursively applying transition model learned abstract state space aggregating predicted rewards state values using tree backup estimate values also propose atreec actor critic variant augments treeqn softmax layer form stochastic policy network approaches trained end end learned model optimised actual use tree show treeqn atreec outperform step dqn box pushing task well step dqn value prediction networks oh et al multiple atari games furthermore present ablation studies demonstrate effect different auxiliary losses learning transition models\n",
            "output sentence:  present treeqn atreec new architectures deep reinforcement learning discrete action domains integrate integrate differentiable line tree planning action value function \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.6, 'f': 0.1499999978125}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.1111111111111111, 'f': 0.019417474133283194}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.5, 'f': 0.12499999781250003}}\n",
            "pair:  deep learning incredibly successful modeling tasks large carefully curated labeled datasets application problems limited labeled data remains challenge aim present work improve label efficiency large neural networks operating audio data combination multitask learning self supervised learning unlabeled data trained end end audio feature extractor based wavenet feeds simple yet versatile task specific neural networks describe several easily implemented self supervised learning tasks operate large unlabeled audio corpus demonstrate scenarios limited labeled training data one significantly improve performance three different supervised classification tasks individually simultaneous training additional self supervised tasks also show incorporating data augmentation multitask setting leads even gains performance\n",
            "output sentence:  label efficient audio classification via multi task learning self supervision \n",
            "\n",
            "{'rouge-1': {'r': 0.08064516129032258, 'p': 0.5555555555555556, 'f': 0.14084506820868875}, 'rouge-2': {'r': 0.013333333333333334, 'p': 0.1, 'f': 0.023529409688581502}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.3333333333333333, 'f': 0.08450704003967473}}\n",
            "pair:  describe simple general neural network weight compression approach network parameters weights biases represented latent space amounting reparameterization space equipped learned probability model used impose entropy penalty parameter representation training compress representation using simple arithmetic coder training classification accuracy model compressibility maximized jointly bitrate accuracy trade specified hyperparameter evaluate method mnist cifar imagenet classification benchmarks using six distinct model architectures results show state art model compression achieved scalable general way without requiring complex procedures multi stage training\n",
            "output sentence:  end end trainable model compression method optimizing accuracy jointly expected model size \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.75, 'f': 0.1935483848491155}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.42857142857142855, 'f': 0.08219177908800905}, 'rouge-l': {'r': 0.09259259259259259, 'p': 0.625, 'f': 0.1612903203329865}}\n",
            "pair:  community detection graphs central importance graph mining machine learning network science detecting overlapping communities especially challenging remains open problem motivated success graph based deep learning graph related tasks study applicability framework overlapping community detection propose probabilistic model overlapping community detection based graph neural network architecture despite simplicity model outperforms existing approaches community recovery task large margin moreover due inductive formulation proposed model able perform sample community detection nodes present training time\n",
            "output sentence:  detecting overlapping communities graphs using graph neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
            "pair:  random matrix theory rmt applied analyze weight matrices deep neural networks dnns including production quality pre trained models alexnet inception smaller models trained scratch lenet miniature alexnet empirical theoretical results clearly indicate empirical spectral density esd dnn layer matrices displays signatures traditionally regularized statistical models even absence exogenously specifying traditional forms regularization dropout weight norm constraints building recent results rmt notably extension universality classes heavy tailed matrices develop theory identify phases training corresponding increasing amounts implicit self regularization smaller older dnns implicit self regularization like traditional tikhonov regularization size scale separating signal noise state art dnns however identify novel form heavy tailed self regularization similar self organization seen statistical physics disordered systems implicit self regularization depend strongly many knobs training process exploiting generalization gap phenomena demonstrate cause small model exhibit phases training simply changing batch size\n",
            "output sentence:  see abstract revision paper identical except page supplementary material serve stand along technical report version paper \n",
            "\n",
            "{'rouge-1': {'r': 0.0963855421686747, 'p': 0.6153846153846154, 'f': 0.16666666432508684}, 'rouge-2': {'r': 0.01834862385321101, 'p': 0.16666666666666666, 'f': 0.033057849452906324}, 'rouge-l': {'r': 0.08433734939759036, 'p': 0.5384615384615384, 'f': 0.14583333099175347}}\n",
            "pair:  model distillation aims distill knowledge complex model simpler one paper consider alternative formulation called dataset distillation keep model fixed instead attempt distill knowledge large training dataset small one idea synthesize small number data points need come correct data distribution given learning algorithm training data approximate model trained original data example show possible compress mnist training images synthetic distilled images one per class achieve close original performance given fixed network initialization evaluate method various initialization settings experiments multiple datasets mnist cifar pascal voc cub demonstrate ad vantage approach compared alternative methods finally include real world application dataset distillation continual learning setting show storing distilled images episodic memory previous tasks alleviate forgetting effectively real images\n",
            "output sentence:  propose distill large dataset small set synthetic data train networks close original performance \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.7777777777777778, 'f': 0.1647058804595156}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.25, 'f': 0.04123711188861734}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.4444444444444444, 'f': 0.09411764516539796}}\n",
            "pair:  carbon footprint natural language processing nlp research increasing recent years due reliance large inefficient neural network implementations distillation network compression technique attempts impart knowledge large model smaller one use teacher student distillation improve efficiency biaffine dependency parser obtains state art performance respect accuracy parsing speed dozat manning distilling original model trainable parameters observe average decrease point uas las across number diverse universal dependency treebanks faster baseline model cpu gpu inference time also observe small increase performance compressing treebanks finally distillation attain parser faster also accurate fastest modern parser penn treebank\n",
            "output sentence:  increase efficiency neural network dependency parsers teacher student distillation \n",
            "\n",
            "{'rouge-1': {'r': 0.05555555555555555, 'p': 0.4444444444444444, 'f': 0.09876543012345682}, 'rouge-2': {'r': 0.011111111111111112, 'p': 0.125, 'f': 0.020408161765930976}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.4444444444444444, 'f': 0.09876543012345682}}\n",
            "pair:  recent work explanation generation decision making problems viewed explanation process one model reconciliation ai agent brings human mental model capabilities beliefs goals page regards task hand formulation succinctly captures many possible types explanations well explicitly addresses various properties social aspects contrastiveness selectiveness explanations studied social sciences among human human interactions however turns process hijacked producing alternative explanations explanations true still satisfy properties proper explanation previous work looked explanations may perceived human loop alluded one possible way generating paper go details curious feature model reconciliation process discuss similar implications overall notion explainable decision making\n",
            "output sentence:  model reconciliation established framework plan explanations easily hijacked produce lies \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.6666666666666666, 'f': 0.159999997888}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.3333333333333333, 'f': 0.06557376871808658}, 'rouge-l': {'r': 0.07954545454545454, 'p': 0.5833333333333334, 'f': 0.139999997888}}\n",
            "pair:  consider two questions heart machine learning predict minimum generalize test set stochastic gradient descent find minima generalize well work responds citet zhang understanding showed deep neural networks easily memorize randomly labeled training data despite generalizing well real labels inputs show phenomenon occurs small linear models observations explained bayesian evidence penalizes sharp minima invariant model parameterization also demonstrate one holds learning rate fixed optimum batch size maximizes test set accuracy propose noise introduced small mini batches drives parameters towards minima whose evidence large interpreting stochastic gradient descent stochastic differential equation identify noise scale epsilon frac approx epsilon epsilon learning rate training set size batch size consequently optimum batch size proportional learning rate size training set opt propto epsilon verify predictions empirically\n",
            "output sentence:  generalization strongly correlated bayesian evidence gradient noise drives sgd towards minima whose evidence large \n",
            "\n",
            "{'rouge-1': {'r': 0.12987012987012986, 'p': 0.7142857142857143, 'f': 0.2197802171766695}, 'rouge-2': {'r': 0.06382978723404255, 'p': 0.42857142857142855, 'f': 0.11111110885459538}, 'rouge-l': {'r': 0.1038961038961039, 'p': 0.5714285714285714, 'f': 0.17582417322062555}}\n",
            "pair:  plethora computer vision tasks optical flow image alignment formulated non linear optimization problems resurgence deep learning dominant family solving optimization problems numerical optimization gauss newton gn recently several attempts made formulate learnable gn steps cascade regression architectures paper investigate recent machine learning architectures deep neural networks residual connections perspective end first demonstrate residual blocks considered discretization odes viewed gn steps go step propose new residual block reminiscent newton method numerical optimization exhibits faster convergence thoroughly evaluate proposed newton resnet conducting experiments image speech classification image generation using datasets experiments demonstrate newton resnet requires less parameters achieve performance original resnet\n",
            "output sentence:  demonstrate residual blocks viewed gauss newton steps propose new residual block exploits second order information \n",
            "\n",
            "{'rouge-1': {'r': 0.13793103448275862, 'p': 0.6666666666666666, 'f': 0.2285714257306123}, 'rouge-2': {'r': 0.09836065573770492, 'p': 0.5454545454545454, 'f': 0.16666666407793213}, 'rouge-l': {'r': 0.1206896551724138, 'p': 0.5833333333333334, 'f': 0.1999999971591837}}\n",
            "pair:  study problem fitting task specific learning rate schedules perspective hyperparameter optimization allows us explicitly search schedules achieve good generalization describe structure gradient validation error learning rates hypergradient based introduce novel online algorithm method adaptively interpolates two recently proposed techniques franceschi et al baydin et al featuring increased stability faster convergence show empirically proposed technique compares favorably baselines related methodsin terms final test accuracy\n",
            "output sentence:  marthe new method fit task specific learning rate schedules perspective hyperparameter optimization \n",
            "\n",
            "{'rouge-1': {'r': 0.2641509433962264, 'p': 0.875, 'f': 0.40579709788699864}, 'rouge-2': {'r': 0.17333333333333334, 'p': 0.8125, 'f': 0.285714282816085}, 'rouge-l': {'r': 0.2641509433962264, 'p': 0.875, 'f': 0.40579709788699864}}\n",
            "pair:  board games often rely visual information location game pieces textual information cards due reliance visual feedback blind players disadvantage cannot read cards see location game pieces may unable play game without sighted help present game changer augmented workspace provides audio descriptions tactile additions make state board game accessible blind visually impaired players paper describe design game changer present findings user study blind participants used game changer play sighted partner players stated game accessible additions game changer felt game changer could used augment games\n",
            "output sentence:  game changer system provides audio descriptions tactile additions make state board game accessible blind visually impaired players \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.8333333333333334, 'f': 0.13513513364499638}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.8, 'f': 0.08421052531855956}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.8333333333333334, 'f': 0.13513513364499638}}\n",
            "pair:  scarcity labeled training data often prohibits internationalization nlp models multiple languages cross lingual understanding made progress area using language universal representations however current approaches focus problem one aligning language address natural domain drift across languages cultures paper address domain gap setting semi supervised cross lingual document classification labeled data available source language unlabeled data available target language combine state art unsupervised learning method masked language modeling pre training recent method semi supervised learning unsupervised data augmentation uda simultaneously close language domain gap show addressing domain gap cross lingual tasks crucial improve strong baselines achieve new state art cross lingual document classification\n",
            "output sentence:  semi supervised cross lingual document classification \n",
            "\n",
            "{'rouge-1': {'r': 0.1518987341772152, 'p': 0.8571428571428571, 'f': 0.2580645135715112}, 'rouge-2': {'r': 0.10309278350515463, 'p': 0.6666666666666666, 'f': 0.1785714262515944}, 'rouge-l': {'r': 0.1518987341772152, 'p': 0.8571428571428571, 'f': 0.2580645135715112}}\n",
            "pair:  paper proposes asal new pool based active learning method generates high entropy samples instead directly annotating synthetic samples asal searches similar samples pool includes training hence quality new samples high annotations reliable asal particularly suitable large data sets achieves better run time complexity sub linear sample selection traditional uncertainty sampling linear present comprehensive set experiments two data sets show asal outperforms similar methods clearly exceeds established baseline random sampling discussion section analyze situations asal performs best sometimes hard outperform random sample selection best knowledge first adversarial active learning technique applied multiple class problems using deep convolutional classifiers demonstrates superior performance random sample selection\n",
            "output sentence:  asal pool based active learning method generates high entropy samples retrieves matching samples pool sub linear time \n",
            "\n",
            "{'rouge-1': {'r': 0.1323529411764706, 'p': 0.5294117647058824, 'f': 0.211764702682353}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.125, 'f': 0.0392156836293735}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.29411764705882354, 'f': 0.1176470556235295}}\n",
            "pair:  training agents operate one environment often yields overfitted models unable generalize changes environment however due numerous variations occur real world agent often required robust order useful case agents trained reinforcement learning rl algorithms paper investigate overfitting rl agents training environments visual navigation tasks experiments show deep rl agents overfit even trained multiple environments simultaneously propose regularization method combines rl supervised learning methods adding term rl objective would encourage invariance policy variations observations ought affect action taken results method called invariance regularization show improvement generalization policies environments seen training\n",
            "output sentence:  propose regularization term added reinforcement learning objective allows policy maximize reward simultaneously learn invariant irrelevant changes within input \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.75, 'f': 0.17142856940408163}, 'rouge-2': {'r': 0.024, 'p': 0.2727272727272727, 'f': 0.04411764557201562}, 'rouge-l': {'r': 0.0967741935483871, 'p': 0.75, 'f': 0.17142856940408163}}\n",
            "pair:  based observation exists dramatic drop singular values fully connected layers single feature map convolutional layer dimension concatenated feature vector almost equals summation dimension feature map propose singular value decomposition svd based approach estimate dimension deep manifolds typical convolutional neural network vgg choose three categories imagenet namely persian cat container ship volcano determine local dimension deep manifolds deep layers tangent space target image several augmentation methods found gaussian noise method closer intrinsic dimension adding random noise image moving arbitrary dimension rank feature matrix augmented images increase close local dimension manifold also estimate dimension deep manifold based tangent space maxpooling layers results show dimensions different categories close decline quickly along convolutional layers fully connected layers furthermore show dimensions decline quickly inside conv layer work provides new insights intrinsic structure deep neural networks helps unveiling inner organization black box deep neural networks\n",
            "output sentence:  propose svd based method explore local dimension activation manifold deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.14864864864864866, 'p': 0.6111111111111112, 'f': 0.2391304316351607}, 'rouge-2': {'r': 0.047058823529411764, 'p': 0.2222222222222222, 'f': 0.07766990002827799}, 'rouge-l': {'r': 0.10810810810810811, 'p': 0.4444444444444444, 'f': 0.1739130403308129}}\n",
            "pair:  recent advances generative adversarial networks facilitated improvements framework successful application various problems resulted extensions multiple domains irgan attempts leverage framework information retrieval ir task described modeling correct conditional probability distribution documents given query work proposes irgan claims optimizing minimax loss function result generator learn distribution setup baseline term steer model away exact adversarial formulation work attempts point certain inaccuracies formulation analyzing loss curves gives insight possible mistakes loss functions better performance obtained using co training like setup propose two models trained co operative rather adversarial fashion\n",
            "output sentence:  points problems loss function used irgan recently proposed gan framework information retrieval model motivated co training proposed better performance \n",
            "\n",
            "{'rouge-1': {'r': 0.06593406593406594, 'p': 0.6666666666666666, 'f': 0.11999999836200001}, 'rouge-2': {'r': 0.023622047244094488, 'p': 0.375, 'f': 0.04444444332949248}, 'rouge-l': {'r': 0.054945054945054944, 'p': 0.5555555555555556, 'f': 0.09999999836200002}}\n",
            "pair:  spiking neural networks investigated biologically plausible models neural computation also potentially efficient type neural network convolutional spiking neural networks demonstrated achieve near state art performance one solution proposed convert gated recurrent neural networks far recurrent neural networks form networks gating memory cells central state art solutions problem domains involve sequence recognition generation design analog gated lstm cell neurons substituted efficient stochastic spiking neurons adaptive spiking neurons implement adaptive form sigma delta coding convert internally computed analog activation values spike trains neurons approximate effective activation function resembles sigmoid show analog neurons activation functions used create analog lstm cell networks cells trained standard backpropagation train lstm networks noisy noiseless version original sequence prediction task hochreiter schmidhuber also noisy noiseless version classical working memory reinforcement learning task maze substituting analog neurons corresponding adaptive spiking neurons show almost resulting spiking neural network equivalents correctly compute original tasks\n",
            "output sentence:  demonstrate gated recurrent asynchronous spiking neural network corresponds lstm unit \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.5555555555555556, 'f': 0.11235954874384549}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.125, 'f': 0.01999999852800011}, 'rouge-l': {'r': 0.05, 'p': 0.4444444444444444, 'f': 0.08988763863148597}}\n",
            "pair:  point clouds form lagrangian representation allow powerful flexible applications large number computational disciplines propose novel deep learning method learn stable temporally coherent feature spaces points clouds change time identify set inherent problems approaches without knowledge time dimension inferred solutions exhibit strong flickering easy solutions suppress flickering result undesirable local minima manifest halo structures propose novel temporal loss function takes account higher time derivatives point positions encourages mingling prevent aforementioned halos combine techniques super resolution method truncation approach flexibly adapt size generated positions show method works large deforming point sets different sources demonstrate flexibility approach\n",
            "output sentence:  propose generative neural network approach temporally coherent point clouds \n",
            "\n",
            "{'rouge-1': {'r': 0.2413793103448276, 'p': 0.875, 'f': 0.37837837498904314}, 'rouge-2': {'r': 0.11688311688311688, 'p': 0.5625, 'f': 0.19354838424788995}, 'rouge-l': {'r': 0.1896551724137931, 'p': 0.6875, 'f': 0.29729729390796206}}\n",
            "pair:  bilingual student learns solve word problems math expect student able solve problem languages student fluent even math lessons taught one language however current representations machine learning language dependent work present method decouple language problem learning language agnostic representations therefore allowing training model one language applying different one zero shot fashion learn representations taking inspiration linguistics specifically universal grammar hypothesis learn universal latent representations language agnostic chomsky montague demonstrate capabilities representations showing models trained single language using language agnostic representations achieve similar accuracies languages\n",
            "output sentence:  taking inspiration linguistics specifically universal grammar hypothesis learn language agnostic universal representations utilize zero shot learning across \n",
            "\n",
            "{'rouge-1': {'r': 0.08928571428571429, 'p': 0.5, 'f': 0.15151514894398535}, 'rouge-2': {'r': 0.015873015873015872, 'p': 0.1111111111111111, 'f': 0.027777775590277953}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.4, 'f': 0.12121211864095506}}\n",
            "pair:  great progress made making neural networks effective across wide range tasks many surprisingly vulnerable small carefully chosen perturbations input known adversarial examples paper advocate experimentally investigate use logit regularization techniques adversarial defense used conjunction methods creating adversarial robustness little cost demonstrate much effectiveness one recent adversarial defense mechanism attributed logit regularization show improve defense white box black box attacks process creating stronger black box attacks pgd based models\n",
            "output sentence:  logit regularization methods help explain improve state art adversarial defenses \n",
            "\n",
            "{'rouge-1': {'r': 0.11224489795918367, 'p': 0.6111111111111112, 'f': 0.1896551697919144}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.3333333333333333, 'f': 0.09090908855371907}, 'rouge-l': {'r': 0.10204081632653061, 'p': 0.5555555555555556, 'f': 0.1724137904815696}}\n",
            "pair:  problem building coherent non monotonous conversational agent proper discourse coverage still area open research current architectures take care semantic contextual information given query fail completely account syntactic external knowledge crucial generating responses chit chat system overcome problem propose end end multi stream deep learning architecture learns unified embeddings query response pairs leveraging contextual information memory networks syntactic information incorporating graph convolution networks gcn dependency parse stream network also utilizes transfer learning pre training bidirectional transformer extract semantic representation input sentence incorporates external knowledge neighbourhood entities knowledge base kb benchmark embeddings next sentence prediction task significantly improve upon existing techniques furthermore use amused represent query responses along context develop retrieval based conversational agent validated expert linguists comprehensive engagement humans\n",
            "output sentence:  paper provides multi stream end end approach learn unified embeddings query response pairs dialogue systems leveraging syntactic semantic semantic \n",
            "\n",
            "{'rouge-1': {'r': 0.10714285714285714, 'p': 0.5, 'f': 0.17647058532871976}, 'rouge-2': {'r': 0.03, 'p': 0.17647058823529413, 'f': 0.05128204879830533}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.3333333333333333, 'f': 0.11764705591695508}}\n",
            "pair:  checkerboard phenomenon one well known visual artifacts computer vision field origins solutions checkerboard artifacts pixel space studied long time effects gradient space rarely investigated paper revisit checkerboard artifacts gradient space turn weak point network architecture explore image agnostic property gradient checkerboard artifacts propose simple yet effective defense method utilizing artifacts introduce defense module dubbed artificial checkerboard enhancer ace induces adversarial attacks designated pixels enables model deflect attacks shifting single pixel image remarkable defense rate provide extensive experiments support effectiveness work various attack scenarios using state art attack methods furthermore show ace even applicable large scale datasets including imagenet dataset easily transferred various pretrained networks\n",
            "output sentence:  propose novel aritificial checkerboard enhancer ace module guides attacks pre specified pixel space successfully defends simple padding operation \n",
            "\n",
            "{'rouge-1': {'r': 0.1643835616438356, 'p': 0.9230769230769231, 'f': 0.2790697648756085}, 'rouge-2': {'r': 0.11702127659574468, 'p': 0.9166666666666666, 'f': 0.20754716780348884}, 'rouge-l': {'r': 0.1643835616438356, 'p': 0.9230769230769231, 'f': 0.2790697648756085}}\n",
            "pair:  work study generalization neural networks gradient based meta learning analyzing various properties objective landscapes experimentally demonstrate meta training progresses meta test solutions obtained adapting meta train solution model new tasks via steps gradient based fine tuning become flatter lower loss away meta train solution also show meta test solutions become flatter even generalization starts degrade thus providing experimental evidence correlation generalization flat minima paradigm gradient based meta leaning furthermore provide empirical evidence generalization new tasks correlated coherence adaptation trajectories parameter space measured average cosine similarity task specific trajectory directions starting meta train solution also show coherence meta test gradients measured average inner product task specific gradient vectors evaluated meta train solution also correlated generalization\n",
            "output sentence:  study generalization neural networks gradient based meta learning analyzing various properties objective landscape \n",
            "\n",
            "{'rouge-1': {'r': 0.10377358490566038, 'p': 0.5789473684210527, 'f': 0.17599999742208006}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.2222222222222222, 'f': 0.056338025955167716}, 'rouge-l': {'r': 0.09433962264150944, 'p': 0.5263157894736842, 'f': 0.15999999742208004}}\n",
            "pair:  community detection graphs solved via spectral methods posterior inference certain probabilistic graphical models focusing random graph families stochastic block model recent research unified approaches identified statistical computational detection thresholds terms signal noise ratio recasting community detection node wise classification problem graphs also study learning perspective present novel family graph neural networks gnns solving community detection problems supervised learning setting show data driven manner without access underlying generative models match even surpass performance belief propagation algorithm binary multiclass stochastic block models believed reach computational threshold cases particular propose augment gnns non backtracking operator defined line graph edge adjacencies gnns achieved good performance real world datasets addition perform first analysis optimization landscape using linear gnns solve community detection problems demonstrating certain simplifications assumptions loss value local minimum close loss value global minimum minima\n",
            "output sentence:  propose novel graph neural network architecture based non backtracking matrix defined edge adjacencies demonstrate effectiveness community detection tasks graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.07547169811320754, 'p': 0.3333333333333333, 'f': 0.12307692006627227}, 'rouge-2': {'r': 0.015873015873015872, 'p': 0.09090909090909091, 'f': 0.027027024495982706}, 'rouge-l': {'r': 0.07547169811320754, 'p': 0.3333333333333333, 'f': 0.12307692006627227}}\n",
            "pair:  inference models replace optimization based inference procedure learned model fundamental advancing bayesian deep learning notable example variational auto encoders vaes paper propose iterative inference models learn optimize variational lower bound repeatedly encoding gradients approach generalizes vaes certain conditions viewing vaes context iterative inference provide insight several recent empirical findings demonstrate inference optimization capabilities iterative inference models explore unique aspects models show outperform standard inference models typical benchmark data sets\n",
            "output sentence:  propose new class inference models iteratively encode gradients estimate approximate posterior distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.09900990099009901, 'p': 0.625, 'f': 0.17094016857915117}, 'rouge-2': {'r': 0.02586206896551724, 'p': 0.1875, 'f': 0.0454545433241507}, 'rouge-l': {'r': 0.06930693069306931, 'p': 0.4375, 'f': 0.1196581172970999}}\n",
            "pair:  modeling hypernymy poodle dog important generalization aid many nlp tasks entailment relation extraction question answering supervised learning labeled hypernym sources wordnet limit coverage models addressed learning hypernyms unlabeled text existing unsupervised methods either scale large vocabularies yield unacceptably poor accuracy paper introduces distributional inclusion vector embedding dive simple implement unsupervised method hypernym discovery via per word non negative vector embeddings preserve inclusion property word contexts experimental evaluations comprehensive previous literature aware evaluating datasets using multiple existing well newly proposed scoring functions find method provides double precision previous unsupervised methods highest average performance using much compact word representation yielding many new state art results addition meaning dimension dive interpretable leads novel approach word sense disambiguation another promising application dive\n",
            "output sentence:  propose novel unsupervised word embedding preserves inclusion property context distribution achieve state art results unsupervised hypernymy detection \n",
            "\n",
            "{'rouge-1': {'r': 0.05714285714285714, 'p': 0.4444444444444444, 'f': 0.10126582076590292}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}, 'rouge-l': {'r': 0.05714285714285714, 'p': 0.4444444444444444, 'f': 0.10126582076590292}}\n",
            "pair:  understanding flow information deep neural networks dnns challenging problem gain increasing attention last years several methods proposed explain network predictions attempts compare theoretical perspective exhaustive empirical comparison performed past work analyze four gradient based attribution methods formally prove conditions equivalence approximation reformulating two methods construct unified framework enables direct comparison well easier implementation finally propose novel evaluation metric called sensitivity test gradient based attribution methods alongside simple perturbation based attribution method several datasets domains image text classification using various network architectures\n",
            "output sentence:  four existing backpropagation based attribution methods fundamentally similar assess \n",
            "\n",
            "{'rouge-1': {'r': 0.09859154929577464, 'p': 0.7, 'f': 0.1728395040085353}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.2222222222222222, 'f': 0.04081632486255733}, 'rouge-l': {'r': 0.056338028169014086, 'p': 0.4, 'f': 0.09876542993446126}}\n",
            "pair:  using modern deep learning models make predictions time series data wearable sensors generally requires large amounts labeled data however labeling large datasets cumbersome costly paper apply weak supervision time series data programmatically label dataset sensors worn patients parkinson built lstm model predicts patients exhibit clinically relevant freezing behavior inability make effective forward stepping show model trained using patient specific data prior sensor sessions come within auroc model trained using hand labeled data assume prior observations subjects weakly supervised model matched performance hand labeled data results demonstrate weak supervision may help reduce need painstakingly hand label time series training data\n",
            "output sentence:  demonstrate feasibility weakly supervised time series classification approach wearable sensor data \n",
            "\n",
            "{'rouge-1': {'r': 0.08450704225352113, 'p': 0.6666666666666666, 'f': 0.149999998003125}, 'rouge-2': {'r': 0.0125, 'p': 0.1111111111111111, 'f': 0.02247190829440742}, 'rouge-l': {'r': 0.04225352112676056, 'p': 0.3333333333333333, 'f': 0.07499999800312505}}\n",
            "pair:  given large database concepts one examples learn models concept generalisable interpretable work aim tackle problem hierarchical bayesian program induction present novel learning algorithm infer concepts short generative stochastic programs learning global prior programs improve generalisation recognition network efficient inference algorithm wake sleep remember wsr combines gradient learning continuous parameters neurally guided search programs show wsr learns compelling latent programs two tough symbolic domains cellular automata gaussian process kernels also collect evaluate new dataset text concepts discovering structured patterns natural text data\n",
            "output sentence:  extend wake sleep algorithm use learn learn structured models examples \n",
            "\n",
            "{'rouge-1': {'r': 0.1038961038961039, 'p': 0.5333333333333333, 'f': 0.17391304074905484}, 'rouge-2': {'r': 0.050505050505050504, 'p': 0.3125, 'f': 0.08695651934366737}, 'rouge-l': {'r': 0.1038961038961039, 'p': 0.5333333333333333, 'f': 0.17391304074905484}}\n",
            "pair:  central goal unsupervised learning acquire representations unlabeled data experience used effective learning downstream tasks modest amounts labeled data many prior unsupervised learning works aim developing proxy objectives based reconstruction disentanglement prediction metrics instead develop unsupervised meta learning method explicitly optimizes ability learn variety tasks small amounts data construct tasks unlabeled data automatic way run meta learning constructed tasks surprisingly find integrated meta learning relatively simple task construction mechanisms clustering embeddings lead good performance variety downstream human specified tasks experiments across four image datasets indicate unsupervised meta learning approach acquires learning algorithm without labeled data applicable wide range downstream classification tasks improving upon embedding learned four prior unsupervised learning methods\n",
            "output sentence:  unsupervised learning method uses meta learning enable efficient learning downstream image classification tasks outperforming state art methods \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.5294117647058824, 'f': 0.21686746662215128}, 'rouge-2': {'r': 0.03896103896103896, 'p': 0.17647058823529413, 'f': 0.06382978427116356}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.35294117647058826, 'f': 0.14457830999564533}}\n",
            "pair:  whereas believed techniques adam batch normalization recently selu nonlinearities solve exploding gradient problem show case range popular mlp architectures exploding gradients exist limit depth networks effectively trained theory practice explain exploding gradients occur highlight collapsing domain problem arise architectures avoid exploding gradients resnets significantly lower gradients thus circumvent exploding gradient problem enabling effective training much deeper networks show consequence surprising mathematical property noticing neural network residual network devise residual trick reveals introducing skip connections simplifies network mathematically simplicity may major cause success\n",
            "output sentence:  show contras popular wisdom exploding gradient problem solved limits depth mlps effectively trained show gradients explode resnet handles \n",
            "\n",
            "{'rouge-1': {'r': 0.07575757575757576, 'p': 0.3333333333333333, 'f': 0.12345678710562423}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.2, 'f': 0.07407407105624156}}\n",
            "pair:  one substitute neuron neural network kernel machine obtain counterpart powered kernel machines new network inherits expressive power architecture original works intuitive way since node enjoys simple interpretation hyperplane reproducing kernel hilbert space using kernel multilayer perceptron example prove classification optimal representation minimizes risk network characterized hidden layer result removes need backpropagation learning model generalized feedforward kernel network moreover unlike backpropagation turns models black boxes optimal hidden representation enjoys intuitive geometric interpretation making dynamics learning deep kernel network simple understand empirical results provided validate theory\n",
            "output sentence:  combine kernel method connectionist models show resulting deep architectures trained layer wise transparent learning dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.08256880733944955, 'p': 0.6, 'f': 0.14516128819588972}, 'rouge-2': {'r': 0.032520325203252036, 'p': 0.26666666666666666, 'f': 0.05797101255513554}, 'rouge-l': {'r': 0.045871559633027525, 'p': 0.3333333333333333, 'f': 0.0806451591636317}}\n",
            "pair:  one main challenges applying graph convolutional neural networks gene interaction data lack understanding vector space belong also inherent difficulties involved representing interactions significantly lower dimension viz euclidean spaces challenge becomes prevalent dealing various types heterogeneous data introduce systematic generalized method called isom gsn used transform multi omic data higher dimensions onto two dimensional grid afterwards apply convolutional neural network predict disease states various types based idea kohonen self organizing map generate two dimensional grid sample given set genes represent gene similarity network tested model predict breast prostate cancer using gene expression dna methylation copy number alteration yielding prediction accuracies range tumor stages breast cancer calculated gleason scores prostate cancer input genes cases scheme outputs nearly perfect classification accuracy also provides enhanced scheme representation learning visualization dimensionality reduction interpretation results\n",
            "output sentence:  paper presents deep learning model combines self organizing maps convolutional neural networks representation learning multi omics data \n",
            "\n",
            "{'rouge-1': {'r': 0.09411764705882353, 'p': 0.8888888888888888, 'f': 0.17021276422589407}, 'rouge-2': {'r': 0.06862745098039216, 'p': 0.875, 'f': 0.12727272592396696}, 'rouge-l': {'r': 0.09411764705882353, 'p': 0.8888888888888888, 'f': 0.17021276422589407}}\n",
            "pair:  paper formalises problem online algorithm selection context reinforcement learning rl setup follows given episodic task finite number policy rl algorithms meta algorithm decide rl algorithm control next episode maximize expected return article presents novel meta algorithm called epochal stochastic bandit algorithm selection esbas principle freeze policy updates epoch leave rebooted stochastic bandit charge algorithm selection assumptions thorough theoretical analysis demonstrates near optimality considering structural sampling budget limitations esbas first empirically evaluated dialogue task shown outperform individual algorithm configurations esbas adapted true online setting algorithms update policies transition call ssbas ssbas evaluated fruit collection task shown adapt stepsize parameter efficiently classical hyperbolic decay atari game improves performance wide margin\n",
            "output sentence:  paper formalises problem online algorithm selection context reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.05172413793103448, 'p': 0.42857142857142855, 'f': 0.09230769038579885}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.034482758620689655, 'p': 0.2857142857142857, 'f': 0.06153845961656811}}\n",
            "pair:  deep learning approaches usually require large amount labeled data generalize however humans learn new concept samples one high cogntition human capablities learn several concepts time paper address task classifying multiple objects seeing samples category best authors knowledge dataset specially designed shot multiclass classification design task mutli object class classification environment easy creating controllable datasets task demonstrate proposed dataset sound using method extension prototypical networks\n",
            "output sentence:  introduce diagnostic task variation shot learning introduce dataset \n",
            "\n",
            "{'rouge-1': {'r': 0.25, 'p': 0.7857142857142857, 'f': 0.3793103411652795}, 'rouge-2': {'r': 0.16071428571428573, 'p': 0.6923076923076923, 'f': 0.2608695621592103}, 'rouge-l': {'r': 0.25, 'p': 0.7857142857142857, 'f': 0.3793103411652795}}\n",
            "pair:  propose non adversarial feature matching based approach train generative models approach generative feature matching networks gfmn leverages pretrained neural networks autoencoders convnet classifiers perform feature extraction perform extensive number experiments different challenging datasets including imagenet experimental results demonstrate due expressiveness features pretrained imagenet classifiers even matching first order statistics approach achieve state art results challenging benchmarks cifar stl\n",
            "output sentence:  new non adversarial feature matching based approach train generative models achieves state art results \n",
            "\n",
            "{'rouge-1': {'r': 0.04819277108433735, 'p': 0.6666666666666666, 'f': 0.08988763919202121}, 'rouge-2': {'r': 0.01, 'p': 0.2, 'f': 0.01904761814058961}, 'rouge-l': {'r': 0.03614457831325301, 'p': 0.5, 'f': 0.06741572907966167}}\n",
            "pair:  recent studies highlighted adversarial examples ubiquitous threat different neural network models many downstream applications nonetheless unique data properties inspired distinct powerful learning principles paper aims explore potentials towards mitigating adversarial inputs particular results reveal importance using temporal dependency audio data gain discriminate power adversarial examples tested automatic speech recognition asr tasks three recent audio adversarial attacks find input transformation developed image adversarial defense provides limited robustness improvement subtle advanced attacks ii temporal dependency exploited gain discriminative power audio adversarial examples resistant adaptive attacks considered experiments results show promising means improving robustness asr systems also offer novel insights exploiting domain specific data properties mitigate negative effects adversarial examples\n",
            "output sentence:  adversarial audio discrimination using temporal dependency \n",
            "\n",
            "{'rouge-1': {'r': 0.1590909090909091, 'p': 0.6363636363636364, 'f': 0.2545454513454546}, 'rouge-2': {'r': 0.078125, 'p': 0.45454545454545453, 'f': 0.13333333083022222}, 'rouge-l': {'r': 0.1590909090909091, 'p': 0.6363636363636364, 'f': 0.2545454513454546}}\n",
            "pair:  introduce quantum graph neural networks qgnn new class quantum neural network ansatze tailored represent quantum processes graph structure particularly suitable executed distributed quantum systems quantum network along general class ansatze introduce specialized architectures namely quantum graph recurrent neural networks qgrnn quantum graph convolutional neural networks qgcnn provide four example applications qgnn learning hamiltonian dynamics quantum systems learning create multipartite entanglement quantum network unsupervised learning spectral clustering supervised learning graph isomorphism classification\n",
            "output sentence:  introducing new class quantum neural networks learning graph based representations quantum computers \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.8, 'f': 0.2857142827806122}, 'rouge-2': {'r': 0.06172839506172839, 'p': 0.35714285714285715, 'f': 0.1052631553817175}, 'rouge-l': {'r': 0.13043478260869565, 'p': 0.6, 'f': 0.21428571135204083}}\n",
            "pair:  argued current machine learning models commonsense therefore must hard coded prior knowledge marcus show surprising evidence language models already learn capture certain common sense knowledge key observation language model compute probability statement probability used evaluate truthfulness statement winograd schema challenge levesque et al language models higher accuracy previous state art supervised methods language models also fine tuned task mining commonsense knowledge conceptnet achieve score outperforming previous best results jastrzebskiet al analysis demonstrates language models discover unique features winograd schema contexts decide correct answers without explicit supervision\n",
            "output sentence:  present evidence lms capture common sense state art results winograd schema challenge commonsense knowledge mining \n",
            "\n",
            "{'rouge-1': {'r': 0.05084745762711865, 'p': 0.6, 'f': 0.09374999855957034}, 'rouge-2': {'r': 0.014492753623188406, 'p': 0.25, 'f': 0.027397259238131022}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.6, 'f': 0.09374999855957034}}\n",
            "pair:  propose method called label embedding network learn label representation label embedding training process deep networks proposed method label embedding adaptively automatically learned back propagation original one hot represented loss function converted new loss function soft distributions originally unrelated labels continuous interactions training process result trained model achieve substantially higher accuracy faster convergence speed experimental results based competitive tasks demonstrate effectiveness proposed method learned label embedding reasonable interpretable proposed method achieves comparable even better results state art systems\n",
            "output sentence:  learning label representation deep networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.5, 'f': 0.13698629900544196}, 'rouge-2': {'r': 0.028985507246376812, 'p': 0.2222222222222222, 'f': 0.05128204924063125}, 'rouge-l': {'r': 0.06349206349206349, 'p': 0.4, 'f': 0.10958903873146937}}\n",
            "pair:  large memory requirements deep neural networks strain capabilities many devices limiting deployment adoption model compression methods effectively reduce memory requirements models usually applying transformations weight pruning quantization paper present novel scheme lossy weight encoding complements conventional compression techniques encoding based bloomier filter probabilistic data structure save space cost introducing random errors leveraging ability neural networks tolerate imperfections training around errors proposed technique weightless compress dnn weights model accuracy results improvement state art\n",
            "output sentence:  propose new way compress neural networks using probabilistic data structures \n",
            "\n",
            "{'rouge-1': {'r': 0.14634146341463414, 'p': 0.7058823529411765, 'f': 0.24242423957963474}, 'rouge-2': {'r': 0.0196078431372549, 'p': 0.11764705882352941, 'f': 0.033613442929171844}, 'rouge-l': {'r': 0.13414634146341464, 'p': 0.6470588235294118, 'f': 0.22222221937761455}}\n",
            "pair:  large pre trained transformers bert tremendously effective many nlp tasks however inference large capacity models prohibitively slow expensive transformers essentially stack self attention layers encode input position using entire input sequence context however find may necessary apply expensive sequence wide self attention layers based observation propose decomposition pre trained transformer allows lower layers process segments input independently enabling parallelism caching show information loss due decomposition recovered upper layers auxiliary supervision fine tuning evaluate de composition pre trained bert models five different paired input tasks question answering sentence similarity natural language inference results show decomposition enables faster inference significant memory reduction retaining original performance release code anonymized url\n",
            "output sentence:  inference large transformers expensive due self attention multiple layers show simple decomposition technique yield faster memory memory model model \n",
            "\n",
            "{'rouge-1': {'r': 0.14754098360655737, 'p': 0.9, 'f': 0.2535211243404087}, 'rouge-2': {'r': 0.075, 'p': 0.6666666666666666, 'f': 0.13483145885620504}, 'rouge-l': {'r': 0.14754098360655737, 'p': 0.9, 'f': 0.2535211243404087}}\n",
            "pair:  give new algorithm learning two layer neural network general class input distributions assuming ground truth two layer network sigma wx xi weight matrices xi represents noise number neurons hidden layer larger input output algorithm guaranteed recover parameters ground truth network requirement input symmetric still allows highly complicated structured input algorithm based method moments framework extends several results tensor decompositions use spectral algorithms avoid complicated non convex optimization learning neural networks experiments show algorithm robustly learn ground truth neural network small number samples many symmetric input distributions\n",
            "output sentence:  give algorithm learning two layer neural network symmetric input distribution \n",
            "\n",
            "{'rouge-1': {'r': 0.14925373134328357, 'p': 0.9090909090909091, 'f': 0.2564102539875082}, 'rouge-2': {'r': 0.1, 'p': 0.8, 'f': 0.17777777580246917}, 'rouge-l': {'r': 0.13432835820895522, 'p': 0.8181818181818182, 'f': 0.23076922834648256}}\n",
            "pair:  learnability different neural architectures characterized directly computable measures data complexity paper reframe problem architecture selection understanding data determines expressive generalizable architectures suited data beyond inductive bias suggesting algebraic topology measure data complexity show power network express topological complexity dataset decision boundary strictly limiting factor ability generalize provide first empirical characterization topological capacity neural networks empirical analysis shows every level dataset complexity neural networks exhibit topological phase transitions stratification observation allowed us connect existing theory empirically driven conjectures choice architectures single hidden layer neural networks\n",
            "output sentence:  show learnability different neural architectures characterized directly computable measures data complexity \n",
            "\n",
            "{'rouge-1': {'r': 0.16304347826086957, 'p': 0.8823529411764706, 'f': 0.27522935516539015}, 'rouge-2': {'r': 0.08943089430894309, 'p': 0.6470588235294118, 'f': 0.15714285500918368}, 'rouge-l': {'r': 0.15217391304347827, 'p': 0.8235294117647058, 'f': 0.2568807313121791}}\n",
            "pair:  deep learning deep reinforcement learning systems demonstrated impressive results domains image classification game playing robotic control data efficiency remains major challenge particularly algorithms learn individual tasks scratch multi task learning emerged promising approach sharing structure across multiple tasks enable efficient learning however multi task setting presents number optimization challenges making difficult realize large efficiency gains compared learning tasks independently reasons multi task learning challenging compared single task learning fully understood motivated insight gradient interference causes optimization challenges develop simple general approach avoiding interference gradients different tasks altering gradients technique refer gradient surgery propose form gradient surgery projects gradient task onto normal plane gradient task conflicting gradient series challenging multi task supervised multi task reinforcement learning problems find approach leads substantial gains efficiency performance effectively combined previously proposed multi task architectures enhanced performance model agnostic way\n",
            "output sentence:  develop simple general approach avoiding interference gradients different tasks improves performance multi task learning supervised reinforcement learning domains \n",
            "\n",
            "{'rouge-1': {'r': 0.0641025641025641, 'p': 0.7142857142857143, 'f': 0.11764705731211073}, 'rouge-2': {'r': 0.028846153846153848, 'p': 0.5, 'f': 0.0545454535140496}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.7142857142857143, 'f': 0.11764705731211073}}\n",
            "pair:  neural embeddings used great success natural language processing nlp provide compact representations encapsulate word similarity attain state art performance range linguistic tasks success neural embeddings prompted significant amounts research applications domains language one domain graph structured data embeddings vertices learned encapsulate vertex similarity improve performance tasks including edge prediction vertex labelling nlp graph based tasks embeddings high dimensional euclidean spaces learned however recent work shown appropriate isometric space embedding complex networks flat euclidean space negatively curved hyperbolic space present new concept exploits recent insights propose learning neural embeddings graphs hyperbolic space provide experimental evidence hyperbolic embeddings significantly outperform euclidean embeddings vertex classification tasks several real world public datasets\n",
            "output sentence:  learn neural embeddings graphs hyperbolic instead euclidean space \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.75, 'f': 0.13953488203353168}, 'rouge-2': {'r': 0.02040816326530612, 'p': 0.2857142857142857, 'f': 0.03809523685079368}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.625, 'f': 0.11627906808004328}}\n",
            "pair:  cloze test widely adopted language exams evaluate students language proficiency paper propose first large scale human designed cloze test dataset cloth questions used middle school high school language exams missing blanks carefully created teachers candidate choices purposely designed confusing cloth requires deeper language understanding wider attention span previous automatically generated cloze datasets show humans outperform dedicated designed baseline models significant margin even model trained sufficiently large external data investigate source performance gap trace model deficiencies distinct properties cloth identify limited ability comprehending long term context key bottleneck addition find human designed data leads larger gap model performance human performance compared automatically generated data\n",
            "output sentence:  cloze test dataset designed teachers assess language proficiency \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.5714285714285714, 'f': 0.21621621314828346}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.19047619047619047, 'f': 0.06399999720448013}, 'rouge-l': {'r': 0.08888888888888889, 'p': 0.38095238095238093, 'f': 0.14414414107621143}}\n",
            "pair:  pruning large neural networks maintaining performance often desirable due reduced space time complexity existing methods pruning done within iterative optimization procedure either heuristically designed pruning schedules additional hyperparameters undermining utility work present new approach prunes given network initialization prior training achieve introduce saliency criterion based connection sensitivity identifies structurally important connections network given task eliminates need pretraining complex pruning schedule making robust architecture variations pruning sparse network trained standard way method obtains extremely sparse networks virtually accuracy reference network mnist cifar tiny imagenet classification tasks broadly applicable various architectures including convolutional residual recurrent networks unlike existing methods approach enables us demonstrate retained connections indeed relevant given task\n",
            "output sentence:  present new approach snip simple versatile interpretable prunes irrelevant connections given task single shot prior training applicable variety neural network network network \n",
            "\n",
            "{'rouge-1': {'r': 0.16363636363636364, 'p': 0.6923076923076923, 'f': 0.2647058792603807}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.3333333333333333, 'f': 0.1111111083333334}, 'rouge-l': {'r': 0.12727272727272726, 'p': 0.5384615384615384, 'f': 0.20588234984861592}}\n",
            "pair:  min max formulations attracted great attention ml community due rise deep generative models adversarial methods understanding dynamics stochastic gradient algorithms solving formulations grand challenge first step restrict bilinear zero sum games give systematic analysis popular gradient updates simultaneous alternating versions provide exact conditions convergence find optimal parameter setup convergence rates particular results offer formal evidence alternating updates converge better simultaneous ones\n",
            "output sentence:  systematically analyze convergence behaviour popular gradient algorithms solving bilinear games simultaneous alternating updates \n",
            "\n",
            "{'rouge-1': {'r': 0.08256880733944955, 'p': 0.42857142857142855, 'f': 0.13846153575266276}, 'rouge-2': {'r': 0.015037593984962405, 'p': 0.1, 'f': 0.026143788577043216}, 'rouge-l': {'r': 0.03669724770642202, 'p': 0.19047619047619047, 'f': 0.061538458829585925}}\n",
            "pair:  ability design biological structures dna proteins would considerable medical industrial impact presents challenging black box optimization problem characterized large batch low round setting due need labor intensive wet lab evaluations response propose using reinforcement learning rl based proximal policy optimization ppo biological sequence design rl provides flexible framework optimization generative sequence models achieve specific criteria diversity among high quality sequences discovered propose model based variant ppo dyna ppo improve sample efficiency policy new round trained offline using simulator fit functional measurements prior rounds accommodate growing number observations across rounds simulator model automatically selected round pool diverse models varying capacity tasks designing dna transcription factor binding sites designing antimicrobial proteins optimizing energy ising models based protein structure find dyna ppo performs significantly better existing methods settings modeling feasible still performing worse situations reliable model cannot learned\n",
            "output sentence:  augment model free policy learning sequence level surrogate reward functions count based visitation bonus demonstrate effectiveness large batch low regime round batch \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.6428571428571429, 'f': 0.1682242967909861}, 'rouge-2': {'r': 0.05128205128205128, 'p': 0.42857142857142855, 'f': 0.09160305152613488}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.42857142857142855, 'f': 0.11214953043584597}}\n",
            "pair:  travelling salesman problem tsp well known combinatorial optimization problem variety real life applications tackle tsp incorporating machine learning methodology leveraging variable neighborhood search strategy precisely search process considered markov decision process mdp opt local search used search within small neighborhood monte carlo tree search mcts method iterates simulation selection back propagation steps used sample number targeted actions within enlarged neighborhood new paradigm clearly distinguishes existing machine learning ml based paradigms solving tsp either uses end end ml model simply applies traditional techniques ml post optimization experiments based two public data sets show approach clearly dominates existing learning based tsp algorithms terms performance demonstrating high potential tsp importantly general framework without complicated hand crafted rules readily extended many combinatorial optimization problems\n",
            "output sentence:  paper combines monte carlo tree search opt local search variable neighborhood mode solve tsp effectively \n",
            "\n",
            "{'rouge-1': {'r': 0.05555555555555555, 'p': 0.38461538461538464, 'f': 0.09708737643510233}, 'rouge-2': {'r': 0.01904761904761905, 'p': 0.16666666666666666, 'f': 0.034188032347140136}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.38461538461538464, 'f': 0.09708737643510233}}\n",
            "pair:  one prevalent symptoms among elderly population dementia detected classifiers trained linguistic features extracted narrative transcripts however linguistic features impacted similar different fashion normal aging process aging therefore confounding factor whose effects hard machine learning classifiers isolate paper show deep neural network dnn classifiers infer ages linguistic features entanglement could lead unfairness across age groups show problem caused undesired activations structures causality diagrams could addressed fair representation learning build neural network classifiers learn low dimensional representations reflecting impacts dementia yet discarding effects age evaluate classifiers specify model agnostic score delta eo measuring classifier results disentangled age best models outperform baseline neural network classifiers disentanglement compromising accuracy little dementiabank famous people dataset respectively\n",
            "output sentence:  show age confounds cognitive impairment detection solve fair representation learning propose metrics models \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.25, 'f': 0.09523809215419511}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.25, 'f': 0.09523809215419511}}\n",
            "pair:  paper presents preliminary ideas work auto mated learning hierarchical goal networks nondeter ministic domains currently implementing ideas expressed paper\n",
            "output sentence:  learning hgns nd domains \n",
            "\n",
            "{'rouge-1': {'r': 0.12371134020618557, 'p': 0.9230769230769231, 'f': 0.21818181609752071}, 'rouge-2': {'r': 0.08108108108108109, 'p': 0.75, 'f': 0.1463414616537775}, 'rouge-l': {'r': 0.12371134020618557, 'p': 0.9230769230769231, 'f': 0.21818181609752071}}\n",
            "pair:  continuous normalizing flows cnfs emerged promising deep generative models wide range tasks thanks invertibility exact likelihood estimation however conditioning cnfs signals interest conditional image generation downstream predictive tasks inefficient due high dimensional latent code generated model needs size input data paper propose infocnf efficient conditional cnf partitions latent space class specific supervised code unsupervised code shared among classes efficient use labeled information since partitioning strategy slightly increases number function evaluations nfes infocnf also employs gating networks learn error tolerances ordinary differential equation ode solvers better speed performance show empirically infocnf improves test accuracy baseline yielding comparable likelihood scores reducing nfes cifar furthermore applying partitioning strategy infocnf time series data helps improve extrapolation performance\n",
            "output sentence:  propose infocnf efficient conditional cnf employs gating networks learn error tolerances ode solvers \n",
            "\n",
            "{'rouge-1': {'r': 0.1323529411764706, 'p': 0.6428571428571429, 'f': 0.21951219229030342}, 'rouge-2': {'r': 0.05, 'p': 0.3076923076923077, 'f': 0.08602150297144186}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.35714285714285715, 'f': 0.12195121668054736}}\n",
            "pair:  unsupervised monocular depth estimation made great progress deep learning involved training binocular stereo images considered good option data easily obtained however depth disparity prediction results show poor performance object boundaries main reason related handling occlusion areas training paper propose novel method overcome issue exploiting disparity maps property generate occlusion mask block back propagation occlusion areas image warping also design new networks flipped stereo images induce networks learn occluded boundaries shows method achieves clearer boundaries better evaluation results kitti driving dataset virtual kitti dataset\n",
            "output sentence:  paper propose mask method solves previous blurred results unsupervised monocular depth estimation caused occlusion \n",
            "\n",
            "{'rouge-1': {'r': 0.13580246913580246, 'p': 0.7857142857142857, 'f': 0.23157894485540165}, 'rouge-2': {'r': 0.08080808080808081, 'p': 0.5714285714285714, 'f': 0.14159291818309974}, 'rouge-l': {'r': 0.08641975308641975, 'p': 0.5, 'f': 0.14736841853961222}}\n",
            "pair:  reinforcement learning promising framework solving control problems use practical situations hampered fact reward functions often difficult engineer specifying goals tasks autonomous machines robots significant challenge conventionally reward functions goal states used communicate objectives people communicate objectives simply describing demonstrating build learning algorithms allow us tell machines want work investigate problem grounding language commands reward functions using inverse reinforcement learning argue language conditioned rewards transferable language conditioned policies new environments propose language conditioned reward learning lc rl grounds language commands reward function represented deep neural network demonstrate model learns rewards transfer novel tasks environments realistic high dimensional visual environments natural language commands whereas directly learning language conditioned policy leads poor performance\n",
            "output sentence:  ground language commands high dimensional visual environment learning language conditioned rewards using inverse reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.02857142857142857, 'p': 0.13333333333333333, 'f': 0.04705882062283755}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02857142857142857, 'p': 0.13333333333333333, 'f': 0.04705882062283755}}\n",
            "pair:  paper proposes method efficient training function continuous state markov decision processes mdp traces resulting policies satisfy linear temporal logic ltl property ltl modal logic express wide range time dependent logical properties including safety liveness convert ltl property limit deterministic buchi automaton synchronized product mdp constructed control policy synthesised reinforcement learning algorithm assuming prior knowledge available mdp proposed method evaluated numerical study test quality generated control policy compared conventional methods policy synthesis mdp abstraction voronoi quantizer approximate dynamic programming fitted value iteration\n",
            "output sentence:  safety becoming critical notion machine learning believe work act foundation number research directions safety aware learning algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.1506849315068493, 'p': 0.5789473684210527, 'f': 0.23913043150519853}, 'rouge-2': {'r': 0.037037037037037035, 'p': 0.15, 'f': 0.05940593741790037}, 'rouge-l': {'r': 0.1232876712328767, 'p': 0.47368421052631576, 'f': 0.1956521706356333}}\n",
            "pair:  consistently checking statistical significance experimental results first mandatory step towards reproducible science paper presents hitchhiker guide rigorous comparisons reinforcement learning algorithms introducing concepts statistical testing review relevant statistical tests compare empirically terms false positive rate statistical power function sample size number seeds effect size investigate robustness tests violations common hypotheses normal distributions distributions equal variances beside simulations compare empirical distributions obtained running soft actor critic twin delayed deep deterministic policy gradient half cheetah conclude providing guidelines code perform rigorous comparisons rl algorithm performances\n",
            "output sentence:  paper compares statistical tests rl comparisons false positive statistical power checks robustness assumptions using simulated distributions empirical td sac td provides rl \n",
            "\n",
            "{'rouge-1': {'r': 0.06741573033707865, 'p': 0.4, 'f': 0.11538461291605034}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.13333333333333333, 'f': 0.03149606090892196}, 'rouge-l': {'r': 0.056179775280898875, 'p': 0.3333333333333333, 'f': 0.09615384368528113}}\n",
            "pair:  prohibitive energy cost running high performance convolutional neural networks cnns limiting deployment resource constrained platforms including mobile wearable devices propose cnn energy aware dynamic routing called energynet achieves adaptive complexity inference based inputs leading overall reduction run time energy cost without noticeably losing even improving accuracy achieved proposing energy loss captures computational data movement costs combine accuracy oriented loss learn dynamic routing policy skipping certain layers networks optimizes hybrid loss empirical results demonstrate compared baseline cnns energynetcan trim energy cost inference cifar tiny imagenet testing sets respectively maintaining testing accuracies encouraging observe energy awareness might serve training regularization even improve prediction accuracy models achieve higher top testing accuracy baseline cifar saving energy higher top testing accuracy tiny imagenet saving energy respectively\n",
            "output sentence:  paper proposes new cnn model combines energy cost dynamic routing strategy enable adaptive energy efficient inference \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.5714285714285714, 'f': 0.21621621314828346}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.11666666666666667, 'p': 0.5, 'f': 0.18918918612125643}}\n",
            "pair:  advance node pooling operations graph neural networks gnns lagged behind feverish design new message passing techniques pooling remains important challenging endeavor design deep architectures paper propose pooling operation gnns leverages differentiable unsupervised loss based mincut optimization objective node method learns soft cluster assignment vector depends node features target inference task graph classification loss thanks mincut objective also connectivity structure graph graph pooling obtained applying matrix assignment vectors adjacency matrix node features validate effectiveness proposed pooling method variety supervised unsupervised tasks\n",
            "output sentence:  new pooling layer gnns learns pool nodes according features graph connectivity dowstream task objective \n",
            "\n",
            "{'rouge-1': {'r': 0.2608695652173913, 'p': 0.9473684210526315, 'f': 0.409090905705062}, 'rouge-2': {'r': 0.18888888888888888, 'p': 0.85, 'f': 0.3090909061157025}, 'rouge-l': {'r': 0.2608695652173913, 'p': 0.9473684210526315, 'f': 0.409090905705062}}\n",
            "pair:  generating visualizations interpretations high dimensional data common problem many fields two key approaches tackling problem clustering representation learning performant deep clustering models one hand interpretable representation learning techniques often relying latent topological structures self organizing maps hand however current methods yet successfully combine two approaches present new deep architecture probabilistic clustering varpsom extension time series data vartpsom composed varpsom modules connected lstm cells show achieve superior clustering performance compared current deep clustering methods static mnist fashion mnist data well medical time series inducing interpretable representation moreover medical time series vartpsom successfully predicts future trajectories original data space\n",
            "output sentence:  present new deep architecture varpsom extension time series data vartpsom achieve superior clustering performance compared current deep clustering methods static temporal data \n",
            "\n",
            "{'rouge-1': {'r': 0.08771929824561403, 'p': 0.35714285714285715, 'f': 0.1408450672564968}, 'rouge-2': {'r': 0.029411764705882353, 'p': 0.15384615384615385, 'f': 0.04938271335467169}, 'rouge-l': {'r': 0.08771929824561403, 'p': 0.35714285714285715, 'f': 0.1408450672564968}}\n",
            "pair:  propose new notion non linearity network layer respect input batch based proximity linear system reflected non negative rank activation matrix measure non linearity applying non negative factorization activation matrix considering batches similar samples find high non linearity deep layers indicative memorization furthermore applying approach layer layer find mechanism memorization consists distinct phases perform experiments fully connected convolutional neural networks trained several image audio datasets results demonstrate indicator memorization technique used perform early stopping\n",
            "output sentence:  use non negative rank relu activation matrices complexity measure show negatively correlates good generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.0851063829787234, 'p': 0.5, 'f': 0.14545454296859506}, 'rouge-2': {'r': 0.01694915254237288, 'p': 0.11764705882352941, 'f': 0.02962962742825805}, 'rouge-l': {'r': 0.07446808510638298, 'p': 0.4375, 'f': 0.12727272478677687}}\n",
            "pair:  implementing correct method invocation important task software developers however challenging work since structure method invocation complicated paper propose invocmap code completion tool allows developers obtain implementation multiple method invocations list method names inside code context invocmap able predict nested method invocations names appear list input method names given developers achieve analyze method invocations four levels abstraction build machine translation engine learn mapping first level third level abstraction multiple method invocations requires developers manually add local variables generated expression get final code evaluate proposed approach six popular libraries jdk android gwt joda time hibernate xstream training corpus million method invocations extracted java github projects testing corpus extracted online forums code snippets invocmap achieves accuracy rate score depending much information context provided along method names shows potential auto code completion\n",
            "output sentence:  paper proposes theory classifying method invocations different abstraction levels conducting statistical approach code completion method name method invocation \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.6923076923076923, 'f': 0.1607142836623087}, 'rouge-2': {'r': 0.025210084033613446, 'p': 0.25, 'f': 0.04580152505331863}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.46153846153846156, 'f': 0.10714285509088013}}\n",
            "pair:  architecture search aims automatically finding neural architectures competitive architectures designed human experts recent approaches achieved state art predictive performance image recognition problematic resource constraints two reasons neural architectures found solely optimized high predictive performance without penalizing excessive resource consumption architecture search methods require vast computational resources address first shortcoming proposing lemonade evolutionary algorithm multi objective architecture search allows approximating pareto front architectures multiple objectives predictive performance number parameters single run method address second shortcoming proposing lamarckian inheritance mechanism lemonade generates children networks warmstarted predictive performance trained parents accomplished using approximate network morphism operators generating children combination two contributions allows finding models par even outperform different sized nasnets mobilenets mobilenets wide residual networks cifar imagenet within one week eight gpus less compute power previous architecture search methods yield state art performance\n",
            "output sentence:  propose method efficient multi objective neural architecture search based lamarckian inheritance evolutionary algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.0851063829787234, 'p': 0.4444444444444444, 'f': 0.14285714015943882}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.25, 'f': 0.06557376821284609}, 'rouge-l': {'r': 0.06382978723404255, 'p': 0.3333333333333333, 'f': 0.10714285444515312}}\n",
            "pair:  graph neural networks combination graph signal processing deep convolutional networks shows great power pattern recognition non euclidean domains paper propose new method deploy two pipelines based duality graph improve accuracy exploring primal graph dual graph nodes edges treated one another exploited benefits vertex features edge features result arrived framework great potential semisupervised unsupervised learning\n",
            "output sentence:  primal dual graph neural network model semi supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.09302325581395349, 'p': 1.0, 'f': 0.1702127644001811}, 'rouge-2': {'r': 0.019230769230769232, 'p': 0.3333333333333333, 'f': 0.036363635332231435}, 'rouge-l': {'r': 0.09302325581395349, 'p': 1.0, 'f': 0.1702127644001811}}\n",
            "pair:  work presents poincar wasserstein autoencoder reformulation recently proposed wasserstein autoencoder framework non euclidean manifold poincar ball model hyperbolic space assuming latent space hyperbolic use intrinsic hierarchy impose structure learned latent space representations show datasets latent hierarchies recover structure low dimensional latent space also demonstrate model visual domain analyze properties show competitive results graph link prediction task\n",
            "output sentence:  wasserstein autoencoder hyperbolic latent space \n",
            "\n",
            "{'rouge-1': {'r': 0.21739130434782608, 'p': 0.7692307692307693, 'f': 0.33898304741166335}, 'rouge-2': {'r': 0.1111111111111111, 'p': 0.5, 'f': 0.18181817884297521}, 'rouge-l': {'r': 0.13043478260869565, 'p': 0.46153846153846156, 'f': 0.2033898270726803}}\n",
            "pair:  show usual training loss augmented lipschitz regularization term networks generalize prove generalization first establishing stronger convergence result along rate convergence second result resolves question posed zhang et al model distinguish case clean labels randomized labels answer lipschitz regularization using lipschitz constant clean data makes distinction case model learns different function hypothesize correctly fails learn dirty labels\n",
            "output sentence:  prove generalization dnns adding lipschitz regularization term training loss resolve question posed zhang et al \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.75, 'f': 0.1935483848491155}, 'rouge-2': {'r': 0.07547169811320754, 'p': 0.6153846153846154, 'f': 0.13445377956641483}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.75, 'f': 0.1935483848491155}}\n",
            "pair:  deep neural networks dnns typically enough capacity fit random data brute force even conventional data dependent regularizations focusing geometry features imposed find reason inconsistency enforced geometry standard softmax cross entropy loss resolve propose new framework data dependent dnn regularization geometrically regularized self validating neural networks grsvnet training geometry enforced one batch features simultaneously validated separate batch using validation loss consistent geometry study particular case grsvnet orthogonal low rank embedding ole grsvnet capable producing highly discriminative features residing orthogonal low rank subspaces numerical experiments show ole grsvnet outperforms dnns conventional regularization trained real data importantly unlike conventional dnns ole grsvnet refuses memorize random data random labels suggesting learns intrinsic patterns reducing memorizing capacity baseline dnn\n",
            "output sentence:  propose new framework data dependent dnn regularization prevent dnns overfitting random data random labels \n",
            "\n",
            "{'rouge-1': {'r': 0.11494252873563218, 'p': 0.9090909090909091, 'f': 0.20408163066014165}, 'rouge-2': {'r': 0.05454545454545454, 'p': 0.6, 'f': 0.09999999847222223}, 'rouge-l': {'r': 0.10344827586206896, 'p': 0.8181818181818182, 'f': 0.18367346739483548}}\n",
            "pair:  robustness neural networks adversarial examples received great attention due security implications despite various attack approaches crafting visually imperceptible adversarial examples little developed towards comprehensive measure robustness paper provide theoretical justification converting robustness analysis local lipschitz constant estimation problem propose use extreme value theory efficient evaluation analysis yields novel robustness metric called clever short cross lipschitz extreme value network robustness proposed clever score attack agnostic computationally feasible large neural networks experimental results various networks including resnet inception mobilenet show clever aligned robustness indication measured ell ell infty norms adversarial examples powerful attacks ii defended networks using defensive distillation bounded relu indeed give better clever scores best knowledge clever first attack independent robustness metric applied neural network classifiers\n",
            "output sentence:  propose first attack independent robustness metric clever applied neural network classifier \n",
            "\n",
            "{'rouge-1': {'r': 0.16470588235294117, 'p': 0.9333333333333333, 'f': 0.27999999745}, 'rouge-2': {'r': 0.10434782608695652, 'p': 0.8571428571428571, 'f': 0.18604650969292713}, 'rouge-l': {'r': 0.16470588235294117, 'p': 0.9333333333333333, 'f': 0.27999999745}}\n",
            "pair:  generating formal language represented relational tuples lisp programs mathematical expressions natural language input extremely challenging task requires explicitly capture discrete symbolic structural information input generate output state art neural sequence models explicitly capture structure information thus perform well tasks paper propose new encoder decoder model based tensor product representations tprs natural formal language generation called tp encoder tp employs tpr binding encode natural language symbolic structure vector space decoder uses tpr unbinding generate sequence relational tuples consisting relation operation number arguments symbolic space tp considerably outperforms lstm based seq seq models creating new state art results two benchmarks mathqa dataset math problem solving algolist dataset program synthesis ablation studies show improvements mainly attributed use tprs encoder decoder explicitly capture relational structure information symbolic reasoning\n",
            "output sentence:  paper propose new encoder decoder model based tensor product representations natural formal language generation called tp \n",
            "\n",
            "{'rouge-1': {'r': 0.08433734939759036, 'p': 0.5833333333333334, 'f': 0.14736841884542937}, 'rouge-2': {'r': 0.010309278350515464, 'p': 0.09090909090909091, 'f': 0.018518516688957657}, 'rouge-l': {'r': 0.060240963855421686, 'p': 0.4166666666666667, 'f': 0.10526315568753467}}\n",
            "pair:  capsule networks shown encouraging results textit defacto benchmark computer vision datasets mnist cifar smallnorb although yet tested tasks entities detected inherently complex internal representations instances per class learn point wise classification suitable hence paper carries experiments face verification controlled uncontrolled settings together address points introduce textit siamese capsule networks new variant used pairwise learning tasks find model improves baselines shot learning setting suggesting capsule networks efficient learning discriminative representations given samples find textit siamese capsule networks perform well strong baselines pairwise learning datasets trained using contrastive loss ell normalized capsule encoded pose features yielding best results shot learning setting image pairs test set contain unseen subjects\n",
            "output sentence:  pairwise learned capsule network performs well face verification tasks given limited labeled data \n",
            "\n",
            "{'rouge-1': {'r': 0.09302325581395349, 'p': 0.6666666666666666, 'f': 0.16326530397334446}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.18181818181818182, 'f': 0.032520323574591926}, 'rouge-l': {'r': 0.06976744186046512, 'p': 0.5, 'f': 0.12244897744273221}}\n",
            "pair:  context optimization gradient neural network indicates amount specific weight change respect loss therefore small gradients indicate good value weight requires change kept frozen training paper provides experimental study importance neural network weights extent need updated wish show starting third epoch freezing weights informative gradient less likely changed training results slight drop overall accuracy sometimes better experiment mnist cifar flickr datasets using several architectures vgg resnet densenet cifar show freezing vgg network parameters third epoch onwards results drop accuracy freezing resnet parameters results drop accuracy finally freezing densnet parameters results drop accuracy furthermore experiemnt real life applications train image captioning model attention mechanism flickr dataset using lstm networks freezing parameters third epoch onwards resulting better bleu score fully trained model source code found appendix\n",
            "output sentence:  experimental paper proves amount redundant weights freezed third epoch slight drop accuracy \n",
            "\n",
            "{'rouge-1': {'r': 0.12857142857142856, 'p': 0.75, 'f': 0.21951219262343843}, 'rouge-2': {'r': 0.052083333333333336, 'p': 0.45454545454545453, 'f': 0.09345794208053108}, 'rouge-l': {'r': 0.11428571428571428, 'p': 0.6666666666666666, 'f': 0.19512194872099942}}\n",
            "pair:  introduce new deep convolutional neural network crescendonet stacking simple building blocks without residual connections crescendo block contains independent convolution paths increased depths numbers convolution layers parameters increased linearly crescendo blocks experiments crescendonet layers outperforms almost networks without residual connections benchmark datasets cifar cifar svhn given sufficient amount data svhn dataset crescendonet layers parameters match performance densenet bc layers parameters crescendonet provides new way construct high performance deep convolutional neural networks without residual connections moreover investigating behavior performance subnetworks crescendonet note high performance crescendonet may come implicit ensemble behavior differs fractalnet also deep convolutional neural network without residual connections furthermore independence paths crescendonet allows us introduce new path wise training procedure reduce memory needed training\n",
            "output sentence:  introduce crescendonet deep cnn architecture stacking simple building blocks without residual connections \n",
            "\n",
            "{'rouge-1': {'r': 0.11956521739130435, 'p': 0.8461538461538461, 'f': 0.20952380735419504}, 'rouge-2': {'r': 0.0423728813559322, 'p': 0.4166666666666667, 'f': 0.07692307524733731}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.6153846153846154, 'f': 0.15238095021133788}}\n",
            "pair:  long short term memory lstm networks allow exhibit temporal dynamic behavior feedback connections seem natural choice learning sequences meshes introduce approach dynamic mesh representations used numerical simulations car crashes bypass complication using meshes transform surface mesh sequences spectral descriptors efficiently encode shape two branch lstm based network architecture chosen learn representations dynamics crash simulation architecture based unsupervised video prediction lstm without convolutional layer uses encoder lstm map input sequence fixed length vector representation representation one decoder lstm performs reconstruction input sequence decoder lstm predicts future behavior receiving initial steps sequence seed spatio temporal error behavior model analysed study well model extrapolate learned spectral descriptors future well learned represent underlying dynamical structural mechanics considering training examples available typical case numerical simulations network performs well\n",
            "output sentence:  two branch lstm based network architecture learns representation dynamics meshes numerical crash simulations \n",
            "\n",
            "{'rouge-1': {'r': 0.10526315789473684, 'p': 0.7692307692307693, 'f': 0.1851851830675583}, 'rouge-2': {'r': 0.05084745762711865, 'p': 0.46153846153846156, 'f': 0.09160305164733994}, 'rouge-l': {'r': 0.08421052631578947, 'p': 0.6153846153846154, 'f': 0.1481481460305213}}\n",
            "pair:  high intra class diversity inter class similarity characteristic remote sensing scene image data sets currently posing significant difficulty deep learning algorithms classification tasks improve accuracy post classification methods proposed smoothing results model predictions however approaches require additional neural network perform smoothing operation adds overhead task propose approach involves learning deep features directly neighboring scene images without requiring use cleanup model approach utilizes siamese network improve discriminative power convolutional neural networks pair neighboring scene images exploits semantic coherence pair enrich feature vector image want predict label empirical results show approach provides viable alternative existing methods example model improved prediction accuracy percentage point dropped mean squared error value baseline disease density estimation task performance gains comparable results existing post classification methods moreover without implementation overheads\n",
            "output sentence:  approach improving prediction accuracy learning deep features neighboring scene images satellite scene image analysis \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.5714285714285714, 'f': 0.11594202716236088}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.42857142857142855, 'f': 0.08695651991598406}}\n",
            "pair:  recommendation prevalent application machine learning affects many users therefore crucial recommender models accurate interpretable work propose method interpret augment predictions black box recommender systems particular propose extract feature interaction interpretations source recommender model explicitly encode interactions target recommender model source target models black boxes assuming structure recommender system approach used general settings experiments focus prominent use machine learning recommendation ad click prediction found interaction interpretations informative predictive significantly outperforming existing recommender models approach interpreting interactions provide new insights domains even beyond recommendation\n",
            "output sentence:  proposed method extract leverage interpretations feature interactions \n",
            "\n",
            "{'rouge-1': {'r': 0.08064516129032258, 'p': 0.8333333333333334, 'f': 0.14705882192041522}, 'rouge-2': {'r': 0.04054054054054054, 'p': 0.6, 'f': 0.07594936590290019}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.6666666666666666, 'f': 0.11764705721453288}}\n",
            "pair:  recent work exhibited surprising cross lingual abilities multilingual bert bert surprising since trained without cross lingual objective aligned data work provide comprehensive study contribution different components bert cross lingual ability study impact linguistic properties languages architecture model learning objectives experimental study done context three typologically different languages spanish hindi russian using two conceptually different nlp tasks textual entailment named entity recognition among key conclusions fact lexical overlap languages plays negligible role cross lingual success depth network important part\n",
            "output sentence:  cross lingual ability multilingual bert empirical study \n",
            "\n",
            "{'rouge-1': {'r': 0.07291666666666667, 'p': 0.5, 'f': 0.1272727250512397}, 'rouge-2': {'r': 0.00847457627118644, 'p': 0.07142857142857142, 'f': 0.01515151325528031}, 'rouge-l': {'r': 0.052083333333333336, 'p': 0.35714285714285715, 'f': 0.09090908868760336}}\n",
            "pair:  computer vision undergone dramatic revolution performance driven large part deep features trained large scale supervised datasets however much improvements focused static image analysis video understanding seen rather modest improvements even though new datasets spatiotemporal models proposed simple frame frame classification methods often still remain competitive posit current video datasets plagued implicit biases scene object structure dwarf variations temporal structure work build video dataset fully observable controllable object scene bias truly requires spatiotemporal understanding order solved dataset named cater rendered synthetically using library standard objects tests ability recognize compositions object movements require long term reasoning addition challenging dataset cater also provides plethora diagnostic tools analyze modern spatiotemporal video architectures completely observable controllable using cater provide insights recent state art deep video architectures\n",
            "output sentence:  propose new video understanding benchmark tasks design require temporal reasoning solved unlike existing video datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.1044776119402985, 'p': 0.5384615384615384, 'f': 0.17499999727812504}, 'rouge-2': {'r': 0.0379746835443038, 'p': 0.25, 'f': 0.06593406364448746}, 'rouge-l': {'r': 0.08955223880597014, 'p': 0.46153846153846156, 'f': 0.14999999727812502}}\n",
            "pair:  recent work cross lingual word embeddings severely anglocentric vast majority lexicon induction evaluation dictionaries english another language english embedding space selected default hub learning multilingual setting work however challenge practices first show choice hub language significantly impact downstream lexicon induction performance second expand current evaluation dictionary collection include language pairs using triangulation also create new dictionaries represented languages evaluating established methods language pairs sheds light suitability presents new challenges field finally analysis identify general guidelines strong cross lingual embeddings baselines based anglocentric experiments\n",
            "output sentence:  choice hub target language affects quality cross lingual embeddings evaluated english centric dictionaries \n",
            "\n",
            "{'rouge-1': {'r': 0.06060606060606061, 'p': 0.5454545454545454, 'f': 0.10909090729090913}, 'rouge-2': {'r': 0.03389830508474576, 'p': 0.4, 'f': 0.06249999855957035}, 'rouge-l': {'r': 0.04040404040404041, 'p': 0.36363636363636365, 'f': 0.07272727092727278}}\n",
            "pair:  compressed representations generalize better shamir et al may crucial learning limited noisy labeled data information bottleneck ib method tishby et al provides insightful principled approach balancing compression prediction representation learning ib objective employs lagrange multiplier tune trade however little theoretical guidance select also lack theoretical understanding relationship dataset model capacity learnability work show improperly chosen learning cannot happen trivial representation becomes global minimum ib objective show avoided identifying sharp phase transition unlearnable learnable arises varies phase transition defines concept ib learnability prove several sufficient conditions ib learnability providing theoretical guidance selecting show ib learnability determined largest confident typical imbalanced subset training examples give practical algorithm estimate minimum given dataset test theoretical results synthetic datasets mnist cifar noisy labels make surprising observation accuracy may non monotonic\n",
            "output sentence:  theory predicts phase transition unlearnable learnable values beta information bottleneck objective \n",
            "\n",
            "{'rouge-1': {'r': 0.09278350515463918, 'p': 0.6923076923076923, 'f': 0.16363636155206615}, 'rouge-2': {'r': 0.017699115044247787, 'p': 0.16666666666666666, 'f': 0.031999998264320095}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.3076923076923077, 'f': 0.07272727064297525}}\n",
            "pair:  deep convolution neural networks cnns rooted pioneer work cite hinton lecun alex summarized cite lecunbengiohinton shown useful variety fields state art cnn machines image rest net cite cvpr described real value inputs kernel convolutions followed local non linear rectified linear outputs understanding role layers accuracy limitations well making efficient fewer parameters ongoing research questions inspired quantum theory propose use complex value kernel functions followed local non linear absolute modulus operator square argue advantage quantum inspired complex kernels robustness realistic unpredictable scenarios clutter noise data deformations study concrete problem shape detection show multiple overlapping shapes deformed clutter noise added convolution layer quantum inspired complex kernels outperforms statistical classical kernel counterpart bayesian shape estimator superior performance due quantum phenomena interference present classical cnns\n",
            "output sentence:  quantum inspired kernel convolution network exhibiting interference phenomena useful compared real value counterpart \n",
            "\n",
            "{'rouge-1': {'r': 0.136986301369863, 'p': 0.7142857142857143, 'f': 0.22988505477077553}, 'rouge-2': {'r': 0.04, 'p': 0.2857142857142857, 'f': 0.07017543644198222}, 'rouge-l': {'r': 0.1232876712328767, 'p': 0.6428571428571429, 'f': 0.2068965490236491}}\n",
            "pair:  orthogonal recurrent neural networks address vanishing gradient problem parameterizing recurrent connections using orthogonal matrix class models particularly effective solve tasks require memorization long sequences propose alternative solution based explicit memorization using linear autoencoders sequences show recently proposed recurrent architecture linear memory network composed nonlinear feedforward layer separate linear recurrence used solve hard memorization tasks propose initialization schema sets weights recurrent architecture approximate linear autoencoder input sequences found closed form solution initialization schema easily adapted recurrent architecture argue approach superior random orthogonal initialization due autoencoder allows memorization long sequences even training empirical analysis show approach achieves competitive results alternative orthogonal models lstm sequential mnist permuted mnist timit\n",
            "output sentence:  show initialize recurrent architectures closed form solution linear autoencoder sequences show advantages approach compared orthogonal rnns \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.4, 'f': 0.16438355837868274}, 'rouge-2': {'r': 0.015873015873015872, 'p': 0.06666666666666667, 'f': 0.025641022534517142}, 'rouge-l': {'r': 0.08620689655172414, 'p': 0.3333333333333333, 'f': 0.13698629810471016}}\n",
            "pair:  recent neural network language models begun rely softmax distributions extremely large number categories context calculating softmax normalizing constant prohibitively expensive spurred growing literature efficiently computable biased estimates softmax paper present first two unbiased algorithms maximizing softmax likelihood whose work per iteration independent number classes datapoints require extra work end epoch compare unbiased methods empirical performance state art seven real world datasets comprehensively outperform competitors\n",
            "output sentence:  propose first methods exactly optimizing softmax distribution using stochastic gradient runtime independent number number datapoints datapoints \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.8888888888888888, 'f': 0.19999999800312504}, 'rouge-2': {'r': 0.08080808080808081, 'p': 0.8888888888888888, 'f': 0.1481481466203704}, 'rouge-l': {'r': 0.11267605633802817, 'p': 0.8888888888888888, 'f': 0.19999999800312504}}\n",
            "pair:  paper study new graph learning problem learning count subgraph isomorphisms although learning based approach inexact able generalize count large patterns data graphs polynomial time compared exponential time original np complete problem different traditional graph learning problems node classification link prediction subgraph isomorphism counting requires global inference oversee whole graph tackle problem propose dynamic intermedium attention memory network diamnet augments different representation learning architectures iteratively attends pattern target data graphs memorize different subgraph isomorphisms global counting develop small graphs subgraph isomorphisms large graphs subgraph isomorphisms sets evaluate different models experimental results show learning based subgraph isomorphism counting help reduce time complexity acceptable accuracy diamnet improve existing representation learning models global problem\n",
            "output sentence:  paper study new graph learning problem learning count subgraph isomorphisms \n",
            "\n",
            "{'rouge-1': {'r': 0.05333333333333334, 'p': 0.4, 'f': 0.09411764498269902}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.1111111111111111, 'f': 0.02040816159725128}, 'rouge-l': {'r': 0.05333333333333334, 'p': 0.4, 'f': 0.09411764498269902}}\n",
            "pair:  convolutional neural networks continuously advance progress image object classification steadfast usage algorithm requires constant evaluation upgrading foundational concepts maintain progress network regularization techniques typically focus convolutional layer operations leaving pooling layer operations without suitable options introduce wavelet pooling another alternative traditional neighborhood pooling method decomposes features second level decomposition discards first level subbands reduce feature dimensions method addresses overfitting problem encountered max pooling reducing features structurally compact manner pooling via neighborhood regions experimental results four benchmark classification datasets demonstrate proposed method outperforms performs comparatively methods like max mean mixed stochastic pooling\n",
            "output sentence:  pooling achieved using wavelets instead traditional neighborhood approaches max average etc \n",
            "\n",
            "{'rouge-1': {'r': 0.012987012987012988, 'p': 0.2, 'f': 0.024390242757287385}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.012987012987012988, 'p': 0.2, 'f': 0.024390242757287385}}\n",
            "pair:  neural networks offer high accuracy solutions range problems computationally costly run production systems propose technique called deep learning approximation take already trained neural network model build faster almost equally accurate network manipulating network structure coefficients without requiring training access training data speedup achieved applying sequential series independent optimizations reduce floating point operations flops required perform forward pass optimal lossy approximation chosen layer weighing relative accuracy loss flop reduction pascal voc yolo network show end end speedup network forward pass drop map gained finetuning enabling network others like deployed compute constrained systems\n",
            "output sentence:  decompose weights use fewer flops svd \n",
            "\n",
            "{'rouge-1': {'r': 0.16129032258064516, 'p': 0.5, 'f': 0.2439024353361095}, 'rouge-2': {'r': 0.06944444444444445, 'p': 0.25, 'f': 0.10869564877126667}, 'rouge-l': {'r': 0.12903225806451613, 'p': 0.4, 'f': 0.19512194753123147}}\n",
            "pair:  present sequence action parsing approach natural language sql task incrementally fills slots sql query feasible actions pre defined inventory account fact typically multiple correct sql queries similar semantics draw inspiration syntactic parsing techniques propose train sequence action models non deterministic oracles evaluate models wikisql dataset achieve execution accuracy test set absolute improvement models trained traditional static oracles assuming single correct target sql query combined execution guided decoding strategy model sets new state art performance execution accuracy\n",
            "output sentence:  design incremental sequence action parsers text sql task achieve sota results improve using non deterministic oracles allow multiple correct correct correct \n",
            "\n",
            "{'rouge-1': {'r': 0.07476635514018691, 'p': 0.8888888888888888, 'f': 0.13793103305142687}, 'rouge-2': {'r': 0.030534351145038167, 'p': 0.4444444444444444, 'f': 0.05714285593979594}, 'rouge-l': {'r': 0.056074766355140186, 'p': 0.6666666666666666, 'f': 0.10344827443073723}}\n",
            "pair:  stochastic gradient descent sgd methods using randomly selected batches widely used train neural network nn models performing design exploration find best nn particular task often requires extensive training different models large dataset computationally expensive straightforward method accelerate computation distribute batch sgd multiple processors however large batch training often times leads degradation accuracy poor generalization even poor robustness adversarial attacks existing solutions large batch training either work require massive hyper parameter tuning address issue propose novel large batch training method combines recent results adversarial training regularize sharp minima second order optimization use curvature information change batch size adaptively training extensively evaluate method cifar svhn tinyimagenet imagenet datasets using multiple nns including residual networks well compressed networks squeezenext new approach exceeds performance existing solutions terms accuracy number sgd iterations times respectively emphasize achieved without additional hyper parameter tuning tailor method experiments\n",
            "output sentence:  large batch size training using adversarial training second order information \n",
            "\n",
            "{'rouge-1': {'r': 0.10638297872340426, 'p': 0.5, 'f': 0.17543859359803018}, 'rouge-2': {'r': 0.03571428571428571, 'p': 0.2, 'f': 0.06060605803489451}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.4, 'f': 0.1403508742997846}}\n",
            "pair:  reinforcement learning learn model future observations rewards use plan agent next actions however jointly modeling future observations computationally expensive even intractable observations high dimensional images reason previous works considered partial models model part observation paper show partial models causally incorrect confounded observations model therefore lead incorrect planning address introduce general family partial models provably causally correct avoid need fully model future observations\n",
            "output sentence:  causally correct partial models generate whole observation remain causally correct stochastic environments \n",
            "\n",
            "{'rouge-1': {'r': 0.23728813559322035, 'p': 0.9333333333333333, 'f': 0.37837837514609207}, 'rouge-2': {'r': 0.1267605633802817, 'p': 0.6, 'f': 0.2093023227014603}, 'rouge-l': {'r': 0.22033898305084745, 'p': 0.8666666666666667, 'f': 0.35135134811906504}}\n",
            "pair:  work first conduct mathematical analysis memory defined function maps element sequence current output three rnn cells namely simple recurrent neural network srn long short term memory lstm gated recurrent unit gru based analysis propose new design called extended long short term memory elstm extend memory length cell next present multi task rnn model robust previous erroneous predictions called dependent bidirectional recurrent neural network dbrnn sequence sequenceout siso problem finally performance dbrnn model elstm cell demonstrated experimental results\n",
            "output sentence:  recurrent neural network cell extended long short term memory multi task rnn model sequence sequence problems \n",
            "\n",
            "{'rouge-1': {'r': 0.26666666666666666, 'p': 0.9230769230769231, 'f': 0.4137930999702736}, 'rouge-2': {'r': 0.22, 'p': 0.9166666666666666, 'f': 0.3548387065556712}, 'rouge-l': {'r': 0.26666666666666666, 'p': 0.9230769230769231, 'f': 0.4137930999702736}}\n",
            "pair:  introduce attention mechanism improve feature extraction deep active learning al semi supervised setting proposed attention mechanism based recent methods visually explain predictions made dnns apply proposed explanation based attention mnist svhn classification conducted experiments show accuracy improvements original class imbalanced datasets number training examples faster long tail convergence compared uncertainty based methods\n",
            "output sentence:  introduce attention mechanism improve feature extraction deep active learning al semi supervised setting \n",
            "\n",
            "{'rouge-1': {'r': 0.1896551724137931, 'p': 0.6875, 'f': 0.29729729390796206}, 'rouge-2': {'r': 0.06578947368421052, 'p': 0.3125, 'f': 0.10869564930056717}, 'rouge-l': {'r': 0.1724137931034483, 'p': 0.625, 'f': 0.27027026688093503}}\n",
            "pair:  last years deep learning tremendously successful many applications however theoretical understanding deep learning thus ability providing principled improvements seems lag behind theoretical puzzle concerns ability deep networks predict well despite intriguing apparent lack generalization classification accuracy training set proxy performance test set possible training performance independent testing performance indeed deep networks require drastically new theory generalization measurements based training data predictive network performance future data show performance measured appropriately training performance fact predictive expected performance consistently classical machine learning theory\n",
            "output sentence:  contrary previous beliefs training performance deep networks measured appropriately predictive test performance consistent classical machine learning theory \n",
            "\n",
            "{'rouge-1': {'r': 0.1323529411764706, 'p': 0.6428571428571429, 'f': 0.21951219229030342}, 'rouge-2': {'r': 0.05, 'p': 0.3076923076923077, 'f': 0.08602150297144186}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.35714285714285715, 'f': 0.12195121668054736}}\n",
            "pair:  unsupervised monocular depth estimation made great progress deep learning involved training binocular stereo images considered good option data easily obtained however depth disparity prediction results show poor performance object boundaries main reason related handling occlusion areas training paper propose novel method overcome issue exploiting disparity maps property generate occlusion mask block back propagation occlusion areas image warping also design new networks flipped stereo images induce networks learn occluded boundaries shows method achieves clearer boundaries better evaluation results kitti driving dataset virtual kitti dataset\n",
            "output sentence:  paper propose mask method solves previous blurred results unsupervised monocular depth estimation caused occlusion \n",
            "\n",
            "{'rouge-1': {'r': 0.07216494845360824, 'p': 0.6363636363636364, 'f': 0.1296296278000686}, 'rouge-2': {'r': 0.04032258064516129, 'p': 0.38461538461538464, 'f': 0.07299269901220101}, 'rouge-l': {'r': 0.07216494845360824, 'p': 0.6363636363636364, 'f': 0.1296296278000686}}\n",
            "pair:  adversarial learning methods proposed wide range applications training adversarial models notoriously unstable effectively balancing performance generator discriminator critical since discriminator achieves high accuracy produce relatively uninformative gradients work propose simple general technique constrain information flow discriminator means information bottleneck enforcing constraint mutual information observations discriminator internal representation effectively modulate discriminator accuracy maintain useful informative gradients demonstrate proposed variational discriminator bottleneck vdb leads significant improvements across three distinct application areas adversarial learning algorithms primary evaluation studies applicability vdb imitation learning dynamic continuous control skills running show method learn skills directly raw video demonstrations substantially outperforming prior adversarial imitation learning methods vdb also combined adversarial inverse reinforcement learning learn parsimonious reward functions transferred optimized new settings finally demonstrate vdb train gans effectively image generation improving upon number prior stabilization methods\n",
            "output sentence:  regularizing adversarial learning information bottleneck applied imitation learning inverse reinforcement learning generative adversarial networks \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.9411764705882353, 'f': 0.28318583815177384}, 'rouge-2': {'r': 0.044642857142857144, 'p': 0.3125, 'f': 0.07812499781250007}, 'rouge-l': {'r': 0.09375, 'p': 0.5294117647058824, 'f': 0.15929203284203935}}\n",
            "pair:  federated learning global model trained iterative parameter averaging locally computed updates promising approach distributed training deep networks provides high communication efficiency privacy preservability allows fit well decentralized data environments mobile cloud ecosystems however despite advantages federated learning based methods still challenge dealing non iid training data local devices learners regard study effects variety hyperparametric conditions non iid environments answer important concerns practical implementations first investigate parameter divergence local updates explain performance degradation non iid data origin parameter divergence also found empirically theoretically ii revisit effects optimizers network depth width regularization techniques observations show well known advantages hyperparameter optimization strategies could rather yield diminishing returns non iid data iii finally provide reasons failure cases categorized way mainly based metrics parameter divergence\n",
            "output sentence:  investigate internal reasons observations diminishing effects well known hyperparameter optimization methods federated learning decentralized non iid data data \n",
            "\n",
            "{'rouge-1': {'r': 0.13157894736842105, 'p': 0.7692307692307693, 'f': 0.224719098628961}, 'rouge-2': {'r': 0.05319148936170213, 'p': 0.38461538461538464, 'f': 0.09345794179054943}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.5384615384615384, 'f': 0.15730336829188238}}\n",
            "pair:  develop reinforcement learning based search assistant assist users set actions sequence interactions enable realize intent approach caters subjective search user seeking digital assets images fundamentally different tasks objective limited search modalities labeled conversational data generally available search tasks training agent human interactions time consuming propose stochastic virtual user impersonates real user used sample user behavior efficiently train agent accelerates bootstrapping agent develop algorithm based context preserving architecture enables agent provide contextual assistance user compare agent learning evaluate performance average rewards state values obtains virtual user validation episodes experiments show agent learns achieve higher rewards better states\n",
            "output sentence:  reinforcement learning based conversational search assistant provides contextual assistance subjective search like digital assets \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 0.7, 'f': 0.2028985482461668}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.4444444444444444, 'f': 0.10389610183167484}, 'rouge-l': {'r': 0.1016949152542373, 'p': 0.6, 'f': 0.17391304099979}}\n",
            "pair:  present new approach defining sequence loss function train summarizer using secondary encoder decoder loss function alleviating shortcoming word level training sequence outputs technique based intuition summary good one contain essential information original article therefore good input sequence lieu original summary generated present experimental results apply additional loss function general abstractive summarizer news summarization dataset result improvement rouge metric especially large improvement human evaluations suggesting enhanced performance competitive specialized state art models\n",
            "output sentence:  present use secondary encoder decoder loss function help train summarizer \n",
            "\n",
            "{'rouge-1': {'r': 0.13114754098360656, 'p': 0.5333333333333333, 'f': 0.21052631262119115}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.21428571428571427, 'f': 0.0697674391346675}, 'rouge-l': {'r': 0.09836065573770492, 'p': 0.4, 'f': 0.15789473367382278}}\n",
            "pair:  present large scale empirical study catastrophic forgetting cf modern deep neural network dnn models perform sequential incremental learning new experimental protocol proposed takes account typical constraints encountered application scenarios investigation empirical evaluate cf behavior hitherto largest number visual classification datasets construct representative number sequential learning tasks slts close alignment previous works cf results clearly indicate model avoids cf investigated datasets slts application conditions conclude discussion potential solutions workarounds cf notably ewc imm models\n",
            "output sentence:  check dnn models catastrophic forgetting using new evaluation scheme reflects typical application conditions surprising results \n",
            "\n",
            "{'rouge-1': {'r': 0.09574468085106383, 'p': 0.75, 'f': 0.16981131874688504}, 'rouge-2': {'r': 0.03571428571428571, 'p': 0.36363636363636365, 'f': 0.06504064877784392}, 'rouge-l': {'r': 0.06382978723404255, 'p': 0.5, 'f': 0.11320754516197937}}\n",
            "pair:  predictive models generalize well distributional shift often desirable sometimes crucial machine learning applications one example estimation treatment effects observational data subtask predict effect treatment subjects systematically different received treatment data related kind distributional shift appears unsupervised domain adaptation tasked generalizing distribution inputs different one observe labels pose problems prediction shift design popular methods overcoming distributional shift often heuristic rely assumptions rarely true practice well specified model knowing policy gave rise observed data methods hindered need pre specified metric comparing observations poor asymptotic properties work devise bound generalization error design shift based integral probability metrics sample weighting combine idea representation learning generalizing tightening existing results space finally propose algorithmic framework inspired bound verify effectiveness causal effect estimation\n",
            "output sentence:  theory algorithmic framework prediction distributional shift including causal effect estimation domain adaptation \n",
            "\n",
            "{'rouge-1': {'r': 0.03488372093023256, 'p': 0.42857142857142855, 'f': 0.06451612764018964}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03488372093023256, 'p': 0.42857142857142855, 'f': 0.06451612764018964}}\n",
            "pair:  growth complexity convolutional neural networks cnns increasing interest partitioning network across multiple accelerators training pipelining backpropagation computations accelerators existing approaches avoid limit use stale weights techniques micro batching weight stashing techniques either underutilize accelerators increase memory footprint explore impact stale weights statistical efficiency performance pipelined backpropagation scheme maximizes accelerator utilization keeps memory overhead modest use cnns lenet alexnet vgg resnet show pipelining limited early layers network training stale weights converges results models comparable inference accuracies resulting non pipelined training mnist cifar datasets drop accuracy networks respectively however pipelining deeper network inference accuracies drop significantly propose combining pipelined non pipelined training hybrid scheme address drop demonstrate implementation performance pipelined backpropagation pytorch gpus using resnet achieving speedups gpu baseline small drop inference accuracy\n",
            "output sentence:  accelerating cnn training pipeline accelerators stale weights \n",
            "\n",
            "{'rouge-1': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.039473684210526314, 'p': 0.3333333333333333, 'f': 0.07058823340069209}}\n",
            "pair:  reconstruction view ray computed tomography ct data highly ill posed problem often used applications require low radiation dose clinical ct rapid industrial scanning fixed gantry ct existing analytic iterative algorithms generally produce poorly reconstructed images severely deteriorated artifacts noise especially number ray projections considerably low paper presents deep network driven approach address extreme view ct incorporating convolutional neural network based inference state art iterative reconstruction proposed method interprets view sinogram data using attention based deep networks infer reconstructed image predicted image used prior knowledge iterative algorithm final reconstruction demonstrate effectiveness proposed approach performing reconstruction experiments chest ct dataset\n",
            "output sentence:  present cnn inference based reconstruction algorithm address extremely view ct \n",
            "\n",
            "{'rouge-1': {'r': 0.12359550561797752, 'p': 0.7333333333333333, 'f': 0.2115384590698965}, 'rouge-2': {'r': 0.036036036036036036, 'p': 0.2857142857142857, 'f': 0.06399999801088006}, 'rouge-l': {'r': 0.0898876404494382, 'p': 0.5333333333333333, 'f': 0.15384615137758878}}\n",
            "pair:  power efficient cnn domain specific accelerator cnn dsa chips currently available wide use mobile devices chips mainly used computer vision applications however recent work super characters method text classification sentiment analysis tasks using two dimensional cnn models also achieved state art results method transfer learning vision text paper implemented text classification sentiment analysis applications mobile devices using cnn dsa chips compact network representations using one bit three bits precision coefficients five bits activations used cnn dsa chip power consumption less mw edge devices memory compute constraints network compressed approximating external fully connected fc layers within cnn dsa chip workshop two system demonstrations nlp tasks first demo classifies input english wikipedia sentence one classes second demo classifies chinese online shopping review positive negative\n",
            "output sentence:  deploy text classification sentiment analysis applications english chinese mw cnn accelerator chip device application scenarios \n",
            "\n",
            "{'rouge-1': {'r': 0.0759493670886076, 'p': 0.75, 'f': 0.137931032812789}, 'rouge-2': {'r': 0.00980392156862745, 'p': 0.14285714285714285, 'f': 0.018348622651292057}, 'rouge-l': {'r': 0.0759493670886076, 'p': 0.75, 'f': 0.137931032812789}}\n",
            "pair:  fundamental trait intelligence ability achieve goals face novel circumstances work address one setting requires solving task novel set actions empowering machines ability requires generalization way agent perceives available actions along way uses actions solve tasks hence propose framework enable generalization aspects understanding action functionality using actions solve tasks reinforcement learning specifically agent interprets action behavior using unsupervised representation learning collection data samples reflecting diverse properties action employ reinforcement learning architecture works action representations propose regularization metrics essential enabling generalization policy illustrate generalizability representation learning method policy enable zero shot generalization previously unseen actions challenging sequential decision making environments results videos found sites google com view action generalization\n",
            "output sentence:  address problem generalization reinforcement learning unseen action spaces \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.5833333333333334, 'f': 0.13592232803845794}, 'rouge-2': {'r': 0.017094017094017096, 'p': 0.18181818181818182, 'f': 0.03124999842895516}, 'rouge-l': {'r': 0.054945054945054944, 'p': 0.4166666666666667, 'f': 0.09708737658214729}}\n",
            "pair:  general problem received considerable recent attention perform multiple tasks network maximizing efficiency prediction accuracy popular approach consists multi branch architecture top shared backbone jointly trained weighted sum losses however many cases shared representation results non optimal performance mainly due interference conflicting gradients uncorrelated tasks recent approaches address problem channel wise modulation feature maps along shared backbone task specific vectors manually dynamically tuned taking approach step propose novel architecture modulate recognition network channel wise well spatial wise efficient top image dependent computation scheme architecture uses task specific branches task specific modules instead uses top modulation network shared tasks show effectiveness scheme achieving par better results alternative approaches correlated uncorrelated sets tasks also demonstrate advantages terms model size addition novel tasks interpretability code released\n",
            "output sentence:  propose top modulation network multi task learning applications several advantages current schemes \n",
            "\n",
            "{'rouge-1': {'r': 0.09782608695652174, 'p': 0.5294117647058824, 'f': 0.1651376120461241}, 'rouge-2': {'r': 0.007936507936507936, 'p': 0.0625, 'f': 0.014084505042650552}, 'rouge-l': {'r': 0.05434782608695652, 'p': 0.29411764705882354, 'f': 0.09174311663328011}}\n",
            "pair:  ensembles multiple neural networks trained individually predictions averaged shown widely successful improving accuracy predictive uncertainty single neural networks however ensemble cost training testing increases linearly number networks paper propose batchensemble ensemble method whose computational memory costs significantly lower typical ensembles batchensemble achieves defining weight matrix hadamard product shared weight among ensemble members rank one matrix per member unlike ensembles batchensemble parallelizable across devices one device trains one member also parallelizable within device multiple ensemble members updated simultaneously given mini batch across cifar cifar wmt en de en fr translation contextual bandits tasks batchensemble yields competitive accuracy uncertainties typical ensembles speedup test time memory reduction ensemble size also apply batchensemble lifelong learning split cifar batchensemble yields comparable performance progressive neural networks much lower computational memory costs show batchensemble easily scale lifelong learning split imagenet involves sequential learning tasks\n",
            "output sentence:  introduced batchensemble efficient method ensembling lifelong learning used improve accuracy uncertainty neural network like typical ensemble methods \n",
            "\n",
            "{'rouge-1': {'r': 0.11956521739130435, 'p': 0.8461538461538461, 'f': 0.20952380735419504}, 'rouge-2': {'r': 0.0423728813559322, 'p': 0.4166666666666667, 'f': 0.07692307524733731}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.6153846153846154, 'f': 0.15238095021133788}}\n",
            "pair:  long short term memory lstm networks allow exhibit temporal dynamic behavior feedback connections seem natural choice learning sequences meshes introduce approach dynamic mesh representations used numerical simulations car crashes bypass complication using meshes transform surface mesh sequences spectral descriptors efficiently encode shape two branch lstm based network architecture chosen learn representations dynamics crash simulation architecture based unsupervised video prediction lstm without convolutional layer uses encoder lstm map input sequence fixed length vector representation representation one decoder lstm performs reconstruction input sequence decoder lstm predicts future behavior receiving initial steps sequence seed spatio temporal error behavior model analysed study well model extrapolate learned spectral descriptors future well learned represent underlying dynamical structural mechanics considering training examples available typical case numerical simulations network performs well\n",
            "output sentence:  two branch lstm based network architecture learns representation dynamics meshes numerical crash simulations \n",
            "\n",
            "{'rouge-1': {'r': 0.18421052631578946, 'p': 0.8235294117647058, 'f': 0.3010752658295757}, 'rouge-2': {'r': 0.08653846153846154, 'p': 0.5625, 'f': 0.14999999768888891}, 'rouge-l': {'r': 0.17105263157894737, 'p': 0.7647058823529411, 'f': 0.27956988948548966}}\n",
            "pair:  employing deep neural networks natural image priors solve inverse problems either requires large amounts data sufficiently train expressive generative models succeed data via untrained neural networks however works considered interpolate high data regimes particular one use availability small amount data even examples one advantage solving inverse problems system performance increase amount data increases well work consider solving linear inverse problems given small number examples images drawn distribution image interest comparing untrained neural networks use data show one pre train neural network given examples improve reconstruction results compressed sensing semantic image recovery problems colorization approach leads improved reconstruction amount available data increases par fully trained generative models requiring less data needed train generative model\n",
            "output sentence:  show pre training untrained neural network examples improve reconstruction results compressed sensing semantic recovery problems like colorization \n",
            "\n",
            "{'rouge-1': {'r': 0.03488372093023256, 'p': 0.42857142857142855, 'f': 0.06451612764018964}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03488372093023256, 'p': 0.42857142857142855, 'f': 0.06451612764018964}}\n",
            "pair:  growth complexity convolutional neural networks cnns increasing interest partitioning network across multiple accelerators training pipelining backpropagation computations accelerators existing approaches avoid limit use stale weights techniques micro batching weight stashing techniques either underutilize accelerators increase memory footprint explore impact stale weights statistical efficiency performance pipelined backpropagation scheme maximizes accelerator utilization keeps memory overhead modest use cnns lenet alexnet vgg resnet show pipelining limited early layers network training stale weights converges results models comparable inference accuracies resulting non pipelined training mnist cifar datasets drop accuracy networks respectively however pipelining deeper network inference accuracies drop significantly propose combining pipelined non pipelined training hybrid scheme address drop demonstrate implementation performance pipelined backpropagation pytorch gpus using resnet achieving speedups gpu baseline small drop inference accuracy\n",
            "output sentence:  accelerating cnn training pipeline accelerators stale weights \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.8235294117647058, 'f': 0.2772277199725517}, 'rouge-2': {'r': 0.05172413793103448, 'p': 0.375, 'f': 0.0909090887786961}, 'rouge-l': {'r': 0.13095238095238096, 'p': 0.6470588235294118, 'f': 0.21782177937849234}}\n",
            "pair:  algorithm introduced learning predictive state representation policy temporal difference td learning used learn steer vehicle reinforcement learning three components learned simultaneously policy predictions compact representation state behavior policy distribution estimating policy predictions deterministic policy gradient learning act behavior policy discriminator learned used estimating important sampling ratios needed learn predictive representation policy general value functions gvfs linear deterministic policy gradient method used train agent predictive representations predictions learned three components combined demonstrated evaluated problem steering vehicle images torcs racing simulator environment steering images challenging problem evaluation completed held set tracks never seen training order measure generalization predictions controller experiments show proposed method able steer smoothly navigate many tracks available torcs performance exceeds ddpg using images input approaches performance ideal non vision based kinematics model\n",
            "output sentence:  algorithm learn predictive state representation general value functions policy learning applied problem vision based steering autonomous driving \n",
            "\n",
            "{'rouge-1': {'r': 0.11392405063291139, 'p': 0.75, 'f': 0.19780219551261927}, 'rouge-2': {'r': 0.042105263157894736, 'p': 0.3076923076923077, 'f': 0.07407407195644725}, 'rouge-l': {'r': 0.0759493670886076, 'p': 0.5, 'f': 0.13186812957855334}}\n",
            "pair:  deep reinforcement learning demonstrated increasing capabilities continuous control problems including agents move skill agility environment open problem setting developing good strategies integrating merging policies multiple skills individual skill specialist specific skill associated state distribution extend policy distillation methods continuous action setting leverage technique combine expert policies evaluated domain simulated bipedal locomotion across different classes terrain also introduce input injection method augmenting existing policy network exploit new input features lastly method uses transfer learning assist efficient acquisition new skills combination methods allows policy incrementally augmented new skills compare progressive learning integration via distillation plaid method three alternative baselines\n",
            "output sentence:  continual learning method uses distillation combine expert policies transfer learning accelerate learning new skills \n",
            "\n",
            "{'rouge-1': {'r': 0.14754098360655737, 'p': 0.75, 'f': 0.24657533971852133}, 'rouge-2': {'r': 0.05970149253731343, 'p': 0.36363636363636365, 'f': 0.10256410014135443}, 'rouge-l': {'r': 0.14754098360655737, 'p': 0.75, 'f': 0.24657533971852133}}\n",
            "pair:  paper propose end end deep learning model called efold rna secondary structure prediction effectively take account inherent constraints problem key idea efold directly predict rna base pairing matrix use unrolled constrained programming algorithm building block architecture enforce constraints comprehensive experiments benchmark datasets demonstrate superior performance efold predicts significantly better structures compared previous sota improvement cases scores even larger improvement pseudoknotted structures runs efficient fastest algorithms terms inference time\n",
            "output sentence:  dl model rna secondary structure prediction uses unrolled algorithm architecture enforce constraints \n",
            "\n",
            "{'rouge-1': {'r': 0.1414141414141414, 'p': 0.7368421052631579, 'f': 0.2372881328914105}, 'rouge-2': {'r': 0.07874015748031496, 'p': 0.5263157894736842, 'f': 0.136986299105836}, 'rouge-l': {'r': 0.10101010101010101, 'p': 0.5263157894736842, 'f': 0.16949152272191903}}\n",
            "pair:  prior work multi agent reinforcement learning marl achieves optimal collaboration directly learning policy agent maximize common reward paper aim address different angle particular consider scenarios self interested agents worker agents minds preferences intentions skills etc dictated perform tasks want achieving optimal coordination among agents train super agent manager manage first inferring minds based current past observations initiating contracts assign suitable tasks workers promise reward corresponding bonuses agree work together objective manager maximize overall productivity well minimize payments made workers ad hoc worker teaming train manager propose mind aware multi agent management reinforcement learning rl consists agent modeling policy learning evaluated approach two environments resource collection crafting simulate multi agent management problems various task settings multiple designs worker agents experimental results validated effectiveness approach modeling worker agents minds online achieving optimal ad hoc teaming good generalization fast adaptation\n",
            "output sentence:  propose mind aware multi agent management reinforcement learning rl training manager motivate self interested achieve achieve optimal collaboration assigning assigning \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.9090909090909091, 'f': 0.24999999762812503}, 'rouge-2': {'r': 0.08045977011494253, 'p': 0.6363636363636364, 'f': 0.14285714086422327}, 'rouge-l': {'r': 0.10144927536231885, 'p': 0.6363636363636364, 'f': 0.17499999762812504}}\n",
            "pair:  knowledge based question answering fundamental problem relax assumption answerable questions simple questions compound questions traditional approaches firstly detect topic entity mentioned questions traverse knowledge graph find relations multi hop path answers propose novel approach leverage simple question answerers answer compound questions model consists two parts novel learning decompose agent learns policy decompose compound question simple questions ii three independent simple question answerers classify corresponding relations simple question experiments demonstrate model learns complex rules compositionality stochastic policy benefits simple neural networks achieve state art results webquestions metaqa analyze interpretable decomposition process well generated partitions\n",
            "output sentence:  propose learning decompose agent helps simple question answerers answer compound question knowledge graph \n",
            "\n",
            "{'rouge-1': {'r': 0.0410958904109589, 'p': 0.375, 'f': 0.07407407229385768}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0410958904109589, 'p': 0.375, 'f': 0.07407407229385768}}\n",
            "pair:  federated learning recent advance privacy protection context trusted curator aggregates parameters optimized decentralized fashion multiple clients resulting model distributed back clients ultimately converging joint representative model without explicitly share data however protocol vulnerable differential attacks could originate party contributing federated optimization attack client contribution training information data set revealed analyzing distributed model tackle problem propose algorithm client sided differential privacy preserving federated optimization aim hide clients contributions training balancing trade privacy loss model performance empirical studies suggest given sufficiently large number participating clients proposed procedure maintain client level differential privacy minor cost model performance\n",
            "output sentence:  ensuring models learned federated fashion reveal client participation \n",
            "\n",
            "{'rouge-1': {'r': 0.1896551724137931, 'p': 0.6111111111111112, 'f': 0.2894736805955679}, 'rouge-2': {'r': 0.04, 'p': 0.16666666666666666, 'f': 0.06451612591051004}, 'rouge-l': {'r': 0.10344827586206896, 'p': 0.3333333333333333, 'f': 0.15789473322714692}}\n",
            "pair:  develop end end learned reconstructions lensless mask based cameras including experimental system capturing aligned lensless lensed images training various reconstruction methods explored scale classic iterative approaches based physical imaging model deep learned methods many learned parameters middle ground present several variations unrolled alternating direction method multipliers admm varying numbers learned parameters network structure combines knowledge physical imaging model learned parameters updated data compensate artifacts caused physical approximations unrolled approach faster classic methods produces better reconstruction quality classic deep methods experimental system\n",
            "output sentence:  improve reconstruction time quality experimental mask based lensless imager using end end learning approach incorporates knowledge imaging model imaging \n",
            "\n",
            "{'rouge-1': {'r': 0.15714285714285714, 'p': 0.6875, 'f': 0.255813950459708}, 'rouge-2': {'r': 0.052083333333333336, 'p': 0.29411764705882354, 'f': 0.08849557266504825}, 'rouge-l': {'r': 0.1, 'p': 0.4375, 'f': 0.16279069464575452}}\n",
            "pair:  language style transfer problem migrating content source sentence target style many applications parallel training data available source sentences transferred may arbitrary unknown styles paper present encoder decoder framework problem setting sentence encoded content style latent representations recombining content target style decode sentence aligned target domain adequately constrain encoding decoding functions couple two loss functions first style discrepancy loss enforcing style representation accurately encodes style information guided discrepancy sentence style target style second cycle consistency loss ensures transferred sentence preserve content original sentence disentangled style validate effectiveness proposed model two tasks sentiment modification restaurant reviews dialog response revision romantic style\n",
            "output sentence:  present encoder decoder framework language style transfer allows use non parallel data source data various unknown language styles \n",
            "\n",
            "{'rouge-1': {'r': 0.1282051282051282, 'p': 0.5555555555555556, 'f': 0.20833333028645837}, 'rouge-2': {'r': 0.046511627906976744, 'p': 0.25, 'f': 0.07843136990388322}, 'rouge-l': {'r': 0.1282051282051282, 'p': 0.5555555555555556, 'f': 0.20833333028645837}}\n",
            "pair:  providing transparency ai planning systems crucial success practical applications order create transparent system user must able query explanations outputs argue key underlying principle use causality within planning model argumentation frameworks provide intuitive representation causality paper discuss argumentation aid extracting causalities plans models create explanations\n",
            "output sentence:  argumentation frameworks used represent causality plans models utilized explanations \n",
            "\n",
            "{'rouge-1': {'r': 0.16, 'p': 0.6666666666666666, 'f': 0.2580645130072841}, 'rouge-2': {'r': 0.021739130434782608, 'p': 0.09523809523809523, 'f': 0.03539822706241705}, 'rouge-l': {'r': 0.12, 'p': 0.5, 'f': 0.19354838397502605}}\n",
            "pair:  neural architecture search nas task finding neural architectures automatically recently emerged promising approach unveiling better models human designed ones however success stories vision tasks quite limited text except small language modeling setup paper explore nas text sequences scale first focusing task language translation later extending reading comprehension standard sequence sequence models translation conduct extensive searches recurrent cells attention similarity functions across two translation tasks iwslt english vietnamese wmt german english report challenges performing cell searches well demonstrate initial success attention searches translation improvements strong baselines addition show results attention searches transferable reading comprehension squad dataset\n",
            "output sentence:  explore neural architecture search language tasks recurrent cell search challenging nmt attention mechanism search works result attention search translation transferable comprehension comprehension \n",
            "\n",
            "{'rouge-1': {'r': 0.2545454545454545, 'p': 1.0, 'f': 0.4057970982146608}, 'rouge-2': {'r': 0.17105263157894737, 'p': 1.0, 'f': 0.2921348289660397}, 'rouge-l': {'r': 0.2545454545454545, 'p': 1.0, 'f': 0.4057970982146608}}\n",
            "pair:  present first verification neural network perception tasks produces correct output within specified tolerance every input interest define correctness relative specification identifies state space consisting relevant states world observation process produces neural network inputs states world tiling state input spaces finite number tiles obtaining ground truth bounds state tiles network output bounds input tiles comparing ground truth network output bounds delivers upper bound network output error input interest results two case studies highlight ability technique deliver tight error bounds inputs interest show error bounds vary state input spaces\n",
            "output sentence:  present first verification neural network perception tasks produces correct output within specified tolerance every input \n",
            "\n",
            "{'rouge-1': {'r': 0.10714285714285714, 'p': 0.6666666666666666, 'f': 0.18461538222958582}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.4444444444444444, 'f': 0.1230769206911243}}\n",
            "pair:  dynamical system models including rnns often lack ability adapt sequence generation prediction given context limiting real world application paper show hierarchical multi task dynamical systems mtdss provide direct user control sequence generation via use latent code specifies customization individual data sequence enables style transfer interpolation morphing within generated sequences show mtds improve predictions via latent code interpolation avoid long term performance degradation standard rnn approaches\n",
            "output sentence:  tailoring predictions sequence models ldss rnns via explicit latent code \n",
            "\n",
            "{'rouge-1': {'r': 0.10256410256410256, 'p': 0.6666666666666666, 'f': 0.17777777546666668}, 'rouge-2': {'r': 0.04807692307692308, 'p': 0.4166666666666667, 'f': 0.08620689469678958}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.5, 'f': 0.13333333102222228}}\n",
            "pair:  existing unsupervised video video translation methods fail produce translated videos frame wise realistic semantic information preserving video level consistent work propose novel unsupervised video video translation model model decomposes style content uses specialized encoder decoder structure propagates inter frame information bidirectional recurrent neural network rnn units style content decomposition mechanism enables us achieve long term style consistent video translation results well provides us good interface modality flexible translation addition changing input frames style codes incorporated translation propose video interpolation loss captures temporal information within sequence train building blocks self supervised manner model produce photo realistic spatio temporal consistent translated videos multimodal way subjective objective experimental results validate superiority model existing methods\n",
            "output sentence:  temporally consistent modality flexible unsupervised video video translation framework trained self supervised manner \n",
            "\n",
            "{'rouge-1': {'r': 0.234375, 'p': 0.8823529411764706, 'f': 0.37037036705380283}, 'rouge-2': {'r': 0.06976744186046512, 'p': 0.35294117647058826, 'f': 0.11650485161278165}, 'rouge-l': {'r': 0.140625, 'p': 0.5294117647058824, 'f': 0.22222221890565466}}\n",
            "pair:  machine learning ml models trained differentially private stochastic gradient descent dp sgd much lower utility non private ones mitigate degradation propose dp laplacian smoothing sgd dp lssgd train ml models differential privacy dp guarantees core dp lssgd laplacian smoothing smooths gaussian noise used gaussian mechanism amount noise used gaussian mechanism dp lssgd attains dp guarantee better utility especially scenarios strong dp guarantees practice dp lssgd makes training convex nonconvex ml models stable enables trained models generalize better proposed algorithm simple implement extra computational complexity memory overhead compared dp sgd negligible dp lssgd applicable train large variety ml models including dnns\n",
            "output sentence:  propose differentially private laplacian smoothing stochastic gradient descent train machine learning models better utility maintain privacy privacy guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.14606741573033707, 'p': 0.8125, 'f': 0.24761904503582766}, 'rouge-2': {'r': 0.05714285714285714, 'p': 0.4, 'f': 0.09999999781250005}, 'rouge-l': {'r': 0.1348314606741573, 'p': 0.75, 'f': 0.22857142598820862}}\n",
            "pair:  introduce two approaches conducting efficient bayesian inference stochastic simulators containing nested stochastic sub procedures internal procedures density cannot calculated directly rejection sampling loops resulting class simulators used extensively throughout sciences interpreted probabilistic generative models however drawing inferences poses substantial challenge due inability evaluate even unnormalised density preventing use many standard inference procedures like markov chain monte carlo mcmc address introduce inference algorithms based two step approach first approximates conditional densities individual sub procedures using approximations run mcmc methods full program sub procedures dealt separately lower dimensional overall problem two step process allows isolated thus tractably dealt without placing restrictions overall dimensionality problem demonstrate utility approach simple artificially constructed simulator\n",
            "output sentence:  introduce two approaches efficient scalable inference stochastic simulators density cannot evaluated directly due example rejection sampling loops \n",
            "\n",
            "{'rouge-1': {'r': 0.09859154929577464, 'p': 1.0, 'f': 0.17948717785338592}, 'rouge-2': {'r': 0.0449438202247191, 'p': 0.6666666666666666, 'f': 0.08421052513240998}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.8571428571428571, 'f': 0.15384615221236028}}\n",
            "pair:  paper present neural phrase based machine translation npmt method explicitly models phrase structures output sequences using sleep wake networks swan recently proposed segmentation based sequence modeling method mitigate monotonic alignment requirement swan introduce new layer perform soft local reordering input sequences different existing neural machine translation nmt approaches npmt use attention based decoding mechanisms instead directly outputs phrases sequential order decode linear time experiments show npmt achieves superior performances iwslt german english english german iwslt english vietnamese machine translation tasks compared strong nmt baselines also observe method produces meaningful phrases output languages\n",
            "output sentence:  neural phrase based machine translation linear decoding time \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 0.7, 'f': 0.2028985482461668}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.4444444444444444, 'f': 0.10389610183167484}, 'rouge-l': {'r': 0.1016949152542373, 'p': 0.6, 'f': 0.17391304099979}}\n",
            "pair:  present new approach defining sequence loss function train summarizer using secondary encoder decoder loss function alleviating shortcoming word level training sequence outputs technique based intuition summary good one contain essential information original article therefore good input sequence lieu original summary generated present experimental results apply additional loss function general abstractive summarizer news summarization dataset result improvement rouge metric especially large improvement human evaluations suggesting enhanced performance competitive specialized state art models\n",
            "output sentence:  present use secondary encoder decoder loss function help train summarizer \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.8571428571428571, 'f': 0.23529411527873895}, 'rouge-2': {'r': 0.0784313725490196, 'p': 0.6666666666666666, 'f': 0.14035087530932594}, 'rouge-l': {'r': 0.11363636363636363, 'p': 0.7142857142857143, 'f': 0.19607842900422914}}\n",
            "pair:  two main families reinforcement learning algorithms learning policy gradients recently proven equivalent using softmax relaxation one part entropic regularization relate result well known convex duality shannon entropy softmax function result also known donsker varadhan formula provides short proof equivalence interpret duality use ideas convex analysis prove new policy inequality relative soft learning\n",
            "output sentence:  short proof equivalence soft learning policy gradients \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.75, 'f': 0.17142856940408163}, 'rouge-2': {'r': 0.024, 'p': 0.2727272727272727, 'f': 0.04411764557201562}, 'rouge-l': {'r': 0.0967741935483871, 'p': 0.75, 'f': 0.17142856940408163}}\n",
            "pair:  based observation exists dramatic drop singular values fully connected layers single feature map convolutional layer dimension concatenated feature vector almost equals summation dimension feature map propose singular value decomposition svd based approach estimate dimension deep manifolds typical convolutional neural network vgg choose three categories imagenet namely persian cat container ship volcano determine local dimension deep manifolds deep layers tangent space target image several augmentation methods found gaussian noise method closer intrinsic dimension adding random noise image moving arbitrary dimension rank feature matrix augmented images increase close local dimension manifold also estimate dimension deep manifold based tangent space maxpooling layers results show dimensions different categories close decline quickly along convolutional layers fully connected layers furthermore show dimensions decline quickly inside conv layer work provides new insights intrinsic structure deep neural networks helps unveiling inner organization black box deep neural networks\n",
            "output sentence:  propose svd based method explore local dimension activation manifold deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.0594059405940594, 'p': 0.6, 'f': 0.10810810646863081}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0297029702970297, 'p': 0.3, 'f': 0.054054052414576795}}\n",
            "pair:  despite impressive performance deep neural networks dnns numerous learning tasks still exhibit uncouth behaviours one puzzling behaviour subtle sensitive reaction dnns various noise attacks nuisance strengthened line research around developing training noise robust networks work propose new training regularizer aims minimize probabilistic expected training loss dnn subject generic gaussian input provide efficient simple approach approximate regularizer arbitrarily deep networks done leveraging analytic expression output mean shallow neural network avoiding need memory computation expensive data augmentation conduct extensive experiments lenet alexnet various datasets including mnist cifar cifar demonstrate effectiveness proposed regularizer particular show networks trained proposed regularizer benefit boost robustness gaussian noise equivalent amount performing folds noisy data augmentation moreover empirically show several architectures datasets improving robustness gaussian noise using new regularizer improve overall robustness types attacks two orders magnitude\n",
            "output sentence:  efficient estimate gaussian first moment dnns regularizer training robust networks \n",
            "\n",
            "{'rouge-1': {'r': 0.05504587155963303, 'p': 0.3333333333333333, 'f': 0.09448818654349316}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.027522935779816515, 'p': 0.16666666666666666, 'f': 0.04724409205530424}}\n",
            "pair:  ability design biological structures dna proteins would considerable medical industrial impact presents challenging black box optimization problem characterized large batch low round setting due need labor intensive wet lab evaluations response propose using reinforcement learning rl based proximal policy optimization ppo biological sequence design rl provides flexible framework optimization generative sequence models achieve specific criteria diversity among high quality sequences discovered propose model based variant ppo dyna ppo improve sample efficiency policy new round trained offline using simulator fit functional measurements prior rounds accommodate growing number observations across rounds simulator model automatically selected round pool diverse models varying capacity tasks designing dna transcription factor binding sites designing antimicrobial proteins optimizing energy ising models based protein structure find dyna ppo performs significantly better existing methods settings modeling feasible still performing worse situations reliable model cannot learned\n",
            "output sentence:  augment model free policy learning sequence level surrogate reward functions count based visitation bonus demonstrate effectiveness large batch low \n",
            "\n",
            "{'rouge-1': {'r': 0.12987012987012986, 'p': 0.7142857142857143, 'f': 0.2197802171766695}, 'rouge-2': {'r': 0.0380952380952381, 'p': 0.3076923076923077, 'f': 0.0677966082088481}, 'rouge-l': {'r': 0.11688311688311688, 'p': 0.6428571428571429, 'f': 0.19780219519864753}}\n",
            "pair:  ever increasing size modern datasets combined difficulty obtaining label information made semi supervised learning significant practical importance modern machine learning applications comparison supervised learning key difficulty semi supervised learning make full use unlabeled data order utilize manifold information provided unlabeled data propose novel regularization called tangent normal adversarial regularization composed two parts two parts complement jointly enforce smoothness along two different directions crucial semi supervised learning one applied along tangent space data manifold aiming enforce local invariance classifier manifold performed normal space orthogonal tangent space intending impose robustness classifier noise causing observed data deviating underlying data manifold two regularizers achieved strategy virtual adversarial training method achieved state art performance semi supervised learning tasks artificial dataset practical datasets\n",
            "output sentence:  propose novel manifold regularization strategy based adversarial training significantly improve performance semi supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.05333333333333334, 'p': 1.0, 'f': 0.10126582182342576}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.25, 'f': 0.02040816248229907}, 'rouge-l': {'r': 0.05333333333333334, 'p': 1.0, 'f': 0.10126582182342576}}\n",
            "pair:  reparameterization trick become one useful tools field variational inference however reparameterization trick based standardization transformation restricts scope application method distributions tractable inverse cumulative distribution functions expressible deterministic transformations distributions paper generalized reparameterization trick allowing general transformation unlike similar works develop generalized transformation based gradient model formally rigorously discover proposed model special case control variate indicating proposed model combine advantages cv generalized reparameterization based proposed gradient model propose new polynomial based gradient estimator better theoretical performance reparameterization trick certain condition applied larger class variational distributions studies synthetic real data show proposed gradient estimator significantly lower gradient variance state art methods thus enabling faster inference procedure\n",
            "output sentence:  generalized transformation generalized transformation model variational model \n",
            "\n",
            "{'rouge-1': {'r': 0.16901408450704225, 'p': 0.8, 'f': 0.2790697645619254}, 'rouge-2': {'r': 0.13253012048192772, 'p': 0.7857142857142857, 'f': 0.2268041212413647}, 'rouge-l': {'r': 0.14084507042253522, 'p': 0.6666666666666666, 'f': 0.23255813665494862}}\n",
            "pair:  social dilemmas situations individuals face temptation increase payoffs cost total welfare building artificially intelligent agents achieve good outcomes situations important many real world interactions include tension selfish interests welfare others show modify modern reinforcement learning methods construct agents act ways simple understand nice begin cooperating provokable try avoid exploited forgiving try return mutual cooperation show theoretically experimentally agents maintain cooperation markov social dilemmas construction require training methods beyond modification self play thus environment good strategies constructed zero sum case eg atari construct agents solve social dilemmas environment\n",
            "output sentence:  build artificial agents solve social dilemmas situations individuals face temptation increase payoffs cost total welfare \n",
            "\n",
            "{'rouge-1': {'r': 0.09523809523809523, 'p': 0.4, 'f': 0.15384615073964505}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.05263157894736842, 'f': 0.017699112246848277}, 'rouge-l': {'r': 0.05952380952380952, 'p': 0.25, 'f': 0.09615384304733737}}\n",
            "pair:  multi agent cooperation important feature natural world many tasks involve individual incentives misaligned common good yet wide range organisms bacteria insects humans able overcome differences collaborate therefore emergence cooperative behavior amongst self interested individuals important question fields multi agent reinforcement learning marl evolutionary theory study particular class multi agent problems called intertemporal social dilemmas isds conflict individual group particularly sharp combining marl appropriately structured natural selection demonstrate individual inductive biases cooperation learned model free way achieve introduce innovative modular architecture deep reinforcement learning agents supports multi level selection present results two challenging environments interpret context cultural ecological evolution\n",
            "output sentence:  introduce biologically inspired modular evolutionary algorithm deep rl agents learn cooperate difficult multi agent social game could help explain altruism \n",
            "\n",
            "{'rouge-1': {'r': 0.09615384615384616, 'p': 0.5, 'f': 0.16129031987513012}, 'rouge-2': {'r': 0.03571428571428571, 'p': 0.2222222222222222, 'f': 0.06153845915266282}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.4, 'f': 0.1290322553590011}}\n",
            "pair:  present new family objective functions term conditional entropy bottleneck ceb objectives motivated minimum necessary information mni criterion demonstrate application ceb classification tasks show ceb gives well calibrated predictions strong detection challenging distribution examples powerful whitebox adversarial examples substantial robustness adversaries finally report ceb fails learn information free datasets providing possible resolution problem generalization observed zhang et al\n",
            "output sentence:  conditional entropy bottleneck information theoretic objective function learning optimal representations \n",
            "\n",
            "{'rouge-1': {'r': 0.047619047619047616, 'p': 0.45454545454545453, 'f': 0.08620689483501787}, 'rouge-2': {'r': 0.007936507936507936, 'p': 0.1, 'f': 0.014705880990484555}, 'rouge-l': {'r': 0.02857142857142857, 'p': 0.2727272727272727, 'f': 0.05172413621432823}}\n",
            "pair:  parameters one critical components machine learning models datasets learning domains change often necessary time consuming learn entire models rather learning parameters scratch replacing learning optimization propose framework building upon theory emph optimal transport adapt model parameters discovering correspondences models data significantly amortizing training cost demonstrate idea challenging problem creating probabilistic spatial representations autonomous robots although recent mapping techniques facilitated robust occupancy mapping learning spatially diverse parameters approximate bayesian models demand considerable computational time discouraging used real world robotic mapping considering fact geometric features robot would observe sensors similar across various environments paper demonstrate use parameters hyperparameters learned different domains adaptation computationally efficient variational inference monte carlo techniques series experiments conducted realistic settings verified possibility transferring thousands parameters negligible time memory cost enabling large scale mapping urban environments\n",
            "output sentence:  present method adapting hyperparameters probabilistic models using optimal transport applications robotics \n",
            "\n",
            "{'rouge-1': {'r': 0.04, 'p': 0.3333333333333333, 'f': 0.07142856951530618}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.125, 'f': 0.019607841691657163}, 'rouge-l': {'r': 0.04, 'p': 0.3333333333333333, 'f': 0.07142856951530618}}\n",
            "pair:  deep learning made remarkable achievement many fields however learning parameters neural networks usually demands large amount labeled data algorithms deep learning therefore encounter difficulties applied supervised learning little data available specific task called shot learning address propose novel algorithm fewshot learning using discrete geometry sense samples class modeled reduced simplex volume simplex used measurement class scatter testing combined test sample points class new simplex formed similarity test sample class quantized ratio volumes new simplex original class simplex moreover present approach constructing simplices using local regions feature maps yielded convolutional neural networks experiments omniglot miniimagenet verify effectiveness simplex algorithm shot learning\n",
            "output sentence:  simplex based geometric method proposed cope shot learning problems \n",
            "\n",
            "{'rouge-1': {'r': 0.15254237288135594, 'p': 0.8181818181818182, 'f': 0.2571428544938776}, 'rouge-2': {'r': 0.05714285714285714, 'p': 0.36363636363636365, 'f': 0.09876542975156231}, 'rouge-l': {'r': 0.11864406779661017, 'p': 0.6363636363636364, 'f': 0.19999999735102042}}\n",
            "pair:  recent research suggests neural machine translation achieves parity professional human translation wmt chinese english news translation task empirically test claim alternative evaluation protocols contrasting evaluation single sentences entire documents pairwise ranking experiment human raters assessing adequacy fluency show stronger preference human machine translation evaluating documents compared isolated sentences findings emphasise need shift towards document level evaluation machine translation improves degree errors hard impossible spot sentence level become decisive discriminating quality different translation outputs\n",
            "output sentence:  raters prefer adequacy human machine translation evaluating entire documents evaluating single sentences \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.5263157894736842, 'f': 0.22727272388688022}, 'rouge-2': {'r': 0.0379746835443038, 'p': 0.15789473684210525, 'f': 0.0612244866701376}, 'rouge-l': {'r': 0.07246376811594203, 'p': 0.2631578947368421, 'f': 0.11363636025051664}}\n",
            "pair:  modern neural networks highly overparameterized capacity substantially overfit training data nevertheless networks often generalize well practice also observed trained networks often compressed much smaller representations purpose paper connect two empirical observations main technical result generalization bound compressed networks based compressed size combined shelf compression algorithms leads state art generalization guarantees particular provide first non vacuous generalization guarantees realistic architectures applied imagenet classification problem additionally show compressibility models tend overfit limited empirical results show increase overfitting increases number bits required describe trained network\n",
            "output sentence:  obtain non vacuous generalization bounds imagenet scale deep neural networks combining original pac bayes bound shelf neural network compression method \n",
            "\n",
            "{'rouge-1': {'r': 0.16363636363636364, 'p': 0.9, 'f': 0.2769230743195267}, 'rouge-2': {'r': 0.11594202898550725, 'p': 0.8888888888888888, 'f': 0.20512820308678503}, 'rouge-l': {'r': 0.16363636363636364, 'p': 0.9, 'f': 0.2769230743195267}}\n",
            "pair:  adversaries neural networks drawn much attention since first debut existing methods aim deceiving image classification models misclassification crafting attacks specific object instances object setection tasks focus creating universal adversaries fool object detectors hide objects detectors adversaries examine universal three ways specific specific object instances image independent transfer different unknown models achieve propose two novel techniques improve transferability adversaries textit piling textit monochromatization techniques prove simplify patterns generated adversaries ultimately result higher transferability\n",
            "output sentence:  focus creating universal adversaries fool object detectors hide objects detectors \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.5714285714285714, 'f': 0.1702127634223631}, 'rouge-2': {'r': 0.031578947368421054, 'p': 0.16666666666666666, 'f': 0.053097342454381836}, 'rouge-l': {'r': 0.0875, 'p': 0.5, 'f': 0.14893616767768222}}\n",
            "pair:  conditional generative adversarial networks cgan led large improvements task conditional image generation lies heart computer vision major focus far performance improvement little effort making cgan robust noise regression generator might lead arbitrarily large errors output makes cgan unreliable real world applications work introduce novel conditional gan model called rocgan leverages structure target space model address issue model augments generator unsupervised pathway promotes outputs generator span target manifold even presence intense noise prove rocgan share similar theoretical properties gan experimentally verify model outperforms existing state art cgan architectures large margin variety domains including images natural scenes faces\n",
            "output sentence:  introduce new type conditional gan aims leverage structure target space generator augment generator new new pathway target target structure \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.8571428571428571, 'f': 0.23529411527873895}, 'rouge-2': {'r': 0.0784313725490196, 'p': 0.6666666666666666, 'f': 0.14035087530932594}, 'rouge-l': {'r': 0.11363636363636363, 'p': 0.7142857142857143, 'f': 0.19607842900422914}}\n",
            "pair:  two main families reinforcement learning algorithms learning policy gradients recently proven equivalent using softmax relaxation one part entropic regularization relate result well known convex duality shannon entropy softmax function result also known donsker varadhan formula provides short proof equivalence interpret duality use ideas convex analysis prove new policy inequality relative soft learning\n",
            "output sentence:  short proof equivalence soft learning policy gradients \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.7058823529411765, 'f': 0.2790697642698756}, 'rouge-2': {'r': 0.0375, 'p': 0.1875, 'f': 0.06249999722222234}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.35294117647058826, 'f': 0.13953488054894544}}\n",
            "pair:  deep learning computer vision depends mainly source supervision photo realistic simulators generate large scale automatically labeled synthetic data introduce domain gap negatively impacting performance propose new unsupervised domain adaptation algorithm called spigan relying simulator privileged information pi generative adversarial networks gan use internal data simulator pi training target task network experimentally evaluate approach semantic segmentation train networks real world cityscapes vistas datasets using unlabeled real world images synthetic labeled data buffer depth pi synthia dataset method improves adaptation state art unsupervised domain adaptation techniques\n",
            "output sentence:  unsupervised sim real domain adaptation method semantic segmentation using privileged information simulator gan based image translation translation \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.4, 'f': 0.18181817830578514}, 'rouge-2': {'r': 0.05063291139240506, 'p': 0.21052631578947367, 'f': 0.08163264993544368}, 'rouge-l': {'r': 0.10294117647058823, 'p': 0.35, 'f': 0.1590909055785125}}\n",
            "pair:  consider simple overarching representation permutation invariant functions sequences set functions approach call janossy pooling expresses permutation invariant function average permutation sensitive function applied reorderings input sequence allows us leverage rich mature literature permutation sensitive functions construct novel flexible permutation invariant functions carried naively janossy pooling computationally prohibitive allow computational tractability consider three kinds approximations canonical orderings sequences functions order interactions stochastic optimization algorithms random permutations framework unifies variety existing work literature suggests possible modeling algorithmic extensions explore experiments demonstrate improved performance current state art methods\n",
            "output sentence:  propose janossy pooling method learning deep permutation invariant functions designed exploit relationships within input sequence tractable inference strategies stochastic optimization \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.75, 'f': 0.17142856940408163}, 'rouge-2': {'r': 0.0125, 'p': 0.125, 'f': 0.022727271074380287}, 'rouge-l': {'r': 0.0967741935483871, 'p': 0.75, 'f': 0.17142856940408163}}\n",
            "pair:  paper establish rigorous benchmarks image classifier robustness first benchmark imagenet standardizes expands corruption robustness topic showing classifiers preferable safety critical applications propose new dataset called imagenet enables researchers benchmark classifier robustness common perturbations unlike recent robustness research benchmark evaluates performance common corruptions perturbations worst case adversarial perturbations find negligible changes relative corruption robustness alexnet classifiers resnet classifiers afterward discover ways enhance corruption perturbation robustness even find bypassed adversarial defense provides substantial common perturbation robustness together benchmarks may aid future work toward networks robustly generalize\n",
            "output sentence:  propose imagenet measure classifier corruption robustness imagenet measure perturbation robustness \n",
            "\n",
            "{'rouge-1': {'r': 0.1864406779661017, 'p': 0.6111111111111112, 'f': 0.28571428213189415}, 'rouge-2': {'r': 0.1, 'p': 0.3888888888888889, 'f': 0.15909090583677693}, 'rouge-l': {'r': 0.1864406779661017, 'p': 0.6111111111111112, 'f': 0.28571428213189415}}\n",
            "pair:  goal survival clustering map subjects users social network patients medical study clusters ranging low risk high risk existing survival methods assume presence clear textit end life signals introduce artificially using pre defined timeout paper forego assumption introduce loss function differentiates empirical lifetime distributions clusters using modified kuiper statistic learn deep neural network optimizing loss performs soft clustering users survival groups apply method social network dataset subjects show significant improvement index compared alternatives\n",
            "output sentence:  goal survival clustering map subjects clusters without end life signals challenging task address task propose new loss function modifying \n",
            "\n",
            "{'rouge-1': {'r': 0.04938271604938271, 'p': 0.3333333333333333, 'f': 0.08602150312868545}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.037037037037037035, 'p': 0.25, 'f': 0.06451612678459945}}\n",
            "pair:  deep neural networks dnns require complex models achieve high performance parameter quantization widely used reducing implementation complexities previous studies quantization mostly based extensive simulation using training data choose different approach attempt measure per parameter capacity dnn models interpret results obtain insights optimum quantization parameters research uses artificially generated data generic forms fully connected dnns convolutional neural networks recurrent neural networks conduct memorization classification tests study effects number precision parameters performance model per parameter capacities assessed measuring mutual information input classified output also extend memorization capacity measurement results image classification language modeling tasks get insight parameter quantization performing real tasks training test performances compared\n",
            "output sentence:  suggest sufficient number bits representing weights dnns optimum bits conservative solving real problems \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5, 'f': 0.19047618739229027}, 'rouge-2': {'r': 0.04918032786885246, 'p': 0.2727272727272727, 'f': 0.08333333074459885}, 'rouge-l': {'r': 0.11764705882352941, 'p': 0.5, 'f': 0.19047618739229027}}\n",
            "pair:  make following striking observation fully convolutional vae models trained imagenet generalize well also far larger photographs changes model use property applying fully convolutional models lossless compression demonstrating method scale vae based bits back ans algorithm lossless compression large color photographs achieving state art compression full size imagenet images release craystack open source library convenient prototyping lossless compression using probabilistic models along full implementations compression results\n",
            "output sentence:  scale lossless compression latent variables beating existing approaches full size imagenet images \n",
            "\n",
            "{'rouge-1': {'r': 0.10606060606060606, 'p': 0.4375, 'f': 0.1707317041760857}, 'rouge-2': {'r': 0.0547945205479452, 'p': 0.26666666666666666, 'f': 0.09090908808109513}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.375, 'f': 0.1463414602736467}}\n",
            "pair:  detection photo manipulation relies subtle statistical traces notoriously removed aggressive lossy compression employed online demonstrate end end modeling complex photo dissemination channels allows codec optimization explicit provenance objectives design lightweight trainable lossy image codec delivers competitive rate distortion performance par best hand engineered alternatives lower computational footprint modern gpu enabled platforms results show significant improvements manipulation detection accuracy possible fractional costs bandwidth storage codec improved accuracy even low bit rates well practicality jpeg qf\n",
            "output sentence:  learn efficient lossy image codec optimized facilitate reliable photo manipulation detection fractional cost payload quality even low \n",
            "\n",
            "{'rouge-1': {'r': 0.0673076923076923, 'p': 0.5833333333333334, 'f': 0.1206896533174792}, 'rouge-2': {'r': 0.007575757575757576, 'p': 0.07692307692307693, 'f': 0.013793101815933607}, 'rouge-l': {'r': 0.04807692307692308, 'p': 0.4166666666666667, 'f': 0.08620689469678958}}\n",
            "pair:  recent advances cross lingual word embeddings primarily relied mapping based methods project pretrained word embeddings different languages shared space linear transformation however approaches assume word embedding spaces isomorphic different languages shown hold practice gaard et al fundamentally limits performance motivates investigating joint learning methods overcome impediment simultaneously learning embeddings across languages via cross lingual term training objective given abundance parallel data available tiedemann propose bilingual extension cbow method leverages sentence aligned corpora obtain robust cross lingual word sentence representations approach significantly improves cross lingual sentence retrieval performance approaches well convincingly outscores mapping methods maintaining parity jointly trained methods word translation also achieves parity deep rnn method zero shot cross lingual document classification task requiring far fewer computational resources training inference additional advantage bilingual method also improves quality monolingual word vectors despite training much smaller datasets make code models publicly available\n",
            "output sentence:  joint method learning cross lingual embeddings state art performance cross lingual tasks mono lingual quality \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.6666666666666666, 'f': 0.23529411474048442}, 'rouge-2': {'r': 0.04040404040404041, 'p': 0.23529411764705882, 'f': 0.06896551473989308}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.4444444444444444, 'f': 0.15686274219146487}}\n",
            "pair:  hierarchical reinforcement learning promising approach long horizon decision making problems sparse rewards unfortunately methods still decouple lower level skill acquisition process training higher level controls skills new task treating skills fixed lead significant sub optimality transfer setting work propose novel algorithm discover set skills continuously adapt along higher level even training new task main contributions two fold first derive new hierarchical policy gradient well unbiased latent dependent baseline introduce hierarchical proximal policy optimization hippo policy method efficiently train levels hierarchy simultaneously second propose method training time abstractions improves robustness obtained skills environment changes code results available sites google com view hippo rl\n",
            "output sentence:  propose hippo stable hierarchical reinforcement learning algorithm train several levels hierarchy simultaneously giving good performance skill discovery adaptation \n",
            "\n",
            "{'rouge-1': {'r': 0.017543859649122806, 'p': 0.1111111111111111, 'f': 0.030303027947658587}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.017543859649122806, 'p': 0.1111111111111111, 'f': 0.030303027947658587}}\n",
            "pair:  significant advances made natural language processing nlp modelling since beginning new approaches allow accurate results even little labelled data nlp models benefit training task agnostic task specific unlabelled data however advantages come significant size computational costs workshop paper outlines proposed convolutional student architecture trained distillation process large scale model achieve inference speedup reduction parameter count cases student model performance surpasses teacher studied tasks\n",
            "output sentence:  train small efficient cnn performance openai transformer text classification tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.8, 'f': 0.11594202764125185}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.25, 'f': 0.020833332534722252}, 'rouge-l': {'r': 0.03125, 'p': 0.4, 'f': 0.05797101314849825}}\n",
            "pair:  colored graphs node classes often associated either neighbors class information incorporated graph associated node propose node classes also associated topological features nodes use association improve graph machine learning general specifically graph convolutional networks gcn first show even absence external information nodes good accuracy obtained prediction node class using either topological features using neighbors class input gcn accuracy slightly less one obtained using content based gcn secondly show explicitly adding topology input gcn improve accuracy combined external information nodes however adding additional adjacency matrix edges distant nodes similar topology gcn significantly improve accuracy leading results better state art methods multiple datasets\n",
            "output sentence:  topology based graph convolutional network gcn \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.5, 'f': 0.10526315601108036}, 'rouge-2': {'r': 0.010309278350515464, 'p': 0.1, 'f': 0.01869158709057575}, 'rouge-l': {'r': 0.047058823529411764, 'p': 0.4, 'f': 0.084210524432133}}\n",
            "pair:  machine learning models achieve human comparable performance sequential data exploiting structured knowledge still challenging problem spatio temporal graphs proved useful tool abstract interaction graphs previous works exploits carefully designed feed forward architecture preserve structure argue scale network design real world problem model needs automatically learn meaningful representation possible relations learning interaction structure trivial one hand model discover hidden relations different problem factors unsupervised way hand mined relations interpretable paper propose attention module able project graph sub structure fixed size embedding preserving influence neighbours exert given vertex comprehensive evaluation done real world well toy task found model competitive strong baselines\n",
            "output sentence:  graph neural network able automatically learn leverage dynamic interactive graph structure \n",
            "\n",
            "{'rouge-1': {'r': 0.14457831325301204, 'p': 0.631578947368421, 'f': 0.23529411461553248}, 'rouge-2': {'r': 0.06, 'p': 0.3333333333333333, 'f': 0.1016949126687734}, 'rouge-l': {'r': 0.13253012048192772, 'p': 0.5789473684210527, 'f': 0.21568627147827762}}\n",
            "pair:  transfer adaptation new unknown environmental dynamics key challenge reinforcement learning rl even greater challenge performing near optimally single attempt test time possibly without access dense rewards addressed current methods require multiple experience rollouts adaptation achieve single episode transfer family environments related dynamics propose general algorithm optimizes probe inference model rapidly estimate underlying latent variables test dynamics immediately used input universal control policy modular approach enables integration state art algorithms variational inference rl moreover approach require access rewards test time allowing perform settings existing adaptive approaches cannot diverse experimental domains single episode test constraint method significantly outperforms existing adaptive approaches shows favorable performance baselines robust transfer\n",
            "output sentence:  single episode policy transfer family environments related dynamics via optimized probing rapid inference latent variables immediate execution universal policy \n",
            "\n",
            "{'rouge-1': {'r': 0.0379746835443038, 'p': 0.6, 'f': 0.07142857030895693}, 'rouge-2': {'r': 0.019417475728155338, 'p': 0.5, 'f': 0.03738317685037995}, 'rouge-l': {'r': 0.0379746835443038, 'p': 0.6, 'f': 0.07142857030895693}}\n",
            "pair:  adam shown able converge optimal solution certain cases researchers recently propose several algorithms avoid issue non convergence adam efficiency turns unsatisfactory practice paper provide new insight non convergence issue adam well adaptive learning rate methods argue exists inappropriate correlation gradient second moment term adam timestep results large gradient likely small step size small gradient may large step size demonstrate unbalanced step sizes fundamental cause non convergence adam prove decorrelating lead unbiased step size gradient thus solving non convergence problem adam finally propose adashift novel adaptive learning rate method decorrelates temporal shifting using temporally shifted gradient calculate experiment results demonstrate adashift able address non convergence issue adam still maintaining competitive performance adam terms training speed generalization\n",
            "output sentence:  analysis solve non convergence issue adam \n",
            "\n",
            "{'rouge-1': {'r': 0.1511627906976744, 'p': 0.9285714285714286, 'f': 0.25999999759200004}, 'rouge-2': {'r': 0.10810810810810811, 'p': 0.9230769230769231, 'f': 0.19354838521982312}, 'rouge-l': {'r': 0.1511627906976744, 'p': 0.9285714285714286, 'f': 0.25999999759200004}}\n",
            "pair:  present adversarial exploration strategy simple yet effective imitation learning scheme incentivizes exploration environment without extrinsic reward human demonstration framework consists deep reinforcement learning drl agent inverse dynamics model contesting former collects training samples latter objective maximize error latter latter trained samples collected former generates rewards former fails predict actual action taken former competitive setting drl agent learns generate samples inverse dynamics model fails predict correctly inverse dynamics model learns adapt challenging samples propose reward structure ensures drl agent collects moderately hard samples overly hard ones prevent inverse model imitating effectively evaluate effectiveness method several openai gym robotic arm hand manipulation tasks number baseline models experimental results show method comparable directly trained expert demonstrations superior baselines even without human priors\n",
            "output sentence:  simple yet effective imitation learning scheme incentivizes exploration environment without extrinsic reward human demonstration \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.8571428571428571, 'f': 0.23529411527873895}, 'rouge-2': {'r': 0.037037037037037035, 'p': 0.2857142857142857, 'f': 0.06557376846009144}, 'rouge-l': {'r': 0.11363636363636363, 'p': 0.7142857142857143, 'f': 0.19607842900422914}}\n",
            "pair:  neural linear model simple adaptive bayesian linear regression method recently used number problems ranging bayesian optimization reinforcement learning despite apparent successes settings best knowledge systematic exploration capabilities simple regression tasks work characterize uci datasets popular benchmark bayesian regression models well recently introduced gap datasets better tests distribution uncertainty demonstrate neural linear model simple method shows competitive performance tasks\n",
            "output sentence:  benchmark neural linear model uci uci gap datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.6923076923076923, 'f': 0.2195121924538965}, 'rouge-2': {'r': 0.03529411764705882, 'p': 0.25, 'f': 0.06185566793495597}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.6153846153846154, 'f': 0.19512194855145748}}\n",
            "pair:  structured tabular data commonly used form data industry according kaggle ml ds survey gradient boosting trees support vector machine random forest logistic regression typically used classification tasks tabular data recent work super characters method using two dimensional word embedding achieved state art results text classification tasks showcasing promise new approach paper propose supertml method borrows idea super characters method two dimensional embedding address problem classification tabular data input tabular data features first projected two dimensional embedding like image image fed fine tuned imagenet cnn models classification experimental results shown proposed supertml method achieved state art results large small datasets\n",
            "output sentence:  deep learning structured tabular data machine two using word embedding cnn model imagenet \n",
            "\n",
            "{'rouge-1': {'r': 0.06557377049180328, 'p': 0.4, 'f': 0.1126760539178735}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.1111111111111111, 'f': 0.02631578738573424}, 'rouge-l': {'r': 0.06557377049180328, 'p': 0.4, 'f': 0.1126760539178735}}\n",
            "pair:  propose new perspective adversarial attacks deep reinforcement learning agents main contribution copycat targeted attack able consistently lure agent following outsider policy pre computed therefore fast inferred could thus usable real time scenario show effectiveness atari games novel read setting latter adversary cannot directly modify agent state representation environment attack agent observation perception environment directly modifying agent state would require write access agent inner workings argue assumption strong realistic settings\n",
            "output sentence:  propose new attack taking full control neural policies realistic settings \n",
            "\n",
            "{'rouge-1': {'r': 0.07317073170731707, 'p': 0.42857142857142855, 'f': 0.1249999975086806}, 'rouge-2': {'r': 0.021052631578947368, 'p': 0.15384615384615385, 'f': 0.037037034919410274}, 'rouge-l': {'r': 0.07317073170731707, 'p': 0.42857142857142855, 'f': 0.1249999975086806}}\n",
            "pair:  many partially observable scenarios reinforcement learning rl agents must rely long term memory order learn optimal policy demonstrate using techniques nlp supervised learning fails rl tasks due stochasticity environment exploration utilizing insights limitations traditional memory methods rl propose amrl class models learn better policies greater sample efficiency resilient noisy inputs specifically models use standard memory module summarize short term context aggregate prior states standard model without respect order show provides advantages terms gradient decay signal noise ratio time evaluating minecraft maze environments test long term memory find model improves average return baseline number parameters stronger baseline far parameters\n",
            "output sentence:  deep rl order invariant functions used conjunction standard memory modules improve gradient decay resilience noise \n",
            "\n",
            "{'rouge-1': {'r': 0.07894736842105263, 'p': 0.35294117647058826, 'f': 0.12903225507688756}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.17647058823529413, 'f': 0.0560747636824178}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.23529411764705882, 'f': 0.08602150238871556}}\n",
            "pair:  machine learning models used high stakes decisions predict accurately fairly responsibly fulfill three requirements model must able output reject option say know qualified make prediction work propose learning defer method model defer judgment downstream decision maker human user show learning defer generalizes rejection learning framework two ways considering effect agents decision making process allowing optimization complex objectives propose learning algorithm accounts potential biases held decision makerslater pipeline experiments real world datasets demonstrate learning defer make model accurate also less biased even operated highly biased users show deferring models still greatly improve fairness entire pipeline\n",
            "output sentence:  incorporating ability say know improve fairness classifier without sacrificing much accuracy improvement magnifies classifier insight downstream decision making \n",
            "\n",
            "{'rouge-1': {'r': 0.16071428571428573, 'p': 0.5, 'f': 0.24324323956172395}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.05555555555555555, 'f': 0.023809520442177348}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.4444444444444444, 'f': 0.21621621253469692}}\n",
            "pair:  paper propose new control framework called moving endpoint control restore images corrupted different degradation levels one model proposed control problem contains restoration dynamics modeled rnn moving endpoint essentially terminal time associated dynamics determined policy network call proposed model dynamically unfolding recurrent restorer durr numerical experiments show durr able achieve state art performances blind image denoising jpeg image deblocking furthermore durr well generalize images higher degradation levels included training stage\n",
            "output sentence:  propose novel method handle image degradations different levels learning diffusion terminal time model generalize unseen degradation level different statistic \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.6363636363636364, 'f': 0.23333333033888887}, 'rouge-2': {'r': 0.0196078431372549, 'p': 0.1, 'f': 0.03278688250470327}, 'rouge-l': {'r': 0.10204081632653061, 'p': 0.45454545454545453, 'f': 0.16666666367222227}}\n",
            "pair:  propose vq wav vec learn discrete representations audio segments wav vec style self supervised context prediction task algorithm uses either gumbel softmax online means clustering quantize dense representations discretization enables direct application algorithms nlp community require discrete inputs experiments show bert pre training achieves new state art timit phoneme classification wsj speech recognition\n",
            "output sentence:  learn quantize speech signal apply algorithms requiring discrete inputs audio data bert \n",
            "\n",
            "{'rouge-1': {'r': 0.09574468085106383, 'p': 0.6428571428571429, 'f': 0.1666666644101509}, 'rouge-2': {'r': 0.04, 'p': 0.3125, 'f': 0.07092198380363167}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.5714285714285714, 'f': 0.1481481458916324}}\n",
            "pair:  neural style transfer become popular technique generating images distinct artistic styles using convolutional neural networks recent success image style transfer raised question whether similar methods leveraged alter style musical audio work attempt long time scale high quality audio transfer texture synthesis time domain captures harmonic rhythmic timbral elements related musical style using examples may different lengths musical keys demonstrate ability use randomly initialized convolutional neural networks transfer aspects musical style one piece onto another using different representations audio log magnitude short time fourier transform stft mel spectrogram constant transform spectrogram propose using representations way generating modifying perceptually significant characteristics musical audio content demonstrate representation shortcomings advantages others carefully designing neural network structures complement nature musical audio finally show compelling style transfer examples make use ensemble representations help capture varying desired characteristics audio signals\n",
            "output sentence:  present long time scale musical audio style transfer algorithm synthesizes audio time domain uses time frequency representations audio \n",
            "\n",
            "{'rouge-1': {'r': 0.23529411764705882, 'p': 0.6666666666666666, 'f': 0.3478260831001891}, 'rouge-2': {'r': 0.08928571428571429, 'p': 0.2777777777777778, 'f': 0.1351351314536159}, 'rouge-l': {'r': 0.19607843137254902, 'p': 0.5555555555555556, 'f': 0.2898550686074355}}\n",
            "pair:  strong inductive biases allow children learn fast adaptable ways children use mutual exclusivity bias help disambiguate words map referents assuming object one label need another paper investigate whether standard neural architectures bias demonstrating lack learning assumption moreover show inductive biases poorly matched lifelong learning formulations classification translation demonstrate compelling case designing neural networks reason mutual exclusivity remains open challenge\n",
            "output sentence:  children use mutual exclusivity bias learn new words standard neural nets show opposite bias hindering learning scenarios lifelong learning \n",
            "\n",
            "{'rouge-1': {'r': 0.061855670103092786, 'p': 0.5454545454545454, 'f': 0.11111110928155009}, 'rouge-2': {'r': 0.01652892561983471, 'p': 0.2, 'f': 0.030534349734864002}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.36363636363636365, 'f': 0.07407407224451307}}\n",
            "pair:  distributed optimization essential training large models large datasets multiple approaches proposed reduce communication overhead distributed training synchronizing performing multiple local sgd steps decentralized methods using gossip algorithms decouple communications among workers although methods run faster allreduce based methods use blocking communication every update resulting models may less accurate number updates inspired bmuf method chen huo propose slow momentum slomo framework workers periodically synchronize perform momentum update multiple iterations base optimization algorithm experiments image classification machine translation tasks demonstrate slomo consistently yields improvements optimization generalization performance relative base optimizer even additional overhead amortized many updates slomo runtime par base optimizer provide theoretical convergence guarantees showing slomo converges stationary point smooth non convex losses since bmuf particular instance slomo framework results also correspond first theoretical convergence guarantees bmuf\n",
            "output sentence:  slowmo improves optimization generalization performance communication efficient decentralized algorithms without sacrificing speed \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.4375, 'f': 0.15053763155971792}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.2, 'f': 0.05555555316358035}, 'rouge-l': {'r': 0.07792207792207792, 'p': 0.375, 'f': 0.1290322552156319}}\n",
            "pair:  seen tremendous success image generating models years generating images neural network usually pixel based fundamentally different humans create artwork using brushes imitate human drawing interactions environment agent required allow trials however environment usually non differentiable leading slow convergence massive computation paper try address discrete nature software environment intermediate differentiable simulation present strokenet novel model agent trained upon well crafted neural approximation painting environment approach agent able learn write characters mnist digits faster reinforcement learning approaches unsupervised manner primary contribution neural simulation real world environment furthermore agent trained emulated environment able directly transfer skills real world software\n",
            "output sentence:  strokenet novel architecture agent trained draw strokes differentiable simulation environment could effectively exploit power back propagation \n",
            "\n",
            "{'rouge-1': {'r': 0.078125, 'p': 0.3333333333333333, 'f': 0.12658227540458267}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.046875, 'p': 0.2, 'f': 0.07594936401217767}}\n",
            "pair:  act plan complex environments posit agents mental simulator world three characteristics build abstract state representing condition world form belief represents uncertainty world go beyond simple step step simulation exhibit temporal abstraction motivated absence model satisfying requirements propose td vae generative sequence model learns representations containing explicit beliefs states several steps future rolled directly without single step transitions td vae trained pairs temporally separated time points using analogue temporal difference learning used reinforcement learning\n",
            "output sentence:  generative model temporal data builds online belief state operates latent space jumpy predictions rollouts states \n",
            "\n",
            "{'rouge-1': {'r': 0.14754098360655737, 'p': 0.6923076923076923, 'f': 0.24324324034696862}, 'rouge-2': {'r': 0.057971014492753624, 'p': 0.3333333333333333, 'f': 0.09876542957476002}, 'rouge-l': {'r': 0.09836065573770492, 'p': 0.46153846153846156, 'f': 0.16216215926588756}}\n",
            "pair:  neural network models shown excellent fluency performance applied abstractive summarization many approaches neural abstractive summarization involve introduction significant inductive bias pointer generator architectures coverage partially extractive procedures designed mimic human summarization show possible attain competitive performance instead directly viewing summarization language modeling introduce simple procedure built upon pre trained decoder transformers obtain competitive rouge scores using language modeling loss alone beam search decoding time optimization instead rely efficient nucleus sampling greedy decoding\n",
            "output sentence:  introduce simple procedure repurpose pre trained transformer based language models perform abstractive summarization well \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.625, 'f': 0.19607842872741257}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.2857142857142857, 'f': 0.05797101266960729}, 'rouge-l': {'r': 0.11627906976744186, 'p': 0.625, 'f': 0.19607842872741257}}\n",
            "pair:  recent work shown separation expressive power depth depth neural networks separation results shown constructing functions input distributions function well approximable depth neural network polynomial size cannot well approximated chosen input distribution depth neural network polynomial size results robust require carefully chosen functions well input distributions show similar separation expressive power depth depth sigmoidal neural networks large class input distributions long weights polynomially bounded also show depth sigmoidal neural networks small width small weights well approximated low degree multivariate polynomials\n",
            "output sentence:  depth vs separation sigmoidal neural networks general distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.15584415584415584, 'p': 0.9230769230769231, 'f': 0.26666666419506174}, 'rouge-2': {'r': 0.10101010101010101, 'p': 0.8333333333333334, 'f': 0.1801801782517653}, 'rouge-l': {'r': 0.15584415584415584, 'p': 0.9230769230769231, 'f': 0.26666666419506174}}\n",
            "pair:  paper introduces network architecture solve structure motion sfm problem via feature metric bundle adjustment ba explicitly enforces multi view geometry constraints form feature metric error whole pipeline differentiable network learn suitable features make ba problem tractable furthermore work introduces novel depth parameterization recover dense per pixel depth network first generates several basis depth maps according input image optimizes final depth linear combination basis depth maps via feature metric ba basis depth maps generator also learned via end end training whole system nicely combines domain knowledge hard coded multi view geometry constraints deep learning feature learning basis depth maps learning address challenging dense sfm problem experiments large scale real data prove success proposed method\n",
            "output sentence:  paper introduces network architecture solve structure motion sfm problem via feature bundle adjustment ba \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.7857142857142857, 'f': 0.19469026331584308}, 'rouge-2': {'r': 0.06349206349206349, 'p': 0.6153846153846154, 'f': 0.11510791197350034}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.7857142857142857, 'f': 0.19469026331584308}}\n",
            "pair:  learning semantic correspondence structured data slot value pairs associated texts core problem many downstream nlp applications data text generation recent neural generation methods require use large scale training data however collected data text pairs training usually loosely corresponded texts contain additional contradicted information compare paired input paper propose local global alignment ga framework learn semantic correspondences loosely related data text pairs first local alignment model based multi instance learning applied build semantic correspondences within data text pair global alignment model built top memory guided conditional random field crf layer designed exploit dependencies among alignments entire training corpus memory used integrate alignment clues provided local alignment model therefore capable inducing missing alignments text spans supported imperfect paired input experiments recent restaurant dataset show proposed method improve alignment accuracy product method also applicable induce semantically equivalent training data text pairs neural generation models\n",
            "output sentence:  propose local global alignment framework learn semantic correspondences noisy data text pairs weak supervision \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.6666666666666666, 'f': 0.11764705721453288}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.2, 'f': 0.02564102444115719}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.5, 'f': 0.08823529250865055}}\n",
            "pair:  state art sequence sequence models large scale tasks perform fixed number computations input sequence regardless whether easy hard process paper train transformer models make output predictions different stages network investigate different ways predict much computation required particular sequence unlike dynamic computation universal transformers applies set layers iteratively apply different layers every step adjust amount computation well model capacity iwslt german english translation approach matches accuracy well tuned baseline transformer using less quarter decoder layers\n",
            "output sentence:  sequence model dynamically adjusts amount computation input \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.8, 'f': 0.11594202764125185}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.25, 'f': 0.020833332534722252}, 'rouge-l': {'r': 0.03125, 'p': 0.4, 'f': 0.05797101314849825}}\n",
            "pair:  colored graphs node classes often associated either neighbors class information incorporated graph associated node propose node classes also associated topological features nodes use association improve graph machine learning general specifically graph convolutional networks gcn first show even absence external information nodes good accuracy obtained prediction node class using either topological features using neighbors class input gcn accuracy slightly less one obtained using content based gcn secondly show explicitly adding topology input gcn improve accuracy combined external information nodes however adding additional adjacency matrix edges distant nodes similar topology gcn significantly improve accuracy leading results better state art methods multiple datasets\n",
            "output sentence:  topology based graph convolutional network gcn \n",
            "\n",
            "{'rouge-1': {'r': 0.10909090909090909, 'p': 0.5, 'f': 0.17910447467141904}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.09090909090909091, 'f': 0.02564102321827768}, 'rouge-l': {'r': 0.07272727272727272, 'p': 0.3333333333333333, 'f': 0.11940298213410566}}\n",
            "pair:  open domain dialogue generation gained increasing attention natural language processing comparing methods requires holistic means dialogue evaluation human ratings deemed gold standard human evaluation inefficient costly automated substitute desirable paper propose holistic evaluation metrics capture quality diversity dialogues metrics consists gpt based context coherence sentences dialogue gpt based fluency phrasing gram based diversity responses augmented queries empirical validity metrics demonstrated strong correlation human judgments provide associated code datasets human ratings\n",
            "output sentence:  propose automatic metrics holistically evaluate open dialogue generation strongly correlate human evaluation \n",
            "\n",
            "{'rouge-1': {'r': 0.04878048780487805, 'p': 0.5, 'f': 0.08888888726913582}, 'rouge-2': {'r': 0.00980392156862745, 'p': 0.125, 'f': 0.018181816833057952}, 'rouge-l': {'r': 0.04878048780487805, 'p': 0.5, 'f': 0.08888888726913582}}\n",
            "pair:  many attempts explain trade accuracy adversarial robustness however clear understanding behaviors robust classifier human like robustness argue need consider adversarial robustness varying magnitudes perturbations focusing fixed perturbation threshold need use different method generate adversarially perturbed samples used train robust classifier measure robustness classifiers need prioritize adversarial accuracies different magnitudes introduce lexicographical genuine robustness lgr classifiers combines requirements also suggest candidate oracle classifier called optimal lexicographically genuinely robust classifier olgrc prioritizes accuracy meaningful adversarially perturbed examples generated smaller magnitude perturbations training algorithm estimating olgrc requires lexicographical optimization unlike existing adversarial training methods apply lexicographical optimization neural network utilize gradient episodic memory gem originally developed continual learning preventing catastrophic forgetting\n",
            "output sentence:  try design train classifier whose adversarial robustness resemblance robustness human \n",
            "\n",
            "{'rouge-1': {'r': 0.16923076923076924, 'p': 0.7333333333333333, 'f': 0.27499999695312505}, 'rouge-2': {'r': 0.06329113924050633, 'p': 0.35714285714285715, 'f': 0.10752687916290908}, 'rouge-l': {'r': 0.15384615384615385, 'p': 0.6666666666666666, 'f': 0.24999999695312503}}\n",
            "pair:  propose new learning based approach solve ill posed inverse problems imaging address case ground truth training samples rare problem severely ill posed underlying physics get measurements setting common geophysical imaging remote sensing show case common approach directly learn mapping measured data reconstruction becomes unstable instead propose first learn ensemble simpler mappings data projections unknown image random piecewise constant subspaces combine projections form final reconstruction solving deconvolution like problem show experimentally proposed method robust measurement noise corruptions seen training directly learned inverse\n",
            "output sentence:  solve ill posed inverse problems scarce ground truth examples estimating ensemble random projections model instead model \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 0.8333333333333334, 'f': 0.3225806420395422}, 'rouge-2': {'r': 0.13793103448275862, 'p': 0.7272727272727273, 'f': 0.23188405529090525}, 'rouge-l': {'r': 0.2, 'p': 0.8333333333333334, 'f': 0.3225806420395422}}\n",
            "pair:  spectral embedding popular technique representation graph data several regularization techniques proposed improve quality embedding respect downstream tasks like clustering paper explain simple block model impact complete graph regularization whereby constant added entries adjacency matrix specifically show regularization forces spectral embedding focus largest blocks making representation less sensitive noise outliers illustrate results synthetic real data showing regularization improves standard clustering scores\n",
            "output sentence:  graph regularization forces spectral embedding focus largest clusters making representation less sensitive \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.5, 'f': 0.183908042975294}, 'rouge-2': {'r': 0.010309278350515464, 'p': 0.05555555555555555, 'f': 0.0173913017073728}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.375, 'f': 0.13793103148104116}}\n",
            "pair:  many environments tiny subset states yield high reward cases interactions environment provide relevant learning signal hence may want preferentially train high reward states probable trajectories leading end advocate use textit backtracking model predicts preceding states terminate given high reward state train model starting high value state one estimated high value predicts samples state action tuples may led high value state traces state action pairs refer recall traces sampled backtracking model starting high value state informative terminate good states hence use traces improve policy provide variational interpretation idea practical algorithm backtracking model samples approximate posterior distribution trajectories lead large rewards method improves sample efficiency policy rl algorithms across several environments tasks\n",
            "output sentence:  backward model previous state action given next state used simulate additional trajectories terminating states improves improves rl rl efficiency \n",
            "\n",
            "{'rouge-1': {'r': 0.12, 'p': 0.6, 'f': 0.19999999722222223}, 'rouge-2': {'r': 0.0625, 'p': 0.4444444444444444, 'f': 0.10958903893413402}, 'rouge-l': {'r': 0.12, 'p': 0.6, 'f': 0.19999999722222223}}\n",
            "pair:  study bert language representation model sequence generation model bert encoder multi label text classification task experiment models explore special qualities setting also introduce examine experimentally mixed model ensemble multi label bert sequence generating bert models experiments demonstrated bert based models mixed model particular outperform current baselines several metrics achieving state art results three well studied multi label classification datasets english texts two private yandex taxi datasets russian texts\n",
            "output sentence:  using bert encoder sequential prediction labels multi label text classification task \n",
            "\n",
            "{'rouge-1': {'r': 0.203125, 'p': 0.65, 'f': 0.3095238058956917}, 'rouge-2': {'r': 0.05333333333333334, 'p': 0.21052631578947367, 'f': 0.08510637975328215}, 'rouge-l': {'r': 0.15625, 'p': 0.5, 'f': 0.23809523446712022}}\n",
            "pair:  transforming one probability distribution another powerful tool bayesian inference machine learning prominent examples constrained unconstrained transformations distributions use hamiltonian monte carlo constructing flexible learnable densities normalizing flows present bijectors jl software package transforming distributions implemented julia available github com turinglang bijectors jl package provides flexible composable way implementing transformations distributions without tied computational framework demonstrate use bijectors jl improving variational inference encoding known statistical dependencies variational posterior using normalizing flows providing general approach relaxing mean field assumption usually made variational inference\n",
            "output sentence:  present software framework transforming distributions demonstrate flexibility relaxing mean field assumptions variational inference use coupling flows replicate structure target generative model \n",
            "\n",
            "{'rouge-1': {'r': 0.06172839506172839, 'p': 0.5, 'f': 0.10989010793382442}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.2222222222222222, 'f': 0.03883494986143847}, 'rouge-l': {'r': 0.04938271604938271, 'p': 0.4, 'f': 0.08791208595580248}}\n",
            "pair:  determinantal point processes dpps provide elegant versatile way sample sets items balance point wise quality set wise diversity selected items reason gained prominence many machine learning applications rely subset selection however sampling dpp ground set size costly operation requiring general preprocessing cost nk sampling cost subsets size approach problem introducing dppnets generative deep models produce dpp like samples arbitrary ground sets develop inhibitive attention mechanism based transformer networks captures notion dissimilarity feature vectors show theoretically approximation sensible maintains guarantees inhibition dissimilarity makes dpp powerful unique empirically demonstrate samples model receive high likelihood expensive dpp alternative\n",
            "output sentence:  approximate determinantal point processes neural nets justify model theoretically empirically \n",
            "\n",
            "{'rouge-1': {'r': 0.17894736842105263, 'p': 0.85, 'f': 0.2956521710396976}, 'rouge-2': {'r': 0.07692307692307693, 'p': 0.45, 'f': 0.13138685882039536}, 'rouge-l': {'r': 0.14736842105263157, 'p': 0.7, 'f': 0.24347825799621928}}\n",
            "pair:  ability decompose complex multi object scenes meaningful abstractions like objects fundamental achieve higher level cognition previous approaches unsupervised object oriented scene representation learning either based spatial attention scene mixture approaches limited scalability main obstacle towards modeling real world scenes paper propose generative latent variable model called space provides uni ed probabilistic modeling framework combines best spatial attention scene mixture approaches space explicitly provide factorized object representations foreground objects also decomposing background segments complex morphology previous models good either space also resolves scalability problems previous methods incorporating parallel spatial attention thus applicable scenes large number objects without performance degradations show experiments atari rooms space achieves properties consistently comparison spair iodine genesis results experiments found project website https sites google com view space project page\n",
            "output sentence:  propose generative latent variable model unsupervised scene decomposition provides factorized object representation per foreground object also decomposing background segments complex morphology \n",
            "\n",
            "{'rouge-1': {'r': 0.07317073170731707, 'p': 0.6, 'f': 0.13043478067107753}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.18181818181818182, 'f': 0.036363634563636456}, 'rouge-l': {'r': 0.036585365853658534, 'p': 0.3, 'f': 0.06521738936672974}}\n",
            "pair:  chemical reactions described stepwise redistribution electrons molecules reactions often depicted using arrow pushing diagrams show movement sequence arrows propose electron path prediction model electro learn sequences directly raw reaction data instead predicting product molecules directly reactant molecules one shot learning model electron movement benefits easy chemists interpret incorporating constraints chemistry balanced atom counts reaction naturally encoding sparsity chemical reactions usually involve changes small number atoms reactants design method extract approximate reaction paths dataset atom mapped reaction smiles strings model achieves excellent performance important subset uspto reaction dataset comparing favorably strongest baselines furthermore show model recovers basic knowledge chemistry without explicitly trained\n",
            "output sentence:  generative model reaction prediction learns mechanistic electron steps reaction directly raw reaction data \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.9230769230769231, 'f': 0.2823529385854672}, 'rouge-2': {'r': 0.07368421052631578, 'p': 0.5833333333333334, 'f': 0.13084111950388683}, 'rouge-l': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}}\n",
            "pair:  paper tackle problem detecting samples drawn training distribution distribution ood samples classification many previous studies attempted solve problem regarding samples low classification confidence ood examples using deep neural networks dnns however difficult datasets models low classification ability methods incorrectly regard distribution samples close decision boundary ood samples problem arises approaches use features close output layer disregard uncertainty features therefore propose method extracts uncertainties features layer dnns using reparameterization trick combines experiments method outperforms existing methods large margin achieving state art detection performance several datasets classification models example method increases auroc score prior work densenet cifar tiny imagenet datasets\n",
            "output sentence:  propose method extracts uncertainties features layer dnns combines detecting ood samples solving classification tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.17333333333333334, 'p': 0.8666666666666667, 'f': 0.28888888611111113}, 'rouge-2': {'r': 0.07954545454545454, 'p': 0.4666666666666667, 'f': 0.1359223276086342}, 'rouge-l': {'r': 0.10666666666666667, 'p': 0.5333333333333333, 'f': 0.17777777500000003}}\n",
            "pair:  order alleviate notorious mode collapse phenomenon generative adversarial networks gans propose novel training method gans certain fake samples reconsidered real ones training process strategy reduce gradient value generator receives region gradient exploding happens show theoretical equilibrium generators discriminations actually seldom realized practice results unbalanced generated distribution deviates target one fake datepoints overfit real ones explains non stability gans also prove penalizing difference discriminator outputs considering certain fake datapoints real adjacent real fake sample pairs gradient exploding alleviated accordingly modified gan training method proposed stable training process better generalization experiments different datasets verify theoretical analysis\n",
            "output sentence:  propose novel gan training method considering certain fake samples real alleviate mode collapse stabilize training process \n",
            "\n",
            "{'rouge-1': {'r': 0.08536585365853659, 'p': 0.5384615384615384, 'f': 0.14736841869030476}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04878048780487805, 'p': 0.3076923076923077, 'f': 0.08421052395346268}}\n",
            "pair:  energy based models ebms un normalized models recent successes continuous spaces however successfully applied model text sequences decreasing energy training samples straightforward mining negative samples energy increased difficult part standard gradient based methods readily applicable input high dimensional discrete side step issue generating negatives using pre trained auto regressive language models ebm works nin em residual language model trained discriminate real text text generated auto regressive models investigate generalization ability residual ebms pre requisite using applications extensively analyze generalization task classifying whether input machine human generated natural task given training loss mine negatives overall observe ebms generalize remarkably well changes architecture generators producing negatives however ebms exhibit sensitivity training set used generators\n",
            "output sentence:  residual ebm text whose formulation equivalent discriminating human machine generated text study generalization behavior \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.7, 'f': 0.2692307661242604}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.3, 'f': 0.11538461227810658}}\n",
            "pair:  peripheral nervous system represents input output system brain cuff electrodes implanted peripheral nervous system allow observation control system however data produced electrodes low signal noise ratio complex signal content paper consider analysis neural data recorded vagus nerve animal models develop unsupervised learner based convolutional neural networks able simultaneously de noise cluster regions data signal content\n",
            "output sentence:  unsupervised analysis data recorded peripheral nervous system denoises categorises signals \n",
            "\n",
            "{'rouge-1': {'r': 0.078125, 'p': 0.35714285714285715, 'f': 0.12820512525969763}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.046875, 'p': 0.21428571428571427, 'f': 0.07692307397764639}}\n",
            "pair:  act plan complex environments posit agents mental simulator world three characteristics build abstract state representing condition world form belief represents uncertainty world go beyond simple step step simulation exhibit temporal abstraction motivated absence model satisfying requirements propose td vae generative sequence model learns representations containing explicit beliefs states several steps future rolled directly without single step transitions td vae trained pairs temporally separated time points using analogue temporal difference learning used reinforcement learning\n",
            "output sentence:  generative model temporal data builds online belief state operates latent space predictions rollouts states \n",
            "\n",
            "{'rouge-1': {'r': 0.0784313725490196, 'p': 0.5333333333333333, 'f': 0.1367521345167653}, 'rouge-2': {'r': 0.024793388429752067, 'p': 0.2, 'f': 0.04411764509623711}, 'rouge-l': {'r': 0.049019607843137254, 'p': 0.3333333333333333, 'f': 0.08547008323471406}}\n",
            "pair:  present simple nearest neighbor nn approach synthesizes high frequency photorealistic images incomplete signal low resolution image surface normal map edges current state art deep generative models designed conditional image synthesis lack two important things unable generate large set diverse outputs due mode collapse problem interpretable making difficult control synthesized output demonstrate nn approaches potentially address limitations suffer accuracy small datasets design simple pipeline combines best worlds first stage uses convolutional neural network cnn map input overly smoothed image second stage uses pixel wise nearest neighbor method map smoothed output multiple high quality high frequency outputs controllable manner importantly pixel wise matching allows method compose novel high frequency content cutting pasting pixels different training exemplars demonstrate approach various input modalities various domains ranging human faces pets shoes handbags\n",
            "output sentence:  pixel wise nearest neighbors used generating multiple images incomplete priors low res images surface normals edges etc \n",
            "\n",
            "{'rouge-1': {'r': 0.14583333333333334, 'p': 0.9333333333333333, 'f': 0.25225224991477974}, 'rouge-2': {'r': 0.0990990990990991, 'p': 0.7857142857142857, 'f': 0.17599999801088}, 'rouge-l': {'r': 0.14583333333333334, 'p': 0.9333333333333333, 'f': 0.25225224991477974}}\n",
            "pair:  paper proposes use spectral element methods citep canuto spectral fast accurate training neural ordinary differential equations ode nets citealp chen neuralod system identification achieved expressing dynamics truncated series legendre polynomials series coefficients well network weights computed minimizing weighted sum loss function violation ode net dynamics problem solved coordinate descent alternately minimizes respect coefficients weights two unconstrained sub problems using standard backpropagation gradient methods resulting optimization scheme fully time parallel results low memory footprint experimental comparison standard methods backpropagation explicit solvers adjoint technique citep chen neuralod training surrogate models small medium scale dynamical systems shows least one order magnitude faster reaching comparable value loss function corresponding testing mse one order magnitude smaller well suggesting generalization capabilities increase\n",
            "output sentence:  paper proposes use spectral element methods fast accurate training neural ordinary differential equations system identification \n",
            "\n",
            "{'rouge-1': {'r': 0.08695652173913043, 'p': 0.5454545454545454, 'f': 0.14999999762812505}, 'rouge-2': {'r': 0.0449438202247191, 'p': 0.4, 'f': 0.08080807899193965}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.5454545454545454, 'f': 0.14999999762812505}}\n",
            "pair:  normalising flows nfs class likelihood based generative models recently gained popularity based idea transforming simple density data seek better understand class models compare previously proposed techniques generative modeling unsupervised representation learning purpose reinterpret nfs framework variational autoencoders vaes present new form vae generalises normalising flows new generalised model also reveals close connection denoising autoencoders therefore call model variational denoising autoencoder vdae using unified model systematically examine model space flows variational autoencoders denoising autoencoders set preliminary experiments mnist handwritten digits experiments shed light modeling assumptions implicit models suggest multiple new directions future research space\n",
            "output sentence:  explore relationship normalising flows variational denoising autoencoders propose novel model generalises \n",
            "\n",
            "{'rouge-1': {'r': 0.18032786885245902, 'p': 0.6111111111111112, 'f': 0.27848100913956103}, 'rouge-2': {'r': 0.07042253521126761, 'p': 0.29411764705882354, 'f': 0.11363636051911165}, 'rouge-l': {'r': 0.13114754098360656, 'p': 0.4444444444444444, 'f': 0.20253164205095342}}\n",
            "pair:  reinforcement learning multi agent scenarios important real world applications presents challenges beyond seen single agent settings present actor critic algorithm trains decentralized policies multi agent settings using centrally computed critics share attention mechanism selects relevant information agent every timestep attention mechanism enables effective scalable learning complex multi agent environments compared recent approaches approach applicable cooperative settings shared rewards also individualized reward settings including adversarial settings makes assumptions action spaces agents flexible enough applied multi agent learning problems\n",
            "output sentence:  propose approach learn decentralized policies multi agent settings using attention based critics demonstrate promising results environments complex interactions \n",
            "\n",
            "{'rouge-1': {'r': 0.08620689655172414, 'p': 0.38461538461538464, 'f': 0.14084506743106534}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.08333333333333333, 'f': 0.02531645311969262}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.3076923076923077, 'f': 0.11267605334655831}}\n",
            "pair:  adversarial neural networks solve many important problems data science notoriously difficult train difficulties come fact optimal weights adversarial nets correspond saddle points minimizers loss function alternating stochastic gradient methods typically used problems reliably converge saddle points convergence happen often highly sensitive learning rates propose simple modification stochastic gradient descent stabilizes adversarial networks show theory practice proposed method reliably converges saddle points makes adversarial networks less likely collapse enables faster training larger learning rates\n",
            "output sentence:  present simple modification alternating sgd method called prediction step improves stability adversarial networks \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.6923076923076923, 'f': 0.1607142836623087}, 'rouge-2': {'r': 0.025210084033613446, 'p': 0.25, 'f': 0.04580152505331863}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.46153846153846156, 'f': 0.10714285509088013}}\n",
            "pair:  architecture search aims automatically finding neural architectures competitive architectures designed human experts recent approaches achieved state art predictive performance image recognition problematic resource constraints two reasons neural architectures found solely optimized high predictive performance without penalizing excessive resource consumption architecture search methods require vast computational resources address first shortcoming proposing lemonade evolutionary algorithm multi objective architecture search allows approximating pareto front architectures multiple objectives predictive performance number parameters single run method address second shortcoming proposing lamarckian inheritance mechanism lemonade generates children networks warmstarted predictive performance trained parents accomplished using approximate network morphism operators generating children combination two contributions allows finding models par even outperform different sized nasnets mobilenets mobilenets wide residual networks cifar imagenet within one week eight gpus less compute power previous architecture search methods yield state art performance\n",
            "output sentence:  propose method efficient multi objective neural architecture search based lamarckian inheritance evolutionary algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.8888888888888888, 'f': 0.22535211046220993}, 'rouge-2': {'r': 0.09722222222222222, 'p': 0.875, 'f': 0.1749999982}, 'rouge-l': {'r': 0.12903225806451613, 'p': 0.8888888888888888, 'f': 0.22535211046220993}}\n",
            "pair:  paper propose nonlinear unsupervised metric learning framework boost performance clustering algorithms framework nonlinear distance metric learning manifold embedding integrated conducted simultaneously increase natural separations among data samples metric learning component implemented feature space transformations regulated nonlinear deformable model called coherent point drifting cpd driven cpd data points get higher level linear separability subsequently picked manifold embedding component generate well separable sample projections clustering experimental results synthetic benchmark datasets show effectiveness proposed approach state art solutions unsupervised metric learning\n",
            "output sentence:  nonlinear unsupervised metric learning framework boost performance clustering algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.5454545454545454, 'f': 0.12765957240153916}, 'rouge-2': {'r': 0.027777777777777776, 'p': 0.2727272727272727, 'f': 0.05042016638937934}, 'rouge-l': {'r': 0.07228915662650602, 'p': 0.5454545454545454, 'f': 0.12765957240153916}}\n",
            "pair:  deep neural networks known annotation hungry numerous efforts devoted reducing annotation cost learning deep networks two prominent directions include learning noisy labels semi supervised learning exploiting unlabeled data work propose dividemix novel framework learning noisy labels leveraging semi supervised learning techniques particular dividemix models per sample loss distribution mixture model dynamically divide training data labeled set clean samples unlabeled set noisy samples trains model labeled unlabeled data semi supervised manner avoid confirmation bias simultaneously train two diverged networks network uses dataset division network semi supervised training phase improve mixmatch strategy performing label co refinement label co guessing labeled unlabeled samples respectively experiments multiple benchmark datasets demonstrate substantial improvements state art methods code available https github com lijunnan dividemix\n",
            "output sentence:  propose novel semi supervised learning approach sota performance combating learning noisy labels \n",
            "\n",
            "{'rouge-1': {'r': 0.17857142857142858, 'p': 0.8823529411764706, 'f': 0.29702970017057156}, 'rouge-2': {'r': 0.12727272727272726, 'p': 0.8235294117647058, 'f': 0.22047243862607727}, 'rouge-l': {'r': 0.17857142857142858, 'p': 0.8823529411764706, 'f': 0.29702970017057156}}\n",
            "pair:  much focus design deep neural networks improving accuracy leading powerful yet highly complex network architectures difficult deploy practical scenarios result recent interest design quantitative metrics evaluating deep neural networks accounts model accuracy sole indicator network performance study continue conversation towards universal metrics evaluating performance deep neural networks practical device edge usage introducing netscore new metric designed specifically provide quantitative assessment balance accuracy computational complexity network architecture complexity deep neural network one largest comparative analysis deep neural networks literature netscore metric top accuracy metric popular information density metric compared across diverse set different deep convolutional neural networks image classification imagenet large scale visual recognition challenge ilsvrc dataset evaluation results across three metrics diverse set networks presented study act reference guide practitioners field\n",
            "output sentence:  introduce netscore new metric designed provide quantitative assessment balance accuracy computational complexity network architecture complexity deep neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.13157894736842105, 'p': 0.7692307692307693, 'f': 0.224719098628961}, 'rouge-2': {'r': 0.05319148936170213, 'p': 0.38461538461538464, 'f': 0.09345794179054943}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.5384615384615384, 'f': 0.15730336829188238}}\n",
            "pair:  develop reinforcement learning based search assistant assist users set actions sequence interactions enable realize intent approach caters subjective search user seeking digital assets images fundamentally different tasks objective limited search modalities labeled conversational data generally available search tasks training agent human interactions time consuming propose stochastic virtual user impersonates real user used sample user behavior efficiently train agent accelerates bootstrapping agent develop algorithm based context preserving architecture enables agent provide contextual assistance user compare agent learning evaluate performance average rewards state values obtains virtual user validation episodes experiments show agent learns achieve higher rewards better states\n",
            "output sentence:  reinforcement learning based conversational search assistant provides contextual assistance subjective search like digital assets \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.4166666666666667, 'f': 0.1818181784066116}, 'rouge-2': {'r': 0.0392156862745098, 'p': 0.16666666666666666, 'f': 0.06349206040816341}, 'rouge-l': {'r': 0.09302325581395349, 'p': 0.3333333333333333, 'f': 0.14545454204297528}}\n",
            "pair:  introduce simple efficient algorithms computing minhash probability distribution suitable sparse dense data equivalent running times state art cases collision probability algorithms new measure similarity positive vectors investigate detail describe sense collision probability optimal locality sensitive hash based sampling argue similarity measure useful probability distributions similarity pursued algorithms weighted minhash natural generalization jaccard index\n",
            "output sentence:  minimum set exponentially distributed hashes useful collision probability generalizes jaccard index probability distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.14772727272727273, 'p': 0.7647058823529411, 'f': 0.24761904490521544}, 'rouge-2': {'r': 0.05357142857142857, 'p': 0.375, 'f': 0.09374999781250005}, 'rouge-l': {'r': 0.10227272727272728, 'p': 0.5294117647058824, 'f': 0.17142856871473924}}\n",
            "pair:  neural networks could misclassify inputs slightly different training data indicates small margin decision boundaries training dataset work study binary classification linearly separable datasets show linear classifiers could also decision boundaries lie close training dataset cross entropy loss used training particular show features training dataset lie low dimensional affine subspace cross entropy loss minimized using gradient method margin training points decision boundary could much smaller optimal value result contrary conclusions recent related works soudry et al identify reason contradiction order improve margin introduce differential training training paradigm uses loss function defined pairs points class show decision boundary linear classifier trained differential training indeed achieves maximum margin results reveal use cross entropy loss one hidden culprits adversarial examples introduces new direction make neural networks robust\n",
            "output sentence:  show minimizing cross entropy loss using gradient method could lead poor margin features lie low dimensional subspace \n",
            "\n",
            "{'rouge-1': {'r': 0.21621621621621623, 'p': 1.0, 'f': 0.3555555526320988}, 'rouge-2': {'r': 0.1744186046511628, 'p': 0.9375, 'f': 0.2941176444136871}, 'rouge-l': {'r': 0.21621621621621623, 'p': 1.0, 'f': 0.3555555526320988}}\n",
            "pair:  introduce novel end end approach learning cluster absence labeled examples clustering objective based optimizing normalized cuts criterion measures intra cluster similarity well inter cluster dissimilarity define differentiable loss function equivalent expected normalized cuts unlike much work unsupervised deep learning trained model directly outputs final cluster assignments rather embeddings need processing usable approach generalizes unseen datasets across wide variety domains including text image specifically achieve state art results popular unsupervised clustering benchmarks mnist reuters cifar cifar outperforming strongest baselines generalization results superior recent top performing clustering approach ability generalize\n",
            "output sentence:  introduce novel end end approach learning cluster absence labeled examples define differentiable loss function equivalent expected normalized cuts \n",
            "\n",
            "{'rouge-1': {'r': 0.10112359550561797, 'p': 0.6, 'f': 0.173076920608358}, 'rouge-2': {'r': 0.02654867256637168, 'p': 0.2, 'f': 0.046874997930908296}, 'rouge-l': {'r': 0.06741573033707865, 'p': 0.4, 'f': 0.11538461291605034}}\n",
            "pair:  recent studies show widely used deep neural networks dnns vulnerable carefully crafted adversarial examples many advanced algorithms proposed generate adversarial examples leveraging distance penalizing perturbations different defense methods also explored defend adversarial attacks effectiveness distance metric perceptual quality remains active research area paper instead focus different type perturbation namely spatial transformation opposed manipulating pixel values directly prior works perturbations generated spatial transformation could result large distance measures extensive experiments show spatially transformed adversarial examples perceptually realistic difficult defend existing defense systems potentially provides new direction adversarial example generation design corresponding defenses visualize spatial transformation based perturbation different examples show technique produce realistic adversarial examples smooth image deformation finally visualize attention deep networks different types adversarial examples better understand examples interpreted\n",
            "output sentence:  propose new approach generating adversarial examples based spatial transformation produces perceptually realistic examples compared existing attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.16494845360824742, 'p': 0.7619047619047619, 'f': 0.2711864377520828}, 'rouge-2': {'r': 0.046296296296296294, 'p': 0.25, 'f': 0.07812499736328134}, 'rouge-l': {'r': 0.1134020618556701, 'p': 0.5238095238095238, 'f': 0.18644067504021836}}\n",
            "pair:  sequence prediction models learned example sequences variety training algorithms maximum likelihood learning simple efficient yet suffer compounding error test time reinforcement learning policy gradient addresses issue prohibitively poor exploration efficiency rich set algorithms data noising raml softmax policy gradient also developed different perspectives paper present formalism entropy regularized policy optimization show apparently distinct algorithms including mle reformulated special instances formulation difference characterized reward function two weight hyperparameters unifying interpretation enables us systematically compare algorithms side side gain new insights trade offs algorithm design new perspective also leads improved approach dynamically interpolates among family algorithms learns model scheduled way experiments machine translation text summarization game imitation learning demonstrate superiority proposed approach\n",
            "output sentence:  entropy regularized policy optimization formalism subsumes set sequence prediction learning algorithms new interpolation algorithm improved results text generation game imitation learning \n",
            "\n",
            "{'rouge-1': {'r': 0.15217391304347827, 'p': 1.0, 'f': 0.26415094110359566}, 'rouge-2': {'r': 0.08928571428571429, 'p': 0.8333333333333334, 'f': 0.1612903208324662}, 'rouge-l': {'r': 0.15217391304347827, 'p': 1.0, 'f': 0.26415094110359566}}\n",
            "pair:  consider new variants optimization algorithms algorithms based observation mini batch stochastic gradients consecutive iterations change drastically consequently may predictable inspired similar setting online learning literature called optimistic online learning propose two new optimistic algorithms amsgrad adam respectively exploiting predictability gradients new algorithms combine idea momentum method adaptive gradient method algorithms optimistic online learning leads speed training deep neural nets practice\n",
            "output sentence:  consider new variants optimization algorithms training deep nets \n",
            "\n",
            "{'rouge-1': {'r': 0.04, 'p': 0.3333333333333333, 'f': 0.07142856951530618}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.125, 'f': 0.019607841691657163}, 'rouge-l': {'r': 0.04, 'p': 0.3333333333333333, 'f': 0.07142856951530618}}\n",
            "pair:  deep learning made remarkable achievement many fields however learning parameters neural networks usually demands large amount labeled data algorithms deep learning therefore encounter difficulties applied supervised learning little data available specific task called shot learning address propose novel algorithm fewshot learning using discrete geometry sense samples class modeled reduced simplex volume simplex used measurement class scatter testing combined test sample points class new simplex formed similarity test sample class quantized ratio volumes new simplex original class simplex moreover present approach constructing simplices using local regions feature maps yielded convolutional neural networks experiments omniglot miniimagenet verify effectiveness simplex algorithm shot learning\n",
            "output sentence:  simplex based geometric method proposed cope shot learning problems \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.6153846153846154, 'f': 0.1467889887248548}, 'rouge-2': {'r': 0.017241379310344827, 'p': 0.16666666666666666, 'f': 0.031249998300781342}, 'rouge-l': {'r': 0.052083333333333336, 'p': 0.38461538461538464, 'f': 0.09174311716522184}}\n",
            "pair:  lifelong learning learner presented sequence tasks incrementally building data driven prior may leveraged speed learning new task work investigate efficiency current lifelong approaches terms sample complexity computational memory cost towards end first introduce new realistic evaluation protocol whereby learners observe example hyper parameter selection done small disjoint set tasks used actual learning experience evaluation second introduce new metric measuring quickly learner acquires new skill third propose improved version gem lopez paz ranzato dubbed averaged gem gem enjoys even better performance gem almost computationally memory efficient ewc kirkpatrick et al regularization based methods finally show algorithms including gem learn even quickly provided task descriptors specifying classification tasks consideration experiments several standard lifelong learning benchmarks demonstrate gem best trade accuracy efficiency\n",
            "output sentence:  efficient lifelong learning algorithm provides better trade accuracy time memory complexity compared algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.2375, 'p': 0.9047619047619048, 'f': 0.37623762046858156}, 'rouge-2': {'r': 0.18181818181818182, 'p': 0.7619047619047619, 'f': 0.2935779785405269}, 'rouge-l': {'r': 0.2375, 'p': 0.9047619047619048, 'f': 0.37623762046858156}}\n",
            "pair:  deep neural networks require extensive computing resources efficiently applied embedded devices mobile phones seriously limits applicability address problem propose novel encoding scheme using decompose quantized neural networks qnns multi branch binary networks efficiently implemented bitwise operations xnor bitcount achieve model compression computational acceleration resource saving method achieve speedup memory saving full precision counterparts therefore users easily achieve different encoding precisions arbitrarily according requirements hardware resources mechanism suitable use fpga asic terms data storage computation provides feasible idea smart chips validate effectiveness method large scale image classification imagenet object detection tasks\n",
            "output sentence:  novel encoding scheme using decompose qnns multi branch binary networks used bitwise operations xnor bitcount achieve model compression computational acceleration acceleration computational \n",
            "\n",
            "{'rouge-1': {'r': 0.10714285714285714, 'p': 0.6428571428571429, 'f': 0.18367346693877554}, 'rouge-2': {'r': 0.037037037037037035, 'p': 0.3076923076923077, 'f': 0.06611570056143712}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.42857142857142855, 'f': 0.12244897714285716}}\n",
            "pair:  paper address challenge limited labeled data class imbalance problem machine learning based rumor detection social media present offline data augmentation method based semantic relatedness rumor detection end unlabeled social media data exploited augment limited labeled data context aware neural language model large credibility focused twitter corpus employed learn effective representations rumor tweets semantic relatedness measurement language model fine tuned large domain specific corpus shows dramatic improvement training data augmentation rumor detection pretrained language models conduct experiments six different real world events based five publicly available data sets one augmented data set experiments show proposed method allows us generate larger training data reasonable quality via weak supervision present preliminary results achieved using state art neural network model augmented data rumor detection\n",
            "output sentence:  propose methodology augmenting publicly available data rumor studies based samantic relatedness limited labeled unlabeled data \n",
            "\n",
            "{'rouge-1': {'r': 0.12727272727272726, 'p': 0.3684210526315789, 'f': 0.18918918537253473}, 'rouge-2': {'r': 0.014705882352941176, 'p': 0.05263157894736842, 'f': 0.02298850233320172}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.2631578947368421, 'f': 0.13513513131848076}}\n",
            "pair:  propose simple technique encouraging generative rnns plan ahead train backward recurrent network generate given sequence reverse order encourage states forward model predict cotemporal states backward model backward network used training plays role sampling inference hypothesize approach eases modeling long term dependencies implicitly forcing forward states hold information longer term future contained backward states show empirically approach achieves relative improvement speech recognition task achieves significant improvement coco caption generation task\n",
            "output sentence:  paper introduces method training generative recurrent networks helps plan ahead run second rnn reverse direction make constraint forward forward forward forward cotemporal \n",
            "\n",
            "{'rouge-1': {'r': 0.045454545454545456, 'p': 0.2727272727272727, 'f': 0.0779220754730984}, 'rouge-2': {'r': 0.012658227848101266, 'p': 0.1, 'f': 0.02247190811766209}, 'rouge-l': {'r': 0.030303030303030304, 'p': 0.18181818181818182, 'f': 0.051948049499072474}}\n",
            "pair:  significant strides made toward designing better generative models recent years despite progress however state art approaches still largely unable capture complex global structure data example images buildings typically contain spatial patterns windows repeating regular intervals state art generative methods easily reproduce structures propose address problem incorporating programs representing global structure generative model loop may represent configuration windows furthermore propose framework learning models leveraging program synthesis generate training data synthetic real world data demonstrate approach substantially better state art generating completing images contain global structure\n",
            "output sentence:  applying program synthesis tasks image completion generation within deep learning framework \n",
            "\n",
            "{'rouge-1': {'r': 0.015151515151515152, 'p': 0.14285714285714285, 'f': 0.027397258540063914}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.015151515151515152, 'p': 0.14285714285714285, 'f': 0.027397258540063914}}\n",
            "pair:  paper propose efficient framework accelerate convolutional neural networks utilize two types acceleration methods pruning hints pruning reduce model size removing channels layers hints improve performance student model transferring knowledge teacher model demonstrate pruning hints complementary one hand hints benefit pruning maintaining similar feature representations hand model pruned teacher networks good initialization student model increases transferability two networks approach performs pruning stage hints stage iteratively improve performance furthermore propose algorithm reconstruct parameters hints layer make pruned model suitable hints experiments conducted various tasks including classification pose estimation results cifar imagenet coco demonstrate generalization superiority framework\n",
            "output sentence:  work aiming boosting existing pruning mimic method \n",
            "\n",
            "{'rouge-1': {'r': 0.16363636363636364, 'p': 0.9, 'f': 0.2769230743195267}, 'rouge-2': {'r': 0.039473684210526314, 'p': 0.3, 'f': 0.06976743980530022}, 'rouge-l': {'r': 0.14545454545454545, 'p': 0.8, 'f': 0.24615384355029585}}\n",
            "pair:  propose end end framework training domain specific models dsms obtain high accuracy computational efficiency object detection tasks dsms trained distillation focus achieving high accuracy limited domain fixed view intersection argue dsms capture essential features well even small model size enabling higher accuracy efficiency traditional techniques addition improve training efficiency reducing dataset size culling easy classify images training set limited domain observed compact dsms significantly surpass accuracy coco trained models size training compact dataset show accuracy drop training time reduced\n",
            "output sentence:  high object detection accuracy obtained training domain specific compact models training short \n",
            "\n",
            "{'rouge-1': {'r': 0.06060606060606061, 'p': 0.5454545454545454, 'f': 0.10909090729090913}, 'rouge-2': {'r': 0.03389830508474576, 'p': 0.4, 'f': 0.06249999855957035}, 'rouge-l': {'r': 0.04040404040404041, 'p': 0.36363636363636365, 'f': 0.07272727092727278}}\n",
            "pair:  compressed representations generalize better shamir et al may crucial learning limited noisy labeled data information bottleneck ib method tishby et al provides insightful principled approach balancing compression prediction representation learning ib objective employs lagrange multiplier tune trade however little theoretical guidance select also lack theoretical understanding relationship dataset model capacity learnability work show improperly chosen learning cannot happen trivial representation becomes global minimum ib objective show avoided identifying sharp phase transition unlearnable learnable arises varies phase transition defines concept ib learnability prove several sufficient conditions ib learnability providing theoretical guidance selecting show ib learnability determined largest confident typical imbalanced subset training examples give practical algorithm estimate minimum given dataset test theoretical results synthetic datasets mnist cifar noisy labels make surprising observation accuracy may non monotonic\n",
            "output sentence:  theory predicts phase transition unlearnable learnable values beta information bottleneck objective \n",
            "\n",
            "{'rouge-1': {'r': 0.07777777777777778, 'p': 0.7777777777777778, 'f': 0.14141413976124886}, 'rouge-2': {'r': 0.018691588785046728, 'p': 0.25, 'f': 0.034782607401134265}, 'rouge-l': {'r': 0.03333333333333333, 'p': 0.3333333333333333, 'f': 0.06060605895316809}}\n",
            "pair:  predicting properties nodes graph important problem applications variety domains graph based semi supervised learning ssl methods aim address problem labeling small subset nodes seeds utilizing graph structure predict label scores rest nodes graph recently graph convolutional networks gcns achieved impressive performance graph based ssl task addition label scores also desirable confidence score associated unfortunately confidence estimation context gcn previously explored fill important gap paper propose confgcn estimates labels scores along confidences jointly gcn based setting confgcn uses estimated confidences determine influence one node another neighborhood aggregation thereby acquiring anisotropic capabilities extensive analysis experiments standard benchmarks find confgcn able significantly outperform state art baselines made confgcn source code available encourage reproducible research\n",
            "output sentence:  propose confidence based graph convolutional network semi supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5333333333333333, 'f': 0.19277108137610685}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.26666666666666666, 'f': 0.07766990042416824}, 'rouge-l': {'r': 0.10294117647058823, 'p': 0.4666666666666667, 'f': 0.16867469583393818}}\n",
            "pair:  policy learning task evaluating improving policies using historic data collected logging policy important policy evaluation usually expensive adverse impacts one major challenge policy learning derive counterfactual estimators also low variance thus low generalization error work inspired learning bounds importance sampling problems present new counterfactual learning principle policy learning bandit feedbacks method regularizes generalization error minimizing distribution divergence logging policy new policy removes need iterating training samples compute sample variance regularization prior work neural network policies end end training algorithms using variational divergence minimization showed significant improvement conventional baseline algorithms also consistent theoretical results\n",
            "output sentence:  policy learning bandit feedbacks propose new variance regularized counterfactual learning algorithm theoretical foundations superior empirical performance \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.5454545454545454, 'f': 0.23999999656800006}, 'rouge-2': {'r': 0.031914893617021274, 'p': 0.13043478260869565, 'f': 0.05128204812331088}, 'rouge-l': {'r': 0.10256410256410256, 'p': 0.36363636363636365, 'f': 0.15999999656800007}}\n",
            "pair:  deep reinforcement learning achieved many recent successes understanding strengths limitations hampered lack rich environments fully characterize optimal behavior correspondingly diagnose individual actions characterization consider family combinatorial games arising work erdos selfridge spencer propose use environments evaluating comparing different approaches reinforcement learning games number appealing features challenging current learning approaches form low dimensional simply parametrized environment ii linear closed form solution optimal behavior state iii difficulty game tuned changing environment parameters interpretable way use erdos selfridge spencer games compare different algorithms also compare approaches based supervised reinforcement learning analyze power multi agent approaches improving performance evaluate generalization environments outside training set\n",
            "output sentence:  adapt family combinatorial games tunable difficulty optimal policy expressible linear network developing rich environment reinforcement learning showing contrasts performance supervised learning analyzing learning generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.34146341463414637, 'p': 0.9333333333333333, 'f': 0.49999999607780615}, 'rouge-2': {'r': 0.28, 'p': 0.875, 'f': 0.42424242056932976}, 'rouge-l': {'r': 0.34146341463414637, 'p': 0.9333333333333333, 'f': 0.49999999607780615}}\n",
            "pair:  work presents method active anomaly detection built upon existing deep learning solutions unsupervised anomaly detection show prior needs assumed anomalies order performance guarantees unsupervised anomaly detection argue active anomaly detection practice cost unsupervised anomaly detection possibility much better results solve problem present new layer attached deep learning model designed unsupervised anomaly detection transform active method presenting results synthetic real anomaly detection datasets\n",
            "output sentence:  method active anomaly detection present new layer attached deep learning model designed unsupervised anomaly detection transform active method \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.7, 'f': 0.21212120955004593}, 'rouge-2': {'r': 0.07462686567164178, 'p': 0.5, 'f': 0.12987012761005232}, 'rouge-l': {'r': 0.125, 'p': 0.7, 'f': 0.21212120955004593}}\n",
            "pair:  propose general deep reinforcement learning method apply robot manipulation tasks approach leverages demonstration data assist reinforcement learning agent learning solve wide range tasks mainly previously unsolved train visuomotor policies end end learn direct mapping rgb camera inputs joint velocities experiments indicate reinforcement imitation approach solve contact rich robot manipulation tasks neither state art reinforcement imitation learning method solve alone also illustrate policies achieved zero shot sim real transfer training large visual dynamics variations\n",
            "output sentence:  combine reinforcement learning imitation learning solve complex robot manipulation tasks pixels \n",
            "\n",
            "{'rouge-1': {'r': 0.06976744186046512, 'p': 0.46153846153846156, 'f': 0.12121211893072138}, 'rouge-2': {'r': 0.008771929824561403, 'p': 0.07692307692307693, 'f': 0.01574802965837953}, 'rouge-l': {'r': 0.046511627906976744, 'p': 0.3076923076923077, 'f': 0.08080807852668102}}\n",
            "pair:  reinforcement learning agents typically trained evaluated according performance averaged distribution environment settings distribution environment settings contain important biases lead agents fail certain cases despite high average case performance work consider worst case analysis agents environment settings order detect whether directions agents may failed generalize specifically consider first person task agents must navigate procedurally generated mazes reinforcement learning agents recently achieved human level average case performance optimizing structure mazes find agents suffer catastrophic failures failing find goal even surprisingly simple mazes despite impressive average case performance additionally find failures transfer different agents even significantly different architectures believe findings highlight important role worst case analysis identifying whether directions agents failed generalize hope ability automatically identify failures generalization facilitate development general robust agents end report initial results enriching training settings causing failure\n",
            "output sentence:  find environment settings sota agents trained navigation tasks display extreme failures suggesting failures generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.3333333333333333, 'f': 0.14285713948979603}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.3333333333333333, 'f': 0.14285713948979603}}\n",
            "pair:  study precise mechanisms allow autoencoders encode decode simple geometric shape disk carefully controlled setting able describe specific form optimal solution minimisation problem training step show autoencoder indeed approximates solution training secondly identify clear failure generalisation capacity autoencoder namely inability interpolate data finally explore several regularisation schemes resolve generalisation problem given great attention recently given generative capacity neural networks believe studying depth simple geometric cases sheds light generation process provide minimal requirement experimental setup complex architectures\n",
            "output sentence:  study functioning autoencoders simple setting advise new strategies regularisation order obtain bettre generalisation latent interpolation mind image sythesis \n",
            "\n",
            "{'rouge-1': {'r': 0.03669724770642202, 'p': 0.4444444444444444, 'f': 0.06779660876041371}, 'rouge-2': {'r': 0.0070921985815602835, 'p': 0.1111111111111111, 'f': 0.013333332205333429}, 'rouge-l': {'r': 0.027522935779816515, 'p': 0.3333333333333333, 'f': 0.050847456218040835}}\n",
            "pair:  artificial intelligence ai becomes integral part life development explainable ai embodied decision making process ai robotic agent becomes imperative robotic teammate ability generate explanations explain behavior one key requirements explainable agency prior work explanation generation focuses supporting reasoning behind robot behavior approaches however fail consider mental workload needed understand received explanation words human teammate expected understand explanation provided often task execution matter much information presented explanation work argue explanation especially complex ones made online fashion execution helps spread information explained thus reducing mental workload humans however challenge different parts explanation dependent must taken account generating online explanations end general formulation online explanation generation presented along three different implementations satisfying different online properties base explanation generation method model reconciliation setting introduced prior work approaches evaluated human subjects standard planning competition ipc domain using nasa task load index tlx well simulation ten different problems across two ipc domains\n",
            "output sentence:  introduce online explanation consider cognitive requirement human understanding generated explanation agent \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}}\n",
            "pair:  deep learning nlp represents word single point single mode region semantic space existing multi mode word embeddings cannot represent longer word sequences like phrases sentences introduce phrase representation also applicable sentences phrase distinct set multi mode codebook embeddings capture different semantic facets phrase meaning codebook embeddings viewed cluster centers summarize distribution possibly co occurring words pre trained word embedding space propose end end trainable neural model directly predicts set cluster centers input text sequence phrase sentence test time find per phrase sentence codebook embeddings provide interpretable semantic representation also outperform strong baselines large margin tasks benchmark datasets unsupervised phrase similarity sentence similarity hypernym detection extractive summarization\n",
            "output sentence:  propose unsupervised way learn multiple embeddings sentences phrases \n",
            "\n",
            "{'rouge-1': {'r': 0.030927835051546393, 'p': 0.6, 'f': 0.058823528479431}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.020618556701030927, 'p': 0.4, 'f': 0.03921568534217611}}\n",
            "pair:  auto encoders commonly used unsupervised representation learning pre training deeper neural networks activation function linear encoding dimension width hidden layer smaller input dimension well known auto encoder optimized learn principal components data distribution oja however activation nonlinear width larger input dimension overcomplete auto encoder behaves differently pca fact known perform well empirically sparse coding problems provide theoretical explanation empirically observed phenomenon rectified linear unit relu adopted activation function hidden layer width set large case show significant probability initializing weight matrix auto encoder sampling spherical gaussian distribution followed stochastic gradient descent sgd training converges towards ground truth representation class sparse dictionary learning models addition show conditioning convergence expected convergence rate number updates analysis quantifies increasing hidden layer width helps training performance random initialization used norm network weights influence speed sgd convergence\n",
            "output sentence:  theoretical analysis nonlinear wide autoencoder \n",
            "\n",
            "{'rouge-1': {'r': 0.0875, 'p': 0.7777777777777778, 'f': 0.15730336896856456}, 'rouge-2': {'r': 0.030303030303030304, 'p': 0.375, 'f': 0.05607476497161328}, 'rouge-l': {'r': 0.0875, 'p': 0.7777777777777778, 'f': 0.15730336896856456}}\n",
            "pair:  introduce new routing algorithm capsule networks child capsule routed parent based agreement parent state child vote unlike previously proposed routing algorithms parent ability reconstruct child explicitly taken account update routing probabilities simplifies routing procedure improves performance benchmark datasets cifar cifar new mechanism designs routing via inverted dot product attention imposes layer normalization normalization replaces sequential iterative routing concurrent iterative routing besides outperforming existing capsule networks model performs par powerful cnn resnet using less parameters different task recognizing digits overlayed digit images proposed capsule model performs favorably cnns given number layers neurons per layer believe work raises possibility applying capsule networks complex real world tasks\n",
            "output sentence:  present new routing method capsule networks performs par resnet cifar \n",
            "\n",
            "{'rouge-1': {'r': 0.039603960396039604, 'p': 0.8, 'f': 0.07547169721431114}, 'rouge-2': {'r': 0.007936507936507936, 'p': 0.25, 'f': 0.015384614788165704}, 'rouge-l': {'r': 0.0297029702970297, 'p': 0.6, 'f': 0.05660377268600926}}\n",
            "pair:  state art performances language comprehension tasks achieved huge language models pre trained massive unlabeled text corpora light subsequent fine tuning task specific supervised manner seems pre training procedure learns good common initialization training various natural language understanding tasks steps need taken parameter space learn task work using bidirectional encoder representations transformers bert example verify hypothesis showing task specific fine tuned language models highly close parameter space pre trained one taking advantage observations show fine tuned versions huge models order floating point parameters made computationally efficient first fine tuning fraction critical layers suffices second fine tuning adequately performed learning binary multiplicative mask pre trained weights textit parameter sparsification result single effort achieve three desired outcomes learning perform specific tasks saving memory storing binary masks certain layers task saving compute appropriate hardware performing sparse operations model parameters\n",
            "output sentence:  sparsification fine tuning language models \n",
            "\n",
            "{'rouge-1': {'r': 0.1320754716981132, 'p': 0.5833333333333334, 'f': 0.21538461237396456}, 'rouge-2': {'r': 0.0410958904109589, 'p': 0.25, 'f': 0.07058823286920424}, 'rouge-l': {'r': 0.09433962264150944, 'p': 0.4166666666666667, 'f': 0.15384615083550301}}\n",
            "pair:  flexibly efficiently reason temporal sequences abstract representations compactly represent important information sequence needed one way constructing representations focusing important events sequence paper propose model learns discover key events keyframes well represent sequence terms using hierarchical keyframe inpainter keyin model first generates keyframes temporal placement inpaints sequences keyframes propose fully differentiable formulation efficiently learning keyframe placement show keyin finds informative keyframes several datasets diverse dynamics evaluated planning task keyin outperforms recent proposals learning hierarchical representations\n",
            "output sentence:  propose model learns discover informative frames future video sequence represent video via keyframes \n",
            "\n",
            "{'rouge-1': {'r': 0.05194805194805195, 'p': 0.3333333333333333, 'f': 0.08988763811639951}, 'rouge-2': {'r': 0.023529411764705882, 'p': 0.18181818181818182, 'f': 0.041666664637586906}, 'rouge-l': {'r': 0.05194805194805195, 'p': 0.3333333333333333, 'f': 0.08988763811639951}}\n",
            "pair:  high performance deep learning models typically comes cost considerable model size computation time factors limit applicability deployment memory battery constraint devices mobile phones embedded systems work propose novel pruning technique eliminates entire filters neurons according relative norm compared rest network yielding compression decreased redundancy parameters resulting network non sparse however much compact requires special infrastructure deployment prove viability method achieving compression lenet resnet resnet respectively exceeding state art compression results reported resnet without losing performance compared baseline approach exhibit good performance also easy implement many architectures\n",
            "output sentence:  propose novel structured class blind pruning technique produce highly compressed neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.02666666666666667, 'p': 0.5, 'f': 0.050632910431020695}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02666666666666667, 'p': 0.5, 'f': 0.050632910431020695}}\n",
            "pair:  neural tangents library working infinite width neural networks provides high level api specifying complex hierarchical neural network architectures networks trained evaluated either finite width usual infinite width limit infinite width networks neural tangents performs exact inference either via bayes rule gradient descent generates corresponding neural network gaussian process neural tangent kernels additionally neural tangents provides tools study gradient descent training dynamics wide finite networks entire library runs box cpu gpu tpu computations automatically distributed multiple accelerators near linear scaling number devices addition repository provide accompanying interactive colab notebook https colab sandbox google com github google neural tangents blob master notebooks neural tangents cookbook ipynb\n",
            "output sentence:  keras infinite neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.05063291139240506, 'p': 0.5, 'f': 0.09195402131853615}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05063291139240506, 'p': 0.5, 'f': 0.09195402131853615}}\n",
            "pair:  paper study adversarial attack defence problem deep learning perspective fourier analysis first explicitly compute fourier transform deep relu neural networks show exist decaying non zero high frequency components fourier spectrum neural networks demonstrate vulnerability neural networks towards adversarial samples attributed insignificant non zero high frequency components based analysis propose use simple post averaging technique smooth high frequency components improve robustness neural networks adversarial attacks experimental results imagenet cifar datasets shown proposed method universally effective defend many existing adversarial attacking methods proposed literature including fgsm pgd deepfool attacks post averaging method simple since require training meanwhile successfully defend adversarial samples generated methods without introducing significant performance degradation less original clean images\n",
            "output sentence:  insight reason adversarial vulnerability effective defense method adversarial attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.1978021978021978, 'p': 0.9473684210526315, 'f': 0.32727272441487604}, 'rouge-2': {'r': 0.1391304347826087, 'p': 0.8888888888888888, 'f': 0.24060150141896097}, 'rouge-l': {'r': 0.1978021978021978, 'p': 0.9473684210526315, 'f': 0.32727272441487604}}\n",
            "pair:  work present new agent architecture called reactor combines multiple algorithmic architectural contributions produce agent higher sample efficiency prioritized dueling dqn wang et al categorical dqn bellemare et al giving better run time performance mnih et al first contribution new policy evaluation algorithm called distributional retrace brings multi step policy updates distributional reinforcement learning setting approach used convert several classes multi step policy evaluation algorithms designed expected value evaluation distributional ones next introduce leaveone policy gradient algorithm improves trade variance bias using action values baseline final algorithmic contribution new prioritized replay algorithm sequences exploits temporal locality neighboring observations efficient replay prioritization using atari benchmarks show innovations contribute sample efficiency final agent performance finally demonstrate reactor reaches state art performance million frames less day training\n",
            "output sentence:  reactor combines multiple algorithmic architectural contributions produce agent higher sample efficiency prioritized dueling dqn giving better run time performance \n",
            "\n",
            "{'rouge-1': {'r': 0.13253012048192772, 'p': 0.6875, 'f': 0.2222222195122947}, 'rouge-2': {'r': 0.009615384615384616, 'p': 0.06666666666666667, 'f': 0.016806720485841684}, 'rouge-l': {'r': 0.07228915662650602, 'p': 0.375, 'f': 0.1212121185021937}}\n",
            "pair:  present fasterseg automatically designed semantic segmentation network state art performance also faster speed current methods utilizing neural architecture search nas fasterseg discovered novel broader search space integrating multi resolution branches recently found vital manually designed segmentation models better calibrate balance goals high accuracy low latency propose decoupled fine grained latency regularization effectively overcomes observed phenomenons searched networks prone collapsing low latency yet poor accuracy models moreover seamlessly extend fasterseg new collaborative search co searching framework simultaneously searching teacher student network single run teacher student distillation boosts student model accuracy experiments popular segmentation benchmarks demonstrate competency fasterseg example fasterseg run faster closest manually designed competitor cityscapes maintaining comparable accuracy\n",
            "output sentence:  present real time segmentation model automatically discovered multi scale nas framework achieving faster state art models \n",
            "\n",
            "{'rouge-1': {'r': 0.04285714285714286, 'p': 0.42857142857142855, 'f': 0.07792207626918539}, 'rouge-2': {'r': 0.022988505747126436, 'p': 0.3333333333333333, 'f': 0.043010751481096114}, 'rouge-l': {'r': 0.04285714285714286, 'p': 0.42857142857142855, 'f': 0.07792207626918539}}\n",
            "pair:  supersymmetric artificial neural network deep learning denoted bar tw espouses importance considering biological constraints aim generalizing backward propagation looking progression solution geometries going representation perceptron like models su representation unitaryrnns guaranteed richer richer representations weight space artificial neural network hence better better hypotheses generatable supersymmetric artificial neural network explores natural step forward namely su representation supersymmetric biological brain representations perez et al represented supercharge compatible special unitary notation su bar tw parameterized bar supersymmetric directions unlike seen typical non supersymmetric deep learning model notably supersymmetric values encode represent information typical deep learning model terms partner potential signals example\n",
            "output sentence:  generalizing backward propagation using formal methods supersymmetry \n",
            "\n",
            "{'rouge-1': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}, 'rouge-2': {'r': 0.016, 'p': 0.25, 'f': 0.03007518683927869}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}}\n",
            "pair:  domain adaptation tackles problem transferring knowledge label rich source domain unlabeled label scarce target domain recently domain adversarial training dat shown promising capacity learn domain invariant feature space reversing gradient propagation domain classifier however dat still vulnerable several aspects including training instability due overwhelming discriminative ability domain classifier adversarial training restrictive feature level alignment lack interpretability systematic explanation learned feature space paper propose novel max margin domain adversarial training mdat designing adversarial reconstruction network arn proposed mdat stabilizes gradient reversing arn replacing domain classifier reconstruction network manner arn conducts feature level pixel level domain alignment without involving extra network structures furthermore arn demonstrates strong robustness wide range hyper parameters settings greatly alleviating task model selection extensive empirical results validate approach outperforms state art domain alignment methods additionally reconstructed target samples visualized interpret domain invariant feature space conforms intuition\n",
            "output sentence:  stable domain adversarial training approach robust comprehensive domain adaptation \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.7, 'f': 0.21212120955004593}, 'rouge-2': {'r': 0.07462686567164178, 'p': 0.5, 'f': 0.12987012761005232}, 'rouge-l': {'r': 0.125, 'p': 0.7, 'f': 0.21212120955004593}}\n",
            "pair:  propose general deep reinforcement learning method apply robot manipulation tasks approach leverages demonstration data assist reinforcement learning agent learning solve wide range tasks mainly previously unsolved train visuomotor policies end end learn direct mapping rgb camera inputs joint velocities experiments indicate reinforcement imitation approach solve contact rich robot manipulation tasks neither state art reinforcement imitation learning method solve alone also illustrate policies achieved zero shot sim real transfer training large visual dynamics variations\n",
            "output sentence:  combine reinforcement learning imitation learning solve complex robot manipulation tasks pixels \n",
            "\n",
            "{'rouge-1': {'r': 0.15151515151515152, 'p': 0.6666666666666666, 'f': 0.24691357722908097}, 'rouge-2': {'r': 0.05128205128205128, 'p': 0.2857142857142857, 'f': 0.08695651915879024}, 'rouge-l': {'r': 0.12121212121212122, 'p': 0.5333333333333333, 'f': 0.19753086117969826}}\n",
            "pair:  promising class generative models maps points simple distribution complex distribution invertible neural network likelihood based training models requires restricting architectures allow cheap computation jacobian determinants alternatively jacobian trace used transformation specified ordinary differential equation paper use hutchinson trace estimator give scalable unbiased estimate log density result continuous time invertible generative model unbiased density estimation one pass sampling allowing unrestricted neural network architectures demonstrate approach high dimensional density estimation image generation variational inference achieving state art among exact likelihood methods efficient sampling\n",
            "output sentence:  use continuous time dynamics define generative model exact likelihoods efficient sampling parameterized unrestricted neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.11956521739130435, 'p': 0.8461538461538461, 'f': 0.20952380735419504}, 'rouge-2': {'r': 0.04504504504504504, 'p': 0.4166666666666667, 'f': 0.0813008112472735}, 'rouge-l': {'r': 0.09782608695652174, 'p': 0.6923076923076923, 'f': 0.17142856925895694}}\n",
            "pair:  hierarchical reinforcement learning methods offer powerful means planning flexible behavior complicated domains however learning appropriate hierarchical decomposition domain subtasks remains substantial challenge present novel algorithm subtask discovery based recently introduced multitask linearly solvable markov decision process mlmdp framework mlmdp perform never seen tasks representing linear combination previously learned basis set tasks setting subtask discovery problem naturally posed finding optimal low rank approximation set tasks agent face domain use non negative matrix factorization discover minimal basis set tasks show technique learns intuitive decompositions variety domains method several qualitatively desirable features limited learning subtasks single goal states instead learning distributed patterns preferred states learns qualitatively different hierarchical decompositions domain depending ensemble tasks agent face may straightforwardly iterated obtain deeper hierarchical decompositions\n",
            "output sentence:  present novel algorithm hierarchical subtask discovery leverages multitask linear markov decision process framework \n",
            "\n",
            "{'rouge-1': {'r': 0.09333333333333334, 'p': 0.6363636363636364, 'f': 0.16279069544348299}, 'rouge-2': {'r': 0.034482758620689655, 'p': 0.3, 'f': 0.06185566825379961}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.45454545454545453, 'f': 0.11627906753650624}}\n",
            "pair:  fundamental still largely unanswered question context generative adversarial networks gans whether gans actually able capture key characteristics datasets trained current approaches examining issue require significant human supervision visual inspection sampled images often offer fairly limited scalability paper propose new techniques employ classification based perspective evaluate synthetic gan distributions capability accurately reflect essential properties training data techniques require minimal human supervision easily scaled adapted evaluate variety state art gans large popular datasets also indicate gans significant problems reproducing distributional properties training dataset particular diversity synthetic data orders magnitude smaller original data\n",
            "output sentence:  propose new methods evaluating quantifying quality synthetic gan distributions perspective classification tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.6, 'f': 0.26086956181474485}, 'rouge-2': {'r': 0.1, 'p': 0.4444444444444444, 'f': 0.16326530312369852}, 'rouge-l': {'r': 0.1388888888888889, 'p': 0.5, 'f': 0.21739130094517964}}\n",
            "pair:  unsupervised domain adaptation aims generalize hypothesis trained source domain unlabeled target domain one popular approach problem learn domain invariant representation domains work study theoretically empirically explicit effect embedding generalization target domain particular complexity class embeddings affects upper bound target domain risk reflected experiments\n",
            "output sentence:  general upper bound target domain risk reflects role embedding complexity \n",
            "\n",
            "{'rouge-1': {'r': 0.08, 'p': 0.5, 'f': 0.1379310321046374}, 'rouge-2': {'r': 0.03508771929824561, 'p': 0.2857142857142857, 'f': 0.06249999805175788}, 'rouge-l': {'r': 0.08, 'p': 0.5, 'f': 0.1379310321046374}}\n",
            "pair:  success modern machine learning becoming increasingly important understand control learning algorithms interact unfortunately negative results game theory show little hope understanding controlling general player games therefore introduce smooth markets sm games class player games pairwise zero sum interactions sm games codify common design pattern machine learning includes gans adversarial training recent algorithms show sm games amenable analysis optimization using first order methods\n",
            "output sentence:  introduce class player games suited gradient based methods \n",
            "\n",
            "{'rouge-1': {'r': 0.09523809523809523, 'p': 0.8571428571428571, 'f': 0.17142856962857145}, 'rouge-2': {'r': 0.06329113924050633, 'p': 0.8333333333333334, 'f': 0.11764705751141871}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.8571428571428571, 'f': 0.17142856962857145}}\n",
            "pair:  work presents modular hierarchical approach learn policies exploring environments approach leverages strengths classical learning based methods using analytical path planners learned mappers global local policies use learning provides flexibility respect input modalities mapper leverages structural regularities world global policies provides robustness errors state estimation local policies use learning within module retains benefits time hierarchical decomposition modular training allow us sidestep high sample complexities associated training end end policies experiments visually physically realistic simulated environments demonstrate effectiveness proposed approach past learning geometry based approaches\n",
            "output sentence:  modular hierarchical approach learn policies exploring environments \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.6666666666666666, 'f': 0.11428571271836736}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.046875, 'p': 0.5, 'f': 0.0857142841469388}}\n",
            "pair:  reinforcement learning actor critic setting relies accurate value estimates critic however combination function approximation temporal difference td learning policy training lead overestimating value function solution use clipped double learning cdq used td algorithm computes minimum two critics td target show cdq induces underestimation bias propose new algorithm accounts using weighted average target cdq target coming single critic weighting parameter adjusted training value estimates match actual discounted return recent episodes balances underestimation empirically obtain accurate value estimates demonstrate state art results several openai gym tasks\n",
            "output sentence:  method accurate critic estimates reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}, 'rouge-2': {'r': 0.028037383177570093, 'p': 0.375, 'f': 0.052173911748960335}, 'rouge-l': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}}\n",
            "pair:  learning communication via deep reinforcement learning recently shown effective way solve cooperative multi agent tasks however learning communicated information beneficial agent decision making remains challenging task order address problem introduce fully differentiable framework communication reasoning enabling agents solve cooperative tasks partially observable environments framework designed facilitate explicit reasoning agents novel memory based attention network learn selectively past memories model communicates series reasoning steps decompose agent intentions learned representations used first compute relevance communicated information second extract information memories given newly received information selectively interacting new information model effectively learns communication protocol directly end end manner empirically demonstrate strength model cooperative multi agent tasks inter agent communication reasoning prior information substantially improves performance compared baselines\n",
            "output sentence:  novel architecture memory based attention mechanism multi agent communication \n",
            "\n",
            "{'rouge-1': {'r': 0.0891089108910891, 'p': 0.6, 'f': 0.1551724115413199}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04950495049504951, 'p': 0.3333333333333333, 'f': 0.0862068942999406}}\n",
            "pair:  skip connections increasingly utilized deep neural networks improve accuracy cost efficiency particular recent densenet efficient computation parameters achieves state art predictions directly connecting feature layer previous ones however densenet extreme connectivity pattern may hinder scalability high depths applications like fully convolutional networks full densenet connections prohibitively expensive work first experimentally shows one key advantage skip connections short distances among feature layers backpropagation specifically using fixed number skip connections connection patterns shorter backpropagation distance among layers accurate predictions following insight propose connection template log densenet comparison densenet slightly increases backpropagation distances among layers log uses log total connections instead hence logdenses easier scale densenets longer require careful gpu memory management demonstrate effectiveness design principle showing better performance densenets tabula rasa semantic segmentation competitive results visual recognition\n",
            "output sentence:  show shortcut connections placed patterns minimize layer distances backpropagation design networks achieve log distances using log connections \n",
            "\n",
            "{'rouge-1': {'r': 0.0875, 'p': 0.6363636363636364, 'f': 0.15384615172080668}, 'rouge-2': {'r': 0.01020408163265306, 'p': 0.1, 'f': 0.01851851683813458}, 'rouge-l': {'r': 0.05, 'p': 0.36363636363636365, 'f': 0.0879120857867408}}\n",
            "pair:  physical design robot policy controls motion inherently coupled however existing approaches largely ignore coupling instead choosing alternate separate design control phases requires expert intuition throughout risks convergence suboptimal designs work propose method jointly optimizes physical design robot corresponding control policy model free fashion without need expert supervision given arbitrary robot morphology method maintains distribution design parameters uses reinforcement learning train neural network controller throughout training refine robot distribution maximize expected reward results assignment robot parameters neural network policy jointly optimal evaluate approach context legged locomotion demonstrate discovers novel robot designs walking gaits several different morphologies achieving performance comparable better hand crafted designs\n",
            "output sentence:  use deep reinforcement learning design physical attributes robot jointly control policy \n",
            "\n",
            "{'rouge-1': {'r': 0.057971014492753624, 'p': 0.23529411764705882, 'f': 0.09302325264196874}, 'rouge-2': {'r': 0.02531645569620253, 'p': 0.125, 'f': 0.04210526035678689}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.17647058823529413, 'f': 0.0697674386884804}}\n",
            "pair:  outline new approaches incorporate ideas deep learning wave based least squares imaging aim main contribution work combination handcrafted constraints deep convolutional neural networks way harness remarkable ease generating natural images mathematical basis underlying method expectation maximization framework data divided batches coupled additional latent unknowns unknowns pairs elements original unknown space coupled specific data batch network inputs setting neural network controls similarity additional parameters acting center variable resulting problem amounts maximum likelihood estimation network parameters augmented data model marginalized latent variables\n",
            "output sentence:  combine hard handcrafted constraints deep prior weak constraint perform seismic imaging reap information posterior distribution leveraging multiplicity data \n",
            "\n",
            "{'rouge-1': {'r': 0.06557377049180328, 'p': 0.4, 'f': 0.1126760539178735}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.1111111111111111, 'f': 0.02631578738573424}, 'rouge-l': {'r': 0.06557377049180328, 'p': 0.4, 'f': 0.1126760539178735}}\n",
            "pair:  propose new perspective adversarial attacks deep reinforcement learning agents main contribution copycat targeted attack able consistently lure agent following outsider policy pre computed therefore fast inferred could thus usable real time scenario show effectiveness atari games novel read setting latter adversary cannot directly modify agent state representation environment attack agent observation perception environment directly modifying agent state would require write access agent inner workings argue assumption strong realistic settings\n",
            "output sentence:  propose new attack taking full control neural policies realistic settings \n",
            "\n",
            "{'rouge-1': {'r': 0.06481481481481481, 'p': 0.6363636363636364, 'f': 0.11764705714568183}, 'rouge-2': {'r': 0.031496062992125984, 'p': 0.4, 'f': 0.05839415923064631}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5454545454545454, 'f': 0.10084033445660619}}\n",
            "pair:  crafting adversarial examples discrete inputs like text sequences fundamentally different generating examples continuous inputs like images paper tries answer question black box setting create adversarial examples automatically effectively fool deep learning classifiers texts making imperceptible changes answer firm yes previous efforts mostly replied using gradient evidence less effective either due finding nearest neighbor word wrt meaning automatically difficult relying heavily hand crafted linguistic rules instead use monte carlo tree search mcts finding important words perturb perform homoglyph attack replacing one character selected word symbol identical shape novel algorithm call mctsbug black box extremely effective time experimental results indicate mctsbug fool deep learning classifiers success rates seven large scale benchmark datasets perturbing characters surprisingly mctsbug without relying gradient information effective gradient based white box baseline thanks nature homoglyph attack generated adversarial perturbations almost imperceptible human eyes\n",
            "output sentence:  use monte carlo tree search homoglyphs generate indistinguishable adversarial samples text data \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.631578947368421, 'f': 0.20869564941550098}, 'rouge-2': {'r': 0.022222222222222223, 'p': 0.15789473684210525, 'f': 0.03896103679794244}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.42105263157894735, 'f': 0.13913043202419662}}\n",
            "pair:  word embedding useful approach capture co occurrence structures large corpus text addition text data often additional covariates associated individual documents corpus demographic author time venue publication etc would like embedding naturally capture information covariates paper propose new tensor decomposition model word embeddings covariates model jointly learns emph base embedding words well weighted diagonal transformation model covariate modifies base embedding obtain specific embedding particular author venue example simply multiply base embedding transformation matrix associated time venue main advantages approach data efficiency interpretability covariate transformation matrix experiments demonstrate joint model learns substantially better embeddings conditioned covariate compared standard approach learning separate embedding covariate using relevant subset data furthermore model encourages embeddings topic aligned sense dimensions specific independent meanings allows covariate specific embeddings compared topic enabling downstream differential analysis empirically evaluate benefits algorithm several datasets demonstrate used address many natural questions effects covariates\n",
            "output sentence:  using embedding across covariates make sense show tensor decomposition algorithm learns sparse covariate specific embeddings naturally separable topics topics efficiently \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.4444444444444444, 'f': 0.08988763863148597}, 'rouge-2': {'r': 0.020833333333333332, 'p': 0.25, 'f': 0.03846153704142017}, 'rouge-l': {'r': 0.05, 'p': 0.4444444444444444, 'f': 0.08988763863148597}}\n",
            "pair:  present capsule architecture processing point clouds equivariant respect rotation group translation permutation unordered input sets network operates sparse set local reference frames computed input point cloud establishes end end equivariance novel quaternion group capsule layer including equivariant dynamic routing procedure capsule layer enables us disentangle geometry pose paving way informative descriptions structured latent space process theoretically connect process dynamic routing capsules well known weiszfeld algorithm scheme solving iterative weighted least squares irls problems provable convergence properties enabling robust pose estimation capsule layers due sparse equivariant quaternion capsules architecture allows joint object classification orientation estimation validate empirically common benchmark datasets\n",
            "output sentence:  deep architectures point clouds equivariant rotations well translations permutations \n",
            "\n",
            "{'rouge-1': {'r': 0.08536585365853659, 'p': 0.5384615384615384, 'f': 0.14736841869030476}, 'rouge-2': {'r': 0.0196078431372549, 'p': 0.16666666666666666, 'f': 0.0350877174145892}, 'rouge-l': {'r': 0.04878048780487805, 'p': 0.3076923076923077, 'f': 0.08421052395346268}}\n",
            "pair:  conventional transforms discrete walsh hadamard transform dwht discrete cosine transform dct widely used feature extractors image processing rarely applied neural networks however found conventional transforms ability capture cross channel correlations without learnable parameters dnns paper firstly proposes apply conventional transforms pointwise convolution showing transforms significantly reduce computational complexity neural networks without accuracy performance degradation especially dwht requires floating point multiplications additions subtractions considerably reduce computation overheads addition fast algorithm reduces complexity floating point addition nlog non parametric low computational properties construct extremely efficient networks number parameters operations enjoying accuracy gain proposed dwht based model gained accuracy increase reduced parameters reduced flops compared baseline model moblienet cifar dataset\n",
            "output sentence:  introduce new pointwise convolution layers equipped extremely fast conventional transforms deep neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.06097560975609756, 'p': 0.29411764705882354, 'f': 0.10101009816549339}, 'rouge-2': {'r': 0.018867924528301886, 'p': 0.11764705882352941, 'f': 0.03252032282107229}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.29411764705882354, 'f': 0.10101009816549339}}\n",
            "pair:  introduce open ended modular self improving omega ai unification architecture refinement solomonoff alpha architecture considered first principles architecture embodies several crucial principles general intelligence including diversity representations diversity data types integrated memory modularity higher order cognition retain basic design fundamental algorithmic substrate called ai kernel problem solving basic cognitive functions like memory larger modular architecture uses kernel many ways omega includes eight representation languages six classes neural networks briefly introduced architecture intended initially address data science automation hence includes many problem solving methods statistical tasks review broad software architecture higher order cognition self improvement modular neural architectures intelligent agents process memory hierarchy hardware abstraction peer peer computing data abstraction facility\n",
            "output sentence:  new agi architecture trans sapient performance high level overview omega agi architecture basis data science automation system submitted workshop \n",
            "\n",
            "{'rouge-1': {'r': 0.136986301369863, 'p': 0.8333333333333334, 'f': 0.23529411522214533}, 'rouge-2': {'r': 0.053763440860215055, 'p': 0.4166666666666667, 'f': 0.09523809321360548}, 'rouge-l': {'r': 0.1232876712328767, 'p': 0.75, 'f': 0.21176470345743945}}\n",
            "pair:  intelligent creatures explore environments learn useful skills without supervision paper propose diversity need diayn method learning useful skills without reward function proposed method learns skills maximizing information theoretic objective using maximum entropy policy variety simulated robotic tasks show simple objective results unsupervised emergence diverse skills walking jumping number reinforcement learning benchmark environments method able learn skill solves benchmark task despite never receiving true task reward show pretrained skills provide good parameter initialization downstream tasks composed hierarchically solve complex sparse reward tasks results suggest unsupervised discovery skills serve effective pretraining mechanism overcoming challenges exploration data efficiency reinforcement learning\n",
            "output sentence:  propose algorithm learning useful skills without reward function show skills used solve downstream tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.75, 'f': 0.17142856940408163}, 'rouge-2': {'r': 0.024, 'p': 0.2727272727272727, 'f': 0.04411764557201562}, 'rouge-l': {'r': 0.0967741935483871, 'p': 0.75, 'f': 0.17142856940408163}}\n",
            "pair:  based observation exists dramatic drop singular values fully connected layers single feature map convolutional layer dimension concatenated feature vector almost equals summation dimension feature map propose singular value decomposition svd based approach estimate dimension deep manifolds typical convolutional neural network vgg choose three categories imagenet namely persian cat container ship volcano determine local dimension deep manifolds deep layers tangent space target image several augmentation methods found gaussian noise method closer intrinsic dimension adding random noise image moving arbitrary dimension rank feature matrix augmented images increase close local dimension manifold also estimate dimension deep manifold based tangent space maxpooling layers results show dimensions different categories close decline quickly along convolutional layers fully connected layers furthermore show dimensions decline quickly inside conv layer work provides new insights intrinsic structure deep neural networks helps unveiling inner organization black box deep neural networks\n",
            "output sentence:  propose svd based method explore local dimension activation manifold deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.2127659574468085, 'p': 0.5555555555555556, 'f': 0.307692303687574}, 'rouge-2': {'r': 0.04918032786885246, 'p': 0.17647058823529413, 'f': 0.07692307351413559}, 'rouge-l': {'r': 0.1702127659574468, 'p': 0.4444444444444444, 'f': 0.24615384214911246}}\n",
            "pair:  reservoir computing powerful tool explain brain learns temporal sequences movements existing learning schemes either biologically implausible inefficient explain animal performance show network learn complicated sequences reward modulated hebbian learning rule network reservoir neurons combined second network serves dynamic working memory provides spatio temporal backbone signal reservoir combination working memory reward modulated hebbian learning readout neurons performs well force learning advantage biologically plausible interpretation learning rule learning paradigm\n",
            "output sentence:  show working memory input reservoir network makes local reward modulated hebbian rule perform well recursive least squares aka force \n",
            "\n",
            "{'rouge-1': {'r': 0.06060606060606061, 'p': 0.5, 'f': 0.10810810617969324}, 'rouge-2': {'r': 0.011764705882352941, 'p': 0.14285714285714285, 'f': 0.021739129028828068}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.5, 'f': 0.10810810617969324}}\n",
            "pair:  paper propose differentiable adversarial grammar model future prediction objective model formal grammar terms differentiable functions latent representations learning possible standard backpropagation learning formal grammar represented latent terminals non terminals productions rules allows capturing sequential structures multiple possibilities data adversarial grammar designed learn stochastic production rules data distribution able select multiple production rules leads different predicted outcomes thus efficiently modeling many plausible futures confirm benefit adversarial grammar two diverse tasks future human pose prediction future activity prediction settings proposed adversarial grammar outperforms state art approaches able predict much accurately future prior work\n",
            "output sentence:  design grammar learned adversarial setting apply future prediction video \n",
            "\n",
            "{'rouge-1': {'r': 0.3333333333333333, 'p': 0.7272727272727273, 'f': 0.45714285283265316}, 'rouge-2': {'r': 0.16, 'p': 0.4, 'f': 0.22857142448979595}, 'rouge-l': {'r': 0.2916666666666667, 'p': 0.6363636363636364, 'f': 0.39999999568979594}}\n",
            "pair:  considering simultaneously finite number tasks multi output learning enables one account similarities tasks via appropriate regularizers propose generalization classical setting continuum tasks using vector valued rkhss\n",
            "output sentence:  propose extension multi output learning continuum tasks using operator valued kernels \n",
            "\n",
            "{'rouge-1': {'r': 0.12222222222222222, 'p': 0.8461538461538461, 'f': 0.2135922308040343}, 'rouge-2': {'r': 0.07142857142857142, 'p': 0.6666666666666666, 'f': 0.12903225631633716}, 'rouge-l': {'r': 0.12222222222222222, 'p': 0.8461538461538461, 'f': 0.2135922308040343}}\n",
            "pair:  applying reinforcement learning rl real world problems require reasoning action reward correlation long time horizons hierarchical reinforcement learning hrl methods handle dividing task hierarchies often hand tuned network structure pre defined subgoals propose novel hrl framework taic learns temporal abstraction past experience expert demonstrations without task specific knowledge formulate temporal abstraction problem learning latent representations action sequences present novel approach regularizing latent space adding information theoretic constraints specifically maximize mutual information latent variables state changes visualization latent space demonstrates algorithm learns effective abstraction long action sequences learned abstraction allows us learn new tasks higher level efficiently convey significant speedup convergence benchmark learning problems results demonstrate learning temporal abstractions effective technique increasing convergence rate sample efficiency rl algorithms\n",
            "output sentence:  propose novel hrl framework formulate temporal abstraction problem learning latent representation action sequence \n",
            "\n",
            "{'rouge-1': {'r': 0.1746031746031746, 'p': 0.9166666666666666, 'f': 0.29333333064533335}, 'rouge-2': {'r': 0.11842105263157894, 'p': 0.75, 'f': 0.20454545219008266}, 'rouge-l': {'r': 0.1746031746031746, 'p': 0.9166666666666666, 'f': 0.29333333064533335}}\n",
            "pair:  propose novel deep network architecture lifelong learning refer dynamically expandable network den dynamically decide network capacity trains sequence tasks learn compact overlapping knowledge sharing structure among tasks den efficiently trained online manner performing selective retraining dynamically expands network capacity upon arrival task necessary number units effectively prevents semantic drift splitting duplicating units timestamping validate den multiple public datasets lifelong learning scenarios multiple public datasets significantly outperforms existing lifelong learning methods deep networks also achieves level performance batch model substantially fewer number parameters\n",
            "output sentence:  propose novel deep network architecture dynamically decide network capacity trains lifelong learning scenario \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.5, 'f': 0.20512820186719266}, 'rouge-2': {'r': 0.04411764705882353, 'p': 0.2, 'f': 0.07228915366526359}, 'rouge-l': {'r': 0.08064516129032258, 'p': 0.3125, 'f': 0.1282051249441158}}\n",
            "pair:  major component overfitting model free reinforcement learning rl involves case agent may mistakenly correlate reward certain spurious features observations generated markov decision process mdp provide general framework analyzing scenario use design multiple synthetic benchmarks modifying observation space mdp agent overfits different observation spaces even underlying mdp dynamics fixed term observational overfitting experiments expose intriguing properties especially regards implicit regularization also corroborate results previous works rl generalization supervised learning sl\n",
            "output sentence:  isolate one factor rl generalization analyzing case agent overfits observations show architectural implicit regularizations occur regime \n",
            "\n",
            "{'rouge-1': {'r': 0.08235294117647059, 'p': 0.5384615384615384, 'f': 0.14285714055601834}, 'rouge-2': {'r': 0.009900990099009901, 'p': 0.08333333333333333, 'f': 0.017699113145900433}, 'rouge-l': {'r': 0.047058823529411764, 'p': 0.3076923076923077, 'f': 0.08163265076010003}}\n",
            "pair:  conventional approach solving recommendation problem greedily ranks individual document candidates prediction scores however method fails optimize slate whole hence often struggles capture biases caused page layout document interdepedencies slate recommendation problem aims directly find optimally ordered subset documents slates best serve users interests solving problem hard due combinatorial explosion document candidates display positions page therefore propose paradigm shift traditional viewpoint solving ranking problem direct slate generation framework paper introduce list conditional variational auto encoders listcvae learn joint distribution documents slate conditioned user responses directly generate full slates experiments simulated real world data show list cvae outperforms greedy ranking methods consistently various scales documents corpora\n",
            "output sentence:  used cvae type model structure learn directly generate slates whole pages recommendation systems \n",
            "\n",
            "{'rouge-1': {'r': 0.14705882352941177, 'p': 0.7142857142857143, 'f': 0.24390243619274243}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.3076923076923077, 'f': 0.08988763795480376}, 'rouge-l': {'r': 0.1323529411764706, 'p': 0.6428571428571429, 'f': 0.21951219229030342}}\n",
            "pair:  training generative adversarial networks requires balancing delicate adversarial dynamics even careful tuning training may diverge end bad equilibrium dropped modes work introduce new form latent optimisation inspired cs gan show improves adversarial dynamics enhancing interactions discriminator generator develop supporting theoretical analysis perspectives differentiable games stochastic approximation experiments demonstrate latent optimisation significantly improve gan training obtaining state art performance imagenet dataset model achieves inception score frechet inception distance fid improvement fid respectively compared baseline biggan deep model architecture number parameters\n",
            "output sentence:  latent optimisation improves adversarial training dynamics present theoretical analysis state art image generation imagenet \n",
            "\n",
            "{'rouge-1': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}, 'rouge-2': {'r': 0.011764705882352941, 'p': 0.2, 'f': 0.022222221172839556}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}}\n",
            "pair:  designing search space critical problem neural architecture search nas algorithms propose fine grained search space comprised atomic blocks minimal search unit much smaller ones used recent nas algorithms search space facilitates direct selection channel numbers kernel sizes convolutions addition propose resource aware architecture search algorithm dynamically selects atomic blocks training algorithm accelerated dynamic network shrinkage technique instead search retrain two stage paradigm method simultaneously search train target architecture end end manner method achieves state art performance several flops configurations imagenet negligible searching cost open entire codebase https github com meijieru atomnas\n",
            "output sentence:  new state art imagenet mobile setting \n",
            "\n",
            "{'rouge-1': {'r': 0.18181818181818182, 'p': 0.9333333333333333, 'f': 0.3043478233577505}, 'rouge-2': {'r': 0.12264150943396226, 'p': 0.9285714285714286, 'f': 0.21666666460555556}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.9333333333333333, 'f': 0.3043478233577505}}\n",
            "pair:  one challenges training generative models variational auto encoder vae avoiding posterior collapse generator much capacity prone ignoring latent code problem exacerbated dataset small latent dimension high root problem elbo objective specifically kullback leibler kl divergence term objective function paper proposes new objective function replace kl term one emulates maximum mean discrepancy mmd objective also introduces new technique named latent clipping used control distance samples latent space probabilistic autoencoder model named mu vae designed trained mnist mnist fashion datasets using new objective function shown outperform models trained elbo beta vae objective mu vae less prone posterior collapse generate reconstructions new samples good quality latent representations learned mu vae shown good used downstream tasks classification\n",
            "output sentence:  paper proposes new objective function replace kl term one emulates maximum mean discrepancy mmd objective \n",
            "\n",
            "{'rouge-1': {'r': 0.03333333333333333, 'p': 0.2222222222222222, 'f': 0.05797101222432271}, 'rouge-2': {'r': 0.014705882352941176, 'p': 0.125, 'f': 0.026315787590027836}, 'rouge-l': {'r': 0.03333333333333333, 'p': 0.2222222222222222, 'f': 0.05797101222432271}}\n",
            "pair:  improve previous end end differentiable neural networks nns fast weight memories gate mechanism updates fast weights every time step sequence two separate outer product based matrices generated slow parts net system trained complex sequence sequence variation associative retrieval problem roughly times temporal memory time varying variables similar sized standard recurrent nns rnns terms accuracy number parameters architecture outperforms variety rnns including long short term memory hypernetworks related fast weight architectures\n",
            "output sentence:  improved fast weight network shows better results general toy task \n",
            "\n",
            "{'rouge-1': {'r': 0.05333333333333334, 'p': 1.0, 'f': 0.10126582182342576}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.5, 'f': 0.04081632574760517}, 'rouge-l': {'r': 0.05333333333333334, 'p': 1.0, 'f': 0.10126582182342576}}\n",
            "pair:  reparameterization trick become one useful tools field variational inference however reparameterization trick based standardization transformation restricts scope application method distributions tractable inverse cumulative distribution functions expressible deterministic transformations distributions paper generalized reparameterization trick allowing general transformation unlike similar works develop generalized transformation based gradient model formally rigorously discover proposed model special case control variate indicating proposed model combine advantages cv generalized reparameterization based proposed gradient model propose new polynomial based gradient estimator better theoretical performance reparameterization trick certain condition applied larger class variational distributions studies synthetic real data show proposed gradient estimator significantly lower gradient variance state art methods thus enabling faster inference procedure\n",
            "output sentence:  generalized transformation generalized transformation based variational model \n",
            "\n",
            "{'rouge-1': {'r': 0.23728813559322035, 'p': 0.9333333333333333, 'f': 0.37837837514609207}, 'rouge-2': {'r': 0.1267605633802817, 'p': 0.6, 'f': 0.2093023227014603}, 'rouge-l': {'r': 0.22033898305084745, 'p': 0.8666666666666667, 'f': 0.35135134811906504}}\n",
            "pair:  work first conduct mathematical analysis memory defined function maps element sequence current output three rnn cells namely simple recurrent neural network srn long short term memory lstm gated recurrent unit gru based analysis propose new design called extended long short term memory elstm extend memory length cell next present multi task rnn model robust previous erroneous predictions called dependent bidirectional recurrent neural network dbrnn sequence sequenceout siso problem finally performance dbrnn model elstm cell demonstrated experimental results\n",
            "output sentence:  recurrent neural network cell extended long short term memory multi task rnn model sequence sequence problems \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.46153846153846156, 'f': 0.14999999727812502}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.3076923076923077, 'f': 0.09999999727812507}}\n",
            "pair:  adversarial examples remain issue contemporary neural networks paper draws background check perello nieto et al technique model calibration assist two class neural networks detecting adversarial examples using one dimensional difference logit values underlying measure method interestingly tends achieve highest average recall image sets generated large perturbation vectors unlike existing literature adversarial attacks cubuk et al proposed method need knowledge attack parameters methods training time unlike great deal literature uses deep learning based methods detect adversarial examples metzen et al imbuing proposed method additional flexibility\n",
            "output sentence:  paper uses principles field calibration machine learning logits neural network defend adversarial attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.16, 'p': 0.6666666666666666, 'f': 0.2580645130072841}, 'rouge-2': {'r': 0.10344827586206896, 'p': 0.42857142857142855, 'f': 0.16666666353395063}, 'rouge-l': {'r': 0.08, 'p': 0.3333333333333333, 'f': 0.12903225494276802}}\n",
            "pair:  propose support guided adversarial imitation learning sail generic imitation learning framework unifies support estimation expert policy family adversarial imitation learning ail algorithms sail addresses two important challenges ail including implicit reward bias potential training instability also show sail least efficient standard ail extensive evaluation demonstrate proposed method effectively handles reward bias achieves better performance training stability baseline methods wide range benchmark control tasks\n",
            "output sentence:  unify support estimation family adversarial imitation learning algorithms support guided adversarial imitation learning robust stable imitation learning framework \n",
            "\n",
            "{'rouge-1': {'r': 0.039603960396039604, 'p': 0.5, 'f': 0.07339449405268918}, 'rouge-2': {'r': 0.007751937984496124, 'p': 0.14285714285714285, 'f': 0.014705881376513906}, 'rouge-l': {'r': 0.0297029702970297, 'p': 0.375, 'f': 0.05504587019947819}}\n",
            "pair:  model interpretability systematic targeted model adaptation present central challenges deep learning domain intuitive physics study task visually predicting stability block towers goal understanding influencing model reasoning contributions two fold firstly introduce neural stethoscopes framework quantifying degree importance specific factors influence deep networks well actively promoting suppressing information appropriate unify concepts multitask learning well training auxiliary adversarial losses secondly deploy stethoscope framework provide depth analysis state art deep neural network stability prediction specifically examining physical reasoning show baseline model susceptible misled incorrect visual cues leads performance breakdown level random guessing training scenarios visual cues inversely correlated stability using stethoscopes promote meaningful feature extraction increases performance prediction accuracy conversely training easy dataset visual cues positively correlated stability baseline model learns bias leading poor performance harder dataset using adversarial stethoscope network successfully de biased leading performance increase\n",
            "output sentence:  combining auxiliary adversarial training interrogate help physical understanding \n",
            "\n",
            "{'rouge-1': {'r': 0.10909090909090909, 'p': 0.6666666666666666, 'f': 0.1874999975830078}, 'rouge-2': {'r': 0.028985507246376812, 'p': 0.2222222222222222, 'f': 0.05128204924063125}, 'rouge-l': {'r': 0.10909090909090909, 'p': 0.6666666666666666, 'f': 0.1874999975830078}}\n",
            "pair:  propose information maximization autoencoder imae information theoretic approach simultaneously learn continuous discrete representations unsupervised setting unlike variational autoencoder framework imae starts stochastic encoder seeks map input data hybrid discrete continuous representation objective maximizing mutual information data representations decoder included approximate posterior distribution data given representations high fidelity approximation achieved leveraging informative representations show proposed objective theoretically valid provides principled framework understanding tradeoffs regarding informativeness representation factor disentanglement representations decoding quality\n",
            "output sentence:  information theoretical approach unsupervised learning unsupervised learning hybrid discrete continuous representations \n",
            "\n",
            "{'rouge-1': {'r': 0.16363636363636364, 'p': 0.9, 'f': 0.2769230743195267}, 'rouge-2': {'r': 0.11594202898550725, 'p': 0.8888888888888888, 'f': 0.20512820308678503}, 'rouge-l': {'r': 0.16363636363636364, 'p': 0.9, 'f': 0.2769230743195267}}\n",
            "pair:  adversaries neural networks drawn much attention since first debut existing methods aim deceiving image classification models misclassification crafting attacks specific object instances object setection tasks focus creating universal adversaries fool object detectors hide objects detectors adversaries examine universal three ways specific specific object instances image independent transfer different unknown models achieve propose two novel techniques improve transferability adversaries textit piling textit monochromatization techniques prove simplify patterns generated adversaries ultimately result higher transferability\n",
            "output sentence:  focus creating universal adversaries fool object detectors hide objects detectors \n",
            "\n",
            "{'rouge-1': {'r': 0.08235294117647059, 'p': 0.6363636363636364, 'f': 0.1458333313042535}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.2, 'f': 0.036697246039895715}, 'rouge-l': {'r': 0.07058823529411765, 'p': 0.5454545454545454, 'f': 0.12499999797092017}}\n",
            "pair:  deep learning demonstrated abilities learn complex structures restricted available data recently consensus networks cns proposed alleviate data sparsity utilizing features multiple modalities limited size labeled data paper extend cn transductive consensus networks tcns suitable semi supervised learning tcns different modalities input compressed latent representations encourage become indistinguishable iterative adversarial training understand tcns two mechanisms consensus classification put forward three variants ablation studies mechanisms investigate tcn models treat latent representations probability distributions measure similarities negative relative jensen shannon divergences show consensus state beneficial classification desires stable imperfect similarity representations overall tcns outperform align best benchmark algorithms given labeled samples bank marketing dementiabank datasets\n",
            "output sentence:  tcn multimodal semi supervised learning ablation study mechanisms interpretations latent representations \n",
            "\n",
            "{'rouge-1': {'r': 0.039473684210526314, 'p': 0.42857142857142855, 'f': 0.07228915508201483}, 'rouge-2': {'r': 0.0297029702970297, 'p': 0.42857142857142855, 'f': 0.05555555434327849}, 'rouge-l': {'r': 0.039473684210526314, 'p': 0.42857142857142855, 'f': 0.07228915508201483}}\n",
            "pair:  paper propose combine imitation reinforcement learning via idea reward shaping using oracle study effectiveness near optimal cost go oracle planning horizon demonstrate cost go oracle shortens learner planning horizon function accuracy globally optimal oracle shorten planning horizon one leading one step greedy markov decision process much easier optimize oracle far away optimality requires planning longer horizon achieve near optimal performance hence new insight bridges gap interpolates imitation learning reinforcement learning motivated mentioned insights propose truncated horizon policy search thor method focuses searching policies maximize total reshaped reward finite planning horizon oracle sub optimal experimentally demonstrate gradient based implementation thor achieve superior performance compared rl baselines il baselines even oracle sub optimal\n",
            "output sentence:  combining imitation learning reinforcement learning learn outperform expert \n",
            "\n",
            "{'rouge-1': {'r': 0.042105263157894736, 'p': 0.4, 'f': 0.07619047446712021}, 'rouge-2': {'r': 0.008771929824561403, 'p': 0.1111111111111111, 'f': 0.016260161245290616}, 'rouge-l': {'r': 0.042105263157894736, 'p': 0.4, 'f': 0.07619047446712021}}\n",
            "pair:  deep neural networks excel regimes large amounts data tend struggle data scarce need adapt quickly changes task response recent work meta learning proposes training meta learner distribution similar tasks hopes generalization novel related tasks learning high level strategy captures essence problem asked solve however many recent meta learning approaches extensively hand designed either using architectures specialized particular application hard coding algorithmic components constrain meta learner solves task propose class simple generic meta learner architectures use novel combination temporal convolutions soft attention former aggregate information past experience latter pinpoint specific pieces information extensive set meta learning experiments date evaluate resulting simple neural attentive learner snail several heavily benchmarked tasks tasks supervised reinforcement learning snail attains state art performance significant margins\n",
            "output sentence:  simple rnn based meta learner achieves sota performance popular benchmarks \n",
            "\n",
            "{'rouge-1': {'r': 0.06666666666666667, 'p': 0.875, 'f': 0.12389380399404809}, 'rouge-2': {'r': 0.047244094488188976, 'p': 0.8571428571428571, 'f': 0.0895522378157719}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.875, 'f': 0.12389380399404809}}\n",
            "pair:  deep neural networks trained wide range datasets demonstrate impressive transferability deep features appear general applicable many datasets tasks property prevalent use real world applications neural network pretrained large datasets imagenet significantly boost generalization accelerate training fine tuned smaller target dataset despite pervasiveness effort devoted uncovering reason transferability deep feature representations paper tries understand transferability perspectives improved generalization optimization feasibility transferability demonstrate transferred models tend find flatter minima since weight matrices stay close original flat region pretrained parameters transferred similar target dataset transferred representations make loss landscape favorable improved lipschitzness accelerates stabilizes training substantially improvement largely attributes fact principal component gradient suppressed pretrained parameters thus stabilizing magnitude gradient back propagation feasibility transferability related similarity input label surprising discovery feasibility also impacted training stages transferability first increases training declines provide theoretical analysis verify observations\n",
            "output sentence:  understand transferability perspectives improved generalization optimization feasibility transferability \n",
            "\n",
            "{'rouge-1': {'r': 0.04040404040404041, 'p': 0.4444444444444444, 'f': 0.07407407254629635}, 'rouge-2': {'r': 0.008620689655172414, 'p': 0.1111111111111111, 'f': 0.015999998663680112}, 'rouge-l': {'r': 0.030303030303030304, 'p': 0.3333333333333333, 'f': 0.055555554027777815}}\n",
            "pair:  adaptive optimization methods adagrad rmsprop adam proposed achieve rapid training process element wise scaling term learning rates though prevailing observed generalize poorly compared sgd even fail converge due unstable extreme learning rates recent work put forward algorithms amsgrad tackle issue failed achieve considerable improvement existing methods paper demonstrate extreme learning rates lead poor performance provide new variants adam amsgrad called adabound amsbound respectively employ dynamic bounds learning rates achieve gradual smooth transition adaptive methods sgd give theoretical proof convergence conduct experiments various popular tasks models often insufficient previous work experimental results show new variants eliminate generalization gap adaptive methods sgd maintain higher learning speed early training time moreover bring significant improvement prototypes especially complex deep networks implementation algorithm found https github com luolc adabound\n",
            "output sentence:  novel variants optimization methods combine benefits adaptive non adaptive methods \n",
            "\n",
            "{'rouge-1': {'r': 0.0851063829787234, 'p': 0.6153846153846154, 'f': 0.14953270814568959}, 'rouge-2': {'r': 0.03361344537815126, 'p': 0.2857142857142857, 'f': 0.060150374056193175}, 'rouge-l': {'r': 0.06382978723404255, 'p': 0.46153846153846156, 'f': 0.11214953057559615}}\n",
            "pair:  formal understanding inductive bias behind deep convolutional networks relation network architectural features functions able model limited work establish fundamental connection fields quantum physics deep learning use obtaining novel theoretical observations regarding inductive bias convolutional networks specifically show structural equivalence function realized convolutional arithmetic circuit convac quantum many body wave function facilitates use quantum entanglement measures quantifiers deep network expressive ability model correlations furthermore construction deep convac terms quantum tensor network enabled allows us perform graph theoretic analysis convolutional network tying expressiveness min cut underlying graph demonstrate practical outcome form direct control inductive bias via number channels width layer empirically validate findings standard convolutional networks involve relu activations max pooling description deep convolutional network well defined graph theoretic tools structural connection quantum entanglement two interdisciplinary bridges brought forth work\n",
            "output sentence:  employing quantum entanglement measures quantifying correlations deep learning using connection fit deep network architecture correlations data \n",
            "\n",
            "{'rouge-1': {'r': 0.08235294117647059, 'p': 0.6363636363636364, 'f': 0.1458333313042535}, 'rouge-2': {'r': 0.028846153846153848, 'p': 0.3, 'f': 0.05263157734687601}, 'rouge-l': {'r': 0.047058823529411764, 'p': 0.36363636363636365, 'f': 0.08333333130425352}}\n",
            "pair:  inverse reinforcement learning irl used infer reward function actions expert running markov decision process mdp novel approach using variational inference learning reward function proposed research using technique intractable posterior distribution continuous latent variable reward function case analytically approximated appear close prior belief trying reconstruct future state conditioned current state action reward function derived using well known deep generative model known conditional variational auto encoder cvae wasserstein loss function thus referred conditional wasserstein auto encoder irl cwae irl analyzed combination backward forward inference form efficient alternative previous approaches irl knowledge system dynamics agent experimental results standard benchmarks objectworld pendulum show proposed algorithm effectively learn latent reward function complex high dimensional environments\n",
            "output sentence:  using supervised latent variable modeling framework determine reward inverse reinforcement learning task \n",
            "\n",
            "{'rouge-1': {'r': 0.09722222222222222, 'p': 0.6363636363636364, 'f': 0.168674696495863}, 'rouge-2': {'r': 0.03488372093023256, 'p': 0.3, 'f': 0.06249999813368061}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.6363636363636364, 'f': 0.168674696495863}}\n",
            "pair:  relational reasoning ability model interactions relations objects valuable robust multi object tracking pivotal trajectory prediction paper propose mohart class agnostic end end multi object tracking trajectory prediction algorithm explicitly accounts permutation invariance relational reasoning explore number permutation invariant architectures show multi headed self attention outperforms provided baselines better accounts complex physical interactions challenging toy experiment show three real world tracking datasets adding relational reasoning capabilities way increases tracking trajectory prediction performance particularly presence ego motion occlusions crowded scenes faulty sensor inputs best knowledge mohart first fully end end multi object tracking vision approach applied real world data reported literature\n",
            "output sentence:  mohart uses self attention mechanism perform relational reasoning multi object tracking \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.6666666666666666, 'f': 0.19047618802721092}, 'rouge-2': {'r': 0.014705882352941176, 'p': 0.11764705882352941, 'f': 0.02614378887436471}, 'rouge-l': {'r': 0.046296296296296294, 'p': 0.2777777777777778, 'f': 0.07936507691609986}}\n",
            "pair:  study problem safe adaptation given model trained variety past experiences task model learn perform task new situation avoiding catastrophic failure problem setting occurs frequently real world reinforcement learning scenarios vehicle adapting drive new city robotic drone adapting policy trained simulation learning without catastrophic failures exceptionally difficult prior experience allow us learn models make much easier models might directly transfer new settings enable cautious adaptation substantially safer na adaptation well learning scratch building intuition propose risk averse domain adaptation rada rada works two steps first trains probabilistic model based rl agents population source domains gain experience capture epistemic uncertainty environment dynamics dropped new environment employs pessimistic exploration policy selecting actions best worst case performance forecasted probabilistic model show simple maximin policy accelerates domain adaptation safety critical driving environment varying vehicle sizes compare approach approaches adapting new environments including meta reinforcement learning\n",
            "output sentence:  adaptation rl agent target environment unknown dynamics fast safe transfer prior experience variety environments select risk averse actions \n",
            "\n",
            "{'rouge-1': {'r': 0.03389830508474576, 'p': 0.3333333333333333, 'f': 0.06153845986272194}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.3333333333333333, 'f': 0.06153845986272194}}\n",
            "pair:  reinforcement learning algorithms though successful tend fit training environments thereby hampering application real world paper proposes text text text robust reinforcement learning algorithm significant robust performance low high dimensional control tasks method formalises robust reinforcement learning novel min max game wasserstein constraint correct convergent solver apart formulation also propose efficient scalable solver following novel zero order optimisation method believe useful numerical optimisation general empirically demonstrate significant gains compared standard robust state art algorithms high dimensional mujuco environments\n",
            "output sentence:  rl algorithm learns robust changes dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.0449438202247191, 'p': 0.5, 'f': 0.08247422529067916}, 'rouge-2': {'r': 0.00980392156862745, 'p': 0.125, 'f': 0.018181816833057952}, 'rouge-l': {'r': 0.033707865168539325, 'p': 0.375, 'f': 0.06185566858964825}}\n",
            "pair:  experimental reproducibility replicability critical topics machine learning authors often raised concerns lack scientific publications improve quality field recently graph representation learning field attracted attention wide research community resulted large stream works several graph neural network models developed effectively tackle graph classification however experimental procedures often lack rigorousness hardly reproducible motivated provide overview common practices avoided fairly compare state art counter troubling trend ran experiments controlled uniform framework evaluate five popular models across nine common benchmarks moreover comparing gnns structure agnostic baselines provide convincing evidence datasets structural information exploited yet believe work contribute development graph learning field providing much needed grounding rigorous evaluations graph classification models\n",
            "output sentence:  provide rigorous comparison different graph neural networks graph classification \n",
            "\n",
            "{'rouge-1': {'r': 0.10227272727272728, 'p': 0.6, 'f': 0.17475727906494487}, 'rouge-2': {'r': 0.02, 'p': 0.14285714285714285, 'f': 0.03508771714373667}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.26666666666666666, 'f': 0.07766990042416824}}\n",
            "pair:  semmelhack et al achieved high classification accuracy distinguishing swim bouts zebrafish using support vector machine svm convolutional neural networks cnns reached superior performance various image recognition tasks svms powerful networks remain black box reaching better transparency helps build trust classifications makes learned features interpretable experts using recently developed technique called deep taylor decomposition generated heatmaps highlight input regions high relevance predictions find cnn makes predictions analyzing steadiness tail trunk markedly differs manually extracted features used semmelhack et al uncovered network paid attention experimental artifacts removing artifacts ensured validity predictions correction best cnn beats svm achieving classification accuracy work thus demonstrates utility ai explainability cnns\n",
            "output sentence:  demonstrate utility recent ai explainability technique visualizing learned features cnn trained binary classification zebrafish movements \n",
            "\n",
            "{'rouge-1': {'r': 0.06315789473684211, 'p': 0.4, 'f': 0.10909090673553726}, 'rouge-2': {'r': 0.014814814814814815, 'p': 0.14285714285714285, 'f': 0.026845635881266722}, 'rouge-l': {'r': 0.06315789473684211, 'p': 0.4, 'f': 0.10909090673553726}}\n",
            "pair:  knowledge distillation kd common method transferring knowledge learned one machine learning model teacher another model student typically teacher greater capacity parameters higher bit widths knowledge existing methods overlook fact although student absorbs extra knowledge teacher models share input data data medium teacher knowledge demonstrated due difference model capacities student may benefit fully data points teacher trained hand human teacher may demonstrate piece knowledge individualized examples adapted particular student instance terms cultural background interests inspired behavior design data augmentation agents distinct roles facilitate knowledge distillation data augmentation agents generate distinct training data teacher student respectively focus specifically kd teacher network greater precision bit width student network find empirically specially tailored data points enable teacher knowledge demonstrated effectively student compare approach existing kd methods training popular neural architectures demonstrate role wise data augmentation improves effectiveness kd strong prior approaches code reproducing results made publicly available\n",
            "output sentence:  study whether adaptive data augmentation knowledge distillation leveraged simultaneously synergistic manner better training student networks \n",
            "\n",
            "{'rouge-1': {'r': 0.2702702702702703, 'p': 0.9090909090909091, 'f': 0.4166666631336806}, 'rouge-2': {'r': 0.20930232558139536, 'p': 0.9, 'f': 0.3396226384478463}, 'rouge-l': {'r': 0.2702702702702703, 'p': 0.9090909090909091, 'f': 0.4166666631336806}}\n",
            "pair:  apply canonical forms gradient complexes barcodes explore neural networks loss surfaces present algorithm calculations objective function barcodes minima experiments confirm two principal observations barcodes minima located small lower part range values objective function increase neural network depth brings minima barcodes natural implications neural network learning ability generalize\n",
            "output sentence:  apply canonical forms gradient complexes barcodes explore neural networks loss surfaces \n",
            "\n",
            "{'rouge-1': {'r': 0.08108108108108109, 'p': 0.5454545454545454, 'f': 0.14117646833494812}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.2, 'f': 0.04166666480034731}, 'rouge-l': {'r': 0.04054054054054054, 'p': 0.2727272727272727, 'f': 0.07058823304083052}}\n",
            "pair:  many biological learning systems mushroom body hippocampus cerebellum built sparsely connected networks neurons new understanding networks study function spaces induced sparse random features characterize functions may may learned network inputs per neuron found equivalent additive model order whereas degree distribution network combines additive terms different orders identify three specific advantages sparsity additive function approximation powerful inductive bias limits curse dimensionality sparse networks stable outlier noise inputs sparse random features scalable thus even simple brain architectures powerful function approximators finally hope work helps popularize kernel theories networks among computational neuroscientists\n",
            "output sentence:  advocate random features theory biological neural networks focusing sparsely connected networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.45454545454545453, 'f': 0.12658227608396094}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.2, 'f': 0.04651162585181188}, 'rouge-l': {'r': 0.04411764705882353, 'p': 0.2727272727272727, 'f': 0.07594936469155592}}\n",
            "pair:  present network embedding algorithms capture information node local distribution node attributes around observed random walks following approach similar skip gram observations neighborhoods different sizes either pooled ae encoded distinctly multi scale approach musae capturing attribute neighborhood relationships multiple scales useful diverse range applications including latent feature identification across disconnected networks similar attributes prove theoretically matrices node feature pointwise mutual information implicitly factorized embeddings experiments show algorithms robust computationally efficient outperform comparable models social web citation network datasets\n",
            "output sentence:  develop efficient multi scale approximate attributed network embedding procedures provable properties \n",
            "\n",
            "{'rouge-1': {'r': 0.10185185185185185, 'p': 0.5789473684210527, 'f': 0.17322834391220784}, 'rouge-2': {'r': 0.05555555555555555, 'p': 0.3684210526315789, 'f': 0.09655172186064215}, 'rouge-l': {'r': 0.07407407407407407, 'p': 0.42105263157894735, 'f': 0.1259842494240189}}\n",
            "pair:  sepsis life threatening complication infection leading cause mortality hospitals early detection sepsis improves patient outcomes little consensus exact treatment guidelines treating septic patients remains open problem work present new deep reinforcement learning method use learn optimal personalized treatment policies septic patients model patient continuous valued physiological time series using multi output gaussian processes probabilistic model easily handles missing values irregularly spaced observation times maintaining estimates uncertainty gaussian process directly tied deep recurrent network learns clinically interpretable treatment policies models learned together end end evaluate approach heterogeneous dataset septic spanning months university health system find learned policy could reduce patient mortality much overall baseline mortality rate algorithm could used make treatment recommendations physicians part decision support tool framework readily applies reinforcement learning problems rely sparsely sampled frequently missing multivariate time series data\n",
            "output sentence:  combine multi output gaussian processes deep recurrent networks learn optimal treatments sepsis show improved performance standard deep reinforcement learning methods \n",
            "\n",
            "{'rouge-1': {'r': 0.12987012987012986, 'p': 1.0, 'f': 0.2298850554366495}, 'rouge-2': {'r': 0.10989010989010989, 'p': 1.0, 'f': 0.19801980019605922}, 'rouge-l': {'r': 0.12987012987012986, 'p': 1.0, 'f': 0.2298850554366495}}\n",
            "pair:  generative models model predict sequences future events principle learn capture complex real world phenomena physical interactions however central challenge video prediction future highly uncertain sequence past observations events imply many possible futures although number recent works studied probabilistic models represent uncertain futures models either extremely expensive computationally case pixel level autoregressive models directly optimize likelihood data knowledge work first propose multi frame video prediction normalizing flows allows direct optimization data likelihood produces high quality stochastic predictions describe approach modeling latent space dynamics demonstrate flow based generative models offer viable competitive approach generative modeling video\n",
            "output sentence:  demonstrate flow based generative models offer viable competitive approach generative modeling video \n",
            "\n",
            "{'rouge-1': {'r': 0.16417910447761194, 'p': 0.55, 'f': 0.2528735596776325}, 'rouge-2': {'r': 0.0759493670886076, 'p': 0.3, 'f': 0.1212121179879605}, 'rouge-l': {'r': 0.08955223880597014, 'p': 0.3, 'f': 0.13793103094200035}}\n",
            "pair:  study emergence cooperative behaviors reinforcement learning agents introducing challenging competitive multi agent soccer environment continuous simulated physics demonstrate decentralized population based training co play lead progression agents behaviors random simple ball chasing finally showing evidence cooperation study highlights several challenges encountered large scale multi agent training continuous control particular demonstrate automatic optimization simple shaping rewards conducive co operative behavior lead long horizon team behavior apply evaluation scheme grounded game theoretic principals assess agent performance absence pre defined evaluation tasks human baselines\n",
            "output sentence:  introduce new mujoco soccer environment continuous multi agent reinforcement learning research show population based training independent reinforcement learners learn cooperative behaviors \n",
            "\n",
            "{'rouge-1': {'r': 0.19117647058823528, 'p': 1.0, 'f': 0.3209876516262765}, 'rouge-2': {'r': 0.15584415584415584, 'p': 1.0, 'f': 0.2696629190152759}, 'rouge-l': {'r': 0.19117647058823528, 'p': 1.0, 'f': 0.3209876516262765}}\n",
            "pair:  graph classification currently dominated graph kernels powerful suffer significant limitations convolutional neural networks cnns offer appealing alternative however processing graphs cnns trivial address challenge many sophisticated extensions cnns recently proposed paper reverse problem rather proposing yet another graph cnn model introduce novel way represent graphs multi channel image like structures allows handled vanilla cnns despite simplicity method proves competitive state art graph kernels graph cnns outperforms wide margin datasets also preferable graph kernels terms time complexity code data publicly available\n",
            "output sentence:  introduce novel way represent graphs multi channel image like structures allows handled vanilla cnns \n",
            "\n",
            "{'rouge-1': {'r': 0.05555555555555555, 'p': 0.4444444444444444, 'f': 0.09876543012345682}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.3333333333333333, 'f': 0.07407407209876549}}\n",
            "pair:  model based reinforcement learning mbrl aims learn dynamic model reduce number interactions real world environments however due estimation error rollouts learned model especially long horizon fail match ones real world environments mismatching seriously impacted sample complexity mbrl phenomenon attributed fact previous works employ supervised learning learn one step transition models inherent difficulty ensuring matching distributions multi step rollouts based claim propose learn synthesized model matching distributions multi step rollouts sampled synthesized model real ones via wgan theoretically show matching two minimize difference cumulative rewards real transition learned one experiments also show proposed model imitation method outperforms state art terms sample complexity average return\n",
            "output sentence:  method incorporates wgan achieve occupancy measure matching transition learning \n",
            "\n",
            "{'rouge-1': {'r': 0.11666666666666667, 'p': 0.6363636363636364, 'f': 0.19718309597302125}, 'rouge-2': {'r': 0.06172839506172839, 'p': 0.45454545454545453, 'f': 0.10869565006852554}, 'rouge-l': {'r': 0.1, 'p': 0.5454545454545454, 'f': 0.16901408188851422}}\n",
            "pair:  paper introduces information theoretic co training objective unsupervised learning consider problem predicting future rather predict future sensations image pixels sound waves predict hypotheses confirmed future sensations formally assume population distribution pairs think past sensation future sensation train predictor model phi confirmation model psi view hypotheses predicted facts confirmed population distribution pairs focus problem measuring mutual information data processing inequality mutual information least large mutual information distribution triples defined confirmation model psi information theoretic training objective phi psi viewed form co training want prediction match confirmation give experiments applications learning phonetics timit dataset\n",
            "output sentence:  presents information theoretic training objective co training demonstrates power unsupervised learning phonetics \n",
            "\n",
            "{'rouge-1': {'r': 0.05128205128205128, 'p': 0.5714285714285714, 'f': 0.09411764554740484}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.038461538461538464, 'p': 0.42857142857142855, 'f': 0.07058823378269899}}\n",
            "pair:  automatic classification objects one important tasks engineering data mining applications although using complex advanced classifiers help improve accuracy classification systems done analyzing data sets features particular problem feature combination one improve quality features paper structure similar feed forward neural network ffnn used generate optimized linear non linear combination features classification genetic algorithm ga applied update weights biases since nature data sets features impact effectiveness combination classification system linear non linear activation functions transfer function used achieve reliable system experiments several uci data sets using minimum distance classifier simple classifier indicate proposed linear non linear intelligent ffnn based feature combination present reliable promising results using feature combination method need use powerful complex classifier anymore\n",
            "output sentence:  method enriching combining features improve classification accuracy \n",
            "\n",
            "{'rouge-1': {'r': 0.15517241379310345, 'p': 0.9, 'f': 0.26470587984429067}, 'rouge-2': {'r': 0.07246376811594203, 'p': 0.5555555555555556, 'f': 0.12820512616370813}, 'rouge-l': {'r': 0.13793103448275862, 'p': 0.8, 'f': 0.23529411513840828}}\n",
            "pair:  generative adversarial networks gans shown provide effective way model complex distributions obtained impressive results various challenging tasks however typical gans require fully observed data training paper present gan based framework learning complex high dimensional incomplete data proposed framework learns complete data generator along mask generator models missing data distribution demonstrate impute missing data equipping framework adversarially trained imputer evaluate proposed framework using series experiments several types missing data processes missing completely random assumption\n",
            "output sentence:  paper presents gan based framework learning distribution high dimensional incomplete data \n",
            "\n",
            "{'rouge-1': {'r': 0.3111111111111111, 'p': 0.7368421052631579, 'f': 0.4374999958251954}, 'rouge-2': {'r': 0.16071428571428573, 'p': 0.45, 'f': 0.2368421013850416}, 'rouge-l': {'r': 0.24444444444444444, 'p': 0.5789473684210527, 'f': 0.34374999582519533}}\n",
            "pair:  handheld virtual panel hvp virtual panel attached non dominant hand controller virtual reality vr hvp go technique enabling menus toolboxes vr devices paper investigate target acquisition performance hvp function four factors target width target distance direction approach respect gravity angle approach results show four factors significant effects user performance based results propose guidelines towards ergonomic performant design hvp interfaces\n",
            "output sentence:  paper investigates target acquisition handheld virtual panels vr shows target width distance direction approach respect gravity angle approach impact user performance \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.5, 'f': 0.24999999625000005}, 'rouge-2': {'r': 0.10638297872340426, 'p': 0.35714285714285715, 'f': 0.16393442269282457}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.42857142857142855, 'f': 0.21428571053571432}}\n",
            "pair:  learn functional organization cortical microcircuits large scale recordings neural activity obtain explicit interpretable model time dependent functional connections neurons establish dynamics cortical information flow develop dynamic neural relational inference dnri study synthetic real world neural spiking data demonstrate developed method able uncover dynamic relations neurons reliably existing baselines\n",
            "output sentence:  develop dynamic neural relational inference variational autoencoder model explicitly interpretably represent hidden dynamic relations neurons \n",
            "\n",
            "{'rouge-1': {'r': 0.10309278350515463, 'p': 0.7142857142857143, 'f': 0.18018017797581368}, 'rouge-2': {'r': 0.03007518796992481, 'p': 0.2857142857142857, 'f': 0.05442176698412704}, 'rouge-l': {'r': 0.061855670103092786, 'p': 0.42857142857142855, 'f': 0.10810810590374163}}\n",
            "pair:  stochastic gradient descent sgd trades noisy gradient updates computational efficiency de facto optimization algorithm solve large scale machine learning problems sgd make rapid learning progress performing updates using subsampled training data noisy updates also lead slow asymptotic convergence several variance reduction algorithms svrg introduce control variates obtain lower variance gradient estimate faster convergence despite appealing asymptotic guarantees svrg like algorithms widely adopted deep learning traditional asymptotic analysis stochastic optimization provides limited insight training deep learning models fixed number epochs paper present non asymptotic analysis svrg noisy least squares regression problem primary focus compare exact loss svrg sgd iteration show learning dynamics regression model closely matches neural networks mnist cifar underparameterized overparameterized models analysis experimental results suggest trade computational cost convergence speed underparametrized neural networks svrg outperforms sgd epochs regime however sgd shown always outperform svrg overparameterized regime\n",
            "output sentence:  non asymptotic analysis sgd svrg showing strength algorithm convergence speed computational cost parametrized parametrized settings \n",
            "\n",
            "{'rouge-1': {'r': 0.16304347826086957, 'p': 0.8823529411764706, 'f': 0.27522935516539015}, 'rouge-2': {'r': 0.08943089430894309, 'p': 0.6470588235294118, 'f': 0.15714285500918368}, 'rouge-l': {'r': 0.15217391304347827, 'p': 0.8235294117647058, 'f': 0.2568807313121791}}\n",
            "pair:  deep learning deep reinforcement learning systems demonstrated impressive results domains image classification game playing robotic control data efficiency remains major challenge particularly algorithms learn individual tasks scratch multi task learning emerged promising approach sharing structure across multiple tasks enable efficient learning however multi task setting presents number optimization challenges making difficult realize large efficiency gains compared learning tasks independently reasons multi task learning challenging compared single task learning fully understood motivated insight gradient interference causes optimization challenges develop simple general approach avoiding interference gradients different tasks altering gradients technique refer gradient surgery propose form gradient surgery projects gradient task onto normal plane gradient task conflicting gradient series challenging multi task supervised multi task reinforcement learning problems find approach leads substantial gains efficiency performance effectively combined previously proposed multi task architectures enhanced performance model agnostic way\n",
            "output sentence:  develop simple general approach avoiding interference gradients different tasks improves performance multi task learning supervised reinforcement learning domains \n",
            "\n",
            "{'rouge-1': {'r': 0.07446808510638298, 'p': 0.6363636363636364, 'f': 0.13333333145759638}, 'rouge-2': {'r': 0.024, 'p': 0.3, 'f': 0.04444444307270237}, 'rouge-l': {'r': 0.07446808510638298, 'p': 0.6363636363636364, 'f': 0.13333333145759638}}\n",
            "pair:  high quality node embeddings learned graph neural networks gnns applied wide range node based applications achieved state art sota performance however applying node embeddings learned gnns generate graph embeddings scalar node representation may suffice preserve node graph properties efficiently resulting sub optimal graph embeddings inspired capsule neural network capsnet propose capsule graph neural network capsgnn adopts concept capsules address weakness existing gnn based graph embeddings algorithms extracting node features form capsules routing mechanism utilized capture important information graph level result model generates multiple embeddings graph capture graph properties different aspects attention module incorporated capsgnn used tackle graphs various sizes also enables model focus critical parts graphs extensive evaluations graph structured datasets demonstrate capsgnn powerful mechanism operates capture macroscopic properties whole graph data driven outperforms sota techniques several graph classification tasks virtue new instrument\n",
            "output sentence:  inspired capsnet propose novel architecture graph embeddings basis node features extracted gnn \n",
            "\n",
            "{'rouge-1': {'r': 0.038834951456310676, 'p': 0.36363636363636365, 'f': 0.07017543685287785}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.019417475728155338, 'p': 0.18181818181818182, 'f': 0.03508771755463228}}\n",
            "pair:  study continuous action reinforcement learning problems crucial agent interacts environment safe policies policies keep agent desirable situations training convergence formulate problems em constrained markov decision processes cmdps present safe policy optimization algorithms based lyapunov approach solve algorithms use standard policy gradient pg method deep deterministic policy gradient ddpg proximal policy optimization ppo train neural network policy guaranteeing near constraint satisfaction every policy update projecting either policy parameter selected action onto set feasible solutions induced state dependent linearized lyapunov constraints compared existing constrained pg algorithms data efficient able utilize policy policy data moreover action projection algorithm often leads less conservative policy updates allows natural integration end end pg training pipeline evaluate algorithms compare state art baselines several simulated mujoco tasks well real world robot obstacle avoidance problem demonstrating effectiveness terms balancing performance constraint satisfaction\n",
            "output sentence:  general framework incorporating long term safety constraints policy based reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.3953488372093023, 'p': 1.0, 'f': 0.5666666626055555}, 'rouge-2': {'r': 0.32, 'p': 1.0, 'f': 0.4848484811753903}, 'rouge-l': {'r': 0.3953488372093023, 'p': 1.0, 'f': 0.5666666626055555}}\n",
            "pair:  point important problems common practice using best single model performance comparing deep learning architectures propose method corrects flaws time model trained one gets different result due random factors training process include random parameter initialization random data shuffling reporting best single model performance appropriately address stochasticity propose normalized expected best performance boo way correct problems\n",
            "output sentence:  point important problems common practice using best single model performance comparing deep learning architectures propose method corrects flaws \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.6, 'f': 0.15584415358407824}, 'rouge-2': {'r': 0.0379746835443038, 'p': 0.3333333333333333, 'f': 0.06818181634555791}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.4, 'f': 0.10389610163602636}}\n",
            "pair:  backpropagation driving today artificial neural networks however despite extensive research remains unclear brain implements algorithm among neuroscientists reinforcement learning rl algorithms often seen realistic alternative however convergence rate learning scales poorly number involved neurons propose hybrid learning approach neuron uses rl type strategy learn approximate gradients backpropagation would provide show approach learns approximate gradient match performance gradient based learning fully connected convolutional networks learning feedback weights provides biologically plausible mechanism achieving good performance without need precise pre specified learning rules\n",
            "output sentence:  perturbations used learn feedback weights large fully connected convolutional networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07575757575757576, 'p': 0.5, 'f': 0.13157894508310253}, 'rouge-2': {'r': 0.024096385542168676, 'p': 0.2222222222222222, 'f': 0.04347825910444242}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.4, 'f': 0.10526315560941835}}\n",
            "pair:  methods calculate dense vector representations features unstructured data words document proven successful knowledge representation study estimate dense representations multiple feature types exist within dataset supervised learning explicit labels available well unsupervised learning labels feat vec calculates embeddings data multiple feature types enforcing different feature types exist common space supervised case show method advantages recently proposed methods enabling higher prediction accuracy providing way avoid cold start problem unsupervised case experiments suggest feat vec significantly outperforms existing algorithms leverage structure data believe first propose method learning unsuper vised embeddings leverage structure multiple feature types\n",
            "output sentence:  learn dense vector representations arbitrary types features labeled unlabeled datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.0641025641025641, 'p': 0.4166666666666667, 'f': 0.11111110880000004}, 'rouge-2': {'r': 0.012048192771084338, 'p': 0.09090909090909091, 'f': 0.021276593678135107}, 'rouge-l': {'r': 0.05128205128205128, 'p': 0.3333333333333333, 'f': 0.08888888657777784}}\n",
            "pair:  deep generative models emulate perceptual properties complex image datasets providing latent representation data however manipulating representation perform meaningful controllable transformations data space remains challenging without form supervision previous work focused exploiting statistical independence textit disentangle latent factors argue requirement advantageously relaxed propose instead non statistical framework relies identifying modular organization network based counterfactual manipulations experiments support modularity groups channels achieved certain degree variety generative models allowed design targeted interventions complex image datasets opening way applications computationally efficient style transfer automated assessment robustness contextual changes pattern recognition systems\n",
            "output sentence:  develop framework find modular internal representations generative models manipulate generate counterfactual examples \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.7058823529411765, 'f': 0.26966291825779576}, 'rouge-2': {'r': 0.12195121951219512, 'p': 0.5555555555555556, 'f': 0.19999999704800003}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.7058823529411765, 'f': 0.26966291825779576}}\n",
            "pair:  dramatic advances generative models resulted near photographic quality artificially rendered faces animals objects natural world spite advances higher level understanding vision imagery arise exhaustively modeling object instead identifying higher level attributes best summarize aspects object work attempt model drawing process fonts building sequential generative models vector graphics model benefit providing scale invariant representation imagery whose latent representation may systematically manipulated exploited perform style propagation demonstrate results large dataset fonts highlight model captures statistical dependencies richness dataset envision model find use tool designers facilitate font design\n",
            "output sentence:  attempt model drawing process fonts building sequential generative models vector graphics svgs highly structured structured font font characters characters \n",
            "\n",
            "{'rouge-1': {'r': 0.061224489795918366, 'p': 0.42857142857142855, 'f': 0.10714285495535718}, 'rouge-2': {'r': 0.017241379310344827, 'p': 0.14285714285714285, 'f': 0.0307692288473374}, 'rouge-l': {'r': 0.04081632653061224, 'p': 0.2857142857142857, 'f': 0.0714285692410715}}\n",
            "pair:  introduce masked translation model mtm combines encoding decoding sequences within model component mtm based idea masked language modeling supports autoregressive non autoregressive decoding strategies simply changing order masking experiments wmt romanian english task mtm shows strong constant time translation performance beating related approaches comparable complexity also extensively compare various decoding strategies supported mtm well several length modeling techniques training settings\n",
            "output sentence:  use transformer encoder translation training style masked translation model \n",
            "\n",
            "{'rouge-1': {'r': 0.1206896551724138, 'p': 0.5833333333333334, 'f': 0.1999999971591837}, 'rouge-2': {'r': 0.02702702702702703, 'p': 0.18181818181818182, 'f': 0.04705882127612468}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.3333333333333333, 'f': 0.11428571144489805}}\n",
            "pair:  uncertainty important feature intelligence helps brain become flexible creative powerful intelligent system crossbar based neuromorphic computing chips computing mainly performed analog circuits uncertainty used imitate brain however current deep neural networks taken uncertainty neuromorphic computing chip consideration therefore performances neuromorphic computing chips good original platforms cpus gpus work proposed uncertainty adaptation training scheme uats tells uncertainty neural network training process experimental results show neural networks achieve comparable inference performances uncertain neuromorphic computing chip compared results original platforms much better performances without training scheme\n",
            "output sentence:  training method make deep learning algorithms work better neuromorphic computing chips uncertainty \n",
            "\n",
            "{'rouge-1': {'r': 0.0875, 'p': 0.875, 'f': 0.15909090743801652}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.8571428571428571, 'f': 0.11009174191734702}, 'rouge-l': {'r': 0.0875, 'p': 0.875, 'f': 0.15909090743801652}}\n",
            "pair:  demonstrate machine learning able model experiments quantum physics quantum entanglement cornerstone upcoming quantum technologies quantum computation quantum cryptography particular interest complex quantum states two particles large number entangled quantum levels given multiparticle high dimensional quantum state usually impossible reconstruct experimental setup produces search interesting experiments one thus randomly create millions setups computer calculate respective output states work show machine learning models provide significant improvement random search demonstrate long short term memory lstm neural network successfully learn model quantum experiments correctly predicting output state characteristics given setups without necessity computing states approach allows faster search also essential step towards automated design multiparticle high dimensional quantum experiments using generative machine learning models\n",
            "output sentence:  demonstrate machine learning able model experiments quantum physics \n",
            "\n",
            "{'rouge-1': {'r': 0.05714285714285714, 'p': 0.4444444444444444, 'f': 0.10126582076590292}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}, 'rouge-l': {'r': 0.05714285714285714, 'p': 0.4444444444444444, 'f': 0.10126582076590292}}\n",
            "pair:  understanding flow information deep neural networks dnns challenging problem gain increasing attention last years several methods proposed explain network predictions attempts compare theoretical perspective exhaustive empirical comparison performed past work analyze four gradient based attribution methods formally prove conditions equivalence approximation reformulating two methods construct unified framework enables direct comparison well easier implementation finally propose novel evaluation metric called sensitivity test gradient based attribution methods alongside simple perturbation based attribution method several datasets domains image text classification using various network architectures\n",
            "output sentence:  four existing backpropagation based attribution methods fundamentally similar assess \n",
            "\n",
            "{'rouge-1': {'r': 0.12, 'p': 0.631578947368421, 'f': 0.20168066958548125}, 'rouge-2': {'r': 0.030534351145038167, 'p': 0.21052631578947367, 'f': 0.053333331120888984}, 'rouge-l': {'r': 0.09, 'p': 0.47368421052631576, 'f': 0.15126050151825438}}\n",
            "pair:  tremendous success deep neural networks motivated need better understand fundamental properties networks many theoretical results proposed shallow networks paper study important primitive understanding meaningful input space deep network span recovery let mathbf mathbb times innermost weight matrix arbitrary feed forward neural network mathbb mathbb written sigma mathbf network sigma mathbb mathbb goal recover row span mathbf given oracle access value show multi layered network relu activation functions partial recovery possible namely provably recover linearly independent vectors row span mathbf using poly non adaptive queries furthermore differentiable activation functions demonstrate textit full span recovery possible even output first passed sign thresholding function case algorithm adaptive empirically confirm full span recovery always possible unrealistically thin layers reasonably wide networks obtain full span recovery random networks networks trained mnist data furthermore demonstrate utility span recovery attack inducing neural networks misclassify data obfuscated controlled random noise sensical inputs\n",
            "output sentence:  provably recover span deep multi layered neural network latent structure empirically apply efficient span recovery algorithms attack networks obfuscating inputs \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.6666666666666666, 'f': 0.13333333153333335}, 'rouge-2': {'r': 0.019801980198019802, 'p': 0.25, 'f': 0.0366972463462672}, 'rouge-l': {'r': 0.07407407407407407, 'p': 0.6666666666666666, 'f': 0.13333333153333335}}\n",
            "pair:  several state art convolutional networks rely inter connecting different layers ease flow information gradient input output layers techniques enabled practitioners successfully train deep convolutional networks hundreds layers particularly novel way interconnecting layers introduced dense convolutional network densenet achieved state art performance relevant image recognition tasks despite notable empirical success theoretical understanding still limited work address problem analyzing effect layer interconnection overall expressive power convolutional network particular connections used densenet compared types inter layer connectivity carry tensor analysis expressive power inter connections convolutional arithmetic circuits convacs relate results standard convolutional networks analysis leads performance bounds practical guidelines design convacs generalization results discussed kinds convolutional networks via generalized tensor decompositions\n",
            "output sentence:  analyze expressive power connections used densenets via tensor decompositions \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.5882352941176471, 'f': 0.25974025629954467}, 'rouge-2': {'r': 0.06172839506172839, 'p': 0.29411764705882354, 'f': 0.10204081345897552}, 'rouge-l': {'r': 0.1, 'p': 0.35294117647058826, 'f': 0.1558441524034408}}\n",
            "pair:  log linear models models widely used machine learning particular ubiquitous deep learning architectures form softmax exact inference learning requires linear time done approximately sub linear time strong concentrations guarantees work present lsh softmax method perform sub linear learning inference softmax layer deep learning setting method relies popular locality sensitive hashing build well concentrated gradient estimator using nearest neighbors uniform samples also present inference scheme sub linear time lsh softmax using gumbel distribution language modeling show recurrent neural networks trained lsh softmax perform par computing exact softmax requiring sub linear computations\n",
            "output sentence:  present lsh softmax softmax approximation layer sub linear learning inference strong theoretical guarantees showcase applicability efficiency evaluating real task \n",
            "\n",
            "{'rouge-1': {'r': 0.02857142857142857, 'p': 0.13333333333333333, 'f': 0.04705882062283755}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02857142857142857, 'p': 0.13333333333333333, 'f': 0.04705882062283755}}\n",
            "pair:  paper proposes method efficient training function continuous state markov decision processes mdp traces resulting policies satisfy linear temporal logic ltl property ltl modal logic express wide range time dependent logical properties including safety liveness convert ltl property limit deterministic buchi automaton synchronized product mdp constructed control policy synthesised reinforcement learning algorithm assuming prior knowledge available mdp proposed method evaluated numerical study test quality generated control policy compared conventional methods policy synthesis mdp abstraction voronoi quantizer approximate dynamic programming fitted value iteration\n",
            "output sentence:  safety becoming critical notion machine learning believe work act foundation number research directions safety aware learning algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.19387755102040816, 'p': 0.95, 'f': 0.32203389548980177}, 'rouge-2': {'r': 0.12295081967213115, 'p': 0.7894736842105263, 'f': 0.21276595511493385}, 'rouge-l': {'r': 0.19387755102040816, 'p': 0.95, 'f': 0.32203389548980177}}\n",
            "pair:  presence bias confounding effects inarguably one critical challenges machine learning applications alluded pivotal debates recent years challenges range spurious associations confounding variables medical studies bias race gender face recognition systems one solution enhance datasets organize reflect biases cumbersome intensive task alternative make use available data build models considering biases traditional statistical methods apply straightforward techniques residualization stratification precomputed features account confounding variables however techniques general applicable end end deep learning methods paper propose method based adversarial training strategy learn discriminative features unbiased invariant confounder enabled incorporating new adversarial loss function encourages vanished correlation bias learned features apply method synthetic medical diagnosis gender classification gender shades dataset results show learned features method result superior prediction performance also uncorrelated bias confounder variables code available http blinded review\n",
            "output sentence:  propose method based adversarial training strategy learn discriminative features unbiased invariant confounder incorporating loss function encourages vanished correlation learned features \n",
            "\n",
            "{'rouge-1': {'r': 0.14666666666666667, 'p': 0.7857142857142857, 'f': 0.2471910085847747}, 'rouge-2': {'r': 0.07317073170731707, 'p': 0.4, 'f': 0.1237113375916676}, 'rouge-l': {'r': 0.12, 'p': 0.6428571428571429, 'f': 0.2022471883600556}}\n",
            "pair:  capsule networks shown encouraging results defacto benchmark computer vision datasets mnist cifar smallnorb although yet tested tasks entities detected inherently complex internal representations instances per class learn point wise classification suitable hence paper carries experiments face verification controlled uncontrolled settings together address points introduce siamese capsule networks new variant used pairwise learning tasks model trained using contrastive loss normalized capsule encoded pose features find siamese capsule networks perform well strong baselines pairwise learning datasets yielding best results shot learning setting image pairs test set contain unseen subjects\n",
            "output sentence:  variant capsule networks used pairwise learning tasks results shows siamese capsule networks work well shot learning setting \n",
            "\n",
            "{'rouge-1': {'r': 0.07894736842105263, 'p': 0.6, 'f': 0.13953488166576528}, 'rouge-2': {'r': 0.02197802197802198, 'p': 0.2222222222222222, 'f': 0.03999999836200007}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.4, 'f': 0.09302325375878857}}\n",
            "pair:  variational autoencoders vaes successful learning low dimensional manifold high dimensional data complex dependencies core consist powerful bayesian probabilistic inference model capture salient features data training exploit power variational inference optimizing lower bound model evidence latent representation performance vaes heavily influenced type bound used cost function significant research work carried development tighter bounds original elbo accurately approximate true log likelihood leveraging deformed logarithm traditional lower bounds elbo iwae upper bound cubo bring contributions direction research proof concept study explore different ways creating deformed bounds tighter classical ones show improvements performance vaes binarized mnist dataset\n",
            "output sentence:  using deformed logarithm derive tighter bounds iwae train variational autoencoders \n",
            "\n",
            "{'rouge-1': {'r': 0.075, 'p': 0.6, 'f': 0.13333333135802472}, 'rouge-2': {'r': 0.019801980198019802, 'p': 0.2, 'f': 0.036036034396558794}, 'rouge-l': {'r': 0.0625, 'p': 0.5, 'f': 0.1111111091358025}}\n",
            "pair:  generative adversarial networks gans evolved one successful unsupervised techniques generating realistic images even though recently shown gan training converges gan models often end local nash equilibria associated mode collapse otherwise fail model target distribution introduce coulomb gans pose gan learning problem potential field generated samples attracted training set samples repel discriminator learns potential field generator decreases energy moving samples along vector force field determined gradient potential field decreasing energy gan model learns generate samples according whole target distribution cover modes prove coulomb gans possess one nash equilibrium optimal sense model distribution equals target distribution show efficacy coulomb gans lsun bedrooms celeba faces cifar google billion word text generation\n",
            "output sentence:  coulomb gans optimally learn distribution posing distribution learning problem optimizing potential field \n",
            "\n",
            "{'rouge-1': {'r': 0.15463917525773196, 'p': 1.0, 'f': 0.26785714053730875}, 'rouge-2': {'r': 0.09565217391304348, 'p': 0.6875, 'f': 0.1679389291533128}, 'rouge-l': {'r': 0.14432989690721648, 'p': 0.9333333333333333, 'f': 0.24999999768016584}}\n",
            "pair:  estimating location image taken based solely contents image challenging task even humans properly labeling image fashion relies heavily contextual information simple identifying single object image thus methods attempt must somehow account complexities single model date completely capable addressing challenges work contributes state research image geolocation inferencing introducing novel global meshing strategy outlining variety training procedures overcome considerable data limitations training models demonstrating incorporating additional information used improve overall performance geolocation inference model work shown delaunay triangles effective type mesh geolocation relatively low volume scenarios compared results state art models use quad trees order magnitude training data addition time posting learned user albuming meta data easily incorporated improve geolocation country level km locality accuracy city level km localities\n",
            "output sentence:  global geolocation inferencing strategy novel meshing strategy demonstrating incorporating additional information used improve overall performance geolocation inference model \n",
            "\n",
            "{'rouge-1': {'r': 0.07777777777777778, 'p': 0.7, 'f': 0.13999999820000003}, 'rouge-2': {'r': 0.02912621359223301, 'p': 0.3333333333333333, 'f': 0.05357142709343116}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  neural networks widely used natural language processing yet despite empirical successes behaviour brittle sensitive small input changes sensitive deletions large fractions input text paper aims tackle sensitivity context natural language inference ensuring models become confident predictions arbitrary subsets words input text deleted develop novel technique formal verification specification models based popular decomposable attention mechanism employing efficient yet effective interval bound propagation ibp approach using method efficiently prove given model whether particular sample free sensitivity problem compare different training methods address sensitivity compare metrics measure experiments snli mnli datasets observe ibp training leads significantly improved verified accuracy snli test set verify samples substantial improvement using standard training\n",
            "output sentence:  formal verification specification model prediction undersensitivity using interval bound propagation \n",
            "\n",
            "{'rouge-1': {'r': 0.05813953488372093, 'p': 0.4166666666666667, 'f': 0.10204081417742612}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.15384615384615385, 'f': 0.029629627889163346}, 'rouge-l': {'r': 0.05813953488372093, 'p': 0.4166666666666667, 'f': 0.10204081417742612}}\n",
            "pair:  meta learning algorithms learn acquire new tasks quickly past experience context reinforcement learning meta learning algorithms acquire reinforcement learning procedures solve new problems efficiently utilizing experience prior tasks performance meta learning algorithms depends tasks available meta training way supervised learning generalizes best test points drawn distribution training points meta learning methods generalize best tasks distribution meta training tasks effect meta reinforcement learning offloads design burden algorithm design task design automate process task design well devise meta learning algorithm truly automated work take step direction proposing family unsupervised meta learning algorithms reinforcement learning motivate describe general recipe unsupervised meta reinforcement learning present instantiation approach conceptual theoretical contributions consist formulating unsupervised meta reinforcement learning problem describing task proposals based mutual information principle used train optimal meta learners experimental results indicate unsupervised meta reinforcement learning effectively acquires accelerated reinforcement learning procedures without need manual task design significantly exceeds performance learning scratch\n",
            "output sentence:  meta learning self proposed task distributions speed reinforcement learning without human specified task distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.039603960396039604, 'p': 0.5, 'f': 0.07339449405268918}, 'rouge-2': {'r': 0.008264462809917356, 'p': 0.14285714285714285, 'f': 0.015624998966064523}, 'rouge-l': {'r': 0.039603960396039604, 'p': 0.5, 'f': 0.07339449405268918}}\n",
            "pair:  paper studies undesired phenomena sensitivity representations learned deep networks semantically irrelevant changes data identify cause shortcoming classical variational auto encoder vae objective evidence lower bound elbo show elbo fails control behaviour encoder support empirical data distribution behaviour vae lead extreme errors learned representation key hurdle effective use representations data efficient learning transfer address problem propose augment data specifications enforce insensitivity representation respect families transformations incorporate specifications propose regularization method based selection mechanism creates fictive data point explicitly perturbing observed true data point certain choices parameters formulation naturally leads minimization entropy regularized wasserstein distance representations illustrate approach standard datasets experimentally show significant improvements downstream adversarial accuracy achieved learning robust representations completely unsupervised manner without reference particular downstream task without costly supervised adversarial training procedure\n",
            "output sentence:  propose method computing adversarially robust representations entirely unsupervised way \n",
            "\n",
            "{'rouge-1': {'r': 0.10227272727272728, 'p': 0.6, 'f': 0.17475727906494487}, 'rouge-2': {'r': 0.02, 'p': 0.14285714285714285, 'f': 0.03508771714373667}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.26666666666666666, 'f': 0.07766990042416824}}\n",
            "pair:  semmelhack et al achieved high classification accuracy distinguishing swim bouts zebrafish using support vector machine svm convolutional neural networks cnns reached superior performance various image recognition tasks svms powerful networks remain black box reaching better transparency helps build trust classifications makes learned features interpretable experts using recently developed technique called deep taylor decomposition generated heatmaps highlight input regions high relevance predictions find cnn makes predictions analyzing steadiness tail trunk markedly differs manually extracted features used semmelhack et al uncovered network paid attention experimental artifacts removing artifacts ensured validity predictions correction best cnn beats svm achieving classification accuracy work thus demonstrates utility ai explainability cnns\n",
            "output sentence:  demonstrate utility recent ai explainability technique visualizing learned features cnn trained binary classification zebrafish movements \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.7777777777777778, 'f': 0.24137930772294885}, 'rouge-2': {'r': 0.08771929824561403, 'p': 0.625, 'f': 0.15384615168757396}, 'rouge-l': {'r': 0.12244897959183673, 'p': 0.6666666666666666, 'f': 0.20689654910225921}}\n",
            "pair:  propose method joint image per pixel annotation synthesis gan demonstrate gan good high level representation target data easily projected semantic segmentation masks method used create training dataset teaching separate semantic segmentation network experiments show segmentation network successfully generalizes real data additionally method outperforms supervised training number training samples small works variety different scenes classes source code proposed method publicly available\n",
            "output sentence:  gan based method joint image per pixel annotation synthesis \n",
            "\n",
            "{'rouge-1': {'r': 0.1864406779661017, 'p': 0.6875, 'f': 0.29333332997688893}, 'rouge-2': {'r': 0.11688311688311688, 'p': 0.6, 'f': 0.19565217118383743}, 'rouge-l': {'r': 0.1864406779661017, 'p': 0.6875, 'f': 0.29333332997688893}}\n",
            "pair:  network pruning widely used reducing heavy computational cost deep models typical pruning algorithm three stage pipeline training large model pruning fine tuning work make rather surprising observation fine tuning pruned model gives comparable even worse performance training model randomly initialized weights results several implications training large parameterized model necessary obtain efficient final model learned important weights large model necessarily useful small pruned model pruned architecture rather set inherited weights leads efficiency benefit final model suggests pruning algorithms could seen performing network architecture search\n",
            "output sentence:  network pruning fine tuning pruned model gives comparable worse performance training scratch advocate rethinking existing pruning \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.7777777777777778, 'f': 0.1647058804595156}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.25, 'f': 0.04123711188861734}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.4444444444444444, 'f': 0.09411764516539796}}\n",
            "pair:  carbon footprint natural language processing nlp research increasing recent years due reliance large inefficient neural network implementations distillation network compression technique attempts impart knowledge large model smaller one use teacher student distillation improve efficiency biaffine dependency parser obtains state art performance respect accuracy parsing speed dozat manning distilling original model trainable parameters observe average decrease point uas las across number diverse universal dependency treebanks faster baseline model cpu gpu inference time also observe small increase performance compressing treebanks finally distillation attain parser faster also accurate fastest modern parser penn treebank\n",
            "output sentence:  increase efficiency neural network dependency parsers teacher student distillation \n",
            "\n",
            "{'rouge-1': {'r': 0.07526881720430108, 'p': 0.6363636363636364, 'f': 0.1346153827237426}, 'rouge-2': {'r': 0.016260162601626018, 'p': 0.18181818181818182, 'f': 0.029850744761639642}, 'rouge-l': {'r': 0.053763440860215055, 'p': 0.45454545454545453, 'f': 0.09615384426220416}}\n",
            "pair:  object recognition real world requires handling long tailed even open ended data ideal visual system needs reliably recognize populated visual concepts meanwhile efficiently learn emerging new categories training instances class balanced many shot learning shot learning tackle one side problem via either learning strong classifiers populated categories learning learn shot classifiers tail classes paper investigate problem generalized shot learning gfsl model deployment required learn tail categories shots simultaneously classify head tail categories propose classifier synthesis learning castle learning framework learns synthesize calibrated shot classifiers addition multi class classifiers head classes leveraging shared neural dictionary castle sheds light upon inductive gfsl optimizing one clean effective gfsl learning objective demonstrates superior performances existing gfsl algorithms strong baselines miniimagenet tieredimagenet data sets interestingly outperforms previous state art methods evaluated standard shot learning\n",
            "output sentence:  propose learn synthesizing shot classifiers many shot classifiers using one single objective function gfsl \n",
            "\n",
            "{'rouge-1': {'r': 0.04285714285714286, 'p': 0.6, 'f': 0.07999999875555558}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.25, 'f': 0.0235294108678201}, 'rouge-l': {'r': 0.04285714285714286, 'p': 0.6, 'f': 0.07999999875555558}}\n",
            "pair:  recurrent neural network rnn effective neural network solving complex supervised unsupervised tasks significant improvement rnn field natural language processing speech processing computer vision multiple domains paper deals rnn application different use cases like incident detection fraud detection android malware classification best performing neural network architecture chosen conducting different chain experiments different network parameters structures network run epochs learning rate set range obviously rnn performed well compared classical machine learning algorithms mainly possible rnns implicitly extracts underlying features also identifies characteristics data lead better accuracy\n",
            "output sentence:  recurrent neural networks cybersecurity use cases \n",
            "\n",
            "{'rouge-1': {'r': 0.04225352112676056, 'p': 0.6, 'f': 0.07894736719182825}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.25, 'f': 0.021505375520869494}, 'rouge-l': {'r': 0.04225352112676056, 'p': 0.6, 'f': 0.07894736719182825}}\n",
            "pair:  paper explores simplicity learned neural networks various settings learned real vs random data varying size architecture using large minibatch size vs small minibatch size notion simplicity used learnability accurately prediction function neural network learned labeled samples learnability different fact often higher test accuracy results herein suggest strong correlation small generalization errors high learnability work also shows exist significant qualitative differences shallow networks compared popular deep networks broadly paper extends new direction previous work understanding properties learned neural networks hope empirical study understanding learned neural networks might shed light right assumptions made theoretical study deep learning\n",
            "output sentence:  exploring learnability learned neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.03389830508474576, 'p': 0.2222222222222222, 'f': 0.05882352711505199}, 'rouge-2': {'r': 0.014492753623188406, 'p': 0.125, 'f': 0.02597402411199204}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.2222222222222222, 'f': 0.05882352711505199}}\n",
            "pair:  propose order learning determine order graph classes representing ranks priorities classify object instance one classes end design pairwise comparator categorize relationship two instances one three cases one instance greater similar smaller comparing input instance reference instances maximizing consistency among comparison results class input estimated reliably apply order learning develop facial age estimator provides state art performance moreover performance improved order graph divided disjoint chains using gender ethnic group information even unsupervised manner\n",
            "output sentence:  notion order learning proposed applied regression problems computer vision \n",
            "\n",
            "{'rouge-1': {'r': 0.07317073170731707, 'p': 0.42857142857142855, 'f': 0.1249999975086806}, 'rouge-2': {'r': 0.021052631578947368, 'p': 0.15384615384615385, 'f': 0.037037034919410274}, 'rouge-l': {'r': 0.07317073170731707, 'p': 0.42857142857142855, 'f': 0.1249999975086806}}\n",
            "pair:  many partially observable scenarios reinforcement learning rl agents must rely long term memory order learn optimal policy demonstrate using techniques nlp supervised learning fails rl tasks due stochasticity environment exploration utilizing insights limitations traditional memory methods rl propose amrl class models learn better policies greater sample efficiency resilient noisy inputs specifically models use standard memory module summarize short term context aggregate prior states standard model without respect order show provides advantages terms gradient decay signal noise ratio time evaluating minecraft maze environments test long term memory find model improves average return baseline number parameters stronger baseline far parameters\n",
            "output sentence:  deep rl order invariant functions used conjunction standard memory modules improve gradient decay resilience noise \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 0.875, 'f': 0.20895522177767878}, 'rouge-2': {'r': 0.05714285714285714, 'p': 0.5714285714285714, 'f': 0.10389610224321134}, 'rouge-l': {'r': 0.0847457627118644, 'p': 0.625, 'f': 0.14925372924036534}}\n",
            "pair:  introduce cgnn framework learn functional causal models generative neural networks networks trained using backpropagation minimize maximum mean discrepancy observed data unlike previous approaches cgnn leverages conditional independences distributional asymmetries seamlessly discover bivariate multivariate causal structures without hidden variables cgnn estimate causal structure full differentiable generative model data throughout extensive variety experiments illustrate competitive esults cgnn state art alternatives observational causal discovery simulated real data tasks cause effect inference structure identification multivariate causal discovery\n",
            "output sentence:  discover structure functional causal models generative neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.8888888888888888, 'f': 0.22535211046220993}, 'rouge-2': {'r': 0.09722222222222222, 'p': 0.875, 'f': 0.1749999982}, 'rouge-l': {'r': 0.12903225806451613, 'p': 0.8888888888888888, 'f': 0.22535211046220993}}\n",
            "pair:  paper propose nonlinear unsupervised metric learning framework boost performance clustering algorithms framework nonlinear distance metric learning manifold embedding integrated conducted simultaneously increase natural separations among data samples metric learning component implemented feature space transformations regulated nonlinear deformable model called coherent point drifting cpd driven cpd data points get higher level linear separability subsequently picked manifold embedding component generate well separable sample projections clustering experimental results synthetic benchmark datasets show effectiveness proposed approach state art solutions unsupervised metric learning\n",
            "output sentence:  nonlinear unsupervised metric learning framework boost performance clustering algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.02857142857142857, 'p': 0.2727272727272727, 'f': 0.05172413621432823}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02857142857142857, 'p': 0.2727272727272727, 'f': 0.05172413621432823}}\n",
            "pair:  music relies heavily repetition build structure meaning self reference occurs multiple timescales motifs phrases reusing entire sections music pieces aba structure transformer vaswani et al sequence model based self attention achieved compelling results many generation tasks require maintaining long range coherence suggests self attention might also well suited modeling music musical composition performance however relative timing critically important existing approaches representing relative positional information transformer modulate attention based pairwise distance shaw et al impractical long sequences musical compositions since memory complexity quadratic sequence length propose algorithm reduces intermediate memory requirements linear sequence length enables us demonstrate transformer modified relative attention mechanism generate minute long thousands steps compositions compelling structure generate continuations coherently elaborate given motif seq seq setup generate accompaniments conditioned melodies evaluate transformer relative attention mechanism two datasets jsb chorales piano competition obtain state art results latter\n",
            "output sentence:  show first successful use transformer generating music exhibits long term structure \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.8571428571428571, 'f': 0.12631578810858723}, 'rouge-2': {'r': 0.043478260869565216, 'p': 0.8333333333333334, 'f': 0.082644627156615}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.8571428571428571, 'f': 0.12631578810858723}}\n",
            "pair:  predicting future real world settings particularly raw sensory observations images exceptionally challenging real world events stochastic unpredictable high dimensionality complexity natural images requires predictive model build intricate understanding natural world many existing methods tackle problem making simplifying assumptions environment one common assumption outcome deterministic one plausible future lead low quality predictions real world settings stochastic dynamics paper develop stochastic variational video prediction sv method predicts different possible future sample latent variables best knowledge model first provide effective stochastic multi frame prediction real world video demonstrate capability proposed method predicting detailed future frames videos multiple real world datasets action free action conditioned find proposed method produces substantially improved video predictions compared model without stochasticity stochastic video prediction methods sv implementation open sourced upon publication\n",
            "output sentence:  stochastic variational video prediction real world settings \n",
            "\n",
            "{'rouge-1': {'r': 0.11235955056179775, 'p': 0.6666666666666666, 'f': 0.19230768983912724}, 'rouge-2': {'r': 0.056074766355140186, 'p': 0.4, 'f': 0.09836065358102665}, 'rouge-l': {'r': 0.0898876404494382, 'p': 0.5333333333333333, 'f': 0.15384615137758878}}\n",
            "pair:  conditional generative adversarial networks cgans finding increasingly widespread use many application domains despite outstanding progress quantitative evaluation models often involves multiple distinct metrics assess different desirable properties image quality conditional consistency intra conditioning diversity setting model benchmarking becomes challenge metric may indicate different best model paper propose frechet joint distance fjd defined frechet distance joint distributions images conditioning allowing implicitly capture aforementioned properties single metric conduct proof concept experiments controllable synthetic dataset consistently highlight benefits fjd compared currently established metrics moreover use newly introduced metric compare existing cgan based models variety conditioning modalities class labels object masks bounding boxes images text captions show fjd used promising single metric model benchmarking\n",
            "output sentence:  propose new metric evaluating conditional gans captures image quality conditional consistency intra conditioning diversity single measure \n",
            "\n",
            "{'rouge-1': {'r': 0.17894736842105263, 'p': 0.8947368421052632, 'f': 0.29824561125731}, 'rouge-2': {'r': 0.03418803418803419, 'p': 0.19047619047619047, 'f': 0.05797101191241349}, 'rouge-l': {'r': 0.09473684210526316, 'p': 0.47368421052631576, 'f': 0.15789473406432752}}\n",
            "pair:  deep reinforcement learning algorithms proven successful variety domains however tasks sparse rewards remain challenging state space large goal oriented tasks among typical problems domain reward received final goal accomplished work propose potential solution problems introduction experience based tendency reward mechanism provides agent additional hints based discriminative learning past experiences automated reverse curriculum mechanism provides dense additional learning signals states lead success also allows agent retain tendency reward instead whole histories experience multi phase curriculum learning extensively study advantages method standard sparse reward domains like maze super mario bros show method performs efficiently robustly prior approaches tasks long time horizons large state space addition demonstrate using optional keyframe scheme small quantity key states approach solve difficult robot manipulation challenges directly perception sparse rewards\n",
            "output sentence:  propose tendency rl efficiently solve goal oriented tasks large state space using automated curriculum discriminative discriminative reward potential shaping shaping robot shaping robot \n",
            "\n",
            "{'rouge-1': {'r': 0.22413793103448276, 'p': 0.7222222222222222, 'f': 0.3421052595429363}, 'rouge-2': {'r': 0.11594202898550725, 'p': 0.47058823529411764, 'f': 0.18604650845592216}, 'rouge-l': {'r': 0.1896551724137931, 'p': 0.6111111111111112, 'f': 0.2894736805955679}}\n",
            "pair:  describe three approaches enabling extremely computationally limited embedded scheduler consider small number alternative activities based resource availability consider case scheduler computationally limited cannot backtrack search first two approaches precompile resource checks called guards enable selection preferred alternative activity sufficient resources estimated available schedule remaining activities final approach mimics backtracking invoking scheduler multiple times alternative activities present evaluation techniques mission scenarios called sol types nasa next planetary rover techniques evaluated inclusion onboard scheduler\n",
            "output sentence:  paper describes three techniques allow non backtracking computationally limited scheduler consider small number alternative activities based resource availability \n",
            "\n",
            "{'rouge-1': {'r': 0.045454545454545456, 'p': 0.2727272727272727, 'f': 0.0779220754730984}, 'rouge-2': {'r': 0.012658227848101266, 'p': 0.1, 'f': 0.02247190811766209}, 'rouge-l': {'r': 0.030303030303030304, 'p': 0.18181818181818182, 'f': 0.051948049499072474}}\n",
            "pair:  significant strides made toward designing better generative models recent years despite progress however state art approaches still largely unable capture complex global structure data example images buildings typically contain spatial patterns windows repeating regular intervals state art generative methods easily reproduce structures propose address problem incorporating programs representing global structure generative model loop may represent configuration windows furthermore propose framework learning models leveraging program synthesis generate training data synthetic real world data demonstrate approach substantially better state art generating completing images contain global structure\n",
            "output sentence:  applying program synthesis tasks image completion generation within deep learning framework \n",
            "\n",
            "{'rouge-1': {'r': 0.07216494845360824, 'p': 0.6363636363636364, 'f': 0.1296296278000686}, 'rouge-2': {'r': 0.04838709677419355, 'p': 0.5454545454545454, 'f': 0.08888888739204391}, 'rouge-l': {'r': 0.061855670103092786, 'p': 0.5454545454545454, 'f': 0.11111110928155009}}\n",
            "pair:  convolutional neural networks cnns generally acknowledged one driving forces advancement computer vision despite promising performances many tasks cnns still face major obstacles road achieving ideal machine intelligence one cnns complex hard interpret another standard cnns require large amounts annotated data sometimes hard obtain desirable able learn examples work address limitations cnns developing novel simple interpretable models shot learn ing models based idea encoding objects terms visual concepts interpretable visual cues represented feature vectors within cnns first adapt learning visual concepts shot setting uncover two key properties feature encoding using visual concepts call category sensitivity spatial pattern motivated properties present two intuitive models problem shot learning experiments show models achieve competitive performances much flexible interpretable alternative state art shot learning methods conclude using visual concepts helps expose natural capability cnns shot learning\n",
            "output sentence:  enable ordinary cnns shot learning exploiting visual concepts interpretable visual cues learnt cnns \n",
            "\n",
            "{'rouge-1': {'r': 0.09302325581395349, 'p': 1.0, 'f': 0.1702127644001811}, 'rouge-2': {'r': 0.019230769230769232, 'p': 0.3333333333333333, 'f': 0.036363635332231435}, 'rouge-l': {'r': 0.09302325581395349, 'p': 1.0, 'f': 0.1702127644001811}}\n",
            "pair:  work presents poincar wasserstein autoencoder reformulation recently proposed wasserstein autoencoder framework non euclidean manifold poincar ball model hyperbolic space assuming latent space hyperbolic use intrinsic hierarchy impose structure learned latent space representations show datasets latent hierarchies recover structure low dimensional latent space also demonstrate model visual domain analyze properties show competitive results graph link prediction task\n",
            "output sentence:  wasserstein autoencoder hyperbolic latent space \n",
            "\n",
            "{'rouge-1': {'r': 0.19101123595505617, 'p': 1.0, 'f': 0.3207547142880029}, 'rouge-2': {'r': 0.14414414414414414, 'p': 0.8888888888888888, 'f': 0.248062013102578}, 'rouge-l': {'r': 0.19101123595505617, 'p': 1.0, 'f': 0.3207547142880029}}\n",
            "pair:  integration knowledge base kb neural dialogue agent one key challenges conversational ai memory networks proven effective encode kb information external memory thus generate fluent informed responses unfortunately memory becomes full latent representations training common strategy overwrite old memory entries randomly paper question approach provide experimental evidence showing conventional memory networks generate many redundant latent vectors resulting overfitting need larger memories introduce memory dropout automatic technique encourages diversity latent space aging redundant memories increase probability overwritten training sampling new memories summarize knowledge acquired redundant memories technique allows us incorporate knowledge bases achieve state art dialogue generation stanford multi turn dialogue dataset considering architecture use provides improvement bleu points automatic generation responses increase recognition named entities\n",
            "output sentence:  conventional memory networks generate many redundant latent vectors resulting overfitting need larger memories introduce memory automatic technique diversity latent space \n",
            "\n",
            "{'rouge-1': {'r': 0.041666666666666664, 'p': 0.1, 'f': 0.05882352525951587}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.1, 'f': 0.05882352525951587}}\n",
            "pair:  data arise multiple latent subpopulations machine learning frameworks typically estimate parameter values independently sub population paper propose overcome limits considering samples tasks multitask learning framework\n",
            "output sentence:  present method estimate collections regression models model personalized single sample \n",
            "\n",
            "{'rouge-1': {'r': 0.1746031746031746, 'p': 0.9166666666666666, 'f': 0.29333333064533335}, 'rouge-2': {'r': 0.11842105263157894, 'p': 0.75, 'f': 0.20454545219008266}, 'rouge-l': {'r': 0.1746031746031746, 'p': 0.9166666666666666, 'f': 0.29333333064533335}}\n",
            "pair:  propose novel deep network architecture lifelong learning refer dynamically expandable network den dynamically decide network capacity trains sequence tasks learn compact overlapping knowledge sharing structure among tasks den efficiently trained online manner performing selective retraining dynamically expands network capacity upon arrival task necessary number units effectively prevents semantic drift splitting duplicating units timestamping validate den multiple public datasets lifelong learning scenarios multiple public datasets significantly outperforms existing lifelong learning methods deep networks also achieves level performance batch model substantially fewer number parameters\n",
            "output sentence:  propose novel deep network architecture dynamically decide network capacity trains lifelong learning scenario \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.42857142857142855, 'f': 0.1621621590942294}, 'rouge-2': {'r': 0.02564102564102564, 'p': 0.15, 'f': 0.04379561794448307}, 'rouge-l': {'r': 0.07777777777777778, 'p': 0.3333333333333333, 'f': 0.1261261230581934}}\n",
            "pair:  paper investigate family functions representable deep neural networks dnn rectified linear units relu give algorithm train relu dnn one hidden layer em global optimality runtime polynomial data size albeit exponential input dimension improve known lower bounds size exponential super exponential approximating relu deep net function shallower relu net gap theorems hold smoothly parametrized families hard functions contrary countable discrete families known literature example consequence gap theorems following every natural number exists function representable relu dnn hidden layers total size relu dnn hidden layers require least frac total nodes finally family dnns relu activations show new lowerbound number affine pieces larger previous constructions certain regimes network architecture distinctively lowerbound demonstrated explicit construction emph smoothly parameterized family functions attaining scaling construction utilizes theory zonotopes polyhedral theory\n",
            "output sentence:  paper characterizes functions representable relu dnns formally studies benefit depth architectures gives algorithm implement empirical risk minimization global optimality two layer relu \n",
            "\n",
            "{'rouge-1': {'r': 0.0759493670886076, 'p': 0.5, 'f': 0.13186812957855334}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05063291139240506, 'p': 0.3333333333333333, 'f': 0.08791208562250942}}\n",
            "pair:  deterministic models approximations reality often easier build interpret stochastic alternatives unfortunately nature capricious observational data never fully explained deterministic models practice observation process noise need added adapt deterministic models behave stochastically capable explaining extrapolating noisy data adding process noise deterministic simulators induce failure simulator resulting return value certain inputs property describe brittle investigate address wasted computation arises failures effect failures downstream inference tasks show performing inference space viewed rejection sampling train conditional normalizing flow proposal noise values low probability simulator crashes increasing computational efficiency inference fidelity fixed sample budget used proposal approximate inference algorithm\n",
            "output sentence:  learn conditional autoregressive flow propose perturbations induce simulator failure improving inference performance \n",
            "\n",
            "{'rouge-1': {'r': 0.054945054945054944, 'p': 0.45454545454545453, 'f': 0.09803921376201463}, 'rouge-2': {'r': 0.009433962264150943, 'p': 0.1, 'f': 0.017241377734839622}, 'rouge-l': {'r': 0.054945054945054944, 'p': 0.45454545454545453, 'f': 0.09803921376201463}}\n",
            "pair:  recent studies attention modules enabled higher performance computer vision tasks capturing global contexts accordingly attending important features paper propose simple highly parametrically efficient module named tree structured attention module tam recursively encourages neighboring channels collaborate order produce spatial attention map output unlike attention modules try capture long range dependencies channel module focuses imposing non linearities tween channels utilizing point wise group convolution module strengthens representational power model also acts gate controls signal flow module allows model achieve higher performance highly parameter efficient manner empirically validate effectiveness module extensive experiments cifar svhn datasets proposed attention module employed resnet resnet models gain accuracy improvement less parameter head pytorch implementation code publicly available\n",
            "output sentence:  paper proposes attention module captures inter channel relationships offers large performance gains \n",
            "\n",
            "{'rouge-1': {'r': 0.15853658536585366, 'p': 0.9285714285714286, 'f': 0.27083333084201394}, 'rouge-2': {'r': 0.08823529411764706, 'p': 0.6428571428571429, 'f': 0.15517241167063023}, 'rouge-l': {'r': 0.15853658536585366, 'p': 0.9285714285714286, 'f': 0.27083333084201394}}\n",
            "pair:  selection initial parameter values gradient based optimization deep neural networks one impactful hyperparameter choices deep learning systems affecting convergence times model performance yet despite significant empirical theoretical analysis relatively little proved concrete effects different initialization schemes work analyze effect initialization deep linear networks provide first time rigorous proof drawing initial weights orthogonal group speeds convergence relative standard gaussian initialization iid weights show deep networks width needed efficient convergence global minimum orthogonal initializations independent depth whereas width needed efficient convergence gaussian initializations scales linearly depth results demonstrate benefits good initialization persist throughout learning suggesting explanation recent empirical successes found initializing deep non linear networks according principle dynamical isometry\n",
            "output sentence:  provide first time rigorous proof orthogonal initialization speeds convergence relative gaussian initialization deep linear networks \n",
            "\n",
            "{'rouge-1': {'r': 0.15, 'p': 0.6521739130434783, 'f': 0.24390243598387207}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.34782608695652173, 'f': 0.11188810918871346}, 'rouge-l': {'r': 0.13, 'p': 0.5652173913043478, 'f': 0.21138211078062003}}\n",
            "pair:  recently neural network based forward dynamics models proposed attempt learn dynamics physical systems deterministic way near term motion predicted accurately long term predictions suffer accumulating input prediction errors lead plausible different trajectories diverge ground truth system predicts distributions future physical states long time horizons based uncertainty thus promising solution work introduce novel robust monte carlo sampling based graph convolutional dropout method allows us sample multiple plausible trajectories initial state given neural network based forward dynamics predictor introducing new shape preservation loss training dynamics model recurrently stabilize long term predictions show model long term forward dynamics prediction errors complicated physical interactions rigid deformable objects various shapes significantly lower existing strong baselines lastly demonstrate generating multiple trajectories monte carlo dropout method used train model free reinforcement learning agents faster better solutions simple manipulation tasks\n",
            "output sentence:  propose stochastic differentiable forward dynamics predictor able sample multiple physically plausible trajectories initial input state show used train model free policies efficiently policies efficiently \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.625, 'f': 0.19607842872741257}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.2857142857142857, 'f': 0.05797101266960729}, 'rouge-l': {'r': 0.11627906976744186, 'p': 0.625, 'f': 0.19607842872741257}}\n",
            "pair:  recent work shown separation expressive power depth depth neural networks separation results shown constructing functions input distributions function well approximable depth neural network polynomial size cannot well approximated chosen input distribution depth neural network polynomial size results robust require carefully chosen functions well input distributions show similar separation expressive power depth depth sigmoidal neural networks large class input distributions long weights polynomially bounded also show depth sigmoidal neural networks small width small weights well approximated low degree multivariate polynomials\n",
            "output sentence:  depth vs separation sigmoidal neural networks general distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.04, 'p': 0.5, 'f': 0.07407407270233198}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.013333333333333334, 'p': 0.16666666666666666, 'f': 0.024691356652949324}}\n",
            "pair:  generative models provide way model structure complex distributions shown useful many tasks practical interest however current techniques training generative models require access fully observed samples many settings expensive even impossible obtain fully observed samples economical obtain partial noisy observations consider task learning implicit generative model given lossy measurements samples distribution interest show true underlying distribution provably recovered even presence per sample information loss class measurement models based propose new method training generative adversarial networks gans call ambientgan three benchmark datasets various measurement models demonstrate substantial qualitative quantitative improvements generative models trained method obtain higher inception scores baselines\n",
            "output sentence:  learn gans noisy distorted partial observations \n",
            "\n",
            "{'rouge-1': {'r': 0.1368421052631579, 'p': 0.8666666666666667, 'f': 0.23636363400826452}, 'rouge-2': {'r': 0.06722689075630252, 'p': 0.47058823529411764, 'f': 0.11764705663602944}, 'rouge-l': {'r': 0.12631578947368421, 'p': 0.8, 'f': 0.21818181582644633}}\n",
            "pair:  inspired recent successes deep generative models text speech tts wavenet van den oord et al tacotron wang et al article proposes use deep generative model tailored automatic speech recognition asr primary acoustic model overall recognition system separate language model lm two dimensions depth considered use mixture density networks autoregressive non autoregressive generate density functions capable modeling acoustic input sequences much powerful conditioning first generation generative models asr gaussian mixture models hidden markov models gmm hmms use standard lstms spirit original tandem approach produce discriminative feature vectors generative modeling combining mixture density networks deep discriminative features leads novel dual stack lstm architecture directly related rnn transducer graves explicit functional form density combining naturally separate language model using bayes rule generative models discussed compared experimentally terms log likelihoods frame accuracies\n",
            "output sentence:  paper proposes use deep generative acoustic model automatic speech recognition combining naturally deep sequence sequence sequence using bayes using bayes bayes \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.5555555555555556, 'f': 0.1388888867013889}, 'rouge-2': {'r': 0.025, 'p': 0.25, 'f': 0.04545454380165295}, 'rouge-l': {'r': 0.07936507936507936, 'p': 0.5555555555555556, 'f': 0.1388888867013889}}\n",
            "pair:  flow based models real nvp extremely powerful approach density estimation however existing flow based models restricted transforming continuous densities continuous input space similarly continuous distributions continuous latent variables makes poorly suited modeling representing discrete structures data distributions example class membership discrete symmetries address difficulty present normalizing flow architecture relies domain partitioning using locally invertible functions possesses real discrete valued latent variables real discrete rad approach retains desirable normalizing flow properties exact sampling exact inference analytically computable probabilities time allowing simultaneous modeling continuous discrete structure data distribution\n",
            "output sentence:  flow based models non invertible also learn discrete variables \n",
            "\n",
            "{'rouge-1': {'r': 0.11320754716981132, 'p': 0.46153846153846156, 'f': 0.18181817865472916}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.25, 'f': 0.07692307431952672}, 'rouge-l': {'r': 0.11320754716981132, 'p': 0.46153846153846156, 'f': 0.18181817865472916}}\n",
            "pair:  autonomy adaptation machines requires able measure errors consider advantages limitations approach machine measure error regression task machine measure error regression sub components ground truth correct predictions compressed sensing approach applied error signal regressors recover precision error without ground truth allows regressors strongly correlated long many related solutions however unique property ground truth inference solutions adding ell minimization condition recover correct solution settings error correction possible briefly discuss similarity mathematics ground truth inference regressors classifiers\n",
            "output sentence:  non parametric method measure error moments regressors without ground truth used biased regressors \n",
            "\n",
            "{'rouge-1': {'r': 0.0410958904109589, 'p': 0.3333333333333333, 'f': 0.07317072975312319}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.125, 'f': 0.01999999852800011}, 'rouge-l': {'r': 0.0410958904109589, 'p': 0.3333333333333333, 'f': 0.07317072975312319}}\n",
            "pair:  present information theoretic framework understanding trade offs unsupervised learning deep latent variables models using variational inference framework emphasizes need consider latent variable models along two dimensions ability reconstruct inputs distortion communication cost rate derive optimal frontier generative models two dimensional rate distortion plane show standard evidence lower bound objective insufficient select points along frontier however performing targeted optimization learn generative models different rates able learn many models achieve similar generative performance make vastly different trade offs terms usage latent variable experiments mnist omniglot variety architectures show framework sheds light many recent proposed extensions variational autoencoder family\n",
            "output sentence:  provide information theoretic experimental analysis state art variational autoencoders \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.3333333333333333, 'f': 0.10526315523545714}, 'rouge-2': {'r': 0.012195121951219513, 'p': 0.09090909090909091, 'f': 0.02150537425829596}, 'rouge-l': {'r': 0.0625, 'p': 0.3333333333333333, 'f': 0.10526315523545714}}\n",
            "pair:  vector semantics especially sentence vectors recently used successfully many areas natural language processing however relatively little work explored internal structure properties spaces sentence vectors paper explore properties sentence vectors studying particular real world application automatic summarization particular show cosine similarity sentence vectors document vectors strongly correlated sentence importance vector semantics identify correct gaps sentences chosen far document addition identify specific dimensions linked effective summaries knowledge first time specific dimensions sentence embeddings connected sentence properties also compare features different methods sentence embeddings many insights applications uses sentence embeddings far beyond summarization\n",
            "output sentence:  comparison detailed analysis various sentence embedding models real world task automatic summarization \n",
            "\n",
            "{'rouge-1': {'r': 0.15463917525773196, 'p': 1.0, 'f': 0.26785714053730875}, 'rouge-2': {'r': 0.09565217391304348, 'p': 0.6875, 'f': 0.1679389291533128}, 'rouge-l': {'r': 0.14432989690721648, 'p': 0.9333333333333333, 'f': 0.24999999768016584}}\n",
            "pair:  estimating location image taken based solely contents image challenging task even humans properly labeling image fashion relies heavily contextual information simple identifying single object image thus methods attempt must somehow account complexities single model date completely capable addressing challenges work contributes state research image geolocation inferencing introducing novel global meshing strategy outlining variety training procedures overcome considerable data limitations training models demonstrating incorporating additional information used improve overall performance geolocation inference model work shown delaunay triangles effective type mesh geolocation relatively low volume scenarios compared results state art models use quad trees order magnitude training data addition time posting learned user albuming meta data easily incorporated improve geolocation country level km locality accuracy city level km localities\n",
            "output sentence:  global geolocation inferencing strategy novel meshing strategy demonstrating incorporating additional information used improve overall performance geolocation inference model \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.6, 'f': 0.1499999978125}, 'rouge-2': {'r': 0.011111111111111112, 'p': 0.1111111111111111, 'f': 0.020202018549127777}, 'rouge-l': {'r': 0.05714285714285714, 'p': 0.4, 'f': 0.09999999781250005}}\n",
            "pair:  impressive lifelong learning animal brains primarily enabled plastic changes synaptic connectivity importantly changes passive actively controlled neuromodulation control brain resulting self modifying abilities brain play important role learning adaptation major basis biological reinforcement learning show first time artificial neural networks neuromodulated plasticity trained gradient descent extending previous work differentiable hebbian plasticity propose differentiable formulation neuromodulation plasticity show neuromodulated plasticity improves performance neural networks reinforcement learning supervised learning tasks one task neuromodulated plastic lstms millions parameters outperform standard lstms benchmark language modeling task controlling number parameters conclude differentiable neuromodulation plasticity offers powerful new framework training neural networks\n",
            "output sentence:  neural networks trained modify connectivity improving online learning performance challenging tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.16393442622950818, 'p': 0.5882352941176471, 'f': 0.256410253001315}, 'rouge-2': {'r': 0.039473684210526314, 'p': 0.17647058823529413, 'f': 0.06451612604462957}, 'rouge-l': {'r': 0.11475409836065574, 'p': 0.4117647058823529, 'f': 0.17948717607823805}}\n",
            "pair:  paper ask main factors determine classifier decision making uncover factors studying latent codes produced auto encoding frameworks deliver explanation classifier behaviour propose method provides series examples highlighting semantic differences classifier decisions generate examples interpolations latent space introduce formalize notion semantic stochastic path suitable stochastic process defined feature space via latent code interpolations introduce concept semantic lagrangians way incorporate desired classifier behaviour find solution associated variational problem allows highlighting differences classifier decision importantly within framework classifier used black box evaluation required\n",
            "output sentence:  generate examples explain classifier desicion via interpolations latent space variational auto encoder cost extended functional classifier example path data \n",
            "\n",
            "{'rouge-1': {'r': 0.16842105263157894, 'p': 0.8421052631578947, 'f': 0.2807017516081871}, 'rouge-2': {'r': 0.03418803418803419, 'p': 0.17391304347826086, 'f': 0.05714285439693891}, 'rouge-l': {'r': 0.09473684210526316, 'p': 0.47368421052631576, 'f': 0.15789473406432752}}\n",
            "pair:  deep reinforcement learning algorithms proven successful variety domains however tasks sparse rewards remain challenging state space large goal oriented tasks among typical problems domain reward received final goal accomplished work propose potential solution problems introduction experience based tendency reward mechanism provides agent additional hints based discriminative learning past experiences automated reverse curriculum mechanism provides dense additional learning signals states lead success also allows agent retain tendency reward instead whole histories experience multi phase curriculum learning extensively study advantages method standard sparse reward domains like maze super mario bros show method performs efficiently robustly prior approaches tasks long time horizons large state space addition demonstrate using optional keyframe scheme small quantity key states approach solve difficult robot manipulation challenges directly perception sparse rewards\n",
            "output sentence:  propose tendency rl efficiently solve goal oriented tasks large state space using automated curriculum discriminative discriminative shaping reward shaping shaping shaping robot shaping tasks manipulation \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}, 'rouge-2': {'r': 0.08860759493670886, 'p': 0.875, 'f': 0.16091953855991545}, 'rouge-l': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}}\n",
            "pair:  propose new model making generalizable diverse retrosynthetic reaction predictions given target compound task predict likely chemical reactants produce target generative task framed sequence sequence problem using smiles representations molecules building top popular transformer architecture propose two novel pre training methods construct relevant auxiliary tasks plausible reactions problem furthermore incorporate discrete latent variable model architecture encourage model produce diverse set alternative predictions subset reaction examples united states patent literature uspto benchmark dataset model greatly improves performance baseline also generating predictions diverse\n",
            "output sentence:  propose new model making generalizable diverse retrosynthetic reaction predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.1016949152542373, 'p': 0.6666666666666666, 'f': 0.17647058593858134}, 'rouge-2': {'r': 0.04, 'p': 0.3333333333333333, 'f': 0.07142856951530618}, 'rouge-l': {'r': 0.1016949152542373, 'p': 0.6666666666666666, 'f': 0.17647058593858134}}\n",
            "pair:  dependency generalization error neural networks model dataset size critical importance practice understanding theory neural networks nevertheless functional form dependency remains elusive work present functional form approximates well generalization error practice capitalizing successful concept model scaling width depth able simultaneously construct form specify exact models attain across model data scales construction follows insights obtained observations conducted range model data scales various model types datasets vision language tasks show form fits observations well across scales provides accurate predictions small large scale models data\n",
            "output sentence:  predict generalization error specify model attains across model data scales \n",
            "\n",
            "{'rouge-1': {'r': 0.16923076923076924, 'p': 0.9166666666666666, 'f': 0.28571428308315067}, 'rouge-2': {'r': 0.0975609756097561, 'p': 0.7272727272727273, 'f': 0.17204300866689792}, 'rouge-l': {'r': 0.16923076923076924, 'p': 0.9166666666666666, 'f': 0.28571428308315067}}\n",
            "pair:  study implicit bias gradient descent methods solving binary classification problem linearly separable dataset classifier described nonlinear relu model objective function adopts exponential loss function first characterize landscape loss function show exist spurious asymptotic local minima besides asymptotic global minima show gradient descent gd converge either global local max margin direction may diverge desired max margin direction general context stochastic gradient descent sgd show converges expectation either global local max margin direction sgd converges explore implicit bias algorithms learning multi neuron network certain stationary conditions show learned classifier maximizes margins sample pattern partition relu activation\n",
            "output sentence:  study implicit bias gradient methods solving binary classification problem nonlinear relu models \n",
            "\n",
            "{'rouge-1': {'r': 0.1188118811881188, 'p': 0.8571428571428571, 'f': 0.20869565003553875}, 'rouge-2': {'r': 0.05426356589147287, 'p': 0.4666666666666667, 'f': 0.0972222203559028}, 'rouge-l': {'r': 0.09900990099009901, 'p': 0.7142857142857143, 'f': 0.17391304133988658}}\n",
            "pair:  existing sequence prediction methods mostly concerned time independent sequences actual time span events irrelevant distance events simply difference order positions sequence time independent view sequences applicable data natural languages dealing words sentence inappropriate inefficient many real world events observed collected unequally spaced points time naturally arise person goes grocery store makes phone call time span events carry important information sequence dependence human behaviors work propose set methods using time sequence prediction neural sequence models rnn amenable handling token like input propose two methods time dependent event representation based intuition time tokenized everyday life previous work embedding contextualization also introduce two methods using next event duration regularization training sequence prediction model discuss methods based recurrent neural nets evaluate methods well baseline models five datasets resemble variety sequence prediction tasks experiments revealed proposed methods offer accuracy gain baseline models range settings\n",
            "output sentence:  proposed methods time dependent event representation regularization sequence prediction evaluated methods five datasets involve range sequence prediction tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.25, 'f': 0.09523809215419511}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.25, 'f': 0.09523809215419511}}\n",
            "pair:  paper presents preliminary ideas work auto mated learning hierarchical goal networks nondeter ministic domains currently implementing ideas expressed paper\n",
            "output sentence:  learning hgns nd domains \n",
            "\n",
            "{'rouge-1': {'r': 0.06779661016949153, 'p': 0.3333333333333333, 'f': 0.11267605352906178}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.09090909090909091, 'f': 0.025974023525046613}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.25, 'f': 0.08450703944455476}}\n",
            "pair:  edge intelligence especially binary neural network bnn attracted considerable attention artificial intelligence community recently bnns significantly reduce computational cost model size memory footprint however still performance gap successful full precision neural network relu activation bnns argue accuracy drop bnns due geometry analyze behaviour full precision neural network relu activation compare binarized counterpart comparison suggests random bias initialization remedy activation saturation full precision networks leads us towards improved bnn training numerical experiments confirm geometric intuition\n",
            "output sentence:  improve saturating activations sigmoid tanh htanh etc binarized neural network bias initialization \n",
            "\n",
            "{'rouge-1': {'r': 0.10416666666666667, 'p': 0.7142857142857143, 'f': 0.18181817959669425}, 'rouge-2': {'r': 0.017543859649122806, 'p': 0.16666666666666666, 'f': 0.03174603002267583}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.5714285714285714, 'f': 0.14545454323305787}}\n",
            "pair:  paper propose perform model ensembling multiclass multilabel learning setting using wasserstein barycenters optimal transport metrics wasserstein distance allow incorporating semantic side information word embeddings using barycenters find consensus models allows us balance confidence semantics finding agreement models show applications wasserstein ensembling attribute based classification multilabel learning image captioning generation results show ensembling viable alternative basic geometric arithmetic mean ensembling\n",
            "output sentence:  propose use wasserstein barycenters semantic model ensembling \n",
            "\n",
            "{'rouge-1': {'r': 0.02702702702702703, 'p': 0.3333333333333333, 'f': 0.04999999861250004}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02702702702702703, 'p': 0.3333333333333333, 'f': 0.04999999861250004}}\n",
            "pair:  plan recognition aims look target plans best explain observed actions based plan libraries domain models despite success previous approaches plan recognition mostly rely correct action observations recent advances visual activity recognition potential enabling applications automated video surveillance effective approaches problems would require ability recognize plans agents video information traditional plan recognition algorithms rely access detailed planning domain models one recent promising direction involves learning approximate shallow domain models directly observed activity sequences plan recognition approaches expect observed action sequences inputs however visual inference results often noisy uncertain typically represented distribution possible actions work develop visual plan recognition framework recognizes plans approximate domain model learned uncertain visual data\n",
            "output sentence:  handling uncertainty visual perception plan recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.22641509433962265, 'p': 0.75, 'f': 0.347826083394245}, 'rouge-2': {'r': 0.1, 'p': 0.375, 'f': 0.15789473351800565}, 'rouge-l': {'r': 0.18867924528301888, 'p': 0.625, 'f': 0.2898550689014913}}\n",
            "pair:  many applications training data machine learning task partitioned across multiple nodes aggregating data may infeasible due storage communication privacy constraints work present good enough model spaces gems novel framework learning global satisficing good enough model within communication rounds carefully combining space local nodes satisficing models experiments benchmark medical datasets approach outperforms baseline aggregation techniques ensembling model averaging performs comparably ideal non distributed models\n",
            "output sentence:  present good enough model spaces gems framework learning aggregate model distributed nodes within small number communication rounds \n",
            "\n",
            "{'rouge-1': {'r': 0.12962962962962962, 'p': 0.5833333333333334, 'f': 0.21212120914600552}, 'rouge-2': {'r': 0.046153846153846156, 'p': 0.2727272727272727, 'f': 0.07894736594529093}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.5, 'f': 0.18181817884297521}}\n",
            "pair:  uncertainty estimation ensembling methods go hand hand uncertainty estimation one main benchmarks assessment ensembling performance time deep learning ensembles provided state art results uncertainty estimation work focus domain uncertainty image classification explore standards quantification point pitfalls existing metrics avoiding pitfalls perform broad study different ensembling techniques provide insight broad comparison introduce deep ensemble equivalent dee show many sophisticated ensembling techniques equivalent ensemble independently trained networks terms test log likelihood\n",
            "output sentence:  highlight problems common metrics domain uncertainty perform broad study modern ensembling techniques \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.8333333333333334, 'f': 0.10989010865837459}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03529411764705882, 'p': 0.5, 'f': 0.06593406470233065}}\n",
            "pair:  building agents interact web would allow significant improvements knowledge understanding representation learning however web navigation tasks difficult current deep reinforcement learning rl models due large discrete action space varying number actions states work introduce dom net novel architecture rl based web navigation address problems parametrizes functions separate networks different action categories clicking dom element typing string input model utilizes graph neural network represent tree structured html standard web page demonstrate capabilities model miniwob environment match outperform existing work without use expert demonstrations furthermore show improvements sample efficiency training multi task setting allowing model transfer learned behaviours across tasks\n",
            "output sentence:  graph based deep network web navigation \n",
            "\n",
            "{'rouge-1': {'r': 0.08450704225352113, 'p': 0.75, 'f': 0.15189873235699408}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.42857142857142855, 'f': 0.0666666652320988}, 'rouge-l': {'r': 0.056338028169014086, 'p': 0.5, 'f': 0.10126582096458903}}\n",
            "pair:  recent trends incorporating attention mechanisms vision led researchers reconsider supremacy convolutional layers primary building block beyond helping cnns handle long range dependencies ramachandran et al showed attention completely replace convolution achieve state art performance vision tasks raises question learned attention layers operate similarly convolutional layers work provides evidence attention layers perform convolution indeed often learn practice specifically prove multi head self attention layer sufficient number heads least expressive convolutional layer numerical experiments show self attention layers attend pixel grid patterns similarly cnn layers corroborating analysis code publicly available\n",
            "output sentence:  self attention layer perform convolution often learns practice \n",
            "\n",
            "{'rouge-1': {'r': 0.0875, 'p': 0.5833333333333334, 'f': 0.15217391077504727}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.36363636363636365, 'f': 0.06611570082644631}, 'rouge-l': {'r': 0.075, 'p': 0.5, 'f': 0.13043478034026468}}\n",
            "pair:  work tackles problem characterizing understanding decision boundaries neural networks piece wise linear non linearity activations use tropical geometry new development area algebraic geometry provide characterization decision boundaries simple neural network form affine relu affine specifically show decision boundaries subset tropical hypersurface intimately related polytope formed convex hull two zonotopes generators zonotopes precise functions neural network parameters utilize geometric characterization shed light new perspective three tasks propose new tropical perspective lottery ticket hypothesis see effect different initializations tropical geometric representation decision boundaries also leverage characterization new set tropical regularizers deal directly decision boundaries network investigate use regularizers neural network pruning removing network parameters contribute tropical geometric representation decision boundaries generating adversarial input attacks input perturbations explicitly perturbing decision boundaries geometry change network prediction input\n",
            "output sentence:  tropical geometry leveraged represent decision boundaries neural networks bring light interesting insights \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.42857142857142855, 'f': 0.1621621590942294}, 'rouge-2': {'r': 0.02564102564102564, 'p': 0.15, 'f': 0.04379561794448307}, 'rouge-l': {'r': 0.07777777777777778, 'p': 0.3333333333333333, 'f': 0.1261261230581934}}\n",
            "pair:  paper investigate family functions representable deep neural networks dnn rectified linear units relu give algorithm train relu dnn one hidden layer em global optimality runtime polynomial data size albeit exponential input dimension improve known lower bounds size exponential super exponential approximating relu deep net function shallower relu net gap theorems hold smoothly parametrized families hard functions contrary countable discrete families known literature example consequence gap theorems following every natural number exists function representable relu dnn hidden layers total size relu dnn hidden layers require least frac total nodes finally family dnns relu activations show new lowerbound number affine pieces larger previous constructions certain regimes network architecture distinctively lowerbound demonstrated explicit construction emph smoothly parameterized family functions attaining scaling construction utilizes theory zonotopes polyhedral theory\n",
            "output sentence:  paper characterizes functions representable relu dnns formally studies benefit depth architectures gives algorithm implement empirical risk minimization global optimality two layer relu \n",
            "\n",
            "{'rouge-1': {'r': 0.1625, 'p': 0.7222222222222222, 'f': 0.2653061194502291}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.3157894736842105, 'f': 0.09917355107164819}, 'rouge-l': {'r': 0.1125, 'p': 0.5, 'f': 0.1836734663890046}}\n",
            "pair:  deep generative models achieved remarkable progress recent years despite progress quantitative evaluation comparison generative models remains one important challenges one popular metrics evaluating generative models log likelihood direct computation log likelihood intractable recently shown log likelihood interesting generative models variational autoencoders vae generative adversarial networks gan efficiently estimated using annealed importance sampling ais work argue log likelihood metric cannot represent different performance characteristics generative models propose use rate distortion curves evaluate compare deep generative models show approximate entire rate distortion curve using one single run ais roughly computational cost single log likelihood estimate evaluate lossy compression rates different deep generative models vaes gans variants adversarial autoencoders aae mnist cifar arrive number insights obtainable log likelihoods alone\n",
            "output sentence:  study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring requiring computational task \n",
            "\n",
            "{'rouge-1': {'r': 0.05660377358490566, 'p': 0.6, 'f': 0.10344827428656364}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03773584905660377, 'p': 0.4, 'f': 0.068965515665874}}\n",
            "pair:  present deep neural network spike assisted feature extraction safe dnn improve robustness classification stochastic perturbation inputs proposed network augments dnn unsupervised learning low level features using spiking neuron network snn spike time dependent plasticity stdp complete network learns ignore local perturbation performing global feature detection classification experimental results cifar imagenet subset demonstrate improved noise robustness multiple dnn architectures without sacrificing accuracy clean images\n",
            "output sentence:  noise robust deep learning architecture \n",
            "\n",
            "{'rouge-1': {'r': 0.03896103896103896, 'p': 0.6, 'f': 0.07317073056216539}, 'rouge-2': {'r': 0.009174311926605505, 'p': 0.25, 'f': 0.017699114361343908}, 'rouge-l': {'r': 0.03896103896103896, 'p': 0.6, 'f': 0.07317073056216539}}\n",
            "pair:  artistic style transfer problem synthesizing image content similar given image style similar another although recent feed forward neural networks generate stylized images real time models produce single stylization given pair style content images user control synthesized output moreover style transfer depends hyper parameters model varying optimum different input images therefore stylized output appealing user try multiple models retrain one different hyper parameters get favorite stylization paper address issues proposing novel method allows adjustment crucial hyper parameters training real time set manually adjustable parameters parameters enable user modify synthesized outputs pair style content images search favorite stylized image quantitative qualitative experiments indicate adjusting parameters comparable retraining model different hyper parameters also demonstrate parameters randomized generate results diverse still similar style content\n",
            "output sentence:  stochastic style transfer adjustable features \n",
            "\n",
            "{'rouge-1': {'r': 0.05333333333333334, 'p': 0.4, 'f': 0.09411764498269902}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.1111111111111111, 'f': 0.02040816159725128}, 'rouge-l': {'r': 0.05333333333333334, 'p': 0.4, 'f': 0.09411764498269902}}\n",
            "pair:  convolutional neural networks continuously advance progress image object classification steadfast usage algorithm requires constant evaluation upgrading foundational concepts maintain progress network regularization techniques typically focus convolutional layer operations leaving pooling layer operations without suitable options introduce wavelet pooling another alternative traditional neighborhood pooling method decomposes features second level decomposition discards first level subbands reduce feature dimensions method addresses overfitting problem encountered max pooling reducing features structurally compact manner pooling via neighborhood regions experimental results four benchmark classification datasets demonstrate proposed method outperforms performs comparatively methods like max mean mixed stochastic pooling\n",
            "output sentence:  pooling achieved using wavelets instead traditional neighborhood approaches max average etc \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.5, 'f': 0.21428571091836737}, 'rouge-2': {'r': 0.025, 'p': 0.11764705882352941, 'f': 0.04123711051121287}, 'rouge-l': {'r': 0.07575757575757576, 'p': 0.2777777777777778, 'f': 0.11904761568027221}}\n",
            "pair:  gans provide framework training generative models mimic data distribution however many cases wish train generative model optimize auxiliary objective function within data generates making aesthetically pleasing images cases objective functions difficult evaluate may require human interaction develop system efficiently training gan increase generic rate positive user interactions example aesthetic ratings build model human behavior targeted domain relatively small set interactions use behavioral model auxiliary loss function improve generative model proof concept demonstrate system successful improving positive interaction rates simulated variety objectives characterize\n",
            "output sentence:  describe improve image generative model according slow difficult evaluate objective human feedback could many applications like making aesthetic \n",
            "\n",
            "{'rouge-1': {'r': 0.11594202898550725, 'p': 0.8, 'f': 0.20253164335843615}, 'rouge-2': {'r': 0.02564102564102564, 'p': 0.2222222222222222, 'f': 0.04597700963931835}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.6, 'f': 0.1518987319660311}}\n",
            "pair:  neural networks vulnerable adversarial examples researchers proposed many heuristic attack defense mechanisms address problem principled lens distributionally robust optimization guarantees performance adversarial input perturbations considering lagrangian penalty formulation perturbing underlying data distribution wasserstein ball provide training procedure augments model parameter updates worst case perturbations training data smooth losses procedure provably achieves moderate levels robustness little computational statistical cost relative empirical risk minimization furthermore statistical guarantees allow us efficiently certify robustness population loss imperceptible perturbations method matches outperforms heuristic approaches\n",
            "output sentence:  provide fast principled adversarial training procedure computational statistical performance guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.07317073170731707, 'p': 0.42857142857142855, 'f': 0.1249999975086806}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07317073170731707, 'p': 0.42857142857142855, 'f': 0.1249999975086806}}\n",
            "pair:  reinforcement learning rl problem solved two different ways value function based approach policy optimization based approach eventually arrive optimal policy given environment one recent breakthroughs reinforcement learning use deep neural networks function approximators approximate value function function reinforcement learning scheme led results agents automatically learning play games like alpha go showing better human performance deep learning networks dqn deep deterministic policy gradient ddpg two methods shown state art results recent times among many variants rl important class problems state action spaces continuous autonomous robots autonomous vehicles optimal control examples problems lend naturally reinforcement based algorithms continuous state action spaces paper adapt combine approaches dqn ddpg novel ways outperform earlier results continuous state action space problems believe results valuable addition fast growing body results reinforcement learning continuous state action space problems\n",
            "output sentence:  improving performance rl agent continuous action state space domain using prioritised experience replay parameter noise \n",
            "\n",
            "{'rouge-1': {'r': 0.07272727272727272, 'p': 0.4444444444444444, 'f': 0.12499999758300785}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.125, 'f': 0.031249997812500156}, 'rouge-l': {'r': 0.05454545454545454, 'p': 0.3333333333333333, 'f': 0.09374999758300787}}\n",
            "pair:  modern neural networks parametrized particular rectified linear hidden unit modified multiplicative factor adjusting input put weights without changing rest network inspired sinkhorn knopp algorithm introduce fast iterative method minimizing norm weights equivalently weight decay regularizer provably converges unique solution interleaving algorithm sgd training improves test accuracy small batches approach offers alternative batch group normalization cifar imagenet resnet\n",
            "output sentence:  fast iterative algorithm balance energy network staying functional equivalence class \n",
            "\n",
            "{'rouge-1': {'r': 0.10576923076923077, 'p': 0.9166666666666666, 'f': 0.18965517055885853}, 'rouge-2': {'r': 0.06451612903225806, 'p': 0.7272727272727273, 'f': 0.11851851702167354}, 'rouge-l': {'r': 0.08653846153846154, 'p': 0.75, 'f': 0.15517241193816883}}\n",
            "pair:  reduce memory footprint run time latency techniques neural net work pruning binarization explored separately however un clear combine best two worlds get extremely small efficient models paper first time define filter level pruning problem binary neural networks cannot solved simply migrating existing structural pruning methods full precision models novel learning based approach proposed prune filters main subsidiary network frame work main network responsible learning representative features optimize prediction performance subsidiary component works filter selector main network avoid gradient mismatch training subsidiary component propose layer wise bottom scheme also provide theoretical experimental comparison learning based greedy rule based methods finally empirically demonstrate effectiveness approach applied several binary models including binarizednin vgg resnet various image classification datasets bi nary resnet imagenet use filters achieve slightly better test error original model\n",
            "output sentence:  define filter level pruning problem binary neural networks first time propose method solve \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.75, 'f': 0.22222221969821673}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.3076923076923077, 'f': 0.07920791854916191}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.6666666666666666, 'f': 0.1975308616735254}}\n",
            "pair:  neural sequence generation commonly approached using maximum likelihood ml estimation reinforcement learning rl however known shortcomings ml presents training testing discrepancy whereas rl suffers sample inefficiency point difficult resolve shortcomings simultaneously tradeoff ml rl order counteract problems propose objective function sequence generation using divergence leads ml rl integrated method exploits better parts ml rl demonstrate proposed objective function generalizes ml rl objective functions includes special cases ml corresponds rl provide proposition stating difference rl objective function proposed one monotonically decreases increasing experimental results machine translation tasks show minimizing proposed objective function achieves better sequence generation performance ml based methods\n",
            "output sentence:  propose new objective function neural sequence generation integrates ml based rl based objective functions \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.7857142857142857, 'f': 0.2749999971125}, 'rouge-2': {'r': 0.13513513513513514, 'p': 0.7692307692307693, 'f': 0.22988505492931696}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.7857142857142857, 'f': 0.2749999971125}}\n",
            "pair:  propose new output layer deep neural networks permits use logged contextual bandit feedback training contextual bandit feedback available huge quantities logs search engines recommender systems little cost opening path training deep networks orders magnitude data effect propose counterfactual risk minimization crm approach training deep networks using equivariant empirical risk estimator variance regularization banditnet show resulting objective decomposed way allows stochastic gradient descent sgd training empirically demonstrate effectiveness method showing deep networks resnets particular trained object recognition without conventionally labeled images\n",
            "output sentence:  paper proposes new output layer deep networks permits use logged contextual bandit feedback training \n",
            "\n",
            "{'rouge-1': {'r': 0.0759493670886076, 'p': 0.75, 'f': 0.137931032812789}, 'rouge-2': {'r': 0.00980392156862745, 'p': 0.14285714285714285, 'f': 0.018348622651292057}, 'rouge-l': {'r': 0.0759493670886076, 'p': 0.75, 'f': 0.137931032812789}}\n",
            "pair:  fundamental trait intelligence ability achieve goals face novel circumstances work address one setting requires solving task novel set actions empowering machines ability requires generalization way agent perceives available actions along way uses actions solve tasks hence propose framework enable generalization aspects understanding action functionality using actions solve tasks reinforcement learning specifically agent interprets action behavior using unsupervised representation learning collection data samples reflecting diverse properties action employ reinforcement learning architecture works action representations propose regularization metrics essential enabling generalization policy illustrate generalizability representation learning method policy enable zero shot generalization previously unseen actions challenging sequential decision making environments results videos found sites google com view action generalization\n",
            "output sentence:  address problem generalization reinforcement learning unseen action spaces \n",
            "\n",
            "{'rouge-1': {'r': 0.0673076923076923, 'p': 0.4117647058823529, 'f': 0.11570247692370746}, 'rouge-2': {'r': 0.02158273381294964, 'p': 0.1875, 'f': 0.03870967556795014}, 'rouge-l': {'r': 0.04807692307692308, 'p': 0.29411764705882354, 'f': 0.08264462568403806}}\n",
            "pair:  cnns widely successful recognizing human actions videos albeit great cost computation cost significantly higher case long range actions video span minutes average goal paper reduce computational cost cnns without sacrificing performance propose videoepitoma neural network architecture comprising two modules timestamp selector video classifier given long range video thousands timesteps selector learns choose representative timesteps video selector resides top lightweight cnn mobilenet uses novel gating module take binary decision consider discard video timestep decision conditioned timestep level feature video level consensus heavyweight cnn model takes selected frames input performs video classification using shelf video classifiers videoepitoma reduces computation without compromising accuracy addition show trained end end selector learns make better choices benefit classifier despite selector classifier residing two different cnns finally report state art results two datasets long range action recognition charades breakfast actions much reduced computation particular match accuracy using less half computation\n",
            "output sentence:  efficient video classification using frame based conditional gating module selecting dominant frames followed temporal modeling classifier classifier \n",
            "\n",
            "{'rouge-1': {'r': 0.1276595744680851, 'p': 0.7058823529411765, 'f': 0.21621621362227092}, 'rouge-2': {'r': 0.04201680672268908, 'p': 0.29411764705882354, 'f': 0.07352940957720595}, 'rouge-l': {'r': 0.1276595744680851, 'p': 0.7058823529411765, 'f': 0.21621621362227092}}\n",
            "pair:  show entropy sgd chaudhari et al viewed learning algorithm optimizes pac bayes bound risk gibbs posterior classifier randomized classifier obtained risk sensitive perturbation weights learned classifier entropy sgd works optimizing bound prior violating hypothesis pac bayes theorem prior chosen independently data indeed available implementations entropy sgd rapidly obtain zero training error random labels holds gibbs posterior order obtain valid generalization bound show differentially private prior yields valid pac bayes bound straightforward consequence results connecting generalization differential privacy using stochastic gradient langevin dynamics sgld approximate well known exponential release mechanism observe generalization error mnist measured held data falls within empirically nonvacuous bounds computed assumption sgld produces perfect samples particular entropy sgld configured yield relatively tight generalization bounds still fit real labels although settings obtain state art performance\n",
            "output sentence:  show entropy sgd optimizes prior pac bayes bound violating requirement prior independent data use differential privacy resolve generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.14102564102564102, 'p': 0.6111111111111112, 'f': 0.2291666636197917}, 'rouge-2': {'r': 0.056074766355140186, 'p': 0.3333333333333333, 'f': 0.09599999753472006}, 'rouge-l': {'r': 0.14102564102564102, 'p': 0.6111111111111112, 'f': 0.2291666636197917}}\n",
            "pair:  bidirectional joint image text modeling develop variational hetero encoder vhe randomized generative adversarial network gan versatile deep generative model integrates probabilistic text decoder probabilistic image encoder gan coherent end end multi modality learning framework vhe randomized gan vhe gan encodes image decode associated text feeds variational posterior source randomness gan image generator plug three shelf modules including deep topic model ladder structured image encoder stackgan vhe gan already achieves competitive performance motivates development vhe raster scan gan generates photo realistic images multi scale low high resolution manner also hierarchical semantic coarse fine fashion capturing relating hierarchical semantic visual concepts end end training vhe raster scan gan achieves state art performance wide variety image text multi modality learning generation tasks\n",
            "output sentence:  novel bayesian deep learning framework captures relates hierarchical semantic visual concepts performing well variety image text generation generation generation \n",
            "\n",
            "{'rouge-1': {'r': 0.16326530612244897, 'p': 0.9411764705882353, 'f': 0.2782608670457467}, 'rouge-2': {'r': 0.1271186440677966, 'p': 0.9375, 'f': 0.22388059491200712}, 'rouge-l': {'r': 0.16326530612244897, 'p': 0.9411764705882353, 'f': 0.2782608670457467}}\n",
            "pair:  paper introduces cloudlstm new branch recurrent neural models tailored forecasting data streams generated geospatial point cloud sources design dynamic point cloud convolution conv operator core component cloudlstms performs convolution directly point clouds extracts local spatial features sets neighboring points surround different elements input operator maintains permutation invariance sequence sequence learning frameworks representing neighboring correlations time step important aspect spatiotemporal predictive learning conv operator resolves grid structural data requirements existing spatiotemporal forecasting models easily plugged traditional lstm architectures sequence sequence learning attention mechanisms apply proposed architecture two representative practical use cases involve point cloud streams mobile service traffic forecasting air quality indicator forecasting results obtained real world datasets collected diverse scenarios use case show cloudlstm delivers accurate long term predictions outperforming variety neural network models\n",
            "output sentence:  paper introduces cloudlstm new branch recurrent neural models tailored forecasting data streams generated geospatial point cloud sources \n",
            "\n",
            "{'rouge-1': {'r': 0.1232876712328767, 'p': 0.75, 'f': 0.21176470345743945}, 'rouge-2': {'r': 0.05, 'p': 0.45454545454545453, 'f': 0.09009008830452077}, 'rouge-l': {'r': 0.0958904109589041, 'p': 0.5833333333333334, 'f': 0.16470587992802768}}\n",
            "pair:  obtaining policies generalise new environments reinforcement learning challenging work demonstrate language understanding via reading policy learner promising vehicle generalisation new environments propose grounded policy learning problem read fight monsters rtfm agent must jointly reason language goal relevant dynamics described document environment observations procedurally generate environment dynamics corresponding language descriptions dynamics agents must read understand new environment dynamics instead memorising particular information addition propose txt model captures three way interactions goal document observations rtfm txt generalises new environments dynamics seen training via reading furthermore model outperforms baselines film language conditioned cnns rtfm curriculum learning txt produces policies excel complex rtfm tasks requiring several reasoning coreference steps\n",
            "output sentence:  show language understanding via reading promising way learn policies generalise new environments \n",
            "\n",
            "{'rouge-1': {'r': 0.1125, 'p': 0.5625, 'f': 0.18749999722222224}, 'rouge-2': {'r': 0.03, 'p': 0.1875, 'f': 0.05172413555291331}, 'rouge-l': {'r': 0.1, 'p': 0.5, 'f': 0.16666666388888893}}\n",
            "pair:  deep learning networks achieved state art accuracies computer vision workloads like image classification object detection performant systems however typically involve big models numerous parameters trained challenging aspect top performing models deployment resource constrained inference systems models often deep networks wide networks compute memory intensive low precision numerics model compression using knowledge distillation popular techniques lower compute requirements memory footprint deployed models paper study combination two techniques show performance low precision networks significantly improved using knowledge distillation techniques call approach apprentice show state art accuracies using ternary precision bit precision many variants resnet architecture imagenet dataset study three schemes one apply knowledge distillation techniques various stages train deploy pipeline\n",
            "output sentence:  show knowledge transfer techniques improve accuracy low precision networks set new state art accuracy ternary bits precision \n",
            "\n",
            "{'rouge-1': {'r': 0.14942528735632185, 'p': 0.9285714285714286, 'f': 0.25742574018625636}, 'rouge-2': {'r': 0.11206896551724138, 'p': 0.9285714285714286, 'f': 0.19999999807810656}, 'rouge-l': {'r': 0.14942528735632185, 'p': 0.9285714285714286, 'f': 0.25742574018625636}}\n",
            "pair:  model free deep reinforcement learning rl algorithms demonstrated range challenging decision making control tasks however methods typically suffer two major challenges high sample complexity brittle convergence properties necessitate meticulous hyperparameter tuning challenges severely limit applicability methods complex real world domains paper propose soft actor critic policy actor critic deep rl algorithm based maximum entropy reinforcement learning framework framework actor aims maximize expected reward also maximizing entropy succeed task acting randomly possible prior deep rl methods based framework formulated either policy learning policy policy gradient methods combining policy updates stable stochastic actor critic formulation method achieves state art performance range continuous control benchmark tasks outperforming prior policy policy methods furthermore demonstrate contrast policy algorithms approach stable achieving similar performance across different random seeds\n",
            "output sentence:  propose soft actor critic policy actor critic deep rl algorithm based maximum entropy reinforcement learning framework \n",
            "\n",
            "{'rouge-1': {'r': 0.16901408450704225, 'p': 0.8, 'f': 0.2790697645619254}, 'rouge-2': {'r': 0.13253012048192772, 'p': 0.7857142857142857, 'f': 0.2268041212413647}, 'rouge-l': {'r': 0.14084507042253522, 'p': 0.6666666666666666, 'f': 0.23255813665494862}}\n",
            "pair:  social dilemmas situations individuals face temptation increase payoffs cost total welfare building artificially intelligent agents achieve good outcomes situations important many real world interactions include tension selfish interests welfare others show modify modern reinforcement learning methods construct agents act ways simple understand nice begin cooperating provokable try avoid exploited forgiving try return mutual cooperation show theoretically experimentally agents maintain cooperation markov social dilemmas construction require training methods beyond modification self play thus environment good strategies constructed zero sum case eg atari construct agents solve social dilemmas environment\n",
            "output sentence:  build artificial agents solve social dilemmas situations individuals face temptation increase payoffs cost total welfare \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.6, 'f': 0.11320754546101817}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.2222222222222222, 'f': 0.03305784986271435}, 'rouge-l': {'r': 0.0625, 'p': 0.6, 'f': 0.11320754546101817}}\n",
            "pair:  fundamental challenging train robust accurate deep neural networks dnns semantically abnormal examples exist although great progress made still one crucial research question thoroughly explored yet training examples focused much emphasised achieve robust learning work study question propose gradient rescaling gr solve gr modifies magnitude logit vector gradient emphasise relatively easier training data points noise becomes severe functions explicit emphasis regularisation improve generalisation performance dnns apart regularisation connect gr examples weighting designing robust loss functions empirically demonstrate gr highly anomaly robust outperforms state art large margin increasing cifar noisy labels also significantly superior standard regularisers clean abnormal settings furthermore present comprehensive ablation studies explore behaviours gr different cases informative applying gr real world scenarios\n",
            "output sentence:  robust discriminative representation learning via gradient rescaling emphasis regularisation perspective \n",
            "\n",
            "{'rouge-1': {'r': 0.14516129032258066, 'p': 0.6428571428571429, 'f': 0.2368421022576178}, 'rouge-2': {'r': 0.027777777777777776, 'p': 0.15384615384615385, 'f': 0.04705882093840845}, 'rouge-l': {'r': 0.11290322580645161, 'p': 0.5, 'f': 0.18421052331024934}}\n",
            "pair:  animals develop novel skills interaction environment also influence others work model social influence scheme reinforcement learning enabling agents learn environment peers specifically first define metric measure distance policies quantitatively derive definition uniqueness unlike previous precarious joint optimization approaches social uniqueness motivation work imposed constraint encourage agent learn policy different existing agents still solve primal task resulting algorithm namely interior policy differentiation ipd brings performance improvement well collection policies solve given task distinct behaviors\n",
            "output sentence:  new rl algorithm called interior policy differentiation proposed learn collection diverse policies given primal task \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.5714285714285714, 'f': 0.14545454323305787}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.3333333333333333, 'f': 0.06060605895316809}, 'rouge-l': {'r': 0.0625, 'p': 0.42857142857142855, 'f': 0.10909090686942154}}\n",
            "pair:  magnitude based pruning one simplest methods pruning neural networks despite simplicity magnitude based pruning variants demonstrated remarkable performances pruning modern architectures based observation magnitude based pruning indeed minimizes frobenius distortion linear operator corresponding single layer develop simple pruning method coined lookahead pruning extending single layer optimization multi layer optimization experimental results demonstrate proposed method consistently outperforms magnitude pruning various networks including vgg resnet particularly high sparsity regime\n",
            "output sentence:  study multi layer generalization magnitude based pruning \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.875, 'f': 0.14141413992857876}, 'rouge-2': {'r': 0.059322033898305086, 'p': 0.875, 'f': 0.1111111099218947}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.875, 'f': 0.14141413992857876}}\n",
            "pair:  present end end design methodology efficient deep learning deployment unlike previous methods separately optimize neural network architecture pruning policy quantization policy jointly optimize end end manner deal larger design space brings train quantization aware accuracy predictor fed evolutionary search select best fit first generate large dataset nn architecture imagenet accuracy pairs without training architecture sampling unified supernet use data train accuracy predictor without quantization using predictor transfer technique get quantization aware predictor reduces amount post quantization fine tuning time extensive experiments imagenet show benefits end end methodology maintains accuracy resnet float model saving bitops comparing bit model obtain level accuracy mobilenetv haq achieving latency energy saving end end optimization outperforms separate optimizations using proxylessnas amc haq accuracy reducing orders magnitude gpu hours co emission\n",
            "output sentence:  present end end design methodology efficient deep learning deployment \n",
            "\n",
            "{'rouge-1': {'r': 0.07042253521126761, 'p': 0.35714285714285715, 'f': 0.11764705607197239}, 'rouge-2': {'r': 0.010752688172043012, 'p': 0.07692307692307693, 'f': 0.01886792237629074}, 'rouge-l': {'r': 0.04225352112676056, 'p': 0.21428571428571427, 'f': 0.07058823254256066}}\n",
            "pair:  modern neural network architectures take advantage increasingly deeper layers various advances structure achieve better performance traditional explicit regularization techniques like dropout weight decay data augmentation still used new models little regularization generalization effects new structures studied besides deeper predecessors could newer architectures like resnet densenet also benefit structures implicit regularization properties work investigate skip connection effect network generalization features experiments show certain neural network architectures contribute generalization abilities specifically study effect low level features generalization performance introduced deeper layers densenet resnet well networks skip connections show low level representations help generalization multiple settings quality quantity training data decreased\n",
            "output sentence:  paper analyses tremendous representational power networks especially skip connections may used method better generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.5, 'f': 0.13333333102222228}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05128205128205128, 'p': 0.3333333333333333, 'f': 0.08888888657777784}}\n",
            "pair:  recent research proposed lottery ticket hypothesis suggesting deep neural network exist trainable sub networks performing equally better original model commensurate training steps discovery insightful finding proper sub networks requires iterative training pruning high cost incurred limits applications lottery ticket hypothesis show exists subset aforementioned sub networks converge significantly faster training process thus mitigate cost issue conduct extensive experiments show sub networks consistently exist across various model structures restrictive setting hyperparameters carefully selected learning rate pruning ratio model capacity practical application findings demonstrate sub networks help cutting total time adversarial training standard approach improve robustness cifar achieve state art robustness\n",
            "output sentence:  show possibility pruning find small sub network significantly higher convergence rate full model \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.46153846153846156, 'f': 0.16901408151557235}, 'rouge-2': {'r': 0.03125, 'p': 0.16666666666666666, 'f': 0.05263157628808878}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.3076923076923077, 'f': 0.11267605334655831}}\n",
            "pair:  application deep recurrent networks audio transcription led impressive gains automatic speech recognition asr systems many demonstrated small adversarial perturbations fool deep neural networks incorrectly predicting specified target high confidence current work fooling asr systems focused white box attacks model architecture parameters known paper adopt black box approach adversarial generation combining approaches genetic algorithms gradient estimation solve task achieve targeted attack similarity generations maintaining audio file similarity\n",
            "output sentence:  present novel black box targeted attack able fool state art speech text transcription \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0392156862745098, 'p': 0.2222222222222222, 'f': 0.06666666411666677}}\n",
            "pair:  power neural networks lies ability generalize unseen data yet underlying reasons phenomenon remain elusive numerous rigorous attempts made explain generalization available bounds still quite loose analysis always lead true understanding goal work make generalization intuitive using visualization methods discuss mystery generalization geometry loss landscapes curse rather blessing dimensionality causes optimizers settle minima generalize well\n",
            "output sentence:  intuitive empirical visual exploration generalization properties deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.11475409836065574, 'p': 0.6363636363636364, 'f': 0.1944444418557099}, 'rouge-2': {'r': 0.025974025974025976, 'p': 0.2, 'f': 0.04597700945963809}, 'rouge-l': {'r': 0.06557377049180328, 'p': 0.36363636363636365, 'f': 0.11111110852237661}}\n",
            "pair:  interpolation data deep neural networks become subject significant research interest prove parameterized single layer fully connected autoencoders merely interpolate rather memorize training data produce outputs non linear version span training examples contrast fully connected autoencoders prove depth necessary memorization convolutional autoencoders moreover observe adding nonlinearity deep convolutional autoencoders results stronger form memorization instead outputting points span training images deep convolutional autoencoders tend output individual training images since convolutional autoencoder components building blocks deep convolutional networks envision findings shed light important question inductive bias parameterized deep networks\n",
            "output sentence:  identify memorization inductive bias interpolation overparameterized fully connected convolutional auto encoders \n",
            "\n",
            "{'rouge-1': {'r': 0.1125, 'p': 0.47368421052631576, 'f': 0.18181817871645756}, 'rouge-2': {'r': 0.02857142857142857, 'p': 0.15789473684210525, 'f': 0.04838709417924051}, 'rouge-l': {'r': 0.0625, 'p': 0.2631578947368421, 'f': 0.1010100979083768}}\n",
            "pair:  state art methods learning cross lingual word embeddings relied bilingual dictionaries parallel corpora recent studies showed need parallel data supervision alleviated character level information methods showed encouraging results par supervised counterparts limited pairs languages sharing common alphabet work show build bilingual dictionary two languages without using parallel corpora aligning monolingual word embedding spaces unsupervised way without using character information model even outperforms existing supervised methods cross lingual tasks language pairs experiments demonstrate method works well also distant language pairs like english russian english chinese finally describe experiments english esperanto low resource language pair exists limited amount parallel data show potential impact method fully unsupervised machine translation code embeddings dictionaries publicly available\n",
            "output sentence:  aligning languages without rosetta stone parallel data construct bilingual dictionaries using adversarial training cross domain local scaling accurate proxy proxy cross \n",
            "\n",
            "{'rouge-1': {'r': 0.09782608695652174, 'p': 0.75, 'f': 0.17307692103550298}, 'rouge-2': {'r': 0.016, 'p': 0.18181818181818182, 'f': 0.02941176321907447}, 'rouge-l': {'r': 0.07608695652173914, 'p': 0.5833333333333334, 'f': 0.13461538257396452}}\n",
            "pair:  artificial neural networks trained gradient descent weights used processing stimuli also used backward passes calculate gradients real brain approximate gradients gradient information would propagated separately one set synaptic weights used processing another set used backward passes produces called weight transport problem biological models learning backward weights used calculate gradients need mirror forward weights used process stimuli weight transport problem considered hard popular proposals biological learning assume backward weights simply random feedback alignment algorithm however random weights appear work well large networks show discontinuity introduced spiking system lead solution problem resulting algorithm special case estimator used causal inference econometrics regression discontinuity design show empirically algorithm rapidly makes backward weights approximate forward weights backward weights become correct improves learning performance feedback alignment tasks fashion mnist cifar results demonstrate simple learning rule spiking network allow neurons produce right backward connections thus solve weight transport problem\n",
            "output sentence:  present learning rule feedback weights spiking neural network addresses weight transport problem \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.5333333333333333, 'f': 0.1860465087479719}, 'rouge-2': {'r': 0.024096385542168676, 'p': 0.14285714285714285, 'f': 0.04123711093208646}, 'rouge-l': {'r': 0.09859154929577464, 'p': 0.4666666666666667, 'f': 0.16279069479448352}}\n",
            "pair:  paper improves upon line research formulates named entity recognition ner sequence labeling problem use called black box long short term memory lstm encoders achieve state art results providing insightful understanding auto regressive model learns parallel self attention mechanism specifically decouple sequence labeling problem ner entity chunking barack obama elected entity typing barack person obama person none elected none analyze model learns difficulties capturing text patterns subtasks insights gain lead us explore sophisticated deep cross bi lstm encoder proves better capturing global interactions given empirical results theoretical justification\n",
            "output sentence:  provide insightful understanding sequence labeling ner propose use two types cross structures bring theoretical improvements \n",
            "\n",
            "{'rouge-1': {'r': 0.10869565217391304, 'p': 0.29411764705882354, 'f': 0.15873015478961966}, 'rouge-2': {'r': 0.05454545454545454, 'p': 0.16666666666666666, 'f': 0.08219177710639912}, 'rouge-l': {'r': 0.10869565217391304, 'p': 0.29411764705882354, 'f': 0.15873015478961966}}\n",
            "pair:  paper presents gumbelclip set modifications actor critic algorithm policy reinforcement learning gumbelclip uses concepts truncated importance sampling along additive noise produce loss function enabling use policy samples modified algorithm achieves increase convergence speed sample efficiency compared policy algorithms competitive existing policy policy gradient methods significantly simpler implement effectiveness gumbelclip demonstrated existing policy policy actor critic algorithms subset atari domain\n",
            "output sentence:  set modifications loc get policy actor critic outperforms performs similarly acer modifications large batchsizes aggressive clamping policy forcing gumbel \n",
            "\n",
            "{'rouge-1': {'r': 0.21428571428571427, 'p': 0.7142857142857143, 'f': 0.32967032612003383}, 'rouge-2': {'r': 0.07368421052631578, 'p': 0.3181818181818182, 'f': 0.11965811660457308}, 'rouge-l': {'r': 0.15714285714285714, 'p': 0.5238095238095238, 'f': 0.24175823820794592}}\n",
            "pair:  meta learning allows intelligent agent leverage prior learning episodes basis quickly improving performance novel task bayesian hierarchical modeling provides theoretical framework formalizing meta learning inference set parameters shared across tasks reformulate model agnostic meta learning algorithm maml finn et al method probabilistic inference hierarchical bayesian model contrast prior methods meta learning via hierarchical bayes maml naturally applicable complex function approximators use scalable gradient descent procedure posterior inference furthermore identification maml hierarchical bayes provides way understand algorithm operation meta learning procedure well opportunity make use computational strategies efficient inference use opportunity propose improvement maml algorithm makes use techniques approximate inference curvature estimation\n",
            "output sentence:  specific gradient based meta learning algorithm maml equivalent inference procedure hierarchical bayesian model use connection improve maml via methods approximate inference curvature estimation \n",
            "\n",
            "{'rouge-1': {'r': 0.12698412698412698, 'p': 0.7272727272727273, 'f': 0.21621621368517166}, 'rouge-2': {'r': 0.01282051282051282, 'p': 0.1, 'f': 0.022727270712810096}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.6363636363636364, 'f': 0.18918918665814466}}\n",
            "pair:  recent shot learning algorithms enabled models quickly adapt new tasks based training samples previous shot learning works mainly focused classification reinforcement learning paper propose shot meta learning system focuses exclusively regression tasks model based idea degree freedom unknown function significantly reduced represented linear combination set sparsifying basis functions enables labeled samples approximate function design basis function learner network encode basis functions task distribution weights generator network generate weight vector novel task show model outperforms current state art meta learning methods various regression tasks\n",
            "output sentence:  propose method shot regression learning set basis functions represent function distribution \n",
            "\n",
            "{'rouge-1': {'r': 0.04938271604938271, 'p': 0.3333333333333333, 'f': 0.08602150312868545}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.037037037037037035, 'p': 0.25, 'f': 0.06451612678459945}}\n",
            "pair:  deep neural networks dnns require complex models achieve high performance parameter quantization widely used reducing implementation complexities previous studies quantization mostly based extensive simulation using training data choose different approach attempt measure per parameter capacity dnn models interpret results obtain insights optimum quantization parameters research uses artificially generated data generic forms fully connected dnns convolutional neural networks recurrent neural networks conduct memorization classification tests study effects number precision parameters performance model per parameter capacities assessed measuring mutual information input classified output also extend memorization capacity measurement results image classification language modeling tasks get insight parameter quantization performing real tasks training test performances compared\n",
            "output sentence:  suggest sufficient number bits representing weights dnns optimum bits conservative solving real problems \n",
            "\n",
            "{'rouge-1': {'r': 0.17857142857142858, 'p': 0.8823529411764706, 'f': 0.29702970017057156}, 'rouge-2': {'r': 0.12727272727272726, 'p': 0.8235294117647058, 'f': 0.22047243862607727}, 'rouge-l': {'r': 0.17857142857142858, 'p': 0.8823529411764706, 'f': 0.29702970017057156}}\n",
            "pair:  much focus design deep neural networks improving accuracy leading powerful yet highly complex network architectures difficult deploy practical scenarios result recent interest design quantitative metrics evaluating deep neural networks accounts model accuracy sole indicator network performance study continue conversation towards universal metrics evaluating performance deep neural networks practical device edge usage introducing netscore new metric designed specifically provide quantitative assessment balance accuracy computational complexity network architecture complexity deep neural network one largest comparative analysis deep neural networks literature netscore metric top accuracy metric popular information density metric compared across diverse set different deep convolutional neural networks image classification imagenet large scale visual recognition challenge ilsvrc dataset evaluation results across three metrics diverse set networks presented study act reference guide practitioners field\n",
            "output sentence:  introduce netscore new metric designed provide quantitative assessment balance accuracy computational complexity network architecture complexity deep neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.06666666666666667, 'p': 0.875, 'f': 0.12389380399404809}, 'rouge-2': {'r': 0.047244094488188976, 'p': 0.8571428571428571, 'f': 0.0895522378157719}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.875, 'f': 0.12389380399404809}}\n",
            "pair:  deep neural networks trained wide range datasets demonstrate impressive transferability deep features appear general applicable many datasets tasks property prevalent use real world applications neural network pretrained large datasets imagenet significantly boost generalization accelerate training fine tuned smaller target dataset despite pervasiveness effort devoted uncovering reason transferability deep feature representations paper tries understand transferability perspectives improved generalization optimization feasibility transferability demonstrate transferred models tend find flatter minima since weight matrices stay close original flat region pretrained parameters transferred similar target dataset transferred representations make loss landscape favorable improved lipschitzness accelerates stabilizes training substantially improvement largely attributes fact principal component gradient suppressed pretrained parameters thus stabilizing magnitude gradient back propagation feasibility transferability related similarity input label surprising discovery feasibility also impacted training stages transferability first increases training declines provide theoretical analysis verify observations\n",
            "output sentence:  understand transferability perspectives improved generalization optimization feasibility transferability \n",
            "\n",
            "{'rouge-1': {'r': 0.18032786885245902, 'p': 0.9166666666666666, 'f': 0.3013698602664665}, 'rouge-2': {'r': 0.13253012048192772, 'p': 0.9166666666666666, 'f': 0.23157894516121888}, 'rouge-l': {'r': 0.18032786885245902, 'p': 0.9166666666666666, 'f': 0.3013698602664665}}\n",
            "pair:  introduce new memory architecture navigation previously unseen environments inspired landmark based navigation animals proposed semi parametric topological memory sptm consists non parametric graph nodes corresponding locations environment parametric deep network capable retrieving nodes graph based observations graph stores metric information connectivity locations corresponding nodes use sptm planning module navigation system given minutes footage previously unseen maze sptm based navigation agent build topological map environment use confidently navigate towards goals average success rate sptm agent goal directed navigation across test environments higher best performing baseline factor three\n",
            "output sentence:  introduce new memory architecture navigation previously unseen environments inspired landmark based navigation animals \n",
            "\n",
            "{'rouge-1': {'r': 0.06097560975609756, 'p': 0.7142857142857143, 'f': 0.11235954911248583}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.3333333333333333, 'f': 0.03999999887200004}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.7142857142857143, 'f': 0.11235954911248583}}\n",
            "pair:  generative adversarial networks gans proven powerful framework learning draw samples complex distributions however gans also notoriously difficult train mode collapse oscillations common problem hypothesize least part due evolution generator distribution catastrophic forgetting tendency neural networks leads discriminator losing ability remember synthesized samples previous instantiations generator recognizing contributions twofold first show gan training makes interesting realistic benchmark continual learning methods evaluation canonical datasets second propose leveraging continual learning techniques augment discriminator preserving ability recognize previous generator samples show resulting methods add light amount computation involve minimal changes model result better overall performance examined image text generation tasks\n",
            "output sentence:  generative adversarial network training continual learning problem \n",
            "\n",
            "{'rouge-1': {'r': 0.14035087719298245, 'p': 0.8, 'f': 0.23880596760971265}, 'rouge-2': {'r': 0.0547945205479452, 'p': 0.4444444444444444, 'f': 0.09756097365556221}, 'rouge-l': {'r': 0.08771929824561403, 'p': 0.5, 'f': 0.1492537288037425}}\n",
            "pair:  combining multiple function approximators machine learning models typically leads better performance robustness compared single function reinforcement learning ensemble algorithms averaging method majority voting method always optimal function learn fundamentally different optimal trajectories exploration paper propose temporal difference weighted tdw algorithm ensemble method adjusts weights contribution based accumulated temporal difference errors advantage algorithm improves ensemble performance reducing weights functions unfamiliar current trajectories provide experimental results gridworld tasks atari tasks show significant performance improvements compared baseline algorithms\n",
            "output sentence:  ensemble method reinforcement learning weights functions based accumulated td errors \n",
            "\n",
            "{'rouge-1': {'r': 0.22727272727272727, 'p': 0.8333333333333334, 'f': 0.35714285377551025}, 'rouge-2': {'r': 0.14814814814814814, 'p': 0.6666666666666666, 'f': 0.24242423944903582}, 'rouge-l': {'r': 0.20454545454545456, 'p': 0.75, 'f': 0.3214285680612245}}\n",
            "pair:  fine tuning language models bert domain specific corpora proven valuable domains like scientific papers biomedical text paper show fine tuning bert legal documents similarly provides valuable improvements nlp tasks legal domain demonstrating outcome significant analyzing commercial agreements obtaining large legal corpora challenging due confidential nature show access large legal corpora competitive advantage commercial applications academic research analyzing contracts\n",
            "output sentence:  fine tuning bert legal corpora provides marginal valuable improvements nlp tasks legal domain \n",
            "\n",
            "{'rouge-1': {'r': 0.10256410256410256, 'p': 0.8888888888888888, 'f': 0.1839080441220769}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.5, 'f': 0.07017543729147431}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.7777777777777778, 'f': 0.16091953837495046}}\n",
            "pair:  paper present general framework distilling expectations respect bayesian posterior distribution deep neural network significantly extending prior work method known bayesian dark knowledge generalized framework applies case classification models takes input architecture teacher network general posterior expectation interest architecture student network distillation method performs online compression selected posterior expectation using iteratively generated monte carlo samples parameter posterior teacher model consider problem optimizing student model architecture respect accuracy speed storage trade present experimental results investigating multiple data sets distillation targets teacher model architectures approaches searching student model architectures establish key result distilling student model architecture matches teacher done bayesian dark knowledge lead sub optimal performance lastly show student architecture search methods identify student models significantly improved performance\n",
            "output sentence:  general framework distilling bayesian posterior expectations deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.7142857142857143, 'f': 0.1333333316408889}, 'rouge-2': {'r': 0.024096385542168676, 'p': 0.3333333333333333, 'f': 0.04494381896730214}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.5714285714285714, 'f': 0.10666666497422224}}\n",
            "pair:  federated learning allows edge devices collaboratively learn shared model keeping training data device decoupling ability model training need store data cloud propose federated matched averaging fedma algorithm designed federated learning modern neural network architectures convolutional neural networks cnns lstms fedma constructs shared global model layer wise manner matching averaging hidden elements channels convolution layers hidden states lstm neurons fully connected layers similar feature extraction signatures experiments indicate fedma outperforms popular state art federated learning algorithms deep cnn lstm architectures trained real world datasets improving communication efficiency\n",
            "output sentence:  communication efficient federated learning layer wise matching \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.5, 'f': 0.17142856858775513}, 'rouge-2': {'r': 0.03076923076923077, 'p': 0.18181818181818182, 'f': 0.052631576471606774}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.3333333333333333, 'f': 0.11428571144489805}}\n",
            "pair:  since deep neural networks parameterized memorize noisy examples address memorizing issue presence annotation noise fact deep neural networks cannot generalize neighborhoods features acquired via memorization hypothesize noisy examples consistently incur small losses network certain perturbation based propose novel training method called learning ensemble consensus lec prevents overfitting noisy examples eliminating using consensus ensemble perturbed networks one proposed lecs ltec outperforms current state art methods noisy mnist cifar cifar efficient manner\n",
            "output sentence:  work presents method generating using ensembles effectively identify noisy examples presence annotation noise \n",
            "\n",
            "{'rouge-1': {'r': 0.3548387096774194, 'p': 0.9166666666666666, 'f': 0.5116279029529476}, 'rouge-2': {'r': 0.19736842105263158, 'p': 0.625, 'f': 0.29999999635200003}, 'rouge-l': {'r': 0.25806451612903225, 'p': 0.6666666666666666, 'f': 0.3720930192320173}}\n",
            "pair:  interactive fiction games text based simulations agent interacts world purely natural language ideal environments studying extend reinforcement learning agents meet challenges natural language understanding partial observability action generation combinatorially large text based action spaces present kg agent builds dynamic knowledge graph exploring generates actions using template based action space contend dual uses knowledge graph reason game state constrain natural language generation keys scalable exploration combinatorially large natural language actions results across wide variety games show kg outperforms current agents despite exponential increase action space size\n",
            "output sentence:  present kg reinforcement learning agent builds dynamic knowledge graph exploring generates natural language using template based action space outperforming current agents wide set text based games \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.5, 'f': 0.13333333102222228}, 'rouge-2': {'r': 0.012987012987012988, 'p': 0.1, 'f': 0.022988503712511744}, 'rouge-l': {'r': 0.046153846153846156, 'p': 0.3, 'f': 0.07999999768888896}}\n",
            "pair:  aim build complex humanoid agents integrate perception motor control memory work partly factor problem low level motor control proprioception high level coordination low level skills informed vision develop architecture capable surprisingly flexible task directed motor control relatively high dof humanoid body combining pre training low level motor controllers high level task focused controller switches among low level sub policies resulting system able control physically simulated humanoid body solve tasks require coupling visual perception unstabilized egocentric rgb camera locomotion environment supplementary video link https youtu fboir pnxpk\n",
            "output sentence:  solve tasks involving vision guided humanoid locomotion reusing locomotion behavior capture \n",
            "\n",
            "{'rouge-1': {'r': 0.05555555555555555, 'p': 0.38461538461538464, 'f': 0.09708737643510233}, 'rouge-2': {'r': 0.01904761904761905, 'p': 0.16666666666666666, 'f': 0.034188032347140136}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.38461538461538464, 'f': 0.09708737643510233}}\n",
            "pair:  one prevalent symptoms among elderly population dementia detected classifiers trained linguistic features extracted narrative transcripts however linguistic features impacted similar different fashion normal aging process aging therefore confounding factor whose effects hard machine learning classifiers isolate paper show deep neural network dnn classifiers infer ages linguistic features entanglement could lead unfairness across age groups show problem caused undesired activations structures causality diagrams could addressed fair representation learning build neural network classifiers learn low dimensional representations reflecting impacts dementia yet discarding effects age evaluate classifiers specify model agnostic score delta eo measuring classifier results disentangled age best models outperform baseline neural network classifiers disentanglement compromising accuracy little dementiabank famous people dataset respectively\n",
            "output sentence:  show age confounds cognitive impairment detection solve fair representation learning propose metrics models \n",
            "\n",
            "{'rouge-1': {'r': 0.08737864077669903, 'p': 0.75, 'f': 0.15652173726124766}, 'rouge-2': {'r': 0.015625, 'p': 0.18181818181818182, 'f': 0.028776976959784765}, 'rouge-l': {'r': 0.06796116504854369, 'p': 0.5833333333333334, 'f': 0.12173912856559549}}\n",
            "pair:  low dimensional vector embeddings computed using lstms simpler techniques popular approach capturing meaning text form unsupervised learning useful downstream tasks however power theoretically understood current paper derives formal understanding looking subcase linear embedding schemes using theory compressed sensing show representations combining constituent word vectors essentially information preserving linear measurements bag grams bong representations text leads new theoretical result lstms low dimensional embeddings derived low memory lstm provably least powerful classification tasks small error linear classifier bong vectors result extensive empirical work thus far unable show experiments support theoretical findings establish strong simple unsupervised baselines standard benchmarks cases state art among word level methods also show surprising new property embeddings glove word vec form good sensing matrix text efficient random matrices standard sparse recovery tool may explain lead better representations practice\n",
            "output sentence:  use theory compressed sensing prove lstms least well linear text classification bag grams \n",
            "\n",
            "{'rouge-1': {'r': 0.19047619047619047, 'p': 0.631578947368421, 'f': 0.2926829232688876}, 'rouge-2': {'r': 0.06976744186046512, 'p': 0.3333333333333333, 'f': 0.11538461252218943}, 'rouge-l': {'r': 0.15873015873015872, 'p': 0.5263157894736842, 'f': 0.24390243546400955}}\n",
            "pair:  approaches generalized zero shot learning rely cross modal mapping image feature space class embedding space generating artificial image features however learning shared cross modal embedding aligning latent spaces modality specific autoencoders shown promising generalized zero shot learning following direction also take artificial feature generation one step propose model shared latent space image features class embeddings learned aligned variational autoencoders purpose generating latent features train softmax classifier evaluate learned latent features conventional benchmark datasets establish new state art generalized zero shot well shot learning moreover results imagenet various zero shot splits show latent features generalize well large scale settings\n",
            "output sentence:  use vaes learn shared latent space embedding image features attributes thereby achieve state art results generalized zero shot learning \n",
            "\n",
            "{'rouge-1': {'r': 0.10576923076923077, 'p': 0.9166666666666666, 'f': 0.18965517055885853}, 'rouge-2': {'r': 0.06451612903225806, 'p': 0.7272727272727273, 'f': 0.11851851702167354}, 'rouge-l': {'r': 0.08653846153846154, 'p': 0.75, 'f': 0.15517241193816883}}\n",
            "pair:  reduce memory footprint run time latency techniques neural net work pruning binarization explored separately however un clear combine best two worlds get extremely small efficient models paper first time define filter level pruning problem binary neural networks cannot solved simply migrating existing structural pruning methods full precision models novel learning based approach proposed prune filters main subsidiary network frame work main network responsible learning representative features optimize prediction performance subsidiary component works filter selector main network avoid gradient mismatch training subsidiary component propose layer wise bottom scheme also provide theoretical experimental comparison learning based greedy rule based methods finally empirically demonstrate effectiveness approach applied several binary models including binarizednin vgg resnet various image classification datasets bi nary resnet imagenet use filters achieve slightly better test error original model\n",
            "output sentence:  define filter level pruning problem binary neural networks first time propose method solve \n",
            "\n",
            "{'rouge-1': {'r': 0.14130434782608695, 'p': 0.8666666666666667, 'f': 0.2429906517949166}, 'rouge-2': {'r': 0.05128205128205128, 'p': 0.42857142857142855, 'f': 0.09160305152613488}, 'rouge-l': {'r': 0.10869565217391304, 'p': 0.6666666666666666, 'f': 0.18691588543977639}}\n",
            "pair:  smallest eigenvectors graph laplacian well known provide succinct representation geometry weighted graph reinforcement learning rl weighted graph may interpreted state transition process induced behavior policy acting environment approximating eigenvectors laplacian provides promising approach state representation learning however existing methods performing approximation ill suited general rl settings two main reasons first computationally expensive often requiring operations large matrices second methods lack adequate justification beyond simple tabular finite state settings paper present fully general scalable method approximating eigenvectors laplacian model free rl context systematically evaluate approach empirically show generalizes beyond tabular finite state setting even tabular finite state settings ability approximate eigenvectors outperforms previous proposals finally show potential benefits using laplacian representation learned using method goal achieving rl tasks providing evidence technique used significantly improve performance rl agent\n",
            "output sentence:  propose scalable method approximate eigenvectors laplacian reinforcement learning context show learned representations improve performance rl agent \n",
            "\n",
            "{'rouge-1': {'r': 0.08823529411764706, 'p': 0.46153846153846156, 'f': 0.14814814545343702}, 'rouge-2': {'r': 0.03529411764705882, 'p': 0.25, 'f': 0.06185566793495597}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.3076923076923077, 'f': 0.09876542940405433}}\n",
            "pair:  using recurrent neural networks rnns sequence modeling tasks promising delivering high quality results challenging meet stringent latency requirements memory bound execution pattern rnns propose big little dual module inference dynamically skip unnecessary memory access computation speedup rnn inference leveraging error resilient feature nonlinear activation functions used rnns propose use lightweight little module approximates original rnn layer referred big module compute activations insensitive region error resilient expensive memory access computation big module reduced results used sensitive region method reduce overall memory access average achieve speedup cpu based server platform negligible impact model quality\n",
            "output sentence:  accelerate rnn inference dynamically reducing redundant memory access using mixture accurate approximate modules \n",
            "\n",
            "{'rouge-1': {'r': 0.1206896551724138, 'p': 0.5833333333333334, 'f': 0.1999999971591837}, 'rouge-2': {'r': 0.039473684210526314, 'p': 0.2727272727272727, 'f': 0.06896551503236895}, 'rouge-l': {'r': 0.08620689655172414, 'p': 0.4166666666666667, 'f': 0.14285714001632657}}\n",
            "pair:  recent studies demonstrated vulnerability deep convolutional neural networks adversarial examples inspired observation intrinsic dimension image data much smaller pixel space dimension vulnerability neural networks grows input dimension propose embed high dimensional input images low dimensional space perform classification however arbitrarily projecting input images low dimensional space without regularization improve robustness deep neural networks propose new framework embedding regularized classifier er classifier improves adversarial robustness classifier embedding regularization experimental results several benchmark datasets show proposed framework achieves state art performance strong adversarial attack methods\n",
            "output sentence:  general easy use framework improves adversarial robustness deep classification models embedding regularization \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.9166666666666666, 'f': 0.21999999788800007}, 'rouge-2': {'r': 0.07216494845360824, 'p': 0.6363636363636364, 'f': 0.1296296278000686}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.5, 'f': 0.11999999788800003}}\n",
            "pair:  many tasks artificial intelligence require collaboration multiple agents exam deep reinforcement learning multi agent domains recent research efforts often take form two seemingly conflicting perspectives decentralized perspective agent supposed controller centralized perspective one assumes larger model controlling agents regard revisit idea master slave architecture incorporating perspectives within one framework hierarchical structure naturally leverages advantages one another idea combining perspective intuitive well motivated many real world systems however variety possible realizations highlights three key ingredients composed action representation learnable communication independent reasoning network designs facilitate explicitly proposal consistently outperforms latest competing methods synthetics experiments applied challenging starcraft micromanagement tasks\n",
            "output sentence:  revisit idea master slave architecture multi agent deep reinforcement learning outperforms state arts \n",
            "\n",
            "{'rouge-1': {'r': 0.16216216216216217, 'p': 0.8571428571428571, 'f': 0.2727272700516529}, 'rouge-2': {'r': 0.12087912087912088, 'p': 0.8461538461538461, 'f': 0.21153845935096155}, 'rouge-l': {'r': 0.16216216216216217, 'p': 0.8571428571428571, 'f': 0.2727272700516529}}\n",
            "pair:  study problem alleviating instability issue gan training procedure via new architecture design discrepancy minimax maximin objective values could serve proxy difficulties alternating gradient descent encounters optimization gans work give new results benefits multi generator architecture gans show minimax gap shrinks epsilon number generators increases rate epsilon improves best known result epsilon core techniques novel application shapley folkman lemma generic minimax problem literature technique known work objective function restricted lagrangian function constraint optimization problem proposed stackelberg gan performs well experimentally synthetic real world datasets improving frechet inception distance previous multi generator gans benchmark datasets\n",
            "output sentence:  study problem alleviating instability issue gan training procedure via new architecture design theoretical guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.75, 'f': 0.12499999847222223}, 'rouge-2': {'r': 0.0196078431372549, 'p': 0.2857142857142857, 'f': 0.036697246504503025}, 'rouge-l': {'r': 0.056818181818181816, 'p': 0.625, 'f': 0.1041666651388889}}\n",
            "pair:  use deep learning wide range data problems increased need understanding diagnosing models deep learning interpretation techniques become essential tool data analysts although numerous model interpretation methods proposed recent years procedures based heuristics little theoretical guarantees work propose statistical framework saliency estimation black box computer vision models build model agnostic estimation procedure statistically consistent passes saliency checks adebayo et al method requires solving linear program whose solution efficiently computed polynomial time theoretical analysis establish upper bound number model evaluations needed recover region importance high probability build new perturbation scheme estimation local gradients shown efficient commonly used random perturbation schemes validity new method demonstrated sensitivity analysis\n",
            "output sentence:  propose statistical framework theoretically consistent procedure saliency estimation \n",
            "\n",
            "{'rouge-1': {'r': 0.05813953488372093, 'p': 0.45454545454545453, 'f': 0.10309278149431397}, 'rouge-2': {'r': 0.018867924528301886, 'p': 0.2, 'f': 0.03448275704518438}, 'rouge-l': {'r': 0.05813953488372093, 'p': 0.45454545454545453, 'f': 0.10309278149431397}}\n",
            "pair:  understanding people represent categories core problem cognitive science flexibility human learning remaining gold standard modern artificial intelligence machine learning aspire decades psychological research yielded variety formal theories categories yet validating theories naturalistic stimuli remains challenge problem human category representations cannot directly observed running informative experiments naturalistic stimuli images requires workable representation stimuli deep neural networks recently successful range computer vision tasks provide way represent features images paper introduce method estimating structure human categories draws ideas cognitive science machine learning blending human based algorithms state art deep representation learners provide qualitative quantitative results proof concept feasibility method samples drawn human distributions rival quality current state art generative models outperform alternative methods estimating structure human categories\n",
            "output sentence:  using deep neural networks clever algorithms capture human mental visual concepts \n",
            "\n",
            "{'rouge-1': {'r': 0.10989010989010989, 'p': 0.5882352941176471, 'f': 0.18518518253257893}, 'rouge-2': {'r': 0.05042016806722689, 'p': 0.35294117647058826, 'f': 0.0882352919301471}, 'rouge-l': {'r': 0.08791208791208792, 'p': 0.47058823529411764, 'f': 0.1481481454955419}}\n",
            "pair:  deep generative models variational autoencoder vae generative adversarial network gan play increasingly important role machine learning computer vision however two fundamental issues hindering real world applications difficulty conducting variational inference vae functional absence encoding real world samples gan paper propose novel algorithm named latently invertible autoencoder lia address two issues one framework invertible network inverse mapping symmetrically embedded latent space vae thus partial encoder first transforms input feature vectors distribution feature vectors reshaped fit prior invertible network decoder proceeds reverse order encoder composite mappings two stage stochasticity free training scheme designed train lia via adversarial learning sense decoder lia first trained standard gan invertible network partial encoder learned autoencoder detaching invertible network lia experiments conducted ffhq face dataset three lsun datasets validate effectiveness lia inference generation\n",
            "output sentence:  new model latently invertible autoencoder proposed solve problem variational inference vae using invertible network two stage adversarial training \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.9090909090909091, 'f': 0.24999999762812503}, 'rouge-2': {'r': 0.08045977011494253, 'p': 0.6363636363636364, 'f': 0.14285714086422327}, 'rouge-l': {'r': 0.10144927536231885, 'p': 0.6363636363636364, 'f': 0.17499999762812504}}\n",
            "pair:  knowledge based question answering fundamental problem relax assumption answerable questions simple questions compound questions traditional approaches firstly detect topic entity mentioned questions traverse knowledge graph find relations multi hop path answers propose novel approach leverage simple question answerers answer compound questions model consists two parts novel learning decompose agent learns policy decompose compound question simple questions ii three independent simple question answerers classify corresponding relations simple question experiments demonstrate model learns complex rules compositionality stochastic policy benefits simple neural networks achieve state art results webquestions metaqa analyze interpretable decomposition process well generated partitions\n",
            "output sentence:  propose learning decompose agent helps simple question answerers answer compound question knowledge graph \n",
            "\n",
            "{'rouge-1': {'r': 0.08247422680412371, 'p': 0.8888888888888888, 'f': 0.1509433946724813}, 'rouge-2': {'r': 0.03508771929824561, 'p': 0.5, 'f': 0.06557376926632627}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.4444444444444444, 'f': 0.07547169655927378}}\n",
            "pair:  ever increasing demand resultant reduced quality services focus shifted towards easing network congestion enable efficient flow systems like traffic supply chains electrical grids step direction imagine traditional heuristics based training systems approach incapable modelling involved dynamics one apply multi agent reinforcement learning marl problems considering vertex network agent marl based models assume agents independent many real world tasks agents need behave group rather collection individuals paper propose framework induces cooperation coordination amongst agents connected via underlying network using emergent communication marl based setup formulate problem general network setting demonstrate utility communication networks help case study traffic systems furthermore study emergent communication protocol show formation distinct communities grounded vocabulary best knowledge work studies emergent language networked marl setting\n",
            "output sentence:  framework studying emergent communication networked multi agent reinforcement learning setup \n",
            "\n",
            "{'rouge-1': {'r': 0.06097560975609756, 'p': 0.29411764705882354, 'f': 0.10101009816549339}, 'rouge-2': {'r': 0.018867924528301886, 'p': 0.11764705882352941, 'f': 0.03252032282107229}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.29411764705882354, 'f': 0.10101009816549339}}\n",
            "pair:  introduce open ended modular self improving omega ai unification architecture refinement solomonoff alpha architecture considered first principles architecture embodies several crucial principles general intelligence including diversity representations diversity data types integrated memory modularity higher order cognition retain basic design fundamental algorithmic substrate called ai kernel problem solving basic cognitive functions like memory larger modular architecture uses kernel many ways omega includes eight representation languages six classes neural networks briefly introduced architecture intended initially address data science automation hence includes many problem solving methods statistical tasks review broad software architecture higher order cognition self improvement modular neural architectures intelligent agents process memory hierarchy hardware abstraction peer peer computing data abstraction facility\n",
            "output sentence:  new agi architecture trans sapient performance high level overview omega agi architecture basis data science automation system submitted workshop \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.45454545454545453, 'f': 0.12658227608396094}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.45454545454545453, 'f': 0.12658227608396094}}\n",
            "pair:  automatic piano fingering hard task computers learn using data data collection hard expensive propose automate process automatically extracting fingerings public videos midi files using computer vision techniques running process videos results largest dataset piano fingering notes show running previously proposed model automatic piano fingering dataset fine tuning manually labeled piano fingering data achieve state art results addition fingering extraction method also introduce novel method transferring deep learning computer vision models work domain data fine tuning domain augmentation proposed generative adversarial network gan demonstration anonymously release visualization output process single video https youtu gfs uwqhr\n",
            "output sentence:  automatically extract fingering information videos piano performances used automatic fingering prediction models \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.36363636363636365, 'f': 0.09195402077949535}, 'rouge-2': {'r': 0.01904761904761905, 'p': 0.2, 'f': 0.03478260710775055}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.36363636363636365, 'f': 0.09195402077949535}}\n",
            "pair:  interpreting generative adversarial network gan training approximate divergence minimization theoretically insightful spurred discussion lead theoretically practically interesting extensions gans wasserstein gans classic gans gans original variant training non saturating variant uses alternative form generator update original variant theoretically easier study alternative variant frequently performs better recommended use practice alternative generator update often regarded simple modification deal optimization issues appears common misconception two variants minimize divergence short note derive divergences approximately minimized original alternative variants gan gan training highlights important differences two variants example show alternative variant kl gan training actually minimizes reverse kl divergence alternative variant conventional gan training minimizes softened version reverse kl hope results may help clarify theoretical discussion surrounding divergence minimization view gan training\n",
            "output sentence:  typical gan training optimize jensen shannon something like reverse kl divergence \n",
            "\n",
            "{'rouge-1': {'r': 0.05747126436781609, 'p': 0.5, 'f': 0.10309278165586143}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.034482758620689655, 'p': 0.3, 'f': 0.06185566825379961}}\n",
            "pair:  stochastic neural networks discrete random variables important class models expressivity interpretability since direct differentiation backpropagation possible monte carlo gradient estimation techniques widely employed training models efficient stochastic gradient estimators straight gumbel softmax work well shallow models one two stochastic layers performance however suffers increasing model complexity work focus stochastic networks multiple layers boolean latent variables analyze networks employ framework harmonic analysis boolean functions use derive analytic formulation source bias biased straight estimator based analysis propose emph foust simple gradient estimation algorithm relies three simple bias reduction steps extensive experiments show foust performs favorably compared state art biased estimators much faster unbiased ones best knowledge foust first gradient estimator train deep stochastic neural networks deterministic stochastic layers\n",
            "output sentence:  present low bias estimator boolean stochastic variable models many stochastic layers \n",
            "\n",
            "{'rouge-1': {'r': 0.07792207792207792, 'p': 0.6, 'f': 0.13793103244814375}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03896103896103896, 'p': 0.3, 'f': 0.06896551520676449}}\n",
            "pair:  two important topics deep learning involve incorporating humans modeling process model priors transfer information humans model regularizing model parameters model attributions transfer information model humans explaining model behavior previous work taken important steps connect topics various forms gradient regularization find however existing methods use attributions align model behavior human intuition ineffective develop efficient theoretically grounded feature attribution method expected gradients novel framework attribution priors enforce prior expectations model behavior training demonstrate attribution priors broadly applicable instantiating three different types data image data gene expression data health care data experiments show models trained attribution priors intuitive achieve better generalization performance equivalent baselines existing methods regularize model behavior\n",
            "output sentence:  method encouraging axiomatic feature attributions deep model match human intuition \n",
            "\n",
            "{'rouge-1': {'r': 0.16216216216216217, 'p': 0.631578947368421, 'f': 0.25806451287778936}, 'rouge-2': {'r': 0.043478260869565216, 'p': 0.21052631578947367, 'f': 0.07207206923464017}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.3684210526315789, 'f': 0.15053763115735933}}\n",
            "pair:  machine translation recently achieved impressive performance thanks recent advances deep learning availability large scale parallel corpora numerous attempts extend successes low resource language pairs yet requiring tens thousands parallel sentences work take research direction extreme investigate whether possible learn translate even without parallel data propose model takes sentences monolingual corpora two different languages maps latent space learning reconstruct languages shared feature space model effectively learns translate without using labeled data demonstrate model two widely used datasets two language pairs reporting bleu scores multi wmt english french datasets without using even single parallel sentence training time\n",
            "output sentence:  propose new unsupervised machine translation model learn without using parallel corpora experimental results show impressive performance multiple corpora pairs languages \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 1.0, 'f': 0.2121212102249771}, 'rouge-2': {'r': 0.07746478873239436, 'p': 0.8461538461538461, 'f': 0.14193548233423517}, 'rouge-l': {'r': 0.0847457627118644, 'p': 0.7142857142857143, 'f': 0.15151514961891646}}\n",
            "pair:  inspired neurophysiological discoveries navigation cells mammalian brain introduce first deep neural network architecture modeling egocentric spatial memory esm learns estimate pose agent progressively construct top global maps egocentric views spatially extended environment exploration proposed esm network model updates belief global map based local observations using recurrent neural network also augments local mapping novel external memory encode store latent representations visited places based corresponding locations egocentric coordinate enables agents perform loop closure mapping correction work contributes following aspects first proposed esm network provides accurate mapping ability vitally important embodied agents navigate goal locations experiments demonstrate functionalities esm network random walks complicated mazes comparing several competitive baselines state art simultaneous localization mapping slam algorithms secondly faithfully hypothesize functionality working mechanism navigation cells brain comprehensive analysis model suggests essential role individual modules proposed architecture demonstrates efficiency communications among modules hope work would advance research collaboration communications fields computer science computational neuroscience\n",
            "output sentence:  first deep neural network modeling egocentric spatial memory inspired neurophysiological discoveries navigation cells mammalian brain \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.6923076923076923, 'f': 0.2195121924538965}, 'rouge-2': {'r': 0.03529411764705882, 'p': 0.25, 'f': 0.06185566793495597}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.6153846153846154, 'f': 0.19512194855145748}}\n",
            "pair:  structured tabular data commonly used form data industry according kaggle ml ds survey gradient boosting trees support vector machine random forest logistic regression typically used classification tasks tabular data recent work super characters method using two dimensional word embedding achieved state art results text classification tasks showcasing promise new approach paper propose supertml method borrows idea super characters method two dimensional embedding address problem classification tabular data input tabular data features first projected two dimensional embedding like image image fed fine tuned imagenet cnn models classification experimental results shown proposed supertml method achieved state art results large small datasets\n",
            "output sentence:  deep learning structured tabular data machine two using word embedding cnn model imagenet \n",
            "\n",
            "{'rouge-1': {'r': 0.1323529411764706, 'p': 0.5294117647058824, 'f': 0.211764702682353}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.125, 'f': 0.0392156836293735}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.29411764705882354, 'f': 0.1176470556235295}}\n",
            "pair:  training agents operate one environment often yields overfitted models unable generalize changes environment however due numerous variations occur real world agent often required robust order useful case agents trained reinforcement learning rl algorithms paper investigate overfitting rl agents training environments visual navigation tasks experiments show deep rl agents overfit even trained multiple environments simultaneously propose regularization method combines rl supervised learning methods adding term rl objective would encourage invariance policy variations observations ought affect action taken results method called invariance regularization show improvement generalization policies environments seen training\n",
            "output sentence:  propose regularization term added reinforcement learning objective allows policy maximize reward simultaneously learn invariant irrelevant changes within input \n",
            "\n",
            "{'rouge-1': {'r': 0.06315789473684211, 'p': 0.4, 'f': 0.10909090673553726}, 'rouge-2': {'r': 0.014814814814814815, 'p': 0.14285714285714285, 'f': 0.026845635881266722}, 'rouge-l': {'r': 0.06315789473684211, 'p': 0.4, 'f': 0.10909090673553726}}\n",
            "pair:  knowledge distillation kd common method transferring knowledge learned one machine learning model teacher another model student typically teacher greater capacity parameters higher bit widths knowledge existing methods overlook fact although student absorbs extra knowledge teacher models share input data data medium teacher knowledge demonstrated due difference model capacities student may benefit fully data points teacher trained hand human teacher may demonstrate piece knowledge individualized examples adapted particular student instance terms cultural background interests inspired behavior design data augmentation agents distinct roles facilitate knowledge distillation data augmentation agents generate distinct training data teacher student respectively focus specifically kd teacher network greater precision bit width student network find empirically specially tailored data points enable teacher knowledge demonstrated effectively student compare approach existing kd methods training popular neural architectures demonstrate role wise data augmentation improves effectiveness kd strong prior approaches code reproducing results made publicly available\n",
            "output sentence:  study whether adaptive data augmentation knowledge distillation leveraged simultaneously synergistic manner better training student networks \n",
            "\n",
            "{'rouge-1': {'r': 0.1702127659574468, 'p': 0.6666666666666666, 'f': 0.27118643743751797}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.3333333333333333, 'f': 0.1111111083333334}, 'rouge-l': {'r': 0.1276595744680851, 'p': 0.5, 'f': 0.20338982726802646}}\n",
            "pair:  recent theoretical experimental results suggest possibility using current near future quantum hardware challenging sampling tasks paper introduce free energy based reinforcement learning ferl application quantum hardware propose method processing quantum annealer measured qubit spin configurations approximating free energy quantum boltzmann machine qbm apply method perform reinforcement learning grid world problem using wave quantum annealer experimental results show technique promising method harnessing power quantum sampling reinforcement learning tasks\n",
            "output sentence:  train quantum boltzmann machines using replica stacking method quantum annealer perform reinforcement learning task \n",
            "\n",
            "{'rouge-1': {'r': 0.030927835051546393, 'p': 0.2727272727272727, 'f': 0.05555555372599457}, 'rouge-2': {'r': 0.007874015748031496, 'p': 0.1, 'f': 0.0145985387926902}, 'rouge-l': {'r': 0.030927835051546393, 'p': 0.2727272727272727, 'f': 0.05555555372599457}}\n",
            "pair:  hyperparameter optimization formulated bilevel optimization problem optimal parameters training set depend hyperparameters aim adapt regularization hyperparameters neural networks fitting compact approximations best response function maps hyperparameters optimal weights biases show construct scalable best response approximations neural networks modeling best response single network whose hidden units gated conditionally regularizer justify approximation showing exact best response shallow linear network regularized jacobian represented similar gating mechanism fit model using gradient based hyperparameter optimization algorithm alternates approximating best response around current hyperparameters optimizing hyperparameters using approximate best response function unlike gradient based approaches require differentiating training loss respect hyperparameters allowing us tune discrete hyperparameters data augmentation hyperparameters dropout probabilities hyperparameters adapted online approach discovers hyperparameter schedules outperform fixed hyperparameter values empirically approach outperforms competing hyperparameter optimization methods large scale deep learning problems call networks update hyperparameters online training self tuning networks stns\n",
            "output sentence:  use hypernetwork predict optimal weights given hyperparameters jointly train everything together \n",
            "\n",
            "{'rouge-1': {'r': 0.14864864864864866, 'p': 0.6111111111111112, 'f': 0.2391304316351607}, 'rouge-2': {'r': 0.047058823529411764, 'p': 0.2222222222222222, 'f': 0.07766990002827799}, 'rouge-l': {'r': 0.10810810810810811, 'p': 0.4444444444444444, 'f': 0.1739130403308129}}\n",
            "pair:  recent advances generative adversarial networks facilitated improvements framework successful application various problems resulted extensions multiple domains irgan attempts leverage framework information retrieval ir task described modeling correct conditional probability distribution documents given query work proposes irgan claims optimizing minimax loss function result generator learn distribution setup baseline term steer model away exact adversarial formulation work attempts point certain inaccuracies formulation analyzing loss curves gives insight possible mistakes loss functions better performance obtained using co training like setup propose two models trained co operative rather adversarial fashion\n",
            "output sentence:  points problems loss function used irgan recently proposed gan framework information retrieval model motivated co training proposed performance performance \n",
            "\n",
            "{'rouge-1': {'r': 0.08536585365853659, 'p': 0.7, 'f': 0.15217391110586015}, 'rouge-2': {'r': 0.008620689655172414, 'p': 0.1111111111111111, 'f': 0.015999998663680112}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.5, 'f': 0.1086956502362949}}\n",
            "pair:  consider setting agent fixed body interacting unknown uncertain external world show models trained predict proprioceptive information agent body come represent objects external world spite trained internally available signals dynamic body models come represent external objects necessity predicting effects agent body model learns holistic persistent representations objects world even though training signals body signals dynamics model able successfully predict distributions sensor readings steps future demonstrate even body longer contact object latent variables dynamics model continue represent shape show active data collection maximizing entropy predictions body touch sensors proprioception vestibular information leads learning dynamic models show superior performance used control also collect data real robotic hand show models used answer questions properties objects real world videos qualitative results models available https goo gl mzuqav\n",
            "output sentence:  train predictive models proprioceptive information show represent properties external objects \n",
            "\n",
            "{'rouge-1': {'r': 0.06944444444444445, 'p': 0.7142857142857143, 'f': 0.1265822768658869}, 'rouge-2': {'r': 0.022988505747126436, 'p': 0.3333333333333333, 'f': 0.043010751481096114}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5714285714285714, 'f': 0.10126582116968436}}\n",
            "pair:  well known many machine learning models susceptible adversarial attacks attacker evades classifier making small perturbations inputs paper discusses industrial copyright detection tools serve central role web susceptible adversarial attacks discuss range copyright detection systems particularly vulnerable attacks vulnerabilities especially apparent neural network based systems proof concept describe well known music identification method implement system form neural net attack system using simple gradient methods adversarial music created way successfully fools industrial systems including audiotag copyright detector youtube content id system goal raise awareness threats posed adversarial examples space highlight importance hardening copyright detection systems attacks\n",
            "output sentence:  adversarial examples fool youtube copyright detection system \n",
            "\n",
            "{'rouge-1': {'r': 0.09523809523809523, 'p': 0.4, 'f': 0.15384615073964505}, 'rouge-2': {'r': 0.028985507246376812, 'p': 0.13333333333333333, 'f': 0.04761904468537433}, 'rouge-l': {'r': 0.06349206349206349, 'p': 0.26666666666666666, 'f': 0.10256409945759377}}\n",
            "pair:  study problem training machine learning models incrementally using active learning access imperfect noisy oracles specifically consider setting batch active learning multiple samples selected opposed single sample classical settings reduce training overhead approach bridges uniform randomness score based importance sampling clusters selecting batch new samples experiments benchmark image classification datasets mnist svhn cifar shows improvement existing active learning strategies introduce extra denoising layer deep networks make active learning robust label noises show significant improvements\n",
            "output sentence:  address active learning batch setting noisy oracles use model uncertainty encode decision quality active learning algorithm acquisition \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.6428571428571429, 'f': 0.1682242967909861}, 'rouge-2': {'r': 0.05128205128205128, 'p': 0.42857142857142855, 'f': 0.09160305152613488}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.42857142857142855, 'f': 0.11214953043584597}}\n",
            "pair:  travelling salesman problem tsp well known combinatorial optimization problem variety real life applications tackle tsp incorporating machine learning methodology leveraging variable neighborhood search strategy precisely search process considered markov decision process mdp opt local search used search within small neighborhood monte carlo tree search mcts method iterates simulation selection back propagation steps used sample number targeted actions within enlarged neighborhood new paradigm clearly distinguishes existing machine learning ml based paradigms solving tsp either uses end end ml model simply applies traditional techniques ml post optimization experiments based two public data sets show approach clearly dominates existing learning based tsp algorithms terms performance demonstrating high potential tsp importantly general framework without complicated hand crafted rules readily extended many combinatorial optimization problems\n",
            "output sentence:  paper combines monte carlo tree search opt local search variable neighborhood mode solve tsp effectively \n",
            "\n",
            "{'rouge-1': {'r': 0.15517241379310345, 'p': 0.9, 'f': 0.26470587984429067}, 'rouge-2': {'r': 0.07246376811594203, 'p': 0.5555555555555556, 'f': 0.12820512616370813}, 'rouge-l': {'r': 0.13793103448275862, 'p': 0.8, 'f': 0.23529411513840828}}\n",
            "pair:  generative adversarial networks gans shown provide effective way model complex distributions obtained impressive results various challenging tasks however typical gans require fully observed data training paper present gan based framework learning complex high dimensional incomplete data proposed framework learns complete data generator along mask generator models missing data distribution demonstrate impute missing data equipping framework adversarially trained imputer evaluate proposed framework using series experiments several types missing data processes missing completely random assumption\n",
            "output sentence:  paper presents gan based framework learning distribution high dimensional incomplete data \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.6666666666666666, 'f': 0.1481481461728395}, 'rouge-2': {'r': 0.05747126436781609, 'p': 0.5555555555555556, 'f': 0.10416666496744793}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.6666666666666666, 'f': 0.1481481461728395}}\n",
            "pair:  human computer conversation systems attracted much attention natural language processing conversation systems roughly divided two categories retrieval based generation based systems retrieval systems search user issued utterance namely query large conversational repository return reply best matches query generative approaches synthesize new replies ways certain advantages suffer disadvantages propose novel ensemble retrieval based generation based conversation system retrieved candidates addition original query fed reply generator via neural network model aware information generated reply together retrieved ones participates ranking process find final reply output experimental results show ensemble system outperforms single module large margin\n",
            "output sentence:  novel ensemble retrieval based generation based open domain conversation systems \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.45, 'f': 0.16363636066115708}, 'rouge-2': {'r': 0.02564102564102564, 'p': 0.14285714285714285, 'f': 0.04347825828922511}, 'rouge-l': {'r': 0.07777777777777778, 'p': 0.35, 'f': 0.12727272429752073}}\n",
            "pair:  paper investigate family functions representable deep neural networks dnn rectified linear units relu give algorithm train relu dnn one hidden layer em global optimality runtime polynomial data size albeit exponential input dimension improve known lower bounds size exponential super exponential approximating relu deep net function shallower relu net gap theorems hold smoothly parametrized families hard functions contrary countable discrete families known literature example consequence gap theorems following every natural number exists function representable relu dnn hidden layers total size relu dnn hidden layers require least frac total nodes finally family dnns relu activations show new lowerbound number affine pieces larger previous constructions certain regimes network architecture distinctively lowerbound demonstrated explicit construction emph smoothly parameterized family functions attaining scaling construction utilizes theory zonotopes polyhedral theory\n",
            "output sentence:  paper characterizes functions representable relu dnns formally studies benefit depth architectures gives algorithm implement empirical risk minimization global optimality layer relu layer relu \n",
            "\n",
            "{'rouge-1': {'r': 0.10309278350515463, 'p': 0.7142857142857143, 'f': 0.18018017797581368}, 'rouge-2': {'r': 0.0423728813559322, 'p': 0.38461538461538464, 'f': 0.07633587607482085}, 'rouge-l': {'r': 0.09278350515463918, 'p': 0.6428571428571429, 'f': 0.16216215995779565}}\n",
            "pair:  paper propose extend recently introduced model agnostic meta learning algorithm maml finn et al low resource neural machine translation nmt frame low resource translation meta learning problem learn adapt low resource languages based multilingual high resource language tasks use universal lexical representation gu et al overcome input output mismatch across different languages evaluate proposed meta learning strategy using eighteen european languages bg cs da de el es et fr hu lt nl pl pt sk sl sv ru source tasks five diverse languages ro lv fi tr ko target tasks show proposed approach significantly outperforms multilingual transfer learning based approach zoph et al enables us train competitive nmt system fraction training examples instance proposed approach achieve high bleu romanian english wmt seeing translated words parallel sentences\n",
            "output sentence:  propose meta learning approach low resource neural machine translation rapidly learn translate new language \n",
            "\n",
            "{'rouge-1': {'r': 0.1282051282051282, 'p': 0.625, 'f': 0.2127659546220009}, 'rouge-2': {'r': 0.018867924528301886, 'p': 0.125, 'f': 0.03278688296694453}, 'rouge-l': {'r': 0.1282051282051282, 'p': 0.625, 'f': 0.2127659546220009}}\n",
            "pair:  introduce distribution regression network drn performs regression input probability distributions output probability distributions compared existing methods drn learns fewer model parameters easily extends multiple input multiple output distributions synthetic real world datasets drn performs similarly better state art furthermore drn generalizes conventional multilayer perceptron mlp framework mlp node encodes real number whereas drn node encodes probability distribution\n",
            "output sentence:  learning network generalizes mlp framework perform distribution distribution regression \n",
            "\n",
            "{'rouge-1': {'r': 0.16049382716049382, 'p': 0.9285714285714286, 'f': 0.2736842080132964}, 'rouge-2': {'r': 0.10989010989010989, 'p': 0.7142857142857143, 'f': 0.19047618816507939}, 'rouge-l': {'r': 0.16049382716049382, 'p': 0.9285714285714286, 'f': 0.2736842080132964}}\n",
            "pair:  increasing model size pretraining natural language representations often results improved performance downstream tasks however point model increases become harder due gpu tpu memory limitations longer training times unexpected model degradation address problems present two parameter reduction techniques lower memory consumption increase training speed bert comprehensive empirical evidence shows proposed methods lead models scale much better compared original bert also use self supervised loss focuses modeling inter sentence coherence show consistently helps downstream tasks multi sentence inputs result best model establishes new state art results glue race squad benchmarks fewer parameters compared bert large\n",
            "output sentence:  new pretraining method establishes new state art results glue race squad benchmarks fewer parameters large large \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.7272727272727273, 'f': 0.13913043305255202}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0673076923076923, 'p': 0.6363636363636364, 'f': 0.12173912870472592}}\n",
            "pair:  reinforcement learning rl agents improve trial error reward sparse agent cannot discover successful action sequences learning stagnates notable problem training deep rl agents perform web based tasks booking flights replying emails single mistake ruin entire sequence actions common remedy warm start agent pre training mimic expert demonstrations prone overfitting instead propose constrain exploration using demonstrations demonstration induce high level workflows constrain allowable actions time step similar demonstration step click textbox step enter text exploration policy learns identify successful workflows samples actions satisfy workflows workflows prune bad exploration directions accelerate agent ability discover rewards use approach train novel neural policy designed handle semi structured nature websites evaluate suite web tasks including recent world bits benchmark achieve new state art results show workflow guided exploration improves sample efficiency behavioral cloning\n",
            "output sentence:  solve sparse rewards problem web ui tasks using exploration guided demonstrations \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.5294117647058824, 'f': 0.17307692034208583}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.1111111111111111, 'f': 0.03076922838343214}, 'rouge-l': {'r': 0.08045977011494253, 'p': 0.4117647058823529, 'f': 0.1346153818805474}}\n",
            "pair:  paper study implicit regularization gradient descent algorithm homogeneous neural networks including fully connected convolutional neural networks relu leakyrelu activations particular study gradient descent gradient flow gradient descent infinitesimal step size optimizing logistic loss cross entropy loss homogeneous model possibly non smooth show training loss decreases certain threshold define smoothed version normalized margin increases time also formulate natural constrained optimization problem related margin maximization prove normalized margin smoothed version converge objective value kkt point optimization problem results generalize previous results logistic regression one layer multi layer linear networks provide quantitative convergence results weaker assumptions previous results homogeneous smooth neural networks conduct several experiments justify theoretical finding mnist cifar datasets finally margin closely related robustness discuss potential benefits training longer improving robustness model\n",
            "output sentence:  study implicit bias gradient descent prove minimal set assumptions parameter direction homogeneous models models kkt kkt points natural maximization \n",
            "\n",
            "{'rouge-1': {'r': 0.029850746268656716, 'p': 0.4, 'f': 0.05555555426311731}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.014925373134328358, 'p': 0.2, 'f': 0.027777776485339568}}\n",
            "pair:  neural conversational models widely used applications like personal assistants chat bots models seem give better performance operating word level however fusion languages like french russian polish vocabulary size sometimes become infeasible since words lots word forms propose neural network architecture transforming normalized text grammatically correct one model efficiently employs correspondence normalized target words significantly outperforms character level models faster training faster evaluation also propose new pipeline building conversational models first generate normalized answer transform grammatically correct one using network proposed pipeline gives better performance character level conversational models according assessor testing\n",
            "output sentence:  proposed architecture solve morphological agreement task \n",
            "\n",
            "{'rouge-1': {'r': 0.1797752808988764, 'p': 0.9411764705882353, 'f': 0.301886789759701}, 'rouge-2': {'r': 0.12605042016806722, 'p': 0.8823529411764706, 'f': 0.22058823310661768}, 'rouge-l': {'r': 0.15730337078651685, 'p': 0.8235294117647058, 'f': 0.2641509407030972}}\n",
            "pair:  existing neural question answering qa models required reason draw complicated inferences long context large scale qa datasets however view qa combined retrieval reasoning task assume existence minimal context necessary sufficient answer given question recent work shown sentence selector module selects shorter context feeds downstream qa model achieves performance comparable qa model trained full context also interpretable recent work also shown state art qa models break adversarially generated sentences appended context humans immune distractor sentences qa models get easily misled selecting answers sentences hypothesize sentence selector module filter extraneous context thereby allowing downstream qa model focus reason parts context relevant question paper show sentence selector susceptible adversarial inputs however demonstrate pipeline consisting sentence selector module followed qa model made robust adversarial attacks comparison qa model trained full context thus provide evidence towards modular approach question answering robust interpretable\n",
            "output sentence:  modular approach consisting sentence selector module followed qa model made robust adversarial attacks comparison qa model trained full context \n",
            "\n",
            "{'rouge-1': {'r': 0.08490566037735849, 'p': 0.6923076923076923, 'f': 0.15126050225549045}, 'rouge-2': {'r': 0.024, 'p': 0.25, 'f': 0.04379561883957595}, 'rouge-l': {'r': 0.05660377358490566, 'p': 0.46153846153846156, 'f': 0.10084033418826356}}\n",
            "pair:  discovering causal structure among set variables fundamental problem many empirical sciences traditional score based casual discovery methods rely various local heuristics search directed acyclic graph dag according predefined score function methods greedy equivalence search may attractive results infinite samples certain model assumptions less satisfactory practice due finite data possible violation assumptions motivated recent advances neural combinatorial optimization propose use reinforcement learning rl search dag best scoring encoder decoder model takes observable data input generates graph adjacency matrices used compute rewards reward incorporates predefined score function two penalty terms enforcing acyclicity contrast typical rl applications goal learn policy use rl search strategy final output would graph among graphs generated training achieves best reward conduct experiments synthetic real datasets show proposed approach improved search ability also allows flexible score function acyclicity constraint\n",
            "output sentence:  apply reinforcement learning score based causal discovery achieve promising results synthetic real datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.09836065573770492, 'p': 0.6, 'f': 0.16901408208688753}, 'rouge-2': {'r': 0.04225352112676056, 'p': 0.3333333333333333, 'f': 0.07499999800312505}, 'rouge-l': {'r': 0.09836065573770492, 'p': 0.6, 'f': 0.16901408208688753}}\n",
            "pair:  introduce new rigorously formulated pac bayes shot meta learning algorithm implicitly learns model prior distribution interest proposed method extends pac bayes framework single task setting shot meta learning setting upper bound generalisation errors unseen tasks also propose generative based approach model shared prior task specific posterior expressively compared usual diagonal gaussian assumption show models trained proposed meta learning algorithm well calibrated accurate state art calibration classification results mini imagenet benchmark competitive results multi modal task distribution regression\n",
            "output sentence:  bayesian meta learning using pac bayes framework implicit prior distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.1038961038961039, 'p': 0.6666666666666666, 'f': 0.17977527856583767}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.09090909090909091, 'f': 0.019999998042000193}, 'rouge-l': {'r': 0.07792207792207792, 'p': 0.5, 'f': 0.13483145834111857}}\n",
            "pair:  reinforcement learning rl typically defines discount factor part markov decision process discount factor values future rewards exponential scheme leads theoretical convergence guarantees bellman equation however evidence psychology economics neuroscience suggests humans animals instead hyperbolic time preferences extend earlier work kurth nelson redish propose efficient deep reinforcement learning agent acts via hyperbolic discounting non exponential discount mechanisms demonstrate simple approach approximates hyperbolic discount functions still using familiar temporal difference learning techniques rl additionally independent hyperbolic discounting make surprising discovery simultaneously learning value functions multiple time horizons effective auxiliary task often improves state art methods\n",
            "output sentence:  deep rl agent learns hyperbolic non exponential values new multi horizon auxiliary task \n",
            "\n",
            "{'rouge-1': {'r': 0.11578947368421053, 'p': 0.8461538461538461, 'f': 0.2037037015860768}, 'rouge-2': {'r': 0.04132231404958678, 'p': 0.38461538461538464, 'f': 0.07462686391958123}, 'rouge-l': {'r': 0.07368421052631578, 'p': 0.5384615384615384, 'f': 0.12962962751200274}}\n",
            "pair:  numerous domains including instance earth observation medical imaging astrophysics available image signal datasets often irregular space time sampling patterns large missing data rates sampling properties critical issue apply state art learning based auto encoders cnns fully benefit available large scale observations reach breakthroughs reconstruction identification processes interest paper address end end learning representations signals images image sequences irregularly sampled data em training data involved missing data analogy bayesian formulation consider energy based representations two energy forms investigated one derived auto encoders one relating gibbs energies learning stage energy based representations priors involve joint interpolation issue resorts solving energy minimization problem observation constraints using neural network based implementation considered energy forms state end end learning scheme irregularly sampled data demonstrate relevance proposed representations different case studies namely multivariate time series sc images image sequences\n",
            "output sentence:  address end end learning energy based representations signal image observation dataset irregular sampling patterns \n",
            "\n",
            "{'rouge-1': {'r': 0.23214285714285715, 'p': 0.9285714285714286, 'f': 0.37142856822857145}, 'rouge-2': {'r': 0.11940298507462686, 'p': 0.6153846153846154, 'f': 0.199999997278125}, 'rouge-l': {'r': 0.17857142857142858, 'p': 0.7142857142857143, 'f': 0.2857142825142857}}\n",
            "pair:  derive unbiased estimator expectations discrete random variables based sampling without replacement reduces variance avoids duplicate samples show estimator derived rao blackwellization three different estimators combining estimator reinforce obtain policy gradient estimator reduce variance using built control variate obtained without additional model evaluations resulting estimator closely related gradient estimators experiments toy problem categorical variational auto encoder structured prediction problem show estimator estimator consistently among best estimators high low entropy settings\n",
            "output sentence:  derive low variance unbiased gradient estimator expectations discrete random variables based sampling without replacement \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5714285714285714, 'f': 0.18604650890210925}, 'rouge-2': {'r': 0.024390243902439025, 'p': 0.15384615384615385, 'f': 0.042105260795568006}, 'rouge-l': {'r': 0.06944444444444445, 'p': 0.35714285714285715, 'f': 0.1162790670416442}}\n",
            "pair:  recent literature demonstrated promising results training generative adversarial networks employing set discriminators opposed traditional game involving one generator single adversary methods perform single objective optimization simple consolidation losses average work revisit multiple discriminator approach framing simultaneous minimization losses provided different models multi objective optimization problem specifically evaluate performance multiple gradient descent hypervolume maximization algorithm number different datasets moreover argue previously proposed methods hypervolume maximization seen variations multiple gradient descent update direction computation done efficiently results indicate hypervolume maximization presents better compromise sample quality diversity computational cost previous methods\n",
            "output sentence:  introduce hypervolume maximization training gans multiple discriminators showing performance improvements terms sample quality diversity \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.6, 'f': 0.16666666427469137}, 'rouge-2': {'r': 0.02564102564102564, 'p': 0.2222222222222222, 'f': 0.04597700963931835}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.3, 'f': 0.08333333094135809}}\n",
            "pair:  generative adversarial networks made data generation possible various use cases case complex high dimensional distributions difficult train convergence problems appearance mode collapse sliced wasserstein gans especially application max sliced wasserstein distance made possible approximate wasserstein distance training efficient stable way helped ease convergence problems architectures method transforms sample assignment distance calculation sorting one dimensional projection samples results sufficient approximation high dimensional wasserstein distance paper demonstrate approximation wasserstein distance sorting samples always optimal approach greedy assignment real fake samples result faster convergence better approximation original distribution\n",
            "output sentence:  apply greedy assignment projected samples instead sorting approximate wasserstein distance \n",
            "\n",
            "{'rouge-1': {'r': 0.15, 'p': 0.6, 'f': 0.2399999968}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.2857142857142857, 'f': 0.09756097277810835}, 'rouge-l': {'r': 0.1125, 'p': 0.45, 'f': 0.17999999680000006}}\n",
            "pair:  spectral graph convolutional networks gcns generalization convolutional networks learning graph structured data applications spectral gcns successful limited problems graph fixed shape correspondence node classification work address limitation revisiting particular family spectral graph networks chebyshev gcns showing efficacy solving graph classification tasks variable graph structure size current gcns also restrict graphs one edge pair nodes end propose novel multigraph network learns multi relational graphs explicitly model different types edges annotated edges learned edges abstract meaning hierarchical edges also experiment different ways fuse representations extracted different edge types restriction sometimes implied dataset however relax restriction kinds datasets achieve state art results variety chemical social vision graph classification benchmarks\n",
            "output sentence:  novel approach graph classification based spectral graph convolutional networks extension multigraphs learnable relations hierarchical structure show state art results social social datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.375, 'f': 0.1237113374513764}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.125, 'f': 0.03478260630018921}, 'rouge-l': {'r': 0.04938271604938271, 'p': 0.25, 'f': 0.08247422404931458}}\n",
            "pair:  deep learning training accesses vast amounts data high velocity posing challenges datasets retrieved commodity networks storage devices introduce way dynamically reduce overhead fetching transporting training data method term progressive compressed records pcrs pcrs deviate previous formats leveraging progressive compression split training example multiple examples increasingly higher fidelity without adding total data size training examples similar fidelity grouped together reduces system overhead data bandwidth needed train model show models trained aggressively compressed representations training data still retain high accuracy pcrs enable speedup average baseline formats using jpeg compression results hold across deep learning architectures wide range datasets imagenet ham stanford cars celeba hq\n",
            "output sentence:  propose simple general space efficient data format accelerate deep learning training allowing sample fidelity dynamically selected training time \n",
            "\n",
            "{'rouge-1': {'r': 0.136986301369863, 'p': 0.5555555555555556, 'f': 0.21978021660669003}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.23529411764705882, 'f': 0.0747663524674645}, 'rouge-l': {'r': 0.0958904109589041, 'p': 0.3888888888888889, 'f': 0.15384615067262414}}\n",
            "pair:  semantic structure extraction spreadsheets includes detecting table regions recognizing structural components classifying cell types automatic semantic structure extraction key automatic data transformation various table structures canonical schema enable data analysis knowledge discovery however challenged diverse table structures spatial correlated semantics cell grids learn spatial correlations capture semantics spreadsheets developed novel learning based framework spreadsheet semantic structure extraction first propose multi task framework learns table region structural components cell types jointly second leverage advances recent language model capture semantics cell value third build large human labeled dataset broad coverage table structures evaluation shows proposed multi task framework highly effective outperforms results training task separately\n",
            "output sentence:  propose novel multi task framework learns table detection semantic component recognition cell type classification spreadsheet tables promising results \n",
            "\n",
            "{'rouge-1': {'r': 0.075, 'p': 0.5, 'f': 0.13043478034026468}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.2727272727272727, 'f': 0.059405938653073295}, 'rouge-l': {'r': 0.0625, 'p': 0.4166666666666667, 'f': 0.1086956499054821}}\n",
            "pair:  biological plausibility backpropagation algorithm long doubted neuroscientists two major reasons neurons would need send two different types signal forward backward phases pairs neurons would need communicate symmetric bidirectional connections present simple two phase learning procedure fixed point recurrent networks addresses issues model neurons perform leaky integration synaptic weights updated local mechanism learning method extends framework equilibrium propagation general dynamics relaxing requirement energy function consequence generalization algorithm compute true gradient objective function rather approximates precision proven directly related degree symmetry feedforward feedback weights show experimentally intrinsic properties system lead alignment feedforward feedback weights algorithm optimizes objective function\n",
            "output sentence:  describe biologically plausible learning algorithm fixed point recurrent networks without tied weights \n",
            "\n",
            "{'rouge-1': {'r': 0.10126582278481013, 'p': 0.6153846153846154, 'f': 0.1739130410515123}, 'rouge-2': {'r': 0.01020408163265306, 'p': 0.08333333333333333, 'f': 0.018181816238016735}, 'rouge-l': {'r': 0.06329113924050633, 'p': 0.38461538461538464, 'f': 0.10869564974716453}}\n",
            "pair:  deep networks run low precision operations inference time offer power space advantages high precision alternatives need overcome challenge maintaining high accuracy precision decreases present method training networks learned step size quantization achieves highest accuracy date imagenet dataset using models variety architectures weights activations quantized bits precision train bit models reach full precision baseline accuracy approach builds upon existing methods learning weights quantized networks improving quantizer configured specifically introduce novel means estimate scale task loss gradient weight activation layer quantizer step size learned conjunction network parameters approach works using different levels precision needed given system requires simple modification existing training code\n",
            "output sentence:  method learning quantization configuration low precision networks achieves state art performance quantized networks \n",
            "\n",
            "{'rouge-1': {'r': 0.11235955056179775, 'p': 0.6666666666666666, 'f': 0.19230768983912724}, 'rouge-2': {'r': 0.056074766355140186, 'p': 0.4, 'f': 0.09836065358102665}, 'rouge-l': {'r': 0.0898876404494382, 'p': 0.5333333333333333, 'f': 0.15384615137758878}}\n",
            "pair:  conditional generative adversarial networks cgans finding increasingly widespread use many application domains despite outstanding progress quantitative evaluation models often involves multiple distinct metrics assess different desirable properties image quality conditional consistency intra conditioning diversity setting model benchmarking becomes challenge metric may indicate different best model paper propose frechet joint distance fjd defined frechet distance joint distributions images conditioning allowing implicitly capture aforementioned properties single metric conduct proof concept experiments controllable synthetic dataset consistently highlight benefits fjd compared currently established metrics moreover use newly introduced metric compare existing cgan based models variety conditioning modalities class labels object masks bounding boxes images text captions show fjd used promising single metric model benchmarking\n",
            "output sentence:  propose new metric evaluating conditional gans captures image quality conditional consistency intra conditioning diversity single measure \n",
            "\n",
            "{'rouge-1': {'r': 0.10204081632653061, 'p': 0.8333333333333334, 'f': 0.1818181798743802}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.5454545454545454, 'f': 0.08391608249596559}, 'rouge-l': {'r': 0.10204081632653061, 'p': 0.8333333333333334, 'f': 0.1818181798743802}}\n",
            "pair:  increasing demand deploy convolutional neural networks cnns mobile platforms sparse kernel approach proposed could save parameters standard convolution maintaining accuracy however despite great potential prior research pointed craft sparse kernel design potential effective design prior works adopt simple combinations existing sparse kernels group convolution meanwhile due large design space also impossible try combinations existing sparse kernels paper first field consider craft effective sparse kernel design eliminating large design space specifically present sparse kernel scheme illustrate reduce space three aspects first terms composition remove designs composed repeated layers second remove designs large accuracy degradation find unified property named emph information field behind various sparse kernel designs could directly indicate final accuracy last remove designs two cases better parameter efficiency could achieved additionally provide detailed efficiency analysis final designs scheme experimental results validate idea scheme showing scheme able find designs efficient using parameters computation similar higher accuracy\n",
            "output sentence:  first field show craft effective sparse kernel design three aspects composition performance \n",
            "\n",
            "{'rouge-1': {'r': 0.24561403508771928, 'p': 0.7, 'f': 0.3636363597908585}, 'rouge-2': {'r': 0.11267605633802817, 'p': 0.38095238095238093, 'f': 0.17391303995510404}, 'rouge-l': {'r': 0.22807017543859648, 'p': 0.65, 'f': 0.3376623338168325}}\n",
            "pair:  present novel multi task training approach learning multilingual distributed representations text system learns word sentence embeddings jointly training multilingual skip gram model together cross lingual sentence similarity model construct sentence embeddings processing word embeddings lstm taking average outputs architecture transparently use monolingual sentence aligned bilingual corpora learn multilingual embeddings thus covering vocabulary significantly larger vocabulary bilingual corpora alone model shows competitive performance standard cross lingual document classification task also show effectiveness method low resource scenario\n",
            "output sentence:  jointly train multilingual skip gram model cross lingual sentence similarity model learn high quality multilingual text embeddings perform well low resource scenario \n",
            "\n",
            "{'rouge-1': {'r': 0.12987012987012986, 'p': 0.7142857142857143, 'f': 0.2197802171766695}, 'rouge-2': {'r': 0.0380952380952381, 'p': 0.3076923076923077, 'f': 0.0677966082088481}, 'rouge-l': {'r': 0.11688311688311688, 'p': 0.6428571428571429, 'f': 0.19780219519864753}}\n",
            "pair:  ever increasing size modern datasets combined difficulty obtaining label information made semi supervised learning significant practical importance modern machine learning applications comparison supervised learning key difficulty semi supervised learning make full use unlabeled data order utilize manifold information provided unlabeled data propose novel regularization called tangent normal adversarial regularization composed two parts two parts complement jointly enforce smoothness along two different directions crucial semi supervised learning one applied along tangent space data manifold aiming enforce local invariance classifier manifold performed normal space orthogonal tangent space intending impose robustness classifier noise causing observed data deviating underlying data manifold two regularizers achieved strategy virtual adversarial training method achieved state art performance semi supervised learning tasks artificial dataset practical datasets\n",
            "output sentence:  propose novel manifold regularization strategy based adversarial training significantly improve performance semi supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.1282051282051282, 'p': 0.6666666666666666, 'f': 0.21505376073534513}, 'rouge-2': {'r': 0.08421052631578947, 'p': 0.5333333333333333, 'f': 0.14545454309917355}, 'rouge-l': {'r': 0.10256410256410256, 'p': 0.5333333333333333, 'f': 0.17204300804717312}}\n",
            "pair:  building deep reinforcement learning agents generalize adapt unseen environments remains fundamental challenge ai paper describes progresses challenge context man made environments visually diverse contain intrinsic semantic regularities propose hybrid model based model free approach learning planning semantics leaps consisting multi target sub policy acts visual inputs bayesian model semantic structures placed unseen environment agent plans semantic model make high level decisions proposes next sub target sub policy execute updates semantic model based new observations perform experiments visual navigation tasks using house environment contains diverse human designed indoor scenes real world objects leaps outperforms strong baselines explicitly plan using semantic content\n",
            "output sentence:  propose hybrid model based model free approach using semantic information improve drl generalization man made environments \n",
            "\n",
            "{'rouge-1': {'r': 0.10606060606060606, 'p': 0.4375, 'f': 0.1707317041760857}, 'rouge-2': {'r': 0.0547945205479452, 'p': 0.26666666666666666, 'f': 0.09090908808109513}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.375, 'f': 0.1463414602736467}}\n",
            "pair:  detection photo manipulation relies subtle statistical traces notoriously removed aggressive lossy compression employed online demonstrate end end modeling complex photo dissemination channels allows codec optimization explicit provenance objectives design lightweight trainable lossy image codec delivers competitive rate distortion performance par best hand engineered alternatives lower computational footprint modern gpu enabled platforms results show significant improvements manipulation detection accuracy possible fractional costs bandwidth storage codec improved accuracy even low bit rates well practicality jpeg qf\n",
            "output sentence:  learn efficient lossy image codec optimized facilitate reliable photo manipulation detection fractional cost payload quality even low \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}, 'rouge-2': {'r': 0.08860759493670886, 'p': 0.875, 'f': 0.16091953855991545}, 'rouge-l': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}}\n",
            "pair:  propose new model making generalizable diverse retrosynthetic reaction predictions given target compound task predict likely chemical reactants produce target generative task framed sequence sequence problem using smiles representations molecules building top popular transformer architecture propose two novel pre training methods construct relevant auxiliary tasks plausible reactions problem furthermore incorporate discrete latent variable model architecture encourage model produce diverse set alternative predictions subset reaction examples united states patent literature uspto benchmark dataset model greatly improves performance baseline also generating predictions diverse\n",
            "output sentence:  propose new model making generalizable diverse retrosynthetic reaction predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.5, 'f': 0.1555555529283951}, 'rouge-2': {'r': 0.010526315789473684, 'p': 0.07142857142857142, 'f': 0.0183486216143425}, 'rouge-l': {'r': 0.07894736842105263, 'p': 0.42857142857142855, 'f': 0.1333333307061729}}\n",
            "pair:  provide novel perspective forward pass block layers deep network particular show forward pass standard dropout layer followed linear layer non linear activation equivalent optimizing convex objective single iteration tau nice proximal stochastic gradient method show replacing standard bernoulli dropout additive dropout equivalent optimizing convex objective variance reduced proximal method expressing fully connected convolutional layers special cases high order tensor product unify underlying convex optimization problem tensor setting derive formula lipschitz constant used determine optimal step size proximal methods conduct experiments standard convolutional networks applied cifar cifar datasets show replacing block layers multiple iterations corresponding solver step size set via consistently improves classification accuracy\n",
            "output sentence:  framework links deep network layers stochastic optimization algorithms used improve model accuracy inform network design \n",
            "\n",
            "{'rouge-1': {'r': 0.08235294117647059, 'p': 0.5, 'f': 0.14141413898581778}, 'rouge-2': {'r': 0.037037037037037035, 'p': 0.26666666666666666, 'f': 0.06504064826492174}, 'rouge-l': {'r': 0.07058823529411765, 'p': 0.42857142857142855, 'f': 0.1212121187837976}}\n",
            "pair:  recently inspired mass researches adversarial examples computer vision growing interest designing adversarial attacks natural language processing nlp tasks followed works adversarial defenses nlp knowledge exists defense method successful synonym substitution based attacks aim satisfy lexical grammatical semantic constraints thus hard perceived humans contribute fill gap propose novel adversarial defense method called synonym encoding method sem inserts encoder input layer model trains model eliminate adversarial perturbations extensive experiments demonstrate sem efficiently defend current best synonym substitution based adversarial attacks little decay accuracy benign examples better evaluate sem also design strong attack method called improved genetic algorithm iga adopts genetic metaheuristic synonym substitution based attacks compared existing genetic based adversarial attack iga achieve higher attack success rate maintaining transferability adversarial examples\n",
            "output sentence:  first text adversarial defense method word level improved generic based attack method synonyms substitution based attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}, 'rouge-2': {'r': 0.08860759493670886, 'p': 0.875, 'f': 0.16091953855991545}, 'rouge-l': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}}\n",
            "pair:  propose new model making generalizable diverse retrosynthetic reaction predictions given target compound task predict likely chemical reactants produce target generative task framed sequence sequence problem using smiles representations molecules building top popular transformer architecture propose two novel pre training methods construct relevant auxiliary tasks plausible reactions problem furthermore incorporate discrete latent variable model architecture encourage model produce diverse set alternative predictions subset reaction examples united states patent literature uspto benchmark dataset model greatly improves performance baseline also generating predictions diverse\n",
            "output sentence:  propose new model making generalizable diverse retrosynthetic reaction predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.1411764705882353, 'p': 0.631578947368421, 'f': 0.23076922778291423}, 'rouge-2': {'r': 0.061946902654867256, 'p': 0.35, 'f': 0.10526315533947658}, 'rouge-l': {'r': 0.11764705882352941, 'p': 0.5263157894736842, 'f': 0.19230768932137576}}\n",
            "pair:  adversarial examples perturbed inputs designed fool machine learning models adversarial training injects examples training data increase robustness scale technique large datasets perturbations crafted using fast single step methods maximize linear approximation model loss show form adversarial training converges degenerate global minimum wherein small curvature artifacts near data points obfuscate linear approximation loss model thus learns generate weak perturbations rather defend strong ones result find adversarial training remains vulnerable black box attacks transfer perturbations computed undefended models well powerful novel single step attack escapes non smooth vicinity input data via small random step introduce ensemble adversarial training technique augments training data perturbations transferred models imagenet ensemble adversarial training yields models strong robustness black box attacks particular robust model first round nips competition defenses adversarial attacks\n",
            "output sentence:  adversarial training single step methods overfits remains vulnerable simple black box white box attacks show including adversarial examples multiple sources helps black \n",
            "\n",
            "{'rouge-1': {'r': 0.1276595744680851, 'p': 0.6666666666666666, 'f': 0.2142857115880102}, 'rouge-2': {'r': 0.03508771929824561, 'p': 0.25, 'f': 0.061538459379881734}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.4444444444444444, 'f': 0.14285714015943882}}\n",
            "pair:  learning domain invariant representation dominant approach domain generalization however previous methods based domain invariance overlooked underlying dependency classes domains responsible trade classification accuracy invariance study proposes novel method em adversarial feature learning accuracy constraint aflac maximizes domain invariance within range interfere accuracy empirical validations show performance aflac superior baseline methods supporting importance considering dependency efficacy proposed method overcome problem\n",
            "output sentence:  address trade caused dependency classes domains improving domain adversarial nets \n",
            "\n",
            "{'rouge-1': {'r': 0.07894736842105263, 'p': 0.6, 'f': 0.13953488166576528}, 'rouge-2': {'r': 0.033707865168539325, 'p': 0.3333333333333333, 'f': 0.061224488127863436}, 'rouge-l': {'r': 0.07894736842105263, 'p': 0.6, 'f': 0.13953488166576528}}\n",
            "pair:  convolutional neural networks cnns achieved state art performance recognizing representing audio images videos volumes domains input characterized regular graph structure however generalizing cnns irregular domains like meshes challenging additionally training data meshes often limited work generalize convolutional autoencoders mesh surfaces perform spectral decomposition meshes apply convolutions directly frequency space addition use max pooling introduce upsampling within network represent meshes low dimensional space construct complex dataset high resolution meshes extreme facial expressions encode using convolutional mesh autoencoder despite limited training data method outperforms state art pca models faces lower error using fewer parameters\n",
            "output sentence:  convolutional autoencoders generalized mesh surfaces encoding reconstructing extreme facial expressions \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.7272727272727273, 'f': 0.161616159640853}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.4, 'f': 0.068965515665874}, 'rouge-l': {'r': 0.07954545454545454, 'p': 0.6363636363636364, 'f': 0.14141413943883277}}\n",
            "pair:  major challenge learning image representations disentangling factors variation underlying image formation typically achieved autoencoder architecture subset latent variables constrained correspond specific factors rest considered nuisance variables approach important drawback dimension nuisance variables increased image reconstruction improved decoder flexibility ignore specified factors thus losing ability condition output work propose overcome trade progressively growing dimension latent code constraining jacobian output image respect disentangled variables remain result obtained models effective disentangling reconstruction demonstrate applicability method unsupervised supervised scenarios learning disentangled representations facial attribute manipulation task obtain high quality image generation smoothly controlling dozens attributes single model order magnitude disentangled factors state art methods obtaining visually similar superior results avoiding adversarial training\n",
            "output sentence:  method learning image representations good disentangling factors variation obtaining faithful reconstructions \n",
            "\n",
            "{'rouge-1': {'r': 0.039603960396039604, 'p': 0.8, 'f': 0.07547169721431114}, 'rouge-2': {'r': 0.007936507936507936, 'p': 0.25, 'f': 0.015384614788165704}, 'rouge-l': {'r': 0.0297029702970297, 'p': 0.6, 'f': 0.05660377268600926}}\n",
            "pair:  state art performances language comprehension tasks achieved huge language models pre trained massive unlabeled text corpora light subsequent fine tuning task specific supervised manner seems pre training procedure learns good common initialization training various natural language understanding tasks steps need taken parameter space learn task work using bidirectional encoder representations transformers bert example verify hypothesis showing task specific fine tuned language models highly close parameter space pre trained one taking advantage observations show fine tuned versions huge models order floating point parameters made computationally efficient first fine tuning fraction critical layers suffices second fine tuning adequately performed learning binary multiplicative mask pre trained weights textit parameter sparsification result single effort achieve three desired outcomes learning perform specific tasks saving memory storing binary masks certain layers task saving compute appropriate hardware performing sparse operations model parameters\n",
            "output sentence:  sparsification fine tuning language models \n",
            "\n",
            "{'rouge-1': {'r': 0.1506849315068493, 'p': 0.7333333333333333, 'f': 0.2499999971720041}, 'rouge-2': {'r': 0.07526881720430108, 'p': 0.4375, 'f': 0.12844036446763743}, 'rouge-l': {'r': 0.136986301369863, 'p': 0.6666666666666666, 'f': 0.22727272444473143}}\n",
            "pair:  end end acoustic word speech recognition models recently gained popularity easy train scale well large amounts training data require lexicon addition word models may also easier integrate downstream tasks spoken language understanding inference search much simplified compared phoneme character sort sub word units paper describe methods construct contextual acoustic word embeddings directly supervised sequence sequence acoustic word speech recognition model using learned attention distribution suite standard sentence evaluation tasks embeddings show competitive performance word vec model trained speech transcriptions addition evaluate embeddings spoken language understanding task observe embeddings match performance text based embeddings pipeline first performing speech recognition constructing word embeddings transcriptions\n",
            "output sentence:  methods learn contextual acoustic word embeddings end end speech recognition model perform competitively text based word embeddings \n",
            "\n",
            "{'rouge-1': {'r': 0.14634146341463414, 'p': 0.46153846153846156, 'f': 0.22222221856652952}, 'rouge-2': {'r': 0.0625, 'p': 0.25, 'f': 0.0999999968000001}, 'rouge-l': {'r': 0.14634146341463414, 'p': 0.46153846153846156, 'f': 0.22222221856652952}}\n",
            "pair:  address problem marginal inference exponential family defined set permutation matrices problem known quickly become intractable size permutation increases since involves computation permanent matrix hard problem introduce sinkhorn variational marginal inference scalable alternative method whose validity ultimately justified called sinkhorn approximation permanent demonstrate efectiveness method problem probabilistic identification neurons worm elegans\n",
            "output sentence:  new methodology variational marginal inference permutations based sinkhorn algorithm applied probabilistic identification neurons \n",
            "\n",
            "{'rouge-1': {'r': 0.13, 'p': 0.7222222222222222, 'f': 0.22033898046538355}, 'rouge-2': {'r': 0.048, 'p': 0.3333333333333333, 'f': 0.08391608171548738}, 'rouge-l': {'r': 0.09, 'p': 0.5, 'f': 0.15254237029589202}}\n",
            "pair:  study problem designing provably optimal adversarial noise algorithms induce misclassification settings learner aggregates decisions multiple classifiers given demonstrated vulnerability state art models adversarial examples recent efforts within field robust machine learning focused use ensemble classifiers way boosting robustness individual models paper design provably optimal attacks set classifiers demonstrate problem framed finding strategies equilibrium two player zero sum game learner adversary consequently illustrate need randomization adversarial attacks main technical challenge consider design best response oracles implemented multiplicative weight updates framework find equilibrium strategies zero sum game develop series scalable noise generation algorithms deep neural networks show outperforms state art attacks various image classification tasks although generally guarantees deep learning show well principled approach provably optimal linear classifiers main insight geometric characterization decision space reduces problem designing best response oracles minimizing quadratic function set convex polytopes\n",
            "output sentence:  paper analyzes problem designing adversarial attacks multiple classifiers introducing algorithms optimal linear classifiers provide state art results deep learning \n",
            "\n",
            "{'rouge-1': {'r': 0.22988505747126436, 'p': 1.0, 'f': 0.37383177266136786}, 'rouge-2': {'r': 0.14035087719298245, 'p': 0.8, 'f': 0.23880596760971265}, 'rouge-l': {'r': 0.21839080459770116, 'p': 0.95, 'f': 0.3551401838763211}}\n",
            "pair:  develop new approximation statistical learning theories convolutional neural networks cnns via resnet type structure channel size filter size width fixed shown resnet type cnn universal approximator expression ability worse fully connected neural networks fnns textit block sparse structure even size layer cnn fixed result general sense automatically translate approximation rate achieved block sparse fnns cnns thanks general theory shown learning cnns satisfies optimality approximation estimation several important function classes applications consider two types function classes estimated barron class older class prove clipped empirical risk minimization erm estimator achieve rate fnns even channel size filter size width cnns constant respect sample size minimax optimal logarithmic factors older class proof based sophisticated evaluations covering number cnns non trivial parameter rescaling technique control lipschitz constant cnns constructed\n",
            "output sentence:  shown resnet type cnns universal approximator expression ability worse fully connected neural networks fnns textit block sparse structure even layer layer layer \n",
            "\n",
            "{'rouge-1': {'r': 0.0821917808219178, 'p': 0.5, 'f': 0.14117646816332183}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0684931506849315, 'p': 0.4166666666666667, 'f': 0.11764705639861596}}\n",
            "pair:  consider problem generating plausible diverse video sequences given start end frame task also known inbetweening belongs broader area stochastic video generation generally approached means recurrent neural networks rnn paper propose instead fully convolutional model generate video sequences directly pixel domain first obtain latent video representation using stochastic fusion mechanism learns incorporate information start end frames model learns produce latent representation progressively increasing temporal resolution decode spatiotemporal domain using convolutions model trained end end minimizing adversarial loss experiments several widely used benchmark datasets show able generate meaningful diverse video sequences according quantitative qualitative evaluations\n",
            "output sentence:  paper presents method stochastically generating video frames given key frames using direct convolutions \n",
            "\n",
            "{'rouge-1': {'r': 0.07, 'p': 0.7777777777777778, 'f': 0.1284403654574531}, 'rouge-2': {'r': 0.023809523809523808, 'p': 0.375, 'f': 0.04477611828024061}, 'rouge-l': {'r': 0.03, 'p': 0.3333333333333333, 'f': 0.055045870044609084}}\n",
            "pair:  recent image super resolution sr studies leverage deep convolutional neural networks rich hierarchical features offered leads better reconstruction performance conventional methods however small receptive fields sampling reconstruction process models stop take full advantage global contextual information causes problems performance improvement paper inspired image reconstruction principles human visual system propose image super resolution global reasoning network srgrn effectively learn correlations different regions image global reasoning specifically propose global reasoning sampling module grum global reasoning reconstruction block grrb construct graph model perform relation reasoning regions low resolution lr images aim reason interactions different regions sampling reconstruction process thus leverage contextual information generate accurate details proposed srgrn robust handle low resolution images corrupted multiple types degradation extensive experiments different benchmark data sets show model outperforms state art methods also model lightweight consumes less computing power makes suitable real life deployment\n",
            "output sentence:  state art model based global reasoning image super resolution \n",
            "\n",
            "{'rouge-1': {'r': 0.171875, 'p': 0.9166666666666666, 'f': 0.2894736815512466}, 'rouge-2': {'r': 0.0963855421686747, 'p': 0.5714285714285714, 'f': 0.16494845113827192}, 'rouge-l': {'r': 0.15625, 'p': 0.8333333333333334, 'f': 0.26315789207756235}}\n",
            "pair:  multi agent systems complex interacting behaviors arise due high correlations among agents however previous work modeling multi agent interactions demonstrations primarily constrained assuming independence among policies reward structures paper cast multi agent interactions modeling problem multi agent imitation learning framework explicit modeling correlated policies approximating opponents policies recover agents policies regenerate similar interactions consequently develop decentralized adversarial imitation learning algorithm correlated policies codail allows decentralized training execution various experiments demonstrate codail better regenerate complex interactions close demonstrators outperforms state art multi agent imitation learning methods code available url https github com apexrl codail\n",
            "output sentence:  modeling complex multi agent interactions multi agent imitation learning framework explicit modeling policies approximating policies opponents \n",
            "\n",
            "{'rouge-1': {'r': 0.16417910447761194, 'p': 0.8461538461538461, 'f': 0.2749999972781251}, 'rouge-2': {'r': 0.1282051282051282, 'p': 0.7692307692307693, 'f': 0.21978021733124017}, 'rouge-l': {'r': 0.16417910447761194, 'p': 0.8461538461538461, 'f': 0.2749999972781251}}\n",
            "pair:  reinforcement learning agent needs pursue different goals across episodes requires goal conditional policy addition potential generalize desirable behavior unseen goals policies may also enable higher level planning based subgoals sparse reward environments capacity exploit information degree arbitrary goal achieved another goal intended appears crucial enable sample efficient learning however reinforcement learning agents recently endowed capacity hindsight paper demonstrate hindsight introduced policy gradient methods generalizing idea broad class successful algorithms experiments diverse selection sparse reward environments show hindsight leads remarkable increase sample efficiency\n",
            "output sentence:  introduce capacity exploit information degree arbitrary goal achieved another goal intended policy gradient methods \n",
            "\n",
            "{'rouge-1': {'r': 0.042105263157894736, 'p': 0.5, 'f': 0.0776699014798756}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.031578947368421054, 'p': 0.375, 'f': 0.05825242575172028}}\n",
            "pair:  make deep neural networks feasible resource constrained environments mobile devices beneficial quantize models using low precision weights one common technique quantizing neural networks straight gradient method enables back propagation quantization mapping despite empirical success little understood straight gradient method works building upon novel observation straight gradient method fact identical well known nesterov dual averaging algorithm quantization constrained optimization problem propose principled alternative approach called proxquant formulates quantized network training regularized learning problem instead optimizes via prox gradient method proxquant back propagation underlying full precision vector applies efficient prox operator stochastic gradient steps encourage quantizedness quantizing resnets lstms proxquant outperforms state art results binary quantization par state art multi bit quantization binary quantization analysis shows theoretically experimentally proxquant stable straight gradient method binaryconnect challenging indispensability straight gradient method providing powerful alternative\n",
            "output sentence:  principled framework model quantization using proximal gradient method \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.8571428571428571, 'f': 0.12631578810858723}, 'rouge-2': {'r': 0.043478260869565216, 'p': 0.8333333333333334, 'f': 0.082644627156615}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.8571428571428571, 'f': 0.12631578810858723}}\n",
            "pair:  predicting future real world settings particularly raw sensory observations images exceptionally challenging real world events stochastic unpredictable high dimensionality complexity natural images requires predictive model build intricate understanding natural world many existing methods tackle problem making simplifying assumptions environment one common assumption outcome deterministic one plausible future lead low quality predictions real world settings stochastic dynamics paper develop stochastic variational video prediction sv method predicts different possible future sample latent variables best knowledge model first provide effective stochastic multi frame prediction real world video demonstrate capability proposed method predicting detailed future frames videos multiple real world datasets action free action conditioned find proposed method produces substantially improved video predictions compared model without stochasticity stochastic video prediction methods sv implementation open sourced upon publication\n",
            "output sentence:  stochastic variational video prediction real world settings \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.8333333333333334, 'f': 0.14492753464398234}, 'rouge-2': {'r': 0.02666666666666667, 'p': 0.4, 'f': 0.04999999882812503}, 'rouge-l': {'r': 0.031746031746031744, 'p': 0.3333333333333333, 'f': 0.057971012904851967}}\n",
            "pair:  paper propose framework leverages semi supervised models improve unsupervised clustering performance leverage semi supervised models first need automatically generate labels called pseudo labels find prior approaches generating pseudo labels hurt clustering performance low accuracy instead use ensemble deep networks construct similarity graph extract high accuracy pseudo labels approach finding high quality pseudo labels using ensembles training semi supervised model iterated yielding continued improvement show approach outperforms state art clustering results multiple image text datasets example achieve accuracy cifar news outperforming state art absolute terms\n",
            "output sentence:  using ensembles pseudo labels unsupervised clustering \n",
            "\n",
            "{'rouge-1': {'r': 0.16279069767441862, 'p': 0.7777777777777778, 'f': 0.26923076636834326}, 'rouge-2': {'r': 0.06140350877192982, 'p': 0.4117647058823529, 'f': 0.10687022674902401}, 'rouge-l': {'r': 0.13953488372093023, 'p': 0.6666666666666666, 'f': 0.23076922790680476}}\n",
            "pair:  hashing based collaborative filtering learns binary vector representations hash codes users items recommendations computed efficiently using hamming distance simply sum differing bits two hash codes problem hashing based collaborative filtering using hamming distance bit equally weighted distance computation practice bits might encode important properties bits importance depends user end propose end end trainable variational hashing based collaborative filtering approach uses novel concept self masking user hash code acts mask items using boolean operation learns encode bits important user rather user preference towards underlying item property bits represent allows binary user level importance weighting item without need store additional weights user experimentally evaluate approach state art baselines datasets obtain significant gains ndcg also make available efficient implementation self masking experimentally yields runtime overhead compared standard hamming distance\n",
            "output sentence:  propose new variational hashing based collaborative filtering approach optimized novel self mask variant hamming distance outperforms state art ndcg \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.9, 'f': 0.17999999820000004}, 'rouge-2': {'r': 0.05172413793103448, 'p': 0.6666666666666666, 'f': 0.09599999866368002}, 'rouge-l': {'r': 0.07777777777777778, 'p': 0.7, 'f': 0.13999999820000003}}\n",
            "pair:  recent work adversarial machine learning started focus visual perception autonomous driving studied adversarial examples aes object detection models however visual perception pipeline detected objects must also tracked process called multiple object tracking mot build moving trajectories surrounding obstacles since mot designed robust errors object detection poses general challenge existing attack techniques blindly target objection detection find success rate needed actually affect tracking results requirement existing attack technique satisfy paper first study adversarial machine learning attacks complete visual perception pipeline autonomous driving discover novel attack technique tracker hijacking effectively fool mot using aes object detection using technique successful aes one single frame move existing object headway autonomous vehicle cause potential safety hazards perform evaluation using berkeley deep drive dataset find average frames attacked attack nearly success rate attacks blindly target object detection\n",
            "output sentence:  study adversarial machine learning attacks multiple object tracking mechanisms first time \n",
            "\n",
            "{'rouge-1': {'r': 0.14084507042253522, 'p': 1.0, 'f': 0.2469135780826094}, 'rouge-2': {'r': 0.10112359550561797, 'p': 1.0, 'f': 0.18367346771970014}, 'rouge-l': {'r': 0.14084507042253522, 'p': 1.0, 'f': 0.2469135780826094}}\n",
            "pair:  deep neural networks use deeper broader structures achieve better performance consequently use increasingly gpu memory well however limited gpu memory restricts many potential designs neural networks paper propose reinforcement learning based variable swapping recomputation algorithm reduce memory cost without sacrificing accuracy models variable swapping transfer variables cpu gpu memory reduce variables stored gpu memory recomputation trade time space removing feature maps forward propagation forward functions executed get feature maps reuse however automatically decide variables swapped recomputed remains challenging problem address issue propose use deep network dqn make plans combining variable swapping recomputation results outperform several well known benchmarks\n",
            "output sentence:  propose reinforcement learning based variable swapping recomputation algorithm reduce memory cost \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.5, 'f': 0.09090908925619837}, 'rouge-2': {'r': 0.022222222222222223, 'p': 0.2857142857142857, 'f': 0.04123711206291853}, 'rouge-l': {'r': 0.0375, 'p': 0.375, 'f': 0.06818181652892566}}\n",
            "pair:  many domains especially enterprise text analysis abundance data used development new ai powered intelligent experiences improve people productivity however strong guarantees privacy prevent broad sampling labeling personal text data learn evaluate models interest fortunately cases like enterprise email manual annotation possible certain public datasets hope models trained public datasets would perform well target private datasets interest paper study challenges transferring information one email dataset another predicting user intent particular present approaches characterizing transfer gap text corpora intrinsic extrinsic point view evaluate several proposed methods literature bridging gap conclude raising issues discussion arena\n",
            "output sentence:  insights domain adaptation challenge predicting user intent enterprise email \n",
            "\n",
            "{'rouge-1': {'r': 0.06315789473684211, 'p': 0.5454545454545454, 'f': 0.11320754530971879}, 'rouge-2': {'r': 0.0078125, 'p': 0.1, 'f': 0.014492752278933122}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.45454545454545453, 'f': 0.09433962078141689}}\n",
            "pair:  deep networks achieved impressive results across variety important tasks however known weakness failure perform well evaluated data differ training distribution even differences small case adversarial examples propose emph fortified networks simple transformation existing networks fortifies hidden layers deep network identifying hidden states data manifold maps hidden states back parts data manifold network performs well principal contribution show fortifying hidden states improves robustness deep networks experiments demonstrate improved robustness standard adversarial attacks black box white box threat models ii suggest improvements primarily due problem deceptively good results due degraded quality gradient signal gradient masking problem iii show advantage fortification hidden layers instead input space demonstrate improvements adversarial robustness three datasets mnist fashion mnist cifar across several attack parameters white box black box settings widely studied attacks fgsm pgd carlini wagner show improvements achieved across wide variety hyperparameters\n",
            "output sentence:  better adversarial training learning map back data manifold autoencoders hidden states \n",
            "\n",
            "{'rouge-1': {'r': 0.14814814814814814, 'p': 0.7272727272727273, 'f': 0.24615384334201185}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.1, 'f': 0.026315787188365854}, 'rouge-l': {'r': 0.09259259259259259, 'p': 0.45454545454545453, 'f': 0.15384615103431953}}\n",
            "pair:  data augmentation da fundamental overfitting large convolutional neural networks especially limited training dataset images da usually based heuristic transformations like geometric color transformations instead using predefined transformations work learns data augmentation directly training data learning transform images encoder decoder architecture combined spatial transformer network transformed images still belong class new complex samples classifier experiments show approach better previous generative data augmentation methods comparable predefined transformation methods training image classifier\n",
            "output sentence:  automatic learning data augmentation using gan based architecture improve image classifier \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.7692307692307693, 'f': 0.24390243635633554}, 'rouge-2': {'r': 0.0449438202247191, 'p': 0.3333333333333333, 'f': 0.0792079186981669}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.6153846153846154, 'f': 0.19512194855145748}}\n",
            "pair:  graphs fundamental data structures required model many important real world data knowledge graphs physical social interactions molecules proteins paper study problem learning generative models graphs dataset graphs interest learning models used generate samples similar properties ones dataset models useful lot applications drug discovery knowledge graph construction task learning generative models graphs however unique challenges particular handle symmetries graphs ordering elements generation process important issues propose generic graph neural net based model capable generating arbitrary graph study performance graph generation tasks compared baselines exploit domain knowledge discuss potential issues open problems generative models going forward\n",
            "output sentence:  study graph generation problem propose powerful deep generative model capable generating arbitrary graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.036036036036036036, 'p': 0.36363636363636365, 'f': 0.06557376885111534}, 'rouge-2': {'r': 0.0072992700729927005, 'p': 0.09090909090909091, 'f': 0.01351351213750927}, 'rouge-l': {'r': 0.036036036036036036, 'p': 0.36363636363636365, 'f': 0.06557376885111534}}\n",
            "pair:  recent advances neural variational inference allowed renaissance latent variable models variety domains involving high dimensional data paper introduce two generic variational inference frameworks generative models knowledge graphs latent fact model latent information model traditional variational methods derive analytical approximation intractable distribution latent variables construct inference network conditioned symbolic representation entities relation types knowledge graph provide variational distributions new framework create models able discover underlying probabilistic semantics symbolic representation utilising parameterisable distributions permit training back propagation context neural variational inference resulting highly scalable method bernoulli sampling framework provide alternative justification commonly used techniques large scale stochastic variational inference drastically reduces training time cost additional approximation variational lower bound generative frameworks flexible enough allow training prior distribution permits parametrisation trick well scoring function permits maximum likelihood estimation parameters experiment results display potential efficiency framework improving upon multiple benchmarks gaussian prior representations code publicly available github\n",
            "output sentence:  working toward generative knowledge graph models better estimate predictive uncertainty knowledge inference \n",
            "\n",
            "{'rouge-1': {'r': 0.16883116883116883, 'p': 0.8666666666666667, 'f': 0.2826086929229679}, 'rouge-2': {'r': 0.07142857142857142, 'p': 0.5, 'f': 0.12499999781250003}, 'rouge-l': {'r': 0.12987012987012986, 'p': 0.6666666666666666, 'f': 0.21739130161862005}}\n",
            "pair:  analyze dynamics training deep relu networks implications generalization capability using teacher student setting discovered novel relationship gradient received hidden student nodes activations teacher nodes deep relu networks relationship assumption small overlapping teacher node activations prove student nodes whose weights initialized close teacher nodes converge faster rate parameterized regimes layer case small set lucky nodes converge teacher nodes fan weights nodes converge zero framework provides insight multiple puzzling phenomena deep learning like parameterization implicit regularization lottery tickets etc verify assumption showing majority batchnorm biases pre trained vgg models negative experiments random deep teacher networks gaussian inputs teacher network pre trained cifar extensive ablation studies validate multiple theoretical predictions\n",
            "output sentence:  theoretical framework deep relu network explains multiple puzzling phenomena like parameterization implicit regularization lottery tickets etc \n",
            "\n",
            "{'rouge-1': {'r': 0.10909090909090909, 'p': 0.6666666666666666, 'f': 0.1874999975830078}, 'rouge-2': {'r': 0.028985507246376812, 'p': 0.2222222222222222, 'f': 0.05128204924063125}, 'rouge-l': {'r': 0.10909090909090909, 'p': 0.6666666666666666, 'f': 0.1874999975830078}}\n",
            "pair:  propose information maximization autoencoder imae information theoretic approach simultaneously learn continuous discrete representations unsupervised setting unlike variational autoencoder framework imae starts stochastic encoder seeks map input data hybrid discrete continuous representation objective maximizing mutual information data representations decoder included approximate posterior distribution data given representations high fidelity approximation achieved leveraging informative representations show proposed objective theoretically valid provides principled framework understanding tradeoffs regarding informativeness representation factor disentanglement representations decoding quality\n",
            "output sentence:  information theoretical approach unsupervised learning unsupervised learning hybrid discrete continuous representations \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.6666666666666666, 'f': 0.20779220516107272}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.4166666666666667, 'f': 0.11494252635751095}, 'rouge-l': {'r': 0.1076923076923077, 'p': 0.5833333333333334, 'f': 0.1818181791870468}}\n",
            "pair:  one main challenges deep learning methods choice appropriate training strategy particular additional steps unsupervised pre training shown greatly improve performances deep structures article propose extra training step called post training optimizes last layer network show procedure analyzed context kernel theory first layers computing embedding data last layer statistical model solve task based embedding step makes sure embedding representation data used best possible way considered task idea tested multiple architectures various data sets showing consistently provides boost performance\n",
            "output sentence:  propose additional training step called post training computes optimal weights last layer network \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.2222222222222222, 'f': 0.1052631542797785}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.034482758620689655, 'p': 0.1111111111111111, 'f': 0.05263157533241022}}\n",
            "pair:  argue symmetry important consideration addressing problem systematicity investigate two forms symmetry relevant symbolic processes implement approach terms convolution show used achieve effective generalisation three toy problems rule learning composition grammar learning\n",
            "output sentence:  use convolution make neural networks behave like symbolic systems \n",
            "\n",
            "{'rouge-1': {'r': 0.0784313725490196, 'p': 0.5333333333333333, 'f': 0.1367521345167653}, 'rouge-2': {'r': 0.024793388429752067, 'p': 0.2, 'f': 0.04411764509623711}, 'rouge-l': {'r': 0.049019607843137254, 'p': 0.3333333333333333, 'f': 0.08547008323471406}}\n",
            "pair:  present simple nearest neighbor nn approach synthesizes high frequency photorealistic images incomplete signal low resolution image surface normal map edges current state art deep generative models designed conditional image synthesis lack two important things unable generate large set diverse outputs due mode collapse problem interpretable making difficult control synthesized output demonstrate nn approaches potentially address limitations suffer accuracy small datasets design simple pipeline combines best worlds first stage uses convolutional neural network cnn map input overly smoothed image second stage uses pixel wise nearest neighbor method map smoothed output multiple high quality high frequency outputs controllable manner importantly pixel wise matching allows method compose novel high frequency content cutting pasting pixels different training exemplars demonstrate approach various input modalities various domains ranging human faces pets shoes handbags\n",
            "output sentence:  pixel wise nearest neighbors used generating multiple images incomplete priors low res images surface normals edges etc \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.6428571428571429, 'f': 0.1682242967909861}, 'rouge-2': {'r': 0.05128205128205128, 'p': 0.42857142857142855, 'f': 0.09160305152613488}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.42857142857142855, 'f': 0.11214953043584597}}\n",
            "pair:  travelling salesman problem tsp well known combinatorial optimization problem variety real life applications tackle tsp incorporating machine learning methodology leveraging variable neighborhood search strategy precisely search process considered markov decision process mdp opt local search used search within small neighborhood monte carlo tree search mcts method iterates simulation selection back propagation steps used sample number targeted actions within enlarged neighborhood new paradigm clearly distinguishes existing machine learning ml based paradigms solving tsp either uses end end ml model simply applies traditional techniques ml post optimization experiments based two public data sets show approach clearly dominates existing learning based tsp algorithms terms performance demonstrating high potential tsp importantly general framework without complicated hand crafted rules readily extended many combinatorial optimization problems\n",
            "output sentence:  paper combines monte carlo tree search opt local search variable neighborhood mode solve tsp effectively \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.8333333333333334, 'f': 0.15151514986225895}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.2, 'f': 0.027777776485339568}, 'rouge-l': {'r': 0.03333333333333333, 'p': 0.3333333333333333, 'f': 0.06060605895316809}}\n",
            "pair:  main goal short paper inform neural art community large ethical ramifications using models trained imagenet dataset using seed images classes bikini two piece brassiere bra bandeau discovered many images belong classes verifiably pornographic shot non consensual setting voyeuristic also entailed underage nudity akin textit ivory carving illegal poaching textit diamond jewelry art blood diamond nexuses posit similar moral conundrum play would like instigate conversation amongst neural artists community\n",
            "output sentence:  non consensual pornographic images imagenet dataset \n",
            "\n",
            "{'rouge-1': {'r': 0.06493506493506493, 'p': 0.5555555555555556, 'f': 0.11627906789345592}, 'rouge-2': {'r': 0.010526315789473684, 'p': 0.125, 'f': 0.019417474295409663}, 'rouge-l': {'r': 0.06493506493506493, 'p': 0.5555555555555556, 'f': 0.11627906789345592}}\n",
            "pair:  field deep reinforcement learning drl recently seen surge popularity maximum entropy reinforcement learning algorithms popularity stems intuitive interpretation maximum entropy objective superior sample efficiency standard benchmarks paper seek understand primary contribution entropy term performance maximum entropy algorithms mujoco benchmark demonstrate entropy term soft actor critic sac principally addresses bounded nature action spaces insight propose simple normalization scheme allows streamlined algorithm without entropy maximization match performance sac experimental results demonstrate need revisit benefits entropy regularization drl also propose simple non uniform sampling method selecting transitions replay buffer training show streamlined algorithm simple non uniform sampling scheme outperforms sac achieves state art performance challenging continuous control tasks\n",
            "output sentence:  propose new drl policy algorithm achieving state art performance \n",
            "\n",
            "{'rouge-1': {'r': 0.09411764705882353, 'p': 0.8888888888888888, 'f': 0.17021276422589407}, 'rouge-2': {'r': 0.0196078431372549, 'p': 0.25, 'f': 0.036363635014876085}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.5555555555555556, 'f': 0.10638297699185154}}\n",
            "pair:  achieving faster execution shorter compilation time foster diversity innovation neural networks however current paradigm executing neural networks either relies hand optimized libraries traditional compilation heuristics recently genetic algorithms stochastic methods methods suffer frequent costly hardware measurements rendering time consuming also suboptimal devise solution learn quickly adapt previously unseen design space code optimization accelerating search improving output performance solution dubbed chameleon leverages reinforcement learning whose solution takes fewer steps converge develops adaptive sampling algorithm focuses costly samples real hardware measurements representative points also uses domain knowledge inspired logic improve samples experimentation real hardware shows chameleon provides speed optimization time autotvm also improving inference time modern deep networks\n",
            "output sentence:  reinforcement learning adaptive sampling optimized compilation deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07058823529411765, 'p': 0.5, 'f': 0.12371133803804871}, 'rouge-2': {'r': 0.019230769230769232, 'p': 0.18181818181818182, 'f': 0.034782606965595556}, 'rouge-l': {'r': 0.07058823529411765, 'p': 0.5, 'f': 0.12371133803804871}}\n",
            "pair:  approaches continual learning aim successfully learn set related tasks arrive online manner recently several frameworks developed enable deep learning deployed learning scenario key modelling decision extent architecture shared across tasks one hand separately modelling task avoids catastrophic forgetting support transfer learning leads large models hand rigidly specifying shared component task specific part enables task transfer limits model size vulnerable catastrophic forgetting restricts form task transfer occur ideally network adaptively identify parts network share data driven way introduce approach called continual learning adaptive weights claw based probabilistic modelling variational inference experiments show claw achieves state art performance six benchmarks terms overall continual learning performance measured classification accuracy terms addressing catastrophic forgetting\n",
            "output sentence:  continual learning framework learns automatically adapt architecture based proposed variational inference algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.078125, 'p': 0.5, 'f': 0.13513513279766257}, 'rouge-2': {'r': 0.02666666666666667, 'p': 0.2222222222222222, 'f': 0.04761904570578239}, 'rouge-l': {'r': 0.046875, 'p': 0.3, 'f': 0.08108107874360854}}\n",
            "pair:  recent trend training neural networks replace data structures crafted hand aim faster execution better accuracy greater compression setting neural data structure instantiated training network many epochs inputs convergence many applications expensive initialization practical example streaming algorithms inputs ephemeral inspected small number times paper explore learning approximate set membership stream data one shot via meta learning propose novel memory architecture neural bloom filter show compressive bloom filters several existing memory augmented neural networks scenarios skewed data structured sets\n",
            "output sentence:  investigate space efficiency memory augmented neural nets learning set membership \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.8333333333333334, 'f': 0.15151514986225895}, 'rouge-2': {'r': 0.02564102564102564, 'p': 0.4, 'f': 0.04819276995209757}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.8333333333333334, 'f': 0.15151514986225895}}\n",
            "pair:  study problem defending deep neural network approaches image classification physically realizable attacks first demonstrate two scalable effective methods learning robust models adversarial training pgd attacks randomized smoothing exhibit limited effectiveness three highest profile physical attacks next propose new abstract adversarial model rectangular occlusion attacks adversary places small adversarially crafted rectangle image develop two approaches efficiently computing resulting adversarial examples finally demonstrate adversarial training using new attack yields image classification models exhibit high robustness physically realizable attacks study offering first effective generic defense attacks\n",
            "output sentence:  defending physically realizable attacks image classification \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.6, 'f': 0.1499999978125}, 'rouge-2': {'r': 0.011111111111111112, 'p': 0.1111111111111111, 'f': 0.020202018549127777}, 'rouge-l': {'r': 0.05714285714285714, 'p': 0.4, 'f': 0.09999999781250005}}\n",
            "pair:  impressive lifelong learning animal brains primarily enabled plastic changes synaptic connectivity importantly changes passive actively controlled neuromodulation control brain resulting self modifying abilities brain play important role learning adaptation major basis biological reinforcement learning show first time artificial neural networks neuromodulated plasticity trained gradient descent extending previous work differentiable hebbian plasticity propose differentiable formulation neuromodulation plasticity show neuromodulated plasticity improves performance neural networks reinforcement learning supervised learning tasks one task neuromodulated plastic lstms millions parameters outperform standard lstms benchmark language modeling task controlling number parameters conclude differentiable neuromodulation plasticity offers powerful new framework training neural networks\n",
            "output sentence:  neural networks trained modify connectivity improving online learning performance challenging tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.04225352112676056, 'p': 0.42857142857142855, 'f': 0.0769230752892834}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04225352112676056, 'p': 0.42857142857142855, 'f': 0.0769230752892834}}\n",
            "pair:  ranking central task machine learning information retrieval task especially important present user slate items appealing whole turn requires taking account interactions items since intuitively placing item slate affects decision items chosen alongside work propose sequence sequence model ranking called seq slate step model predicts next item place slate given items already chosen recurrent nature model allows complex dependencies items captured directly flexible scalable way show learn model end end weak supervision form easily obtained click data demonstrate usefulness approach experiments standard ranking benchmarks well real world recommendation system\n",
            "output sentence:  pointer network architecture ranking items learned click logs \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.8571428571428571, 'f': 0.23529411527873895}, 'rouge-2': {'r': 0.08181818181818182, 'p': 0.6428571428571429, 'f': 0.14516128831945893}, 'rouge-l': {'r': 0.11363636363636363, 'p': 0.7142857142857143, 'f': 0.19607842900422914}}\n",
            "pair:  existing black box attacks deep neural networks dnns far largely focused transferability adversarial instance generated locally trained model transfer attack learning models paper propose novel gradient estimation black box attacks adversaries query access target model class probabilities rely transferability also propose strategies decouple number queries required generate adversarial sample dimensionality input iterative variant attack achieves close adversarial success rates targeted untargeted attacks dnns carry extensive experiments thorough comparative evaluation black box attacks show proposed gradient estimation attacks outperform transferability based black box attacks tested mnist cifar datasets achieving adversarial success rates similar well known state art white box attacks also apply gradient estimation attacks successfully real world content moderation classi er hosted clarifai furthermore evaluate black box attacks state art defenses show gradient estimation attacks effective even defenses\n",
            "output sentence:  query based black box attacks deep neural networks adversarial success rates matching white box attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.10666666666666667, 'p': 0.5714285714285714, 'f': 0.179775278247696}, 'rouge-2': {'r': 0.037383177570093455, 'p': 0.26666666666666666, 'f': 0.06557376833512503}, 'rouge-l': {'r': 0.09333333333333334, 'p': 0.5, 'f': 0.15730336813533646}}\n",
            "pair:  competitive situations agents may take actions achieve goals unwittingly facilitate opponent goals consider domain three agents operate user human attacker human software agent observer software agent user attacker compete achieve different goals disparity domain knowledge user attacker possess attacker may use user unfamiliarity domain advantage goal situation observer whose goal support user may need intervene intervention needs occur online time accurate formalize online plan intervention problem propose solution uses decision tree classifier identify intervention points situations agents unwittingly facilitate opponent goal trained classifier using domain independent features extracted observer decision space evaluate criticality current state trained model used online setting ipc benchmarks identify observations warrant intervention contributions lay foundation work area deciding intervene\n",
            "output sentence:  introduce machine learning model uses domain independent features estimate criticality current state cause known state state state state \n",
            "\n",
            "{'rouge-1': {'r': 0.09859154929577464, 'p': 0.7, 'f': 0.1728395040085353}, 'rouge-2': {'r': 0.03488372093023256, 'p': 0.3333333333333333, 'f': 0.0631578930216067}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.6, 'f': 0.14814814598384393}}\n",
            "pair:  present real time method synthesizing highly complex human motions using novel training regime call auto conditioned recurrent neural network acrnn recently researchers attempted synthesize new motion using autoregressive techniques existing methods tend freeze diverge couple seconds due accumulation errors fed back network furthermore methods shown reliable relatively simple human motions walking running contrast approach synthesize arbitrary motions highly complex styles including dances martial arts addition locomotion acrnn able accomplish explicitly accommodating autoregressive noise accumulation training work first knowledge demonstrates ability generate continuous frames seconds new complex human motion different styles\n",
            "output sentence:  synthesize complex extended human motions using auto conditioned lstm network \n",
            "\n",
            "{'rouge-1': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.4444444444444444, 'f': 0.09411764516539796}}\n",
            "pair:  central goal study primate visual cortex hierarchical models object recognition understanding single units trade invariance versus sensitivity image transformations example deep networks visual cortex substantial variation layer layer unit unit degree translation invariance provide theoretical insight variation consequences encoding deep network critical insight comes fact rectification simultaneously decreases response variance correlation across responses transformed stimuli naturally inducing positive relationship invariance dynamic range invariant input units tend drive network sensitive small image transformations discuss consequences relationship ai deep nets naturally weight invariant units sensitive units strengthened training perhaps contributing generalization performance results predict signature relationship invariance dynamic range tested future neurophysiological studies\n",
            "output sentence:  rectification deep neural networks naturally leads favor invariant representation \n",
            "\n",
            "{'rouge-1': {'r': 0.10909090909090909, 'p': 0.6666666666666666, 'f': 0.1874999975830078}, 'rouge-2': {'r': 0.028985507246376812, 'p': 0.2222222222222222, 'f': 0.05128204924063125}, 'rouge-l': {'r': 0.10909090909090909, 'p': 0.6666666666666666, 'f': 0.1874999975830078}}\n",
            "pair:  propose information maximization autoencoder imae information theoretic approach simultaneously learn continuous discrete representations unsupervised setting unlike variational autoencoder framework imae starts stochastic encoder seeks map input data hybrid discrete continuous representation objective maximizing mutual information data representations decoder included approximate posterior distribution data given representations high fidelity approximation achieved leveraging informative representations show proposed objective theoretically valid provides principled framework understanding tradeoffs regarding informativeness representation factor disentanglement representations decoding quality\n",
            "output sentence:  information theoretical approach unsupervised learning unsupervised learning hybrid discrete continuous representations \n",
            "\n",
            "{'rouge-1': {'r': 0.031914893617021274, 'p': 0.3333333333333333, 'f': 0.05825242558959378}, 'rouge-2': {'r': 0.007518796992481203, 'p': 0.1111111111111111, 'f': 0.01408450585498919}, 'rouge-l': {'r': 0.02127659574468085, 'p': 0.2222222222222222, 'f': 0.03883494986143847}}\n",
            "pair:  long known single layer fully connected neural network prior parameters equivalent gaussian process gp limit infinite network width correspondence enables exact bayesian inference infinite width neural networks regression tasks means evaluating corresponding gp recently kernel functions mimic multi layer random neural networks developed outside bayesian framework previous work identified kernels used covariance functions gps allow fully bayesian prediction deep neural network work derive exact equivalence infinitely wide deep networks gps particular covariance function develop computationally efficient pipeline compute covariance function use resulting gp perform bayesian inference deep neural networks mnist cifar observe trained neural network accuracy approaches corresponding gp increasing layer width gp uncertainty strongly correlated trained network prediction error find test performance increases finite width trained networks made wider similar gp gp based predictions typically outperform finite width networks finally connect prior distribution weights variances gp formulation recent development signal propagation random neural networks\n",
            "output sentence:  show make predictions using deep networks without training deep networks \n",
            "\n",
            "{'rouge-1': {'r': 0.044444444444444446, 'p': 0.2, 'f': 0.07272726975206624}, 'rouge-2': {'r': 0.018867924528301886, 'p': 0.1111111111111111, 'f': 0.03225806203433942}, 'rouge-l': {'r': 0.044444444444444446, 'p': 0.2, 'f': 0.07272726975206624}}\n",
            "pair:  tdespite growing literature explaining neural networks consensus reached explain neural network decision evaluate explanation contributions paper twofold first investigate schemes combine explanation methods reduce model uncertainty obtain single aggregated explanation aggregation robust aligns better neural network single explanation method second propose new approach evaluating explanation methods circumvents need manual evaluation reliant alignment neural networks humans decision processes\n",
            "output sentence:  show theory practice combining multiple explanation methods dnn benefits explanation \n",
            "\n",
            "{'rouge-1': {'r': 0.078125, 'p': 0.3333333333333333, 'f': 0.12658227540458267}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.046875, 'p': 0.2, 'f': 0.07594936401217767}}\n",
            "pair:  act plan complex environments posit agents mental simulator world three characteristics build abstract state representing condition world form belief represents uncertainty world go beyond simple step step simulation exhibit temporal abstraction motivated absence model satisfying requirements propose td vae generative sequence model learns representations containing explicit beliefs states several steps future rolled directly without single step transitions td vae trained pairs temporally separated time points using analogue temporal difference learning used reinforcement learning\n",
            "output sentence:  generative model temporal data builds online belief state operates latent space jumpy predictions rollouts states \n",
            "\n",
            "{'rouge-1': {'r': 0.0975609756097561, 'p': 0.6666666666666666, 'f': 0.17021276373019467}, 'rouge-2': {'r': 0.05309734513274336, 'p': 0.5454545454545454, 'f': 0.09677419193158171}, 'rouge-l': {'r': 0.0975609756097561, 'p': 0.6666666666666666, 'f': 0.17021276373019467}}\n",
            "pair:  deep neural networks achieved state art performance various fields scaled used real world applications means reduce size neural network preserving performance knowledge transfer brought lot attention one popular method knowledge transfer knowledge distillation kd softened outputs pre trained teacher network help train student networks since kd transfer methods proposed mainly focus loss functions activations hidden layers additional modules transfer knowledge well teacher networks student networks work focus structure teacher network get effect multiple teacher networks without additional resources propose changing structure teacher network stochastic blocks skip connections teacher network becomes aggregate huge number paths training phase sub network generated dropping stochastic blocks randomly used teacher network allows training student network multiple teacher networks enhances student network resources single teacher network verify proposed structure brings improvement student networks benchmark datasets\n",
            "output sentence:  goal paper get effect multiple teacher networks exploiting stochastic blocks skip connections \n",
            "\n",
            "{'rouge-1': {'r': 0.11224489795918367, 'p': 0.6111111111111112, 'f': 0.1896551697919144}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.3157894736842105, 'f': 0.0902255614607949}, 'rouge-l': {'r': 0.10204081632653061, 'p': 0.5555555555555556, 'f': 0.1724137904815696}}\n",
            "pair:  problem building coherent non monotonous conversational agent proper discourse coverage still area open research current architectures take care semantic contextual information given query fail completely account syntactic external knowledge crucial generating responses chit chat system overcome problem propose end end multi stream deep learning architecture learns unified embeddings query response pairs leveraging contextual information memory networks syntactic information incorporating graph convolution networks gcn dependency parse stream network also utilizes transfer learning pre training bidirectional transformer extract semantic representation input sentence incorporates external knowledge neighbourhood entities knowledge base kb benchmark embeddings next sentence prediction task significantly improve upon existing techniques furthermore use amused represent query responses along context develop retrieval based conversational agent validated expert linguists comprehensive engagement humans\n",
            "output sentence:  paper provides multi stream end end approach learn unified embeddings query response pairs dialogue systems leveraging semantic semantic external information \n",
            "\n",
            "{'rouge-1': {'r': 0.08620689655172414, 'p': 0.35714285714285715, 'f': 0.13888888575617292}, 'rouge-2': {'r': 0.015873015873015872, 'p': 0.07142857142857142, 'f': 0.025974022998819703}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.2857142857142857, 'f': 0.11111110797839514}}\n",
            "pair:  recent neural network language models begun rely softmax distributions extremely large number categories context calculating softmax normalizing constant prohibitively expensive spurred growing literature efficiently computable biased estimates softmax paper present first two unbiased algorithms maximizing softmax likelihood whose work per iteration independent number classes datapoints require extra work end epoch compare unbiased methods empirical performance state art seven real world datasets comprehensively outperform competitors\n",
            "output sentence:  propose first methods exactly optimizing softmax distribution using stochastic gradient runtime independent number number datapoints \n",
            "\n",
            "{'rouge-1': {'r': 0.10909090909090909, 'p': 0.5, 'f': 0.17910447467141904}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.09090909090909091, 'f': 0.02564102321827768}, 'rouge-l': {'r': 0.07272727272727272, 'p': 0.3333333333333333, 'f': 0.11940298213410566}}\n",
            "pair:  open domain dialogue generation gained increasing attention natural language processing comparing methods requires holistic means dialogue evaluation human ratings deemed gold standard human evaluation inefficient costly automated substitute desirable paper propose holistic evaluation metrics capture quality diversity dialogues metrics consists gpt based context coherence sentences dialogue gpt based fluency phrasing gram based diversity responses augmented queries empirical validity metrics demonstrated strong correlation human judgments provide associated code datasets human ratings\n",
            "output sentence:  propose automatic metrics holistically evaluate open dialogue generation strongly correlate human evaluation \n",
            "\n",
            "{'rouge-1': {'r': 0.22448979591836735, 'p': 0.7333333333333333, 'f': 0.34374999641113285}, 'rouge-2': {'r': 0.13793103448275862, 'p': 0.5, 'f': 0.21621621282688094}, 'rouge-l': {'r': 0.22448979591836735, 'p': 0.7333333333333333, 'f': 0.34374999641113285}}\n",
            "pair:  giving provable guarantees learning neural networks core challenge machine learning theory prior work gives parameter recovery guarantees one hidden layer networks however networks used practice multiple non linear layers work show strengthen results deeper networks address problem uncovering lowest layer deep neural network assumption lowest layer uses high threshold applying activation upper network modeled well behaved polynomial input distribution gaussian\n",
            "output sentence:  provably recover lowest layer deep neural network assuming lowest layer uses high threshold activation network well behaved polynomial \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.875, 'f': 0.16666666494331067}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.42857142857142855, 'f': 0.06185566876394944}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}}\n",
            "pair:  describe kernel rnn learning kernl reduced rank temporal eligibility trace based approximation backpropagation time bptt training recurrent neural networks rnns gives competitive performance bptt long time dependence tasks approximation replaces rank gradient learning tensor describes past hidden unit activations affect current state simple reduced rank product sensitivity weight temporal eligibility trace structured approximation motivated node perturbation sensitivity weights eligibility kernel time scales learned applying perturbations rule represents another step toward biologically plausible neurally inspired ml lower complexity terms relaxed architectural requirements symmetric return weights smaller memory demand unfolding storage states time shorter feedback time\n",
            "output sentence:  biologically plausible learning rule training recurrent neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.17142857142857143, 'p': 0.6666666666666666, 'f': 0.2727272694731405}, 'rouge-2': {'r': 0.04, 'p': 0.15789473684210525, 'f': 0.06382978400860134}, 'rouge-l': {'r': 0.12857142857142856, 'p': 0.5, 'f': 0.20454545129132234}}\n",
            "pair:  experimental evidence indicates simple models outperform complex deep networks many unsupervised similarity tasks introducing concept optimal representation space provide simple theoretical resolution apparent paradox addition present straightforward procedure without retraining architectural modifications allows deep recurrent models perform equally well sometimes better compared shallow models validate analysis conduct set consistent empirical evaluations introduce several new sentence embedding models process even though work presented within context natural language processing insights readily applicable domains rely distributed representations transfer tasks\n",
            "output sentence:  introducing notion optimal representation space provide theoretical argument experimental validation unsupervised model sentences perform well supervised similarity similarity transfer unsupervised tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.6666666666666666, 'f': 0.19047618802721092}, 'rouge-2': {'r': 0.04854368932038835, 'p': 0.45454545454545453, 'f': 0.08771929650200062}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5833333333333334, 'f': 0.1666666642176871}}\n",
            "pair:  existing public face image datasets strongly biased toward caucasian faces races latino significantly underrepresented models trained datasets suffer inconsistent classification accuracy limits applicability face analytic systems non white race groups mitigate race bias problem datasets constructed novel face image dataset containing images balanced race define race groups white black indian east asian southeast asian middle eastern latino images collected yfcc flickr dataset labeled race gender age groups evaluations performed existing face attribute datasets well novel image datasets measure generalization performance find model trained dataset substantially accurate novel datasets accuracy consistent across race gender groups also compare several commercial computer vision apis report balanced accuracy across gender race age groups\n",
            "output sentence:  new face image dataset balanced race gender age used bias measurement mitigation \n",
            "\n",
            "{'rouge-1': {'r': 0.13793103448275862, 'p': 0.6666666666666666, 'f': 0.2285714257306123}, 'rouge-2': {'r': 0.09836065573770492, 'p': 0.5454545454545454, 'f': 0.16666666407793213}, 'rouge-l': {'r': 0.1206896551724138, 'p': 0.5833333333333334, 'f': 0.1999999971591837}}\n",
            "pair:  study problem fitting task specific learning rate schedules perspective hyperparameter optimization allows us explicitly search schedules achieve good generalization describe structure gradient validation error learning rates hypergradient based introduce novel online algorithm method adaptively interpolates two recently proposed techniques franceschi et al baydin et al featuring increased stability faster convergence show empirically proposed technique compares favorably baselines related methodsin terms final test accuracy\n",
            "output sentence:  marthe new method fit task specific learning rate schedules perspective hyperparameter optimization \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.5, 'f': 0.13698629900544196}, 'rouge-2': {'r': 0.028985507246376812, 'p': 0.2222222222222222, 'f': 0.05128204924063125}, 'rouge-l': {'r': 0.06349206349206349, 'p': 0.4, 'f': 0.10958903873146937}}\n",
            "pair:  large memory requirements deep neural networks strain capabilities many devices limiting deployment adoption model compression methods effectively reduce memory requirements models usually applying transformations weight pruning quantization paper present novel scheme lossy weight encoding complements conventional compression techniques encoding based bloomier filter probabilistic data structure save space cost introducing random errors leveraging ability neural networks tolerate imperfections training around errors proposed technique weightless compress dnn weights model accuracy results improvement state art\n",
            "output sentence:  propose new way compress neural networks using probabilistic data structures \n",
            "\n",
            "{'rouge-1': {'r': 0.0975609756097561, 'p': 0.42105263157894735, 'f': 0.15841583852955599}, 'rouge-2': {'r': 0.009615384615384616, 'p': 0.05263157894736842, 'f': 0.016260159989424705}, 'rouge-l': {'r': 0.08536585365853659, 'p': 0.3684210526315789, 'f': 0.1386138583315362}}\n",
            "pair:  solving tasks sparse rewards one important challenges reinforcement learning single agent setting challenge addressed introducing intrinsic rewards motivate agents explore unseen regions state spaces applying techniques naively multi agent setting results agents exploring independently without coordination among argue learning cooperative multi agent settings accelerated improved agents coordinate respect explored paper propose approach learning dynamically select different types intrinsic rewards consider individual agent explored agents agents coordinate exploration maximize extrinsic returns concretely formulate approach hierarchical policy high level controller selects among sets policies trained different types intrinsic rewards low level controllers learn action policies agents specific rewards demonstrate effectiveness proposed approach multi agent gridworld domain sparse rewards show method scales complex settings evaluating vizdoom platform\n",
            "output sentence:  propose several intrinsic reward functions encouraging coordinated exploration multi agent problems introduce approach dynamically selecting best exploration method given task task \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.35714285714285715, 'f': 0.13333333029688896}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03278688524590164, 'p': 0.14285714285714285, 'f': 0.05333333029688906}}\n",
            "pair:  basis pursuit compressed sensing optimization norm minimized subject model error constraints use deep neural network prior instead regularization using known noise statistics jointly learn prior reconstruct images without access ground truth data training use alternating minimization across unrolled iterative network jointly solve neural network weights training set image reconstructions inference fix weights pass measurements network compare reconstruction performance unsupervised supervised ground truth methods hypothesize technique could used learn reconstruction ground truth data unavailable high resolution dynamic mri\n",
            "output sentence:  present unsupervised deep learning reconstruction imaging inverse problems combines neural networks model based constraints \n",
            "\n",
            "{'rouge-1': {'r': 0.34146341463414637, 'p': 0.9333333333333333, 'f': 0.49999999607780615}, 'rouge-2': {'r': 0.28, 'p': 0.875, 'f': 0.42424242056932976}, 'rouge-l': {'r': 0.34146341463414637, 'p': 0.9333333333333333, 'f': 0.49999999607780615}}\n",
            "pair:  work presents method active anomaly detection built upon existing deep learning solutions unsupervised anomaly detection show prior needs assumed anomalies order performance guarantees unsupervised anomaly detection argue active anomaly detection practice cost unsupervised anomaly detection possibility much better results solve problem present new layer attached deep learning model designed unsupervised anomaly detection transform active method presenting results synthetic real anomaly detection datasets\n",
            "output sentence:  method active anomaly detection present new layer attached deep learning model designed unsupervised anomaly detection transform active method \n",
            "\n",
            "{'rouge-1': {'r': 0.06930693069306931, 'p': 0.7, 'f': 0.12612612448664884}, 'rouge-2': {'r': 0.025, 'p': 0.3333333333333333, 'f': 0.04651162660897786}, 'rouge-l': {'r': 0.06930693069306931, 'p': 0.7, 'f': 0.12612612448664884}}\n",
            "pair:  graph neural networks shown promising results representing analyzing diverse graph structured data social citation protein interaction networks existing approaches commonly suffer oversmoothing issue regardless whether policies edge based node based neighborhood aggregation methods also focus transductive scenarios fixed graphs leading poor generalization performance unseen graphs address issues propose new graph neural network model considers edge based neighborhood relationships node based entity features graph entities step mixture via random walk gesm gesm employs mixture various steps random walk alleviate oversmoothing problem attention use node information explicitly two mechanisms allow weighted neighborhood aggregation considers properties entities relations intensive experiments show proposed gesm achieves state art comparable performances four benchmark graph datasets comprising transductive inductive learning tasks furthermore empirically demonstrate significance considering global information source code publicly available near future\n",
            "output sentence:  simple effective graph neural network mixture random walk steps attention \n",
            "\n",
            "{'rouge-1': {'r': 0.2898550724637681, 'p': 1.0, 'f': 0.44943819876278257}, 'rouge-2': {'r': 0.2111111111111111, 'p': 0.7307692307692307, 'f': 0.3275862034185493}, 'rouge-l': {'r': 0.2898550724637681, 'p': 1.0, 'f': 0.44943819876278257}}\n",
            "pair:  propose neural language model capable unsupervised syntactic structure induction model leverages structure information form better semantic representations better language modeling standard recurrent neural networks limited structure fail efficiently use syntactic information hand tree structured recursive networks usually require additional structural supervision cost human expert annotation paper propose novel neural language model called parsing reading predict networks prpn simultaneously induce syntactic structure unannotated sentences leverage inferred structure learn better language model model gradient directly back propagated language model loss neural parsing network experiments show proposed model discover underlying syntactic structure achieve state art performance word character level language model tasks\n",
            "output sentence:  paper propose novel neural language model called parsing reading predict networks prpn simultaneously induce syntactic structure unannotated sentences leverage leverage learn structure language structure learn model learn model \n",
            "\n",
            "{'rouge-1': {'r': 0.1797752808988764, 'p': 0.8421052631578947, 'f': 0.29629629339677643}, 'rouge-2': {'r': 0.09649122807017543, 'p': 0.6111111111111112, 'f': 0.1666666643112948}, 'rouge-l': {'r': 0.16853932584269662, 'p': 0.7894736842105263, 'f': 0.27777777487825794}}\n",
            "pair:  paper propose novel approach improve given surface mapping local refinement approach receives established mapping two surfaces follows four phases inspection mapping creation sparse nset landmarks mismatching regions ii segmentation low distortion region growing process based flattening nsegmented parts iii optimization deformation segmented parts align landmarks planar parameterization domain nand iv aggregation mappings segments update surface mapping addition propose new method deform mesh order meet constraints case landmark alignment phase iii incrementally adjust cotangent weights constraints apply deformation fashion guarantees deformed mesh free flipped faces low conformal distortion new deformation approach iterative least squares conformal mapping ilscm outperforms low distortion deformation methods approach general tested improving mappings different existing surface mapping methods also tested effectiveness editing mappings variety objects\n",
            "output sentence:  propose novel approach improve given cross surface mapping local refinement new iterative method deform mesh order meet user constraints \n",
            "\n",
            "{'rouge-1': {'r': 0.23728813559322035, 'p': 0.9333333333333333, 'f': 0.37837837514609207}, 'rouge-2': {'r': 0.1267605633802817, 'p': 0.6, 'f': 0.2093023227014603}, 'rouge-l': {'r': 0.22033898305084745, 'p': 0.8666666666666667, 'f': 0.35135134811906504}}\n",
            "pair:  work first conduct mathematical analysis memory defined function maps element sequence current output three rnn cells namely simple recurrent neural network srn long short term memory lstm gated recurrent unit gru based analysis propose new design called extended long short term memory elstm extend memory length cell next present multi task rnn model robust previous erroneous predictions called dependent bidirectional recurrent neural network dbrnn sequence sequenceout siso problem finally performance dbrnn model elstm cell demonstrated experimental results\n",
            "output sentence:  recurrent neural network cell extended long short term memory multi task rnn model sequence sequence problems \n",
            "\n",
            "{'rouge-1': {'r': 0.2361111111111111, 'p': 0.9444444444444444, 'f': 0.3777777745777778}, 'rouge-2': {'r': 0.15625, 'p': 0.8333333333333334, 'f': 0.26315789207756235}, 'rouge-l': {'r': 0.2361111111111111, 'p': 0.9444444444444444, 'f': 0.3777777745777778}}\n",
            "pair:  introduce three generic point cloud processing blocks improve accuracy memory consumption multiple state art networks thus allowing design deeper accurate networks novel processing blocks facilitate efficient information flow convolution type operation block point sets blends neighborhood information memory efficient manner multi resolution point cloud processing block crosslink block efficiently shares information across low high resolution processing branches combining blocks design significantly wider deeper architectures extensively evaluate proposed architectures multiple point segmentation benchmarks shapenetpart scannet partnet report systematic improvements terms accuracy memory consumption using generic modules conjunction multiple recent architectures pointnet dgcnn spidercnn pointcnn report increase iou partnet dataset complex decreasing memory footprint\n",
            "output sentence:  introduce three generic point cloud processing blocks improve accuracy memory consumption multiple state art networks thus design design design networks \n",
            "\n",
            "{'rouge-1': {'r': 0.06976744186046512, 'p': 0.5454545454545454, 'f': 0.1237113381953449}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.2, 'f': 0.032786883740929924}, 'rouge-l': {'r': 0.05813953488372093, 'p': 0.45454545454545453, 'f': 0.10309278149431397}}\n",
            "pair:  touch interactions current mobile devices limited expressiveness augmenting devices additional degrees freedom add power interaction several augmentations proposed tested however still little known effects learning multiple sets augmented interactions mapped different applications better understand whether multiple command mappings interfere one another affect transfer retention developed prototype three pushbuttons smartphone case used provide augmented input system buttons chorded provide seven possible shortcuts transient mode switches mapped buttons three different sets actions carried study see multiple mappings affect learning performance transfer retention results show mappings quickly learned reduction performance multiple mappings transfer realistic task successful although slight reduction accuracy retention one week initially poor expert performance quickly restored work provides new information design use augmented input mobile interactions\n",
            "output sentence:  describes study investigating interference transfer retention multiple mappings set chorded buttons \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 0.8181818181818182, 'f': 0.32142856827168376}, 'rouge-2': {'r': 0.057692307692307696, 'p': 0.3, 'f': 0.09677419084287209}, 'rouge-l': {'r': 0.15555555555555556, 'p': 0.6363636363636364, 'f': 0.24999999684311228}}\n",
            "pair:  work propose novel formulation planning views probabilistic inference problem future optimal trajectories enables us use sampling methods thus tackle planning continuous domains using fixed computational budget design new algorithm sequential monte carlo planning leveraging classical methods sequential monte carlo bayesian smoothing context control inference furthermore show sequential monte carlo planning capture multimodal policies quickly learn continuous control tasks\n",
            "output sentence:  leveraging control inference sequential monte carlo methods proposed probabilistic planning algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.21428571428571427, 'p': 0.8333333333333334, 'f': 0.3409090876549587}, 'rouge-2': {'r': 0.10465116279069768, 'p': 0.5, 'f': 0.17307692021449708}, 'rouge-l': {'r': 0.21428571428571427, 'p': 0.8333333333333334, 'f': 0.3409090876549587}}\n",
            "pair:  propose procedures evaluating strengthening contextual embedding alignment show useful analyzing improving multilingual bert particular proposed alignment procedure bert exhibits significantly improved zero shot performance xnli compared base model remarkably matching pseudo fully supervised translate train models bulgarian greek measure degree alignment introduce contextual version word retrieval show correlates well downstream zero shot transfer using word retrieval task also analyze bert find exhibits systematic deficiencies worse alignment open class parts speech word pairs written different scripts corrected alignment procedure results support contextual alignment useful concept understanding large multilingual pre trained models\n",
            "output sentence:  propose procedures evaluating strengthening contextual embedding alignment show improve multilingual bert zero shot xnli transfer provide useful useful insights \n",
            "\n",
            "{'rouge-1': {'r': 0.011494252873563218, 'p': 0.09090909090909091, 'f': 0.020408161272386705}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.011494252873563218, 'p': 0.09090909090909091, 'f': 0.020408161272386705}}\n",
            "pair:  practice often found large parameterized neural networks generalize better smaller counterparts observation appears conflict classical notions function complexity typically favor smaller models work investigate tension complexity generalization extensive empirical exploration two natural metrics complexity related sensitivity input perturbations experiments survey thousands models different architectures optimizers hyper parameters well four different image classification datasets find trained neural networks robust input perturbations vicinity training data manifold measured input output jacobian network correlates well generalization establish factors associated poor generalization full batch training using random labels correspond higher sensitivity factors associated good generalization data augmentation relu non linearities give rise robust functions finally demonstrate input output jacobian norm predictive generalization level individual test points\n",
            "output sentence:  perform massive experimental studies characterizing relationships jacobian norms linear regions generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.07142857142857142, 'p': 0.5833333333333334, 'f': 0.12727272532892564}, 'rouge-2': {'r': 0.00847457627118644, 'p': 0.09090909090909091, 'f': 0.01550387440899}, 'rouge-l': {'r': 0.05102040816326531, 'p': 0.4166666666666667, 'f': 0.0909090889652893}}\n",
            "pair:  hierarchical bayesian methods potential unify many related tasks shot classification conditional unconditional generation framing inference within single generative model show existing approaches learning models fail expressive generative networks pixelcnns describing global distribution little reliance latent variables address develop modification variational autoencoder encoded observations decoded new elements class result call variational homoencoder vhe may understood training hierarchical latent variable model better utilises latent variables cases using framework enables us train hierarchical pixelcnn omniglot dataset outperforming existing models test set likelihood single model achieve strong one shot generation near human level classification competitive state art discriminative classifiers vhe objective extends naturally richer dataset structures factorial hierarchical categories illustrate training models separate character content simple variations drawing style generalise style alphabet new characters\n",
            "output sentence:  technique learning deep generative models shared latent variables applied omniglot pixelcnn decoder \n",
            "\n",
            "{'rouge-1': {'r': 0.16393442622950818, 'p': 0.7142857142857143, 'f': 0.26666666363022223}, 'rouge-2': {'r': 0.09090909090909091, 'p': 0.5384615384615384, 'f': 0.15555555308395064}, 'rouge-l': {'r': 0.11475409836065574, 'p': 0.5, 'f': 0.18666666363022225}}\n",
            "pair:  resnet batch normalization bn achieved high performance even labeled data available however reasons high performance unclear clear reasons analyzed effect skip connection resnet bn data separation ability important ability classification problem results show multilayer perceptron randomly initialized weights angle two input vectors converges zero exponential order depth skip connection makes exponential decrease sub exponential decrease bn relaxes sub exponential decrease reciprocal decrease moreover analysis shows preservation angle initialization encourages trained neural networks separate points different classes imply skip connection bn improve data separation ability achieve high performance even labeled data available\n",
            "output sentence:  skip connection resnet batch normalization improve data separation ability help train deep neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.1951219512195122, 'p': 0.7272727272727273, 'f': 0.3076923043565089}, 'rouge-2': {'r': 0.1276595744680851, 'p': 0.6, 'f': 0.21052631289627577}, 'rouge-l': {'r': 0.1951219512195122, 'p': 0.7272727272727273, 'f': 0.3076923043565089}}\n",
            "pair:  pattern databases foundation strongest admissible heuristics optimal classical planning experiments showed informative way combining information multiple pattern databases use saturated cost partitioning previous work selected patterns computed saturated cost partitionings resulting pattern database heuristics two separate steps introduce new method uses saturated cost partitioning select patterns show outperforms existing pattern selection algorithms\n",
            "output sentence:  using saturated cost partitioning select patterns preferable existing pattern selection algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.12048192771084337, 'p': 0.5555555555555556, 'f': 0.19801979905107345}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.2222222222222222, 'f': 0.06557376797635056}, 'rouge-l': {'r': 0.08433734939759036, 'p': 0.3888888888888889, 'f': 0.13861385845701407}}\n",
            "pair:  intrinsically motivated goal exploration algorithms enable machines discover repertoires policies produce diversity effects complex environments exploration algorithms shown allow real world robots acquire skills tool use high dimensional continuous state action spaces however far assumed self generated goals sampled specifically engineered feature space limiting autonomy work propose approach using deep representation learning algorithms learn adequate goal space developmental stage approach first perceptual learning stage deep learning algorithms use passive raw sensor observations world changes learn corresponding latent space goal exploration happens second stage sampling goals latent space present experiments simulated robot arm interacting object show exploration algorithms using learned representations closely match even sometimes improve performance obtained using engineered representations\n",
            "output sentence:  propose novel intrinsically motivated goal exploration architecture unsupervised learning goal space representations evaluate various implementations enable discovery diversity policies \n",
            "\n",
            "{'rouge-1': {'r': 0.03614457831325301, 'p': 0.3333333333333333, 'f': 0.065217389539225}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03614457831325301, 'p': 0.3333333333333333, 'f': 0.065217389539225}}\n",
            "pair:  binarized neural networks bnns shown effective improving network efficiency inference phase network trained however bnns binarize model parameters activations propagations therefore bnns offer significant efficiency improvements training since gradients still propagated used high precision show inherent difficulty training bnns using binarized backpropagation bbp also binarize gradients avoid significant degradation test accuracy simply increase number filter maps convolution layer using bbp dedicated hardware potentially significantly improve execution efficiency emph reduce dynamic memory footprint memory bandwidth computational energy speed training process appropriate hardware support even increase network size moreover method ideal distributed learning reduces communication costs significantly using method demonstrate minimal loss classification accuracy several datasets topologies\n",
            "output sentence:  binarized back propagation need completely binarized training inflate size network \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.8333333333333334, 'f': 0.10989010865837459}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03529411764705882, 'p': 0.5, 'f': 0.06593406470233065}}\n",
            "pair:  building agents interact web would allow significant improvements knowledge understanding representation learning however web navigation tasks difficult current deep reinforcement learning rl models due large discrete action space varying number actions states work introduce dom net novel architecture rl based web navigation address problems parametrizes functions separate networks different action categories clicking dom element typing string input model utilizes graph neural network represent tree structured html standard web page demonstrate capabilities model miniwob environment match outperform existing work without use expert demonstrations furthermore show improvements sample efficiency training multi task setting allowing model transfer learned behaviours across tasks\n",
            "output sentence:  graph based deep network web navigation \n",
            "\n",
            "{'rouge-1': {'r': 0.14666666666666667, 'p': 0.8461538461538461, 'f': 0.24999999748192153}, 'rouge-2': {'r': 0.06542056074766354, 'p': 0.5384615384615384, 'f': 0.11666666473472223}, 'rouge-l': {'r': 0.14666666666666667, 'p': 0.8461538461538461, 'f': 0.24999999748192153}}\n",
            "pair:  injecting adversarial examples training known adversarial training improve robustness one step attacks unknown iterative attacks address challenge first show iteratively generated adversarial images easily transfer networks trained strategy inspired observation propose cascade adversarial training transfers knowledge end results adversarial training train network scratch injecting iteratively generated adversarial images crafted already defended networks addition one step adversarial images network trained also propose utilize embedding space classification low level pixel level similarity learning ignore unknown pixel level perturbation training inject adversarial images without replacing corresponding clean images penalize distance two embeddings clean adversarial experimental results show cascade adversarial training together proposed low level similarity learning efficiently enhances robustness iterative attacks expense decreased robustness one step attacks show combining two techniques also improve robustness worst case black box attack scenario\n",
            "output sentence:  cascade adversarial training low level similarity learning improve robustness white box black box attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.6666666666666666, 'f': 0.23809523516156467}, 'rouge-2': {'r': 0.023529411764705882, 'p': 0.14285714285714285, 'f': 0.04040403797571691}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.5333333333333333, 'f': 0.19047618754251705}}\n",
            "pair:  structured tabular data commonly used form data industry according kaggle ml ds survey gradient boosting trees support vector machine random forest logistic regression typically used classification tasks tabular data recent work super characters method using two dimensional word embeddings achieved state art results text classification tasks showcasing promise new approach paper propose supertml method borrows idea super characters method two dimensional embeddings address problem classification tabular data input tabular data features first projected two dimensional embeddings like image image fed fine tuned imagenet cnn models classification experimental results shown proposed supertml method achieved state art results large small datasets\n",
            "output sentence:  deep learning structured tabular data machine two using word cnn fine model imagenet pre trained cnn \n",
            "\n",
            "{'rouge-1': {'r': 0.12941176470588237, 'p': 0.8461538461538461, 'f': 0.22448979361724286}, 'rouge-2': {'r': 0.0660377358490566, 'p': 0.5833333333333334, 'f': 0.11864406596954902}, 'rouge-l': {'r': 0.08235294117647059, 'p': 0.5384615384615384, 'f': 0.14285714055601834}}\n",
            "pair:  despite advances deep learning artificial neural networks learn way humans today neural networks learn multiple tasks trained jointly cannot maintain performance learnt tasks tasks presented one time phenomenon called catastrophic forgetting fundamental challenge overcome neural networks learn continually incoming data work derive inspiration human memory develop architecture capable learning continuously sequentially incoming tasks averting catastrophic forgetting specifically model consists dual memory architecture emulate complementary learning systems hippocampus neocortex human brain maintains consolidated long term memory via generative replay past experiences substantiate claim replay generative ii show benefits generative replay dual memory via experiments iii demonstrate improved performance retention even small models low capacity architecture displays many important characteristics human memory provides insights connection sleep learning humans\n",
            "output sentence:  dual memory architecture inspired human brain learn sequentially incoming tasks averting catastrophic forgetting \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.26666666666666666, 'f': 0.11940298159946547}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.06666666666666667, 'f': 0.0243902409131473}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.26666666666666666, 'f': 0.11940298159946547}}\n",
            "pair:  amortized inference led efficient approximate inference large datasets quality posterior inference largely determined two factors ability variational distribution model true posterior capacity recognition network generalize inference datapoints analyze approximate inference variational autoencoders terms factors find suboptimal inference often due amortizing inference rather limited complexity approximating distribution show due partly generator learning accommodate choice approximation furthermore show parameters used increase expressiveness approximation play role generalizing inference rather simply improving complexity approximation\n",
            "output sentence:  decompose gap marginal log likelihood evidence lower bound study effect approximate posterior true posterior distribution vaes vaes \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.7058823529411765, 'f': 0.2790697642698756}, 'rouge-2': {'r': 0.0375, 'p': 0.1875, 'f': 0.06249999722222234}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.35294117647058826, 'f': 0.13953488054894544}}\n",
            "pair:  deep learning computer vision depends mainly source supervision photo realistic simulators generate large scale automatically labeled synthetic data introduce domain gap negatively impacting performance propose new unsupervised domain adaptation algorithm called spigan relying simulator privileged information pi generative adversarial networks gan use internal data simulator pi training target task network experimentally evaluate approach semantic segmentation train networks real world cityscapes vistas datasets using unlabeled real world images synthetic labeled data buffer depth pi synthia dataset method improves adaptation state art unsupervised domain adaptation techniques\n",
            "output sentence:  unsupervised sim real domain adaptation method semantic segmentation using privileged information simulator gan based image translation translation \n",
            "\n",
            "{'rouge-1': {'r': 0.18032786885245902, 'p': 0.6111111111111112, 'f': 0.27848100913956103}, 'rouge-2': {'r': 0.07042253521126761, 'p': 0.29411764705882354, 'f': 0.11363636051911165}, 'rouge-l': {'r': 0.13114754098360656, 'p': 0.4444444444444444, 'f': 0.20253164205095342}}\n",
            "pair:  reinforcement learning multi agent scenarios important real world applications presents challenges beyond seen single agent settings present actor critic algorithm trains decentralized policies multi agent settings using centrally computed critics share attention mechanism selects relevant information agent every timestep attention mechanism enables effective scalable learning complex multi agent environments compared recent approaches approach applicable cooperative settings shared rewards also individualized reward settings including adversarial settings makes assumptions action spaces agents flexible enough applied multi agent learning problems\n",
            "output sentence:  propose approach learn decentralized policies multi agent settings using attention based critics demonstrate promising results environments complex interactions \n",
            "\n",
            "{'rouge-1': {'r': 0.037037037037037035, 'p': 1.0, 'f': 0.07142857073979593}, 'rouge-2': {'r': 0.013513513513513514, 'p': 1.0, 'f': 0.02666666640355556}, 'rouge-l': {'r': 0.037037037037037035, 'p': 1.0, 'f': 0.07142857073979593}}\n",
            "pair:  paper design harmonic acoustic model pitch detection model arranges conventional convolution sparse convolution way global harmonic patterns captured sparse convolution composed enough number local patterns captured layers conventional convolution trained maps dataset harmonic model outperforms existing pitch detection systems trained dataset impressively trained maps simple data augmentation harmonic model lstm layer top surpasses date complex pitch detection system trained maestro dataset complicated data augmentation applied whose training split order magnitude larger training split maps harmonic model demonstrated potential used advanced automatic music transcription amt systems\n",
            "output sentence:  harmonic acoustic model \n",
            "\n",
            "{'rouge-1': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.25, 'f': 0.04123711188861734}, 'rouge-l': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}}\n",
            "pair:  convolutional neural networks cnns achieved state art performance recognizing representing audio images videos volumes domains input characterized regular graph structure however generalizing cnns irregular domains like meshes challenging additionally training data meshes often limited work generalize convolutional autoencoders mesh surfaces perform spectral decomposition meshes apply convolutions directly frequency space addition use max pooling introduce upsampling within network represent meshes low dimensional space construct complex dataset high resolution meshes extreme facial expressions encode using convolutional mesh autoencoder despite limited training data method outperforms state art pca models faces lower error using fewer parameters\n",
            "output sentence:  convolutional autoencoders generalized mesh surfaces encoding reconstructing extreme facial \n",
            "\n",
            "{'rouge-1': {'r': 0.2127659574468085, 'p': 0.5555555555555556, 'f': 0.307692303687574}, 'rouge-2': {'r': 0.04918032786885246, 'p': 0.17647058823529413, 'f': 0.07692307351413559}, 'rouge-l': {'r': 0.1702127659574468, 'p': 0.4444444444444444, 'f': 0.24615384214911246}}\n",
            "pair:  reservoir computing powerful tool explain brain learns temporal sequences movements existing learning schemes either biologically implausible inefficient explain animal performance show network learn complicated sequences reward modulated hebbian learning rule network reservoir neurons combined second network serves dynamic working memory provides spatio temporal backbone signal reservoir combination working memory reward modulated hebbian learning readout neurons performs well force learning advantage biologically plausible interpretation learning rule learning paradigm\n",
            "output sentence:  show working memory input reservoir network makes local reward modulated hebbian rule perform well recursive least squares aka force \n",
            "\n",
            "{'rouge-1': {'r': 0.1282051282051282, 'p': 0.625, 'f': 0.2127659546220009}, 'rouge-2': {'r': 0.04878048780487805, 'p': 0.2857142857142857, 'f': 0.08333333084201397}, 'rouge-l': {'r': 0.1282051282051282, 'p': 0.625, 'f': 0.2127659546220009}}\n",
            "pair:  introduce neural architecture perform amortized approximate bayesian inference latent random permutations two sets objects method involves approximating permanents matrices pairwise probabilities using recent ideas functions defined sets sampled permutation comes probability estimate quantity unavailable mcmc approaches illustrate method sets points mnist images\n",
            "output sentence:  novel neural architecture efficient amortized inference latent permutations \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.625, 'f': 0.14492753418189458}, 'rouge-2': {'r': 0.02702702702702703, 'p': 0.2857142857142857, 'f': 0.049382714470355187}, 'rouge-l': {'r': 0.06557377049180328, 'p': 0.5, 'f': 0.11594202693551779}}\n",
            "pair:  deep image prior dip ulyanov et al fascinating recent approach recovering images appear natural yet fully understood work aims shedding light approach investigating properties early outputs dip first show early iterations demonstrate invariance adversarial perturbations classifying progressive dip outputs using novel saliency map approach next explore using dip defence adversaries showing good potential finally examine adversarial invariancy early dip outputs hypothesize outputs may remove non robust image features comparing classification confidence values show evidence confirming hypothesis\n",
            "output sentence:  investigate properties recently introduced deep image prior ulyanov et al \n",
            "\n",
            "{'rouge-1': {'r': 0.2328767123287671, 'p': 0.7083333333333334, 'f': 0.3505154601934319}, 'rouge-2': {'r': 0.10416666666666667, 'p': 0.38461538461538464, 'f': 0.16393442287557117}, 'rouge-l': {'r': 0.1643835616438356, 'p': 0.5, 'f': 0.24742267668827722}}\n",
            "pair:  introduce search amortized value estimates save approach combining model free learning model based monte carlo tree search mcts save learned prior state action values used guide mcts estimates improved set state action values new estimates used combination real experience update prior effectively amortizes value computation performed mcts resulting cooperative relationship model free learning model based search save implemented top learning agent access model demonstrate incorporating agents perform challenging physical reasoning tasks atari save consistently achieves higher rewards fewer training steps contrast typical model based search approaches yields strong performance small search budgets combining real experience information computed search save demonstrates possible improve performance model free learning computational cost planning\n",
            "output sentence:  propose model based method called search amortized value estimates save leverages real planned experience combining learning monte carlo tree search achieving strong performance search small search budgets \n",
            "\n",
            "{'rouge-1': {'r': 0.06329113924050633, 'p': 0.35714285714285715, 'f': 0.10752687916290908}, 'rouge-2': {'r': 0.02197802197802198, 'p': 0.14285714285714285, 'f': 0.03809523578412713}, 'rouge-l': {'r': 0.05063291139240506, 'p': 0.2857142857142857, 'f': 0.08602150281882306}}\n",
            "pair:  wasserstein distance received lot attention recently community machine learning especially principled way comparing distributions found numerous applications several hard problems domain adaptation dimensionality reduction generative models however use still limited heavy computational cost goal alleviate problem providing approximation mechanism allows break inherent complexity relies search embedding euclidean distance mimics wasserstein distance show embedding found siamese architecture associated decoder network allows move embedding space back original input space embedding found computing optimization problems wasserstein space barycenters principal directions even archetypes conducted extremely fast numerical experiments supporting idea conducted image datasets show wide potential benefits method\n",
            "output sentence:  show possible fastly approximate wasserstein distances computation finding appropriate embedding euclidean distance emulates wasserstein distance \n",
            "\n",
            "{'rouge-1': {'r': 0.12745098039215685, 'p': 0.6842105263157895, 'f': 0.2148760304104911}, 'rouge-2': {'r': 0.07692307692307693, 'p': 0.5, 'f': 0.13333333102222228}, 'rouge-l': {'r': 0.12745098039215685, 'p': 0.6842105263157895, 'f': 0.2148760304104911}}\n",
            "pair:  present generic dynamic architecture employs problem specific differentiable forking mechanism leverage discrete logical information problem data structure adapt apply model clevr visual question answering giving rise ddrprog architecture compared previous approaches model achieves higher accuracy half many epochs five times fewer learnable parameters model directly models underlying question logic using recurrent controller jointly predicts executes functional neural modules explicitly forks subprocesses handle logical branching film competitive models static architectures less supervision argue inclusion program labels enables learning higher level logical operations architecture achieves particularly high performance questions requiring counting integer comparison demonstrate generality approach though ddrstack application method reverse polish notation expression evaluation inclusion stack assumption allows approach generalize long expressions significantly outperforming lstm ten times many learnable parameters\n",
            "output sentence:  generic dynamic architecture employs problem specific differentiable forking mechanism encode hard data structure assumptions applied clevr vqa expression evaluation \n",
            "\n",
            "{'rouge-1': {'r': 0.10526315789473684, 'p': 0.7692307692307693, 'f': 0.1851851830675583}, 'rouge-2': {'r': 0.05084745762711865, 'p': 0.46153846153846156, 'f': 0.09160305164733994}, 'rouge-l': {'r': 0.08421052631578947, 'p': 0.6153846153846154, 'f': 0.1481481460305213}}\n",
            "pair:  high intra class diversity inter class similarity characteristic remote sensing scene image data sets currently posing significant difficulty deep learning algorithms classification tasks improve accuracy post classification methods proposed smoothing results model predictions however approaches require additional neural network perform smoothing operation adds overhead task propose approach involves learning deep features directly neighboring scene images without requiring use cleanup model approach utilizes siamese network improve discriminative power convolutional neural networks pair neighboring scene images exploits semantic coherence pair enrich feature vector image want predict label empirical results show approach provides viable alternative existing methods example model improved prediction accuracy percentage point dropped mean squared error value baseline disease density estimation task performance gains comparable results existing post classification methods moreover without implementation overheads\n",
            "output sentence:  approach improving prediction accuracy learning deep features neighboring scene images satellite scene image analysis \n",
            "\n",
            "{'rouge-1': {'r': 0.10204081632653061, 'p': 0.38461538461538464, 'f': 0.16129031926638923}, 'rouge-2': {'r': 0.03389830508474576, 'p': 0.15384615384615385, 'f': 0.05555555259645077}, 'rouge-l': {'r': 0.061224489795918366, 'p': 0.23076923076923078, 'f': 0.09677419023413122}}\n",
            "pair:  propose novel subgraph image representation classification network fragments target parent networks graph image representation based image embeddings adjacency matrices use image representation two modes first input machine learning algorithm second input pure transfer learner conclusions multiple datasets deep learning using structured image features performs best compared graph kernel classical features based methods pure transfer learning works effectively minimum interference user robust small data\n",
            "output sentence:  convert subgraphs structured images classify using deep learning transfer learning caffe achieve stunning results \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.875, 'f': 0.16470588064775088}, 'rouge-2': {'r': 0.030612244897959183, 'p': 0.42857142857142855, 'f': 0.057142855898412726}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.875, 'f': 0.16470588064775088}}\n",
            "pair:  learning learn powerful paradigm enabling models learn data effectively efficiently popular approach meta learning train recurrent model read training dataset input output parameters learned model output predictions new test inputs alternatively recent approach meta learning aims acquire deep representations effectively fine tuned via standard gradient descent new tasks paper consider meta learning problem perspective universality formalizing notion learning algorithm approximation comparing expressive power aforementioned recurrent models recent approaches embed gradient descent meta learner particular seek answer following question deep representation combined standard gradient descent sufficient capacity approximate learning algorithm find indeed true find experiments gradient based meta learning consistently leads learning strategies generalize widely compared represented recurrent models\n",
            "output sentence:  deep representations combined gradient descent approximate learning algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.11392405063291139, 'p': 0.75, 'f': 0.19780219551261927}, 'rouge-2': {'r': 0.042105263157894736, 'p': 0.3076923076923077, 'f': 0.07407407195644725}, 'rouge-l': {'r': 0.0759493670886076, 'p': 0.5, 'f': 0.13186812957855334}}\n",
            "pair:  deep reinforcement learning demonstrated increasing capabilities continuous control problems including agents move skill agility environment open problem setting developing good strategies integrating merging policies multiple skills individual skill specialist specific skill associated state distribution extend policy distillation methods continuous action setting leverage technique combine expert policies evaluated domain simulated bipedal locomotion across different classes terrain also introduce input injection method augmenting existing policy network exploit new input features lastly method uses transfer learning assist efficient acquisition new skills combination methods allows policy incrementally augmented new skills compare progressive learning integration via distillation plaid method three alternative baselines\n",
            "output sentence:  continual learning method uses distillation combine expert policies transfer learning accelerate learning new skills \n",
            "\n",
            "{'rouge-1': {'r': 0.12345679012345678, 'p': 0.625, 'f': 0.20618556425550005}, 'rouge-2': {'r': 0.04081632653061224, 'p': 0.26666666666666666, 'f': 0.07079645787453998}, 'rouge-l': {'r': 0.07407407407407407, 'p': 0.375, 'f': 0.1237113374513764}}\n",
            "pair:  multivariate time series missing values common areas healthcare finance grown number complexity years raises question whether deep learning methodologies outperform classical data imputation methods domain however naive applications deep learning fall short giving reliable confidence estimates lack interpretability propose new deep sequential latent variable model dimensionality reduction data imputation modeling assumption simple interpretable high dimensional time series lower dimensional representation evolves smoothly time according gaussian process non linear dimensionality reduction presence missing data achieved using vae approach novel structured variational approximation demonstrate approach outperforms several classical deep learning based data imputation methods high dimensional data domains computer vision healthcare additionally improving smoothness imputations providing interpretable uncertainty estimates\n",
            "output sentence:  perform amortized variational inference latent gaussian process model achieve superior imputation performance multivariate time series missing data \n",
            "\n",
            "{'rouge-1': {'r': 0.06779661016949153, 'p': 0.5, 'f': 0.11940298297170863}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.375, 'f': 0.08955223670305197}}\n",
            "pair:  contextualized word representations elmo bert shown perform well various semantic structural syntactic task work tackle task unsupervised disentanglement semantics structure neural language representations aim learn transformation contextualized vectors discards lexical semantics keeps structural information end automatically generate groups sentences structurally similar semantically different use metric learning approach learn transformation emphasizes structural component encoded vectors demonstrate transformation clusters vectors space structural properties rather lexical semantics finally demonstrate utility distilled representations showing outperform original contextualized representations shot parsing setting\n",
            "output sentence:  distill language models representations syntax unsupervised metric learning \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.46153846153846156, 'f': 0.19672130812147276}, 'rouge-2': {'r': 0.0392156862745098, 'p': 0.16666666666666666, 'f': 0.06349206040816341}, 'rouge-l': {'r': 0.0625, 'p': 0.23076923076923078, 'f': 0.09836065238376793}}\n",
            "pair:  propose effective multitask learning setup reducing distant supervision noise leveraging sentence level supervision show sentence level supervision used improve encoding individual sentences learn input sentences likely express relationship pair entities also introduce novel neural architecture collecting signals multiple input sentences combines benefits attention maxpooling proposed method increases auc outperforms recently published results fb nyt dataset\n",
            "output sentence:  new form attention works well distant supervision setting multitask learning approach add level level \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 0.875, 'f': 0.20895522177767878}, 'rouge-2': {'r': 0.05714285714285714, 'p': 0.5714285714285714, 'f': 0.10389610224321134}, 'rouge-l': {'r': 0.0847457627118644, 'p': 0.625, 'f': 0.14925372924036534}}\n",
            "pair:  introduce cgnn framework learn functional causal models generative neural networks networks trained using backpropagation minimize maximum mean discrepancy observed data unlike previous approaches cgnn leverages conditional independences distributional asymmetries seamlessly discover bivariate multivariate causal structures without hidden variables cgnn estimate causal structure full differentiable generative model data throughout extensive variety experiments illustrate competitive esults cgnn state art alternatives observational causal discovery simulated real data tasks cause effect inference structure identification multivariate causal discovery\n",
            "output sentence:  discover structure functional causal models generative neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07017543859649122, 'p': 0.4, 'f': 0.11940298253508581}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.1111111111111111, 'f': 0.02439024194824525}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.3, 'f': 0.08955223626642911}}\n",
            "pair:  recent work studied emergence language among deep reinforcement learning agents must collaborate solve task particular interest factors cause language compositional express meaning combining words meaning evolutionary linguists found addition structural priors like already studied deep learning dynamics transmitting language generation generation contribute significantly emergence compositionality paper introduce cultural evolutionary dynamics language emergence periodically replacing agents population create knowledge gap implicitly inducing cultural transmission language show implicit cultural transmission encourages resulting languages exhibit better compositional generalization\n",
            "output sentence:  use cultural transmission encourage compositionality languages emerge interactions neural agents \n",
            "\n",
            "{'rouge-1': {'r': 0.0975609756097561, 'p': 0.6666666666666666, 'f': 0.17021276373019467}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.4, 'f': 0.07547169640441441}, 'rouge-l': {'r': 0.0975609756097561, 'p': 0.6666666666666666, 'f': 0.17021276373019467}}\n",
            "pair:  consider problem information compression high dimensional data many studies consider problem compression non invertible trans formations emphasize importance invertible compression introduce new class likelihood based auto encoders pseudo bijective architecture call pseudo invertible encoders provide theoretical explanation principles evaluate gaussian pseudo invertible encoder mnist model outperform wae vae sharpness generated images\n",
            "output sentence:  new class autoencoders pseudo invertible architecture \n",
            "\n",
            "{'rouge-1': {'r': 0.0673076923076923, 'p': 0.5833333333333334, 'f': 0.1206896533174792}, 'rouge-2': {'r': 0.007575757575757576, 'p': 0.07692307692307693, 'f': 0.013793101815933607}, 'rouge-l': {'r': 0.04807692307692308, 'p': 0.4166666666666667, 'f': 0.08620689469678958}}\n",
            "pair:  recent advances cross lingual word embeddings primarily relied mapping based methods project pretrained word embeddings different languages shared space linear transformation however approaches assume word embedding spaces isomorphic different languages shown hold practice gaard et al fundamentally limits performance motivates investigating joint learning methods overcome impediment simultaneously learning embeddings across languages via cross lingual term training objective given abundance parallel data available tiedemann propose bilingual extension cbow method leverages sentence aligned corpora obtain robust cross lingual word sentence representations approach significantly improves cross lingual sentence retrieval performance approaches well convincingly outscores mapping methods maintaining parity jointly trained methods word translation also achieves parity deep rnn method zero shot cross lingual document classification task requiring far fewer computational resources training inference additional advantage bilingual method also improves quality monolingual word vectors despite training much smaller datasets make code models publicly available\n",
            "output sentence:  joint method learning cross lingual embeddings state art performance cross lingual tasks mono lingual quality \n",
            "\n",
            "{'rouge-1': {'r': 0.05128205128205128, 'p': 0.5714285714285714, 'f': 0.09411764554740484}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.038461538461538464, 'p': 0.42857142857142855, 'f': 0.07058823378269899}}\n",
            "pair:  automatic classification objects one important tasks engineering data mining applications although using complex advanced classifiers help improve accuracy classification systems done analyzing data sets features particular problem feature combination one improve quality features paper structure similar feed forward neural network ffnn used generate optimized linear non linear combination features classification genetic algorithm ga applied update weights biases since nature data sets features impact effectiveness combination classification system linear non linear activation functions transfer function used achieve reliable system experiments several uci data sets using minimum distance classifier simple classifier indicate proposed linear non linear intelligent ffnn based feature combination present reliable promising results using feature combination method need use powerful complex classifier anymore\n",
            "output sentence:  method enriching combining features improve classification accuracy \n",
            "\n",
            "{'rouge-1': {'r': 0.07476635514018691, 'p': 0.8888888888888888, 'f': 0.13793103305142687}, 'rouge-2': {'r': 0.030534351145038167, 'p': 0.4444444444444444, 'f': 0.05714285593979594}, 'rouge-l': {'r': 0.056074766355140186, 'p': 0.6666666666666666, 'f': 0.10344827443073723}}\n",
            "pair:  stochastic gradient descent sgd methods using randomly selected batches widely used train neural network nn models performing design exploration find best nn particular task often requires extensive training different models large dataset computationally expensive straightforward method accelerate computation distribute batch sgd multiple processors however large batch training often times leads degradation accuracy poor generalization even poor robustness adversarial attacks existing solutions large batch training either work require massive hyper parameter tuning address issue propose novel large batch training method combines recent results adversarial training regularize sharp minima second order optimization use curvature information change batch size adaptively training extensively evaluate method cifar svhn tinyimagenet imagenet datasets using multiple nns including residual networks well compressed networks squeezenext new approach exceeds performance existing solutions terms accuracy number sgd iterations times respectively emphasize achieved without additional hyper parameter tuning tailor method experiments\n",
            "output sentence:  large batch size training using adversarial training second order information \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.7222222222222222, 'f': 0.27083333028645834}, 'rouge-2': {'r': 0.06818181818181818, 'p': 0.3157894736842105, 'f': 0.11214952978950134}, 'rouge-l': {'r': 0.15384615384615385, 'p': 0.6666666666666666, 'f': 0.24999999695312503}}\n",
            "pair:  paper introduce symplectic ode net symoden deep learning framework infer dynamics physical system observed state trajectories achieve better generalization fewer training samples symoden incorporates appropriate inductive bias designing associated computation graph physics informed manner particular enforce hamiltonian dynamics control learn underlying dynamics transparent way leveraged draw insight relevant physical aspects system mass potential energy addition propose parametrization enforce hamiltonian formalism even generalized coordinate data embedded high dimensional space access velocity data instead generalized momentum framework offering interpretable physically consistent models physical systems opens new possibilities synthesizing model based control strategies\n",
            "output sentence:  work enforces hamiltonian dynamics control learn system models embedded position velocity data exploits physically consistent dynamics model model based control \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}}\n",
            "pair:  recently deep neural networks shown capacity memorize training data even noisy labels hurts generalization performance mitigate issue propose simple effective method robust noisy labels even severe noise objective involves variance regularization term implicitly penalizes jacobian norm neural network whole training set including noisy labeled data encourages generalization prevents overfitting corrupted labels experiments noisy benchmarks demonstrate approach achieves state art performance high tolerance severe noise\n",
            "output sentence:  paper proposed simple yet effective baseline learning noisy labels \n",
            "\n",
            "{'rouge-1': {'r': 0.13846153846153847, 'p': 0.6923076923076923, 'f': 0.23076922799145302}, 'rouge-2': {'r': 0.0125, 'p': 0.07142857142857142, 'f': 0.02127659320959741}, 'rouge-l': {'r': 0.09230769230769231, 'p': 0.46153846153846156, 'f': 0.15384615106837612}}\n",
            "pair:  deep convolutional network architectures often assumed guarantee generalization small image translations deformations paper show modern cnns vgg resnet inceptionresnetv drastically change output image translated image plane pixels failure generalization also happens realistic small image transformations furthermore see failures generalize frequently modern networks show failures related fact architecture modern cnns ignores classical sampling theorem generalization guaranteed also show biases statistics commonly used image datasets makes unlikely cnns learn invariant transformations taken together results suggest performance cnns object recognition falls far short generalization capabilities humans\n",
            "output sentence:  modern deep cnns invariant translations scalings realistic image transformations lack invariance related operation operation operation image image \n",
            "\n",
            "{'rouge-1': {'r': 0.06557377049180328, 'p': 0.4, 'f': 0.1126760539178735}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.1111111111111111, 'f': 0.02631578738573424}, 'rouge-l': {'r': 0.06557377049180328, 'p': 0.4, 'f': 0.1126760539178735}}\n",
            "pair:  propose new perspective adversarial attacks deep reinforcement learning agents main contribution copycat targeted attack able consistently lure agent following outsider policy pre computed therefore fast inferred could thus usable real time scenario show effectiveness atari games novel read setting latter adversary cannot directly modify agent state representation environment attack agent observation perception environment directly modifying agent state would require write access agent inner workings argue assumption strong realistic settings\n",
            "output sentence:  propose new attack taking full control neural policies realistic settings \n",
            "\n",
            "{'rouge-1': {'r': 0.10714285714285714, 'p': 0.6428571428571429, 'f': 0.18367346693877554}, 'rouge-2': {'r': 0.037037037037037035, 'p': 0.3076923076923077, 'f': 0.06611570056143712}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.42857142857142855, 'f': 0.12244897714285716}}\n",
            "pair:  paper address challenge limited labeled data class imbalance problem machine learning based rumor detection social media present offline data augmentation method based semantic relatedness rumor detection end unlabeled social media data exploited augment limited labeled data context aware neural language model large credibility focused twitter corpus employed learn effective representations rumor tweets semantic relatedness measurement language model fine tuned large domain specific corpus shows dramatic improvement training data augmentation rumor detection pretrained language models conduct experiments six different real world events based five publicly available data sets one augmented data set experiments show proposed method allows us generate larger training data reasonable quality via weak supervision present preliminary results achieved using state art neural network model augmented data rumor detection\n",
            "output sentence:  propose methodology augmenting publicly available data rumor studies based samantic relatedness limited labeled unlabeled data \n",
            "\n",
            "{'rouge-1': {'r': 0.047619047619047616, 'p': 0.5, 'f': 0.08695652015122875}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.25, 'f': 0.037383176186566565}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.5, 'f': 0.08695652015122875}}\n",
            "pair:  many problems large scale labeled training data impressively solved deep learning however unseen class categorization ucc minimal information provided target classes commonly encountered setting industry remains challenging research problem machine learning previous approaches ucc either fail generate powerful discriminative feature extractor fail learn flexible classifier easily adapted unseen classes paper propose address issues network reparameterization textit reparametrizing learnable weights network function variables decouple feature extraction part classification part deep classification model suit special setting ucc securing strong discriminability excellent adaptability extensive experiments ucc several widely used benchmark datasets settings zero shot shot learning demonstrate method network reparameterization achieves state art performance\n",
            "output sentence:  unified frame shot learning zero shot learning based network reparameterization \n",
            "\n",
            "{'rouge-1': {'r': 0.031914893617021274, 'p': 0.3333333333333333, 'f': 0.05825242558959378}, 'rouge-2': {'r': 0.007518796992481203, 'p': 0.1111111111111111, 'f': 0.01408450585498919}, 'rouge-l': {'r': 0.02127659574468085, 'p': 0.2222222222222222, 'f': 0.03883494986143847}}\n",
            "pair:  long known single layer fully connected neural network prior parameters equivalent gaussian process gp limit infinite network width correspondence enables exact bayesian inference infinite width neural networks regression tasks means evaluating corresponding gp recently kernel functions mimic multi layer random neural networks developed outside bayesian framework previous work identified kernels used covariance functions gps allow fully bayesian prediction deep neural network work derive exact equivalence infinitely wide deep networks gps particular covariance function develop computationally efficient pipeline compute covariance function use resulting gp perform bayesian inference deep neural networks mnist cifar observe trained neural network accuracy approaches corresponding gp increasing layer width gp uncertainty strongly correlated trained network prediction error find test performance increases finite width trained networks made wider similar gp gp based predictions typically outperform finite width networks finally connect prior distribution weights variances gp formulation recent development signal propagation random neural networks\n",
            "output sentence:  show make predictions using deep networks without training deep networks \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.7777777777777778, 'f': 0.2121212097658402}, 'rouge-2': {'r': 0.030303030303030304, 'p': 0.2222222222222222, 'f': 0.05333333122133342}, 'rouge-l': {'r': 0.12280701754385964, 'p': 0.7777777777777778, 'f': 0.2121212097658402}}\n",
            "pair:  introduce adaptive input representations neural language modeling extend adaptive softmax grave et al input representations variable capacity several choices factorize input output layers whether model words characters sub word units perform systematic comparison popular choices self attentional architecture experiments show models equipped adaptive embeddings twice fast train popular character input cnn lower number parameters wikitext benchmark achieve perplexity improvement perplexity compared previously best published result billion word benchmark achieve perplexity\n",
            "output sentence:  variable capacity input word embeddings sota wikitext billion word benchmarks \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}, 'rouge-2': {'r': 0.00980392156862745, 'p': 0.125, 'f': 0.018181816833057952}, 'rouge-l': {'r': 0.024096385542168676, 'p': 0.2222222222222222, 'f': 0.04347825910444242}}\n",
            "pair:  counterfactual regret minimization cfr successful algorithm finding approximate nash equilibria imperfect information games however cfr reliance full game tree traversals limits scalability generality therefore game state action space often abstracted simplified cfr resulting strategy mapped back full game requires extensive expert knowledge practical many games outside poker often converges highly exploitable policies recently proposed method deep cfr applies deep learning directly cfr allowing agent intrinsically abstract generalize state space samples without requiring expert knowledge paper introduce single deep cfr sd cfr variant deep cfr lower overall approximation error avoiding training average strategy network show sd cfr attractive theoretical perspective empirically outperforms deep cfr respect exploitability one one play poker\n",
            "output sentence:  better deep reinforcement learning algorithm approximate counterfactual regret minimization \n",
            "\n",
            "{'rouge-1': {'r': 0.0891089108910891, 'p': 0.6, 'f': 0.1551724115413199}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04950495049504951, 'p': 0.3333333333333333, 'f': 0.0862068942999406}}\n",
            "pair:  skip connections increasingly utilized deep neural networks improve accuracy cost efficiency particular recent densenet efficient computation parameters achieves state art predictions directly connecting feature layer previous ones however densenet extreme connectivity pattern may hinder scalability high depths applications like fully convolutional networks full densenet connections prohibitively expensive work first experimentally shows one key advantage skip connections short distances among feature layers backpropagation specifically using fixed number skip connections connection patterns shorter backpropagation distance among layers accurate predictions following insight propose connection template log densenet comparison densenet slightly increases backpropagation distances among layers log uses log total connections instead hence logdenses easier scale densenets longer require careful gpu memory management demonstrate effectiveness design principle showing better performance densenets tabula rasa semantic segmentation competitive results visual recognition\n",
            "output sentence:  show shortcut connections placed patterns minimize layer distances backpropagation design networks achieve log distances using log connections \n",
            "\n",
            "{'rouge-1': {'r': 0.03333333333333333, 'p': 0.2857142857142857, 'f': 0.05970149066607268}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03333333333333333, 'p': 0.2857142857142857, 'f': 0.05970149066607268}}\n",
            "pair:  increasing use neural networks music information retrieval tasks paper empirically investigate different ways improving performance convolutional neural networks cnns spectral audio features specifically explore three aspects cnn design depth network use residual blocks along use grouped convolution global aggregation time application context singer classification singing performance embedding believe conclusions extend types music analysis using convolutional neural networks results show global time aggregation helps improve performance cnns another contribution paper release singing recording dataset used training evaluation\n",
            "output sentence:  using deep learning techniques singing voice related tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.21212121212121213, 'p': 1.0, 'f': 0.3499999971125}, 'rouge-2': {'r': 0.1728395061728395, 'p': 1.0, 'f': 0.29473683959224384}, 'rouge-l': {'r': 0.21212121212121213, 'p': 1.0, 'f': 0.3499999971125}}\n",
            "pair:  propose novel projection based way incorporate conditional information discriminator gans respects role conditional information underlining probabilistic model approach contrast frameworks conditional gans used application today use conditional information concatenating embedded conditional vector feature vectors modification able significantly improve quality class conditional image generation ilsvrc imagenet dataset current state art result achieved single pair discriminator generator also able extend application super resolution succeeded producing highly discriminative super resolution images new structure also enabled high quality category transformation based parametric functional transformation conditional batch normalization layers generator\n",
            "output sentence:  propose novel projection based way incorporate conditional information discriminator gans respects role conditional information underlining probabilistic model \n",
            "\n",
            "{'rouge-1': {'r': 0.0975609756097561, 'p': 0.4, 'f': 0.1568627419454057}, 'rouge-2': {'r': 0.009615384615384616, 'p': 0.047619047619047616, 'f': 0.01599999720448049}, 'rouge-l': {'r': 0.08536585365853659, 'p': 0.35, 'f': 0.13725489880815078}}\n",
            "pair:  solving tasks sparse rewards one important challenges reinforcement learning single agent setting challenge addressed introducing intrinsic rewards motivate agents explore unseen regions state spaces applying techniques naively multi agent setting results agents exploring independently without coordination among argue learning cooperative multi agent settings accelerated improved agents coordinate respect explored paper propose approach learning dynamically select different types intrinsic rewards consider individual agent explored agents agents coordinate exploration maximize extrinsic returns concretely formulate approach hierarchical policy high level controller selects among sets policies trained different types intrinsic rewards low level controllers learn action policies agents specific rewards demonstrate effectiveness proposed approach multi agent gridworld domain sparse rewards show method scales complex settings evaluating vizdoom platform\n",
            "output sentence:  propose several intrinsic reward functions encouraging coordinated exploration multi agent problems introduce approach dynamically selecting best exploration method given task task online \n",
            "\n",
            "{'rouge-1': {'r': 0.21739130434782608, 'p': 0.7692307692307693, 'f': 0.33898304741166335}, 'rouge-2': {'r': 0.1111111111111111, 'p': 0.5, 'f': 0.18181817884297521}, 'rouge-l': {'r': 0.13043478260869565, 'p': 0.46153846153846156, 'f': 0.2033898270726803}}\n",
            "pair:  show usual training loss augmented lipschitz regularization term networks generalize prove generalization first establishing stronger convergence result along rate convergence second result resolves question posed zhang et al model distinguish case clean labels randomized labels answer lipschitz regularization using lipschitz constant clean data makes distinction case model learns different function hypothesize correctly fails learn dirty labels\n",
            "output sentence:  prove generalization dnns adding lipschitz regularization term training loss resolve question posed zhang et al \n",
            "\n",
            "{'rouge-1': {'r': 0.039603960396039604, 'p': 0.5, 'f': 0.07339449405268918}, 'rouge-2': {'r': 0.007751937984496124, 'p': 0.14285714285714285, 'f': 0.014705881376513906}, 'rouge-l': {'r': 0.0297029702970297, 'p': 0.375, 'f': 0.05504587019947819}}\n",
            "pair:  model interpretability systematic targeted model adaptation present central challenges deep learning domain intuitive physics study task visually predicting stability block towers goal understanding influencing model reasoning contributions two fold firstly introduce neural stethoscopes framework quantifying degree importance specific factors influence deep networks well actively promoting suppressing information appropriate unify concepts multitask learning well training auxiliary adversarial losses secondly deploy stethoscope framework provide depth analysis state art deep neural network stability prediction specifically examining physical reasoning show baseline model susceptible misled incorrect visual cues leads performance breakdown level random guessing training scenarios visual cues inversely correlated stability using stethoscopes promote meaningful feature extraction increases performance prediction accuracy conversely training easy dataset visual cues positively correlated stability baseline model learns bias leading poor performance harder dataset using adversarial stethoscope network successfully de biased leading performance increase\n",
            "output sentence:  combining auxiliary adversarial training interrogate help physical understanding \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.5555555555555556, 'f': 0.09174311775103107}, 'rouge-2': {'r': 0.007936507936507936, 'p': 0.125, 'f': 0.014925372011583955}, 'rouge-l': {'r': 0.03, 'p': 0.3333333333333333, 'f': 0.055045870044609084}}\n",
            "pair:  effectiveness convolutional neural networks stems large part ability exploit translation invariance inherent many learning problems recently shown cnns exploit invariances rotation invariance using group convolutions instead planar convolutions however reasons performance ease implementation necessary limit group convolution transformations applied filters without interpolation thus images square pixels integer translations rotations multiples degrees reflections admissible whereas square tiling provides fold rotational symmetry hexagonal tiling plane fold rotational symmetry paper show one efficiently implement planar convolution group convolution hexagonal lattices using existing highly optimized convolution routines find due reduced anisotropy hexagonal filters planar hexaconv provides better accuracy planar convolution square filters given fixed parameter budget furthermore find increased degree symmetry hexagonal grid increases effectiveness group convolutions allowing parameter sharing show method significantly outperforms conventional cnns aid aerial scene classification dataset even outperforming imagenet pre trained models\n",
            "output sentence:  introduce hexaconv group equivariant convolutional neural network hexagonal lattices \n",
            "\n",
            "{'rouge-1': {'r': 0.1590909090909091, 'p': 0.6363636363636364, 'f': 0.2545454513454546}, 'rouge-2': {'r': 0.078125, 'p': 0.45454545454545453, 'f': 0.13333333083022222}, 'rouge-l': {'r': 0.1590909090909091, 'p': 0.6363636363636364, 'f': 0.2545454513454546}}\n",
            "pair:  introduce quantum graph neural networks qgnn new class quantum neural network ansatze tailored represent quantum processes graph structure particularly suitable executed distributed quantum systems quantum network along general class ansatze introduce specialized architectures namely quantum graph recurrent neural networks qgrnn quantum graph convolutional neural networks qgcnn provide four example applications qgnn learning hamiltonian dynamics quantum systems learning create multipartite entanglement quantum network unsupervised learning spectral clustering supervised learning graph isomorphism classification\n",
            "output sentence:  introducing new class quantum neural networks learning graph based representations quantum computers \n",
            "\n",
            "{'rouge-1': {'r': 0.10869565217391304, 'p': 0.9090909090909091, 'f': 0.19417475537373932}, 'rouge-2': {'r': 0.0743801652892562, 'p': 0.8181818181818182, 'f': 0.13636363483585862}, 'rouge-l': {'r': 0.10869565217391304, 'p': 0.9090909090909091, 'f': 0.19417475537373932}}\n",
            "pair:  goal standard compressive sensing estimate unknown vector linear measurements assumption sparsity basis recently shown significantly fewer measurements may required sparsity assumption replaced assumption unknown vector lies near range suitably chosen generative model particular bora em et al shown roughly log random gaussian measurements suffice accurate recovery input generative model bounded lipschitz kd log measurements suffice input relu networks depth width paper establish corresponding algorithm independent lower bounds sample complexity using tools minimax statistical analysis accordance upper bounds results summarized follows construct lipschitz generative model capable generating group sparse signals show resulting necessary number measurements omega log ii using similar ideas construct two layer relu networks high width requiring omega log measurements well lower width deep relu networks requiring omega measurements result establish scaling laws derived bora em et al optimal near optimal absence assumptions\n",
            "output sentence:  establish scaling laws derived bora et al optimal near optimal absence assumptions \n",
            "\n",
            "{'rouge-1': {'r': 0.017543859649122806, 'p': 0.1111111111111111, 'f': 0.030303027947658587}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.017543859649122806, 'p': 0.1111111111111111, 'f': 0.030303027947658587}}\n",
            "pair:  significant advances made natural language processing nlp modelling since beginning new approaches allow accurate results even little labelled data nlp models benefit training task agnostic task specific unlabelled data however advantages come significant size computational costs workshop paper outlines proposed convolutional student architecture trained distillation process large scale model achieve inference speedup reduction parameter count cases student model performance surpasses teacher studied tasks\n",
            "output sentence:  train small efficient cnn performance openai transformer text classification tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.5, 'f': 0.11999999788800003}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.3333333333333333, 'f': 0.07999999788800007}}\n",
            "pair:  paper puts forward new text tensor representation relies information compression techniques assign shorter codes frequently used characters representation language independent need pretraining produces encoding information loss provides adequate description morphology text able represent prefixes declensions inflections similar vectors able represent even unseen words training dataset similarly compact yet sparse ideal speed training times using tensor processing libraries part paper show technique especially effective coupled convolutional neural networks cnns text classification character level apply two variants cnn coupled experimental results show drastically reduces number parameters optimized resulting competitive classification accuracy values fraction time spent one hot encoding representations thus enabling training commodity hardware\n",
            "output sentence:  using compressing tecniques encoding words possibility faster training cnn dimensionality reduction representation \n",
            "\n",
            "{'rouge-1': {'r': 0.06944444444444445, 'p': 0.625, 'f': 0.12499999820000002}, 'rouge-2': {'r': 0.011627906976744186, 'p': 0.14285714285714285, 'f': 0.021505374952017663}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  paper presents novel two step approach fundamental problem learning optimal map one distribution another first learn optimal transport ot plan thought one many map two distributions end propose stochastic dual approach regularized ot show empirically scales better recent related approach amount samples large second estimate monge map deep neural network learned approximating barycentric projection previously obtained ot plan parameterization allows generalization mapping outside support input measure prove two theoretical stability results regularized ot show estimations converge ot monge map underlying continuous measures showcase proposed approach two applications domain adaptation generative modeling\n",
            "output sentence:  learning optimal mapping deepnn distributions along theoretical guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.5555555555555556, 'f': 0.24096385202496737}, 'rouge-2': {'r': 0.05555555555555555, 'p': 0.2777777777777778, 'f': 0.09259258981481489}, 'rouge-l': {'r': 0.12307692307692308, 'p': 0.4444444444444444, 'f': 0.19277108094063003}}\n",
            "pair:  demonstrate possibility call sparse learning accelerated training deep neural networks maintain sparse weights throughout training achieving dense performance levels accomplish developing sparse momentum algorithm uses exponentially smoothed gradients momentum identify layers weights reduce error efficiently sparse momentum redistributes pruned weights across layers according mean momentum magnitude layer within layer sparse momentum grows weights according momentum magnitude zero valued weights demonstrate state art sparse performance mnist cifar imagenet decreasing mean error relative compared sparse algorithms furthermore show sparse momentum reliably reproduces dense performance levels providing faster training analysis ablations show benefits momentum redistribution growth increase depth size network\n",
            "output sentence:  redistributing growing weights according momentum magnitude enables training sparse networks random initializations reach dense performance levels weights accelerating training \n",
            "\n",
            "{'rouge-1': {'r': 0.12698412698412698, 'p': 0.8, 'f': 0.21917807982735973}, 'rouge-2': {'r': 0.0410958904109589, 'p': 0.3333333333333333, 'f': 0.07317072975312319}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.6, 'f': 0.16438355927941453}}\n",
            "pair:  success reinforcement learning real world limited instrumented laboratory scenarios often requiring arduous human supervision enable continuous learning work discuss required elements robotic system continually autonomously improve data collected real world propose particular instantiation system subsequently investigate number challenges learning without instrumentation including lack episodic resets state estimation hand engineered rewards propose simple scalable solutions challenges demonstrate efficacy proposed system dexterous robotic manipulation tasks simulation real world also provide insightful analysis ablation study challenges associated learning paradigm\n",
            "output sentence:  system learn robotic tasks real world reinforcement learning without instrumentation \n",
            "\n",
            "{'rouge-1': {'r': 0.08974358974358974, 'p': 0.7, 'f': 0.1590909070764463}, 'rouge-2': {'r': 0.03488372093023256, 'p': 0.3333333333333333, 'f': 0.0631578930216067}, 'rouge-l': {'r': 0.05128205128205128, 'p': 0.4, 'f': 0.09090908889462815}}\n",
            "pair:  learning mahalanobis metric spaces important problem found numerous applications several algorithms designed problem including information theoretic metric learning itml davis et al large margin nearest neighbor lmnn classification weinberger saul consider formulation mahalanobis metric learning optimization problem objective minimize number violated similarity dissimilarity constraints show fixed ambient dimension exists fully polynomial time approximation scheme fptas nearly linear running time result obtained using tools theory linear programming low dimensions also discuss improvements algorithm practice present experimental results synthetic real world data sets algorithm fully parallelizable performs favorably presence adversarial noise\n",
            "output sentence:  fully parallelizable adversarial noise resistant metric learning algorithm theoretical guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.2, 'f': 0.021276594737437805}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}}\n",
            "pair:  paper proposes metagross meta gated recursive controller new neural sequence modeling unit proposed unit characterized recursive parameterization gating functions gating mechanisms metagross controlled instances repeatedly called recursive fashion interpreted form meta gating recursively parameterizing recurrent model postulate proposed inductive bias provides modeling benefits pertaining learning inherently hierarchically structured sequence data language logical music tasks end conduct extensive experiments recursive logic tasks sorting tree traversal logical inference sequential pixel pixel classification semantic parsing code generation machine translation polyphonic music modeling demonstrating widespread utility proposed approach achieving state art close performance tasks\n",
            "output sentence:  recursive parameterization recurrent models improve performance \n",
            "\n",
            "{'rouge-1': {'r': 0.18421052631578946, 'p': 0.8235294117647058, 'f': 0.3010752658295757}, 'rouge-2': {'r': 0.08653846153846154, 'p': 0.5625, 'f': 0.14999999768888891}, 'rouge-l': {'r': 0.17105263157894737, 'p': 0.7647058823529411, 'f': 0.27956988948548966}}\n",
            "pair:  employing deep neural networks natural image priors solve inverse problems either requires large amounts data sufficiently train expressive generative models succeed data via untrained neural networks however works considered interpolate high data regimes particular one use availability small amount data even examples one advantage solving inverse problems system performance increase amount data increases well work consider solving linear inverse problems given small number examples images drawn distribution image interest comparing untrained neural networks use data show one pre train neural network given examples improve reconstruction results compressed sensing semantic image recovery problems colorization approach leads improved reconstruction amount available data increases par fully trained generative models requiring less data needed train generative model\n",
            "output sentence:  show pre training untrained neural network examples improve reconstruction results compressed sensing semantic recovery problems like colorization \n",
            "\n",
            "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
            "pair:  paper gives rigorous analysis trained generalized hamming networks ghn proposed fan discloses interesting finding ghns stacked convolution layers ghn equivalent single yet wide convolution layer revealed equivalence theoretical side regarded constructive manifestation universal approximation theorem cybenko hornik practice profound multi fold implications network visualization constructed deep epitomes layer provide visualization network internal representation rely input data moreover deep epitomes allows direct extraction features one step without resorting regularized optimizations used existing visualization tools\n",
            "output sentence:  bridge gap soft computing \n",
            "\n",
            "{'rouge-1': {'r': 0.11666666666666667, 'p': 0.6363636363636364, 'f': 0.19718309597302125}, 'rouge-2': {'r': 0.06172839506172839, 'p': 0.45454545454545453, 'f': 0.10869565006852554}, 'rouge-l': {'r': 0.1, 'p': 0.5454545454545454, 'f': 0.16901408188851422}}\n",
            "pair:  paper introduces information theoretic co training objective unsupervised learning consider problem predicting future rather predict future sensations image pixels sound waves predict hypotheses confirmed future sensations formally assume population distribution pairs think past sensation future sensation train predictor model phi confirmation model psi view hypotheses predicted facts confirmed population distribution pairs focus problem measuring mutual information data processing inequality mutual information least large mutual information distribution triples defined confirmation model psi information theoretic training objective phi psi viewed form co training want prediction match confirmation give experiments applications learning phonetics timit dataset\n",
            "output sentence:  presents information theoretic training objective co training demonstrates power unsupervised learning phonetics \n",
            "\n",
            "{'rouge-1': {'r': 0.1188118811881188, 'p': 0.8571428571428571, 'f': 0.20869565003553875}, 'rouge-2': {'r': 0.05426356589147287, 'p': 0.4666666666666667, 'f': 0.0972222203559028}, 'rouge-l': {'r': 0.09900990099009901, 'p': 0.7142857142857143, 'f': 0.17391304133988658}}\n",
            "pair:  existing sequence prediction methods mostly concerned time independent sequences actual time span events irrelevant distance events simply difference order positions sequence time independent view sequences applicable data natural languages dealing words sentence inappropriate inefficient many real world events observed collected unequally spaced points time naturally arise person goes grocery store makes phone call time span events carry important information sequence dependence human behaviors work propose set methods using time sequence prediction neural sequence models rnn amenable handling token like input propose two methods time dependent event representation based intuition time tokenized everyday life previous work embedding contextualization also introduce two methods using next event duration regularization training sequence prediction model discuss methods based recurrent neural nets evaluate methods well baseline models five datasets resemble variety sequence prediction tasks experiments revealed proposed methods offer accuracy gain baseline models range settings\n",
            "output sentence:  proposed methods time dependent event representation regularization sequence prediction evaluated methods five datasets involve range sequence prediction tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.2222222222222222, 'p': 0.9411764705882353, 'f': 0.35955055870723396}, 'rouge-2': {'r': 0.17045454545454544, 'p': 0.8823529411764706, 'f': 0.28571428300045354}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.9411764705882353, 'f': 0.35955055870723396}}\n",
            "pair:  propose warped residual network warpnet using parallelizable warp operator forward backward propagation distant layers trains faster original residual neural network apply perturbation theory residual networks decouple interactions residual units resulting warp operator first order approximation output multiple layers first order perturbation theory exhibits properties binomial path lengths exponential gradient scaling found experimentally veit et al demonstrate extensive performance study proposed network achieves comparable predictive performance original residual network number parameters achieving significant speed total training time warpnet performs model parallelism residual network training weights distributed different gpus offers speed capability train larger networks compared original residual networks\n",
            "output sentence:  propose warped residual network using parallelizable warp operator forward backward propagation distant layers trains faster original residual neural \n",
            "\n",
            "{'rouge-1': {'r': 0.0392156862745098, 'p': 0.3333333333333333, 'f': 0.07017543671283476}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0196078431372549, 'p': 0.16666666666666666, 'f': 0.0350877174145892}}\n",
            "pair:  propose new approach known iterative regularized dual averaging irda improve efficiency convolutional neural networks cnn significantly reducing redundancy model without reducing accuracy method tested various data sets proven significantly efficient existing compressing techniques deep learning literature many popular data sets mnist cifar weights zeroed without losing accuracy particular able make resnet sparsity accuracy comparable much larger model resnet best sparsity reported literature\n",
            "output sentence:  sparse optimization algorithm deep cnn models \n",
            "\n",
            "{'rouge-1': {'r': 0.09195402298850575, 'p': 0.8, 'f': 0.1649484517589542}, 'rouge-2': {'r': 0.03669724770642202, 'p': 0.4444444444444444, 'f': 0.06779660876041371}, 'rouge-l': {'r': 0.09195402298850575, 'p': 0.8, 'f': 0.1649484517589542}}\n",
            "pair:  twe present new approach novel architecture termed wsnet learning compact efficient deep neural networks existing approaches conventionally learn full model parameters independently compress via emph ad hoc processing model pruning filter factorization alternatively wsnet proposes learning model parameters sampling compact set learnable parameters naturally enforces parameter sharing throughout learning process demonstrate novel weight sampling approach induced wsnet promotes weights computation sharing favorably employing method efficiently learn much smaller networks competitive performance compared baseline networks equal numbers convolution filters specifically consider learning compact efficient convolutional neural networks audio classification extensive experiments multiple audio classification datasets verify effectiveness wsnet combined weight quantization resulted models textbf times smaller theoretically textbf times faster well established baselines without noticeable performance drop\n",
            "output sentence:  present novel network architecture learning compact efficient deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.05084745762711865, 'p': 0.5, 'f': 0.0923076906319527}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.3333333333333333, 'f': 0.06153845986272194}}\n",
            "pair:  work offers new method domain translation semantic label maps computer graphic cg simulation edge map images photo realistic im ages train generative adversarial network gan conditional way generate photo realistic version given cg scene existing architectures gans still lack photo realism capabilities needed train dnns computer vision tasks address issue embedding edge maps training adversarial mode also offer extension model uses gan architecture create visually appealing temporally coherent videos\n",
            "output sentence:  simulation real images translation video generation \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.9090909090909091, 'f': 0.2197802176548726}, 'rouge-2': {'r': 0.08737864077669903, 'p': 0.9, 'f': 0.15929203378494794}, 'rouge-l': {'r': 0.125, 'p': 0.9090909090909091, 'f': 0.2197802176548726}}\n",
            "pair:  understanding groundbreaking performance deep neural networks one greatest challenges scientific community today work introduce information theoretic viewpoint behavior deep networks optimization processes generalization abilities studying information plane plane mutual information input variable desired label hidden layer specifically show training network characterized rapid increase mutual information mi layers target label followed longer decrease mi layers input variable explicitly show two fundamental information theoretic quantities correspond generalization error network result introducing new generalization bound exponential representation compression analysis focuses typical patterns large scale problems purpose introduce novel analytic bound mutual information consecutive layers network important consequence analysis super linear boost training time number non degenerate hidden layers demonstrating computational benefit hidden layers\n",
            "output sentence:  introduce information theoretic viewpoint behavior deep networks optimization processes generalization abilities \n",
            "\n",
            "{'rouge-1': {'r': 0.19047619047619047, 'p': 0.8421052631578947, 'f': 0.31067960864171934}, 'rouge-2': {'r': 0.08653846153846154, 'p': 0.47368421052631576, 'f': 0.14634146080243246}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.7368421052631579, 'f': 0.2718446571854086}}\n",
            "pair:  recent powerful pre trained language models achieved remarkable performance popular datasets reading comprehension time introduce challenging datasets push development field towards comprehensive reasoning text paper introduce new reading comprehension dataset requiring logical reasoning reclor extracted standardized graduate admission examinations earlier studies suggest human annotated datasets usually contain biases often exploited models achieve high accuracy without truly understanding text order comprehensively evaluate logical reasoning ability models reclor propose identify biased data points separate easy set rest hard set empirical results show state art models outstanding ability capture biases contained dataset high accuracy easy set however struggle hard set poor performance near random guess indicating research needed essentially enhance logical reasoning ability current models\n",
            "output sentence:  introduce reclor reading comprehension dataset requiring logical reasoning find current state art models struggle real logical reasoning poor performance near random \n",
            "\n",
            "{'rouge-1': {'r': 0.11320754716981132, 'p': 0.5714285714285714, 'f': 0.18897637519251043}, 'rouge-2': {'r': 0.032520325203252036, 'p': 0.2, 'f': 0.05594405353807043}, 'rouge-l': {'r': 0.0660377358490566, 'p': 0.3333333333333333, 'f': 0.1102362177121955}}\n",
            "pair:  ability autonomously explore navigate physical space fundamental requirement virtually mobile autonomous agent household robotic vacuums autonomous vehicles traditional slam based approaches exploration navigation largely focus leveraging scene geometry fail model dynamic objects agents semantic constraints wet floors doorways learning based rl agents attractive alternative incorporate semantic geometric information notoriously sample inefficient difficult generalize novel settings difficult interpret paper combine best worlds modular approach em learns spatial representation scene trained effective coupled traditional geometric planners specifically design agent learns predict spatial affordance map elucidates parts scene navigable active self supervised experience gathering contrast simulation environments assume static world evaluate approach vizdoom simulator using large scale randomly generated maps containing variety dynamic actors hazards show learned affordance maps used augment traditional approaches exploration navigation providing significant improvements performance\n",
            "output sentence:  address task autonomous exploration navigation using spatial affordance maps learned self supervised manner outperform classic geometric baselines sample efficient contemporary contemporary \n",
            "\n",
            "{'rouge-1': {'r': 0.017857142857142856, 'p': 0.2857142857142857, 'f': 0.03361344427088486}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.008928571428571428, 'p': 0.14285714285714285, 'f': 0.016806721581809268}}\n",
            "pair:  generative models shown great success generating high dimensional samples conditional low dimensional descriptors learning stroke thickness mnist hair color celeba speaker identity wavenet generation sample poses fundamental problems conditional variational autoencoder cvae simple conditional generative model explicitly relate conditions training hence incentive learning compact joint distribution across conditions overcome limitation matching distributions using maximum mean discrepancy mmd decoder layer follows bottleneck introduces strong regularization reconstructing samples within condition transforming samples across conditions resulting much improved generalization refer architecture transformer vae trvae benchmarking trvae high dimensional image tabular data demonstrate higher robustness higher accuracy existing approaches particular show qualitatively improved predictions cellular perturbation response treatment disease based high dimensional single cell gene expression data tackling previously problematic minority classes multiple conditions generic tasks improve pearson correlations high dimensional estimated means variances ground truths respectively\n",
            "output sentence:  generates never seen data training desired condition \n",
            "\n",
            "{'rouge-1': {'r': 0.10526315789473684, 'p': 0.8571428571428571, 'f': 0.18749999805175782}, 'rouge-2': {'r': 0.03076923076923077, 'p': 0.3333333333333333, 'f': 0.05633802662170209}, 'rouge-l': {'r': 0.08771929824561403, 'p': 0.7142857142857143, 'f': 0.15624999805175782}}\n",
            "pair:  large transformer models routinely achieve state art results number tasks training models prohibitively costly especially long sequences introduce two techniques improve efficiency transformers one replace dot product attention one uses locality sensitive hashing changing complexity length sequence furthermore use reversible residual layers instead standard residuals allows storing activations training process instead times number layers resulting model reformer performs par transformer models much memory efficient much faster long sequences\n",
            "output sentence:  efficient transformer locality sensitive hashing reversible layers \n",
            "\n",
            "{'rouge-1': {'r': 0.11392405063291139, 'p': 0.6, 'f': 0.1914893590199185}, 'rouge-2': {'r': 0.019230769230769232, 'p': 0.13333333333333333, 'f': 0.03361344317491717}, 'rouge-l': {'r': 0.0759493670886076, 'p': 0.4, 'f': 0.12765957178587603}}\n",
            "pair:  investigate training performance generative adversarial networks using maximum mean discrepancy mmd critic termed mmd gans main theoretical contribution clarify situation bias gan loss functions raised recent work show gradient estimators used optimization process mmd gans wasserstein gans unbiased learning discriminator based samples leads biased gradients generator parameters also discuss issue kernel choice mmd critic characterize kernel corresponding energy distance used cram gan critic integral probability metric mmd benefits training strategies recently developed wasserstein gans experiments mmd gan able employ smaller critic network wasserstein gan resulting simpler faster training algorithm matching performance also propose improved measure gan convergence kernel inception distance show use dynamically adapt learning rates gan training\n",
            "output sentence:  explain bias situation mmd gans mmd gans work smaller critic networks wgan gps new gan evaluation metric \n",
            "\n",
            "{'rouge-1': {'r': 0.11666666666666667, 'p': 0.6363636363636364, 'f': 0.19718309597302125}, 'rouge-2': {'r': 0.06172839506172839, 'p': 0.45454545454545453, 'f': 0.10869565006852554}, 'rouge-l': {'r': 0.1, 'p': 0.5454545454545454, 'f': 0.16901408188851422}}\n",
            "pair:  paper introduces information theoretic co training objective unsupervised learning consider problem predicting future rather predict future sensations image pixels sound waves predict hypotheses confirmed future sensations formally assume population distribution pairs think past sensation future sensation train predictor model phi confirmation model psi view hypotheses predicted facts confirmed population distribution pairs focus problem measuring mutual information data processing inequality mutual information least large mutual information distribution triples defined confirmation model psi information theoretic training objective phi psi viewed form co training want prediction match confirmation give experiments applications learning phonetics timit dataset\n",
            "output sentence:  presents information theoretic training objective co training demonstrates power unsupervised learning phonetics \n",
            "\n",
            "{'rouge-1': {'r': 0.1206896551724138, 'p': 0.5833333333333334, 'f': 0.1999999971591837}, 'rouge-2': {'r': 0.02702702702702703, 'p': 0.18181818181818182, 'f': 0.04705882127612468}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.3333333333333333, 'f': 0.11428571144489805}}\n",
            "pair:  uncertainty important feature intelligence helps brain become flexible creative powerful intelligent system crossbar based neuromorphic computing chips computing mainly performed analog circuits uncertainty used imitate brain however current deep neural networks taken uncertainty neuromorphic computing chip consideration therefore performances neuromorphic computing chips good original platforms cpus gpus work proposed uncertainty adaptation training scheme uats tells uncertainty neural network training process experimental results show neural networks achieve comparable inference performances uncertain neuromorphic computing chip compared results original platforms much better performances without training scheme\n",
            "output sentence:  training method make deep learning algorithms work better neuromorphic computing chips uncertainty \n",
            "\n",
            "{'rouge-1': {'r': 0.14942528735632185, 'p': 0.9285714285714286, 'f': 0.25742574018625636}, 'rouge-2': {'r': 0.11206896551724138, 'p': 0.9285714285714286, 'f': 0.19999999807810656}, 'rouge-l': {'r': 0.14942528735632185, 'p': 0.9285714285714286, 'f': 0.25742574018625636}}\n",
            "pair:  model free deep reinforcement learning rl algorithms demonstrated range challenging decision making control tasks however methods typically suffer two major challenges high sample complexity brittle convergence properties necessitate meticulous hyperparameter tuning challenges severely limit applicability methods complex real world domains paper propose soft actor critic policy actor critic deep rl algorithm based maximum entropy reinforcement learning framework framework actor aims maximize expected reward also maximizing entropy succeed task acting randomly possible prior deep rl methods based framework formulated either policy learning policy policy gradient methods combining policy updates stable stochastic actor critic formulation method achieves state art performance range continuous control benchmark tasks outperforming prior policy policy methods furthermore demonstrate contrast policy algorithms approach stable achieving similar performance across different random seeds\n",
            "output sentence:  propose soft actor critic policy actor critic deep rl algorithm based maximum entropy reinforcement learning framework \n",
            "\n",
            "{'rouge-1': {'r': 0.1746031746031746, 'p': 0.9166666666666666, 'f': 0.29333333064533335}, 'rouge-2': {'r': 0.11842105263157894, 'p': 0.75, 'f': 0.20454545219008266}, 'rouge-l': {'r': 0.1746031746031746, 'p': 0.9166666666666666, 'f': 0.29333333064533335}}\n",
            "pair:  propose novel deep network architecture lifelong learning refer dynamically expandable network den dynamically decide network capacity trains sequence tasks learn compact overlapping knowledge sharing structure among tasks den efficiently trained online manner performing selective retraining dynamically expands network capacity upon arrival task necessary number units effectively prevents semantic drift splitting duplicating units timestamping validate den multiple public datasets lifelong learning scenarios multiple public datasets significantly outperforms existing lifelong learning methods deep networks also achieves level performance batch model substantially fewer number parameters\n",
            "output sentence:  propose novel deep network architecture dynamically decide network capacity trains lifelong learning scenario \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.5454545454545454, 'f': 0.14814814580094499}, 'rouge-2': {'r': 0.04395604395604396, 'p': 0.4, 'f': 0.07920791900794046}, 'rouge-l': {'r': 0.08571428571428572, 'p': 0.5454545454545454, 'f': 0.14814814580094499}}\n",
            "pair:  generative adversarial networks learning framework rely training discriminator estimate measure difference target generated distributions gans normally formulated rely generated samples completely differentiable generative parameters thus work discrete data introduce method training gans discrete data uses estimated difference measure discriminator compute importance weights generated samples thus providing policy gradient training generator importance weights strong connection decision boundary discriminator call method boundary seeking gans bgans demonstrate effectiveness proposed algorithm discrete image character based natural language generation addition boundary seeking objective extends continuous data used improve stability training demonstrate celeba large scale scene understanding lsun bedrooms imagenet without conditioning\n",
            "output sentence:  address training gans discrete data formulating policy gradient generalizes across divergences \n",
            "\n",
            "{'rouge-1': {'r': 0.05952380952380952, 'p': 0.5555555555555556, 'f': 0.10752687997225113}, 'rouge-2': {'r': 0.02654867256637168, 'p': 0.375, 'f': 0.04958677562461584}, 'rouge-l': {'r': 0.05952380952380952, 'p': 0.5555555555555556, 'f': 0.10752687997225113}}\n",
            "pair:  inspired combination feedforward iterative computations visual cortex taking advantage ability denoising autoencoders estimate score joint distribution propose novel approach iterative inference capturing exploiting complex joint distribution output variables conditioned input variables approach applied image pixel wise segmentation estimated conditional score used perform gradient ascent towards mode estimated conditional distribution extends previous work score estimation denoising autoencoders case conditional distribution novel use corrupted feedforward predictor replacing gaussian corruption advantage approach classical ways perform iterative inference structured outputs like conditional random fields crfs necessary define explicit energy function linking output variables keep computations tractable energy function parametrizations typically fairly constrained involving neighbors output variables clique experimentally find proposed iterative inference conditional score estimation conditional denoising autoencoders performs better comparable models based crfs using explicit modeling conditional joint distribution outputs\n",
            "output sentence:  refining segmentation proposals performing iterative inference conditional denoising autoencoders \n",
            "\n",
            "{'rouge-1': {'r': 0.13157894736842105, 'p': 0.625, 'f': 0.21739130147448016}, 'rouge-2': {'r': 0.046511627906976744, 'p': 0.2857142857142857, 'f': 0.07999999759200008}, 'rouge-l': {'r': 0.13157894736842105, 'p': 0.625, 'f': 0.21739130147448016}}\n",
            "pair:  extend recent results arora et al spectral analysis representations corresponding kernel neural embeddings showed simple single layer network alignment labels eigenvectors corresponding gram matrix determines convergence optimization training well generalization properties generalize result kernel neural representations show extensions improve optimization generalization basic setup studied arora et al\n",
            "output sentence:  spectral analysis understanding different representations improve optimization generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.1038961038961039, 'p': 0.6666666666666666, 'f': 0.17977527856583767}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.09090909090909091, 'f': 0.019999998042000193}, 'rouge-l': {'r': 0.07792207792207792, 'p': 0.5, 'f': 0.13483145834111857}}\n",
            "pair:  reinforcement learning rl typically defines discount factor part markov decision process discount factor values future rewards exponential scheme leads theoretical convergence guarantees bellman equation however evidence psychology economics neuroscience suggests humans animals instead hyperbolic time preferences extend earlier work kurth nelson redish propose efficient deep reinforcement learning agent acts via hyperbolic discounting non exponential discount mechanisms demonstrate simple approach approximates hyperbolic discount functions still using familiar temporal difference learning techniques rl additionally independent hyperbolic discounting make surprising discovery simultaneously learning value functions multiple time horizons effective auxiliary task often improves state art methods\n",
            "output sentence:  deep rl agent learns hyperbolic non exponential values new multi horizon auxiliary task \n",
            "\n",
            "{'rouge-1': {'r': 0.12371134020618557, 'p': 0.631578947368421, 'f': 0.2068965489848395}, 'rouge-2': {'r': 0.008, 'p': 0.05555555555555555, 'f': 0.013986011785417725}, 'rouge-l': {'r': 0.061855670103092786, 'p': 0.3157894736842105, 'f': 0.10344827312277059}}\n",
            "pair:  work propose goal driven collaborative task contains language vision action virtual environment core components specifically develop collaborative image drawing game two agents called codraw game grounded virtual world contains movable clip art objects game involves two players teller drawer teller sees abstract scene containing multiple clip art pieces semantically meaningful configuration drawer tries reconstruct scene empty canvas using available clip art pieces two players communicate via two way communication using natural language collect codraw dataset dialogs consisting messages exchanged human agents define protocols metrics evaluate effectiveness learned agents testbed highlighting need novel crosstalk condition pairs agents trained independently disjoint subsets training data evaluation present models task including simple effective baselines neural network approaches trained using combination imitation learning goal driven training models benchmarked using fully automated evaluation playing game live human agents\n",
            "output sentence:  introduce dataset models training evaluation protocols collaborative drawing task allows studying goal driven perceptually actionably grounded language generation understanding \n",
            "\n",
            "{'rouge-1': {'r': 0.19047619047619047, 'p': 0.8421052631578947, 'f': 0.31067960864171934}, 'rouge-2': {'r': 0.04672897196261682, 'p': 0.2777777777777778, 'f': 0.07999999753472008}, 'rouge-l': {'r': 0.13095238095238096, 'p': 0.5789473684210527, 'f': 0.21359223000094266}}\n",
            "pair:  belief persists long machine learning enlargement margins training data accounts resistance models overfitting increasing robustness yet breiman shows dilemma breiman uniform improvement margin distribution emph necessarily reduces generalization error paper revisit breiman dilemma deep neural networks recently proposed normalized margins using lipschitz constant bound spectral norm products simplified theory extensive experiments breiman dilemma shown rely dynamics normalized margin distributions reflects trade model expression power data complexity complexity data comparable model expression power sense training test data share similar phase transitions normalized margin dynamics two efficient ways derived via classic margin based generalization bounds successfully predict trend generalization error hand expressed models exhibit uniform improvements training normalized margins may lose prediction power fail prevent overfitting\n",
            "output sentence:  bregman dilemma shown deep learning improvement margins parameterized models may result overfitting dynamics normalized margin distributions predict generalization error error \n",
            "\n",
            "{'rouge-1': {'r': 0.1388888888888889, 'p': 0.7692307692307693, 'f': 0.23529411505605538}, 'rouge-2': {'r': 0.046511627906976744, 'p': 0.3333333333333333, 'f': 0.08163265091212}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5384615384615384, 'f': 0.16470587976193776}}\n",
            "pair:  propose evaluate new techniques compressing speeding dense matrix multiplications found fully connected recurrent layers neural networks embedded large vocabulary continuous speech recognition lvcsr compression introduce study trace norm regularization technique training low rank factored versions matrix multiplications compared standard low rank training show method leads good accuracy versus number parameter trade offs used speed training large models speedup enable faster inference arm processors new open sourced kernels optimized small batch sizes resulting speed ups widely used gemmlowp library beyond lvcsr expect techniques kernels generally applicable embedded neural networks large fully connected recurrent layers\n",
            "output sentence:  compress speed speech recognition models embedded devices trace norm regularization technique optimized kernels \n",
            "\n",
            "{'rouge-1': {'r': 0.047058823529411764, 'p': 0.36363636363636365, 'f': 0.08333333130425352}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.18181818181818182, 'f': 0.032520323574591926}, 'rouge-l': {'r': 0.047058823529411764, 'p': 0.36363636363636365, 'f': 0.08333333130425352}}\n",
            "pair:  propose active learning algorithmic architecture capable organizing learning process order achieve field complex tasks learning sequences primitive motor policies socially guided intrinsic motivation procedure babbling sgim pb learner generalize experience continuously learn new outcomes choosing actively learn guided empirical measures progress paper considering learning set interrelated complex outcomes hierarchically organized introduce new framework called procedures enables autonomous discovery combine previously learned skills order learn increasingly complex motor policies combinations primitive motor policies architecture actively decide outcome focus exploration strategy apply strategies could autonomous exploration active social guidance relies expertise human teacher providing demonstrations learner request show simulated environment new architecture capable tackling learning complex motor policies adapt complexity policies task hand also show procedures increases agent capability learn complex tasks\n",
            "output sentence:  paper describes strategic intrinsically motivated learning algorithm tackles learning complex motor policies \n",
            "\n",
            "{'rouge-1': {'r': 0.11650485436893204, 'p': 0.8571428571428571, 'f': 0.20512820302140405}, 'rouge-2': {'r': 0.02962962962962963, 'p': 0.3076923076923077, 'f': 0.05405405245160707}, 'rouge-l': {'r': 0.07766990291262135, 'p': 0.5714285714285714, 'f': 0.1367521346453357}}\n",
            "pair:  pre trained deep neural network language models elmo gpt bert xlnet recently achieved state art performance variety language understanding tasks however size makes impractical number scenarios especially mobile edge devices particular input word embedding matrix accounts significant proportion model memory footprint due large input vocabulary embedding dimensions knowledge distillation techniques success compressing large neural network models ineffective yielding student models vocabularies different original teacher models introduce novel knowledge distillation technique training student model significantly smaller vocabulary well lower embedding hidden state dimensions specifically employ dual training mechanism trains teacher student models simultaneously obtain optimal word embeddings student vocabulary combine approach learning shared projection matrices transfer layer wise knowledge teacher model student model method able compress bert base model minor drop downstream task metrics resulting language model footprint mb experimental results also demonstrate higher compression efficiency accuracy compared state art compression techniques\n",
            "output sentence:  present novel distillation techniques enable training student models different vocabularies compress bert minor performance drop \n",
            "\n",
            "{'rouge-1': {'r': 0.15, 'p': 0.8, 'f': 0.25263157628808863}, 'rouge-2': {'r': 0.0673076923076923, 'p': 0.4666666666666667, 'f': 0.1176470566202952}, 'rouge-l': {'r': 0.1125, 'p': 0.6, 'f': 0.18947368155124658}}\n",
            "pair:  conventional generative adversarial networks gans text generation tend issues reward sparsity mode collapse affect quality diversity generated samples address issues propose novel self adversarial learning sal paradigm improving gans performance text generation contrast standard gans use binary classifier discriminator predict whether sample real generated sal employs comparative discriminator pairwise classifier comparing text quality pair samples training sal rewards generator currently generated sentence found better previously generated samples self improvement reward mechanism allows model receive credits easily avoid collapsing towards limited number real samples helps alleviate reward sparsity issue also reduces risk mode collapse experiments text generation benchmark datasets show proposed approach substantially improves quality diversity yields stable performance compared previous gans text generation\n",
            "output sentence:  propose self adversarial learning sal paradigm improves generator self play fashion improving gans performance text generation \n",
            "\n",
            "{'rouge-1': {'r': 0.06, 'p': 0.5454545454545454, 'f': 0.10810810632253876}, 'rouge-2': {'r': 0.024793388429752067, 'p': 0.3, 'f': 0.045801525307383066}, 'rouge-l': {'r': 0.06, 'p': 0.5454545454545454, 'f': 0.10810810632253876}}\n",
            "pair:  recurrent neural networks rnns shown excellent performance processing sequence data however complex memory intensive due recursive nature limitations make rnns difficult embed mobile devices requiring real time processes limited hardware resources address issues introduce method learn binary ternary weights training phase facilitate hardware implementations rnns result using approach replaces multiply accumulate operations simple accumulations bringing significant benefits custom hardware terms silicon area power consumption software side evaluate performance terms accuracy method using long short term memories lstms gated recurrent units grus various sequential models including sequence classification language modeling demonstrate method achieves competitive results aforementioned tasks using binary ternary weights runtime hardware side present custom hardware accelerating recurrent computations lstms binary ternary weights ultimately show lstms binary ternary weights achieve memory saving inference speedup compared full precision hardware implementation design\n",
            "output sentence:  propose high performance lstms binary ternary weights greatly reduce implementation complexity \n",
            "\n",
            "{'rouge-1': {'r': 0.06097560975609756, 'p': 0.7142857142857143, 'f': 0.11235954911248583}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.3333333333333333, 'f': 0.03999999887200004}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.7142857142857143, 'f': 0.11235954911248583}}\n",
            "pair:  generative adversarial networks gans proven powerful framework learning draw samples complex distributions however gans also notoriously difficult train mode collapse oscillations common problem hypothesize least part due evolution generator distribution catastrophic forgetting tendency neural networks leads discriminator losing ability remember synthesized samples previous instantiations generator recognizing contributions twofold first show gan training makes interesting realistic benchmark continual learning methods evaluation canonical datasets second propose leveraging continual learning techniques augment discriminator preserving ability recognize previous generator samples show resulting methods add light amount computation involve minimal changes model result better overall performance examined image text generation tasks\n",
            "output sentence:  generative adversarial network training continual learning problem \n",
            "\n",
            "{'rouge-1': {'r': 0.08256880733944955, 'p': 0.6, 'f': 0.14516128819588972}, 'rouge-2': {'r': 0.032520325203252036, 'p': 0.26666666666666666, 'f': 0.05797101255513554}, 'rouge-l': {'r': 0.045871559633027525, 'p': 0.3333333333333333, 'f': 0.0806451591636317}}\n",
            "pair:  one main challenges applying graph convolutional neural networks gene interaction data lack understanding vector space belong also inherent difficulties involved representing interactions significantly lower dimension viz euclidean spaces challenge becomes prevalent dealing various types heterogeneous data introduce systematic generalized method called isom gsn used transform multi omic data higher dimensions onto two dimensional grid afterwards apply convolutional neural network predict disease states various types based idea kohonen self organizing map generate two dimensional grid sample given set genes represent gene similarity network tested model predict breast prostate cancer using gene expression dna methylation copy number alteration yielding prediction accuracies range tumor stages breast cancer calculated gleason scores prostate cancer input genes cases scheme outputs nearly perfect classification accuracy also provides enhanced scheme representation learning visualization dimensionality reduction interpretation results\n",
            "output sentence:  paper presents deep learning model combines self organizing maps convolutional neural networks representation learning multi omics data \n",
            "\n",
            "{'rouge-1': {'r': 0.056818181818181816, 'p': 0.625, 'f': 0.1041666651388889}, 'rouge-2': {'r': 0.03418803418803419, 'p': 0.5714285714285714, 'f': 0.06451612796696152}, 'rouge-l': {'r': 0.056818181818181816, 'p': 0.625, 'f': 0.1041666651388889}}\n",
            "pair:  weight initialization activation function deep neural networks crucial impact performance training procedure inappropriate selection lead loss information input forward propagation exponential vanishing exploding gradients back propagation understanding theoretical properties untrained random networks key identifying deep networks may trained successfully recently demonstrated schoenholz et al showed deep feedforward neural networks specific choice hyperparameters known edge chaos lead good performance complete analysis providing quantitative results showing class relu like activation functions information propagates indeed deeper initialization edge chaos extending analysis identify class activation functions improve information propagation relu like functions class includes swish activation phi swish cdot text sigmoid used hendrycks gimpel elfwing et al ramachandran et al provides theoretical grounding excellent empirical performance phi swish observed contributions complement previous results illustrating benefit using random initialization edge chaos context\n",
            "output sentence:  effectively choose initialization activation function deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.1348314606741573, 'p': 0.8, 'f': 0.23076922830066568}, 'rouge-2': {'r': 0.037383177570093455, 'p': 0.2857142857142857, 'f': 0.06611570043303058}, 'rouge-l': {'r': 0.0898876404494382, 'p': 0.5333333333333333, 'f': 0.15384615137758878}}\n",
            "pair:  growing number learning methods actually differentiable games whose players optimise multiple interdependent objectives parallel gans intrinsic curiosity multi agent rl opponent shaping powerful approach improve learning dynamics games accounting player influence others updates learning opponent learning awareness lola recent algorithm exploits response leads cooperation settings like iterated prisoner dilemma although experimentally successful show lola agents exhibit arrogant behaviour directly odds convergence fact remarkably algorithms theoretical guarantees applying across player non convex games paper present stable opponent shaping sos new method interpolates lola stable variant named lookahead prove lookahead converges locally equilibria avoids strict saddles differentiable games sos inherits essential guarantees also shaping learning opponents consistently either matching outperforming lola experimentally\n",
            "output sentence:  opponent shaping powerful approach multi agent learning prevent convergence sos algorithm fixes strong guarantees differentiable games \n",
            "\n",
            "{'rouge-1': {'r': 0.3902439024390244, 'p': 0.9411764705882353, 'f': 0.5517241337871582}, 'rouge-2': {'r': 0.3181818181818182, 'p': 0.875, 'f': 0.4666666627555556}, 'rouge-l': {'r': 0.36585365853658536, 'p': 0.8823529411764706, 'f': 0.5172413751664686}}\n",
            "pair:  paper introduces agent makes efficient use demonstrations solve hard exploration problems partially observable environments highly variable initial conditions also introduce suite eight tasks combine three properties show solve several tasks state art methods without demonstrations fail see even single successful trajectory tens billions steps exploration\n",
            "output sentence:  introduce agent makes efficient use demonstrations solve hard exploration problems partially observable environments highly variable initial conditions \n",
            "\n",
            "{'rouge-1': {'r': 0.175, 'p': 0.875, 'f': 0.2916666638888889}, 'rouge-2': {'r': 0.11711711711711711, 'p': 0.7222222222222222, 'f': 0.20155038519560123}, 'rouge-l': {'r': 0.15, 'p': 0.75, 'f': 0.24999999722222221}}\n",
            "pair:  adversarial training demonstrated one effective methods training robust models defend adversarial examples however adversarially trained models often lack adversarially robust generalization unseen testing data recent works show adversarially trained models biased towards global structure features instead work would like investigate relationship generalization adversarial training robust local features robust local features generalize well unseen shape variation learn robust local features develop random block shuffle rbs transformation break global structure features normal adversarial examples continue propose new approach called robust local features adversarial training rlfat first learns robust local features adversarial training rbs transformed adversarial examples transfers robust local features training normal adversarial examples demonstrate generality argument implement rlfat currently state art adversarial training frameworks extensive experiments stl cifar cifar show rlfat significantly improves adversarially robust generalization standard generalization adversarial training additionally demonstrate models capture local features object images aligning better human perception\n",
            "output sentence:  propose new stream adversarial training approach called robust local features adversarial training rlfat significantly improves adversarially robust generalization generalization generalization generalization generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.08247422680412371, 'p': 0.8888888888888888, 'f': 0.1509433946724813}, 'rouge-2': {'r': 0.03508771929824561, 'p': 0.5, 'f': 0.06557376926632627}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.4444444444444444, 'f': 0.07547169655927378}}\n",
            "pair:  ever increasing demand resultant reduced quality services focus shifted towards easing network congestion enable efficient flow systems like traffic supply chains electrical grids step direction imagine traditional heuristics based training systems approach incapable modelling involved dynamics one apply multi agent reinforcement learning marl problems considering vertex network agent marl based models assume agents independent many real world tasks agents need behave group rather collection individuals paper propose framework induces cooperation coordination amongst agents connected via underlying network using emergent communication marl based setup formulate problem general network setting demonstrate utility communication networks help case study traffic systems furthermore study emergent communication protocol show formation distinct communities grounded vocabulary best knowledge work studies emergent language networked marl setting\n",
            "output sentence:  framework studying emergent communication networked multi agent reinforcement learning setup \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.4166666666666667, 'f': 0.1818181784066116}, 'rouge-2': {'r': 0.0392156862745098, 'p': 0.16666666666666666, 'f': 0.06349206040816341}, 'rouge-l': {'r': 0.09302325581395349, 'p': 0.3333333333333333, 'f': 0.14545454204297528}}\n",
            "pair:  introduce simple efficient algorithms computing minhash probability distribution suitable sparse dense data equivalent running times state art cases collision probability algorithms new measure similarity positive vectors investigate detail describe sense collision probability optimal locality sensitive hash based sampling argue similarity measure useful probability distributions similarity pursued algorithms weighted minhash natural generalization jaccard index\n",
            "output sentence:  minimum set exponentially distributed hashes useful collision probability generalizes jaccard index probability distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.4166666666666667, 'f': 0.12987012723899483}, 'rouge-2': {'r': 0.012987012987012988, 'p': 0.09090909090909091, 'f': 0.02272727053977294}, 'rouge-l': {'r': 0.06153846153846154, 'p': 0.3333333333333333, 'f': 0.10389610126496887}}\n",
            "pair:  ability generalize quickly observations crucial intelligent systems paper introduce apl algorithm approximates probability distributions remembering surprising observations encountered past observations recalled external memory module processed decoder network combine information different memory slots generalize beyond direct recall show algorithm perform well state art baselines shot classification benchmarks smaller memory footprint addition memory compression allows scale thousands unknown labels finally introduce meta learning reasoning task challenging direct classification setting apl able generalize fewer one example per class via deductive reasoning\n",
            "output sentence:  introduce model generalizes quickly observations storing surprising information attending relevant data time time \n",
            "\n",
            "{'rouge-1': {'r': 0.10526315789473684, 'p': 0.7692307692307693, 'f': 0.1851851830675583}, 'rouge-2': {'r': 0.05084745762711865, 'p': 0.46153846153846156, 'f': 0.09160305164733994}, 'rouge-l': {'r': 0.08421052631578947, 'p': 0.6153846153846154, 'f': 0.1481481460305213}}\n",
            "pair:  high intra class diversity inter class similarity characteristic remote sensing scene image data sets currently posing significant difficulty deep learning algorithms classification tasks improve accuracy post classification methods proposed smoothing results model predictions however approaches require additional neural network perform smoothing operation adds overhead task propose approach involves learning deep features directly neighboring scene images without requiring use cleanup model approach utilizes siamese network improve discriminative power convolutional neural networks pair neighboring scene images exploits semantic coherence pair enrich feature vector image want predict label empirical results show approach provides viable alternative existing methods example model improved prediction accuracy percentage point dropped mean squared error value baseline disease density estimation task performance gains comparable results existing post classification methods moreover without implementation overheads\n",
            "output sentence:  approach improving prediction accuracy learning deep features neighboring scene images satellite scene image analysis \n",
            "\n",
            "{'rouge-1': {'r': 0.057971014492753624, 'p': 1.0, 'f': 0.1095890400600488}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.057971014492753624, 'p': 1.0, 'f': 0.1095890400600488}}\n",
            "pair:  reparameterization trick become one useful tools field variational inference however reparameterization trick based standardization transformation restricts scope application method distributions tractable inverse cumulative distribution functions expressible deterministic transformations distributions paper generalized reparameterization trick allowing general transformation discover proposed model special case control variate indicating proposed model combine advantages cv generalized reparameterization based proposed gradient model propose new polynomial based gradient estimator better theoretical performance reparameterization trick certain condition applied larger class variational distributions studies synthetic real data show proposed gradient estimator significantly lower gradient variance state art methods thus enabling faster inference procedure\n",
            "output sentence:  generalized transformation generalized transformation based variational model \n",
            "\n",
            "{'rouge-1': {'r': 0.12941176470588237, 'p': 0.8461538461538461, 'f': 0.22448979361724286}, 'rouge-2': {'r': 0.0660377358490566, 'p': 0.5833333333333334, 'f': 0.11864406596954902}, 'rouge-l': {'r': 0.08235294117647059, 'p': 0.5384615384615384, 'f': 0.14285714055601834}}\n",
            "pair:  despite advances deep learning artificial neural networks learn way humans today neural networks learn multiple tasks trained jointly cannot maintain performance learnt tasks tasks presented one time phenomenon called catastrophic forgetting fundamental challenge overcome neural networks learn continually incoming data work derive inspiration human memory develop architecture capable learning continuously sequentially incoming tasks averting catastrophic forgetting specifically model consists dual memory architecture emulate complementary learning systems hippocampus neocortex human brain maintains consolidated long term memory via generative replay past experiences substantiate claim replay generative ii show benefits generative replay dual memory via experiments iii demonstrate improved performance retention even small models low capacity architecture displays many important characteristics human memory provides insights connection sleep learning humans\n",
            "output sentence:  dual memory architecture inspired human brain learn sequentially incoming tasks averting catastrophic forgetting \n",
            "\n",
            "{'rouge-1': {'r': 0.07017543859649122, 'p': 0.3076923076923077, 'f': 0.11428571126122457}, 'rouge-2': {'r': 0.016129032258064516, 'p': 0.08333333333333333, 'f': 0.027027024309715396}, 'rouge-l': {'r': 0.07017543859649122, 'p': 0.3076923076923077, 'f': 0.11428571126122457}}\n",
            "pair:  paper show novel transfer reinforcement learning techniques applied complex task target driven navigation using photorealisticai thor simulator specifically build concept universal successorfeatures agent introduce novel architectural contribution successor feature dependent policy sfdp adopt concept variationalinformation bottlenecks achieve state art performance vusfa final architecture straightforward approach implemented using open source repository approach generalizable showed greater stability training outperformed recent approaches terms transfer learning ability\n",
            "output sentence:  present improved version universal successor features based drl method improve transfer learning agents \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.5, 'f': 0.1408450680023805}, 'rouge-2': {'r': 0.039473684210526314, 'p': 0.3, 'f': 0.06976743980530022}, 'rouge-l': {'r': 0.08196721311475409, 'p': 0.5, 'f': 0.1408450680023805}}\n",
            "pair:  propose novel unsupervised generative model elastic infogan learns disentangle object identity low level aspects class imbalanced datasets first investigate issues surrounding assumptions uniformity made infogan demonstrate ineffectiveness properly disentangle object identity imbalanced data key idea make discovery discrete latent factor variation invariant identity preserving transformations real images use signal learn latent distribution parameters experiments artificial mnist real world youtube faces datasets demonstrate effectiveness approach imbalanced data better disentanglement object identity latent factor variation ii better approximation class imbalance data reflected learned parameters latent distribution\n",
            "output sentence:  elastic infogan modification infogan learns without supervision disentangled representations class imbalanced data \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.9, 'f': 0.2195121929803689}, 'rouge-2': {'r': 0.022727272727272728, 'p': 0.2222222222222222, 'f': 0.04123711171856739}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.7, 'f': 0.1707317051754908}}\n",
            "pair:  many irregular domains social networks financial transactions neuron connections natural language structures represented graphs recent years variety graph neural networks gnns successfully applied representation learning prediction graphs however many applications underlying graph changes time existing gnns inadequate handling dynamic graphs paper propose novel technique learning embeddings dynamic graphs based tensor algebra framework method extends popular graph convolutional network gcn learning representations dynamic graphs using recently proposed tensor product technique theoretical results establish connection proposed tensor approach spectral convolution tensors developed numerical experiments real datasets demonstrate usefulness proposed method edge classification task dynamic graphs\n",
            "output sentence:  propose novel tensor based method graph convolutional networks dynamic graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.5, 'f': 0.09090908925619837}, 'rouge-2': {'r': 0.010752688172043012, 'p': 0.14285714285714285, 'f': 0.019999998698000086}, 'rouge-l': {'r': 0.05, 'p': 0.5, 'f': 0.09090908925619837}}\n",
            "pair:  graph neural networks gnns prediction tasks like node classification edge prediction received increasing attention recent machine learning graphically structured data however large quantity labeled graphs difficult obtain significantly limit true success gnns although active learning widely studied addressing label sparse issues data types like text images etc make effective graphs open question research paper present investigation active learning gnns node classification tasks specifically propose new method uses node feature propagation followed medoids clustering nodes instance selection active learning theoretical bound analysis justify design choice approach experiments four benchmark dataset proposed method outperforms representative baseline methods consistently significantly\n",
            "output sentence:  paper introduces clustering based active learning algorithm graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.109375, 'p': 0.6363636363636364, 'f': 0.18666666416355557}, 'rouge-2': {'r': 0.013888888888888888, 'p': 0.1, 'f': 0.024390241760856822}, 'rouge-l': {'r': 0.078125, 'p': 0.45454545454545453, 'f': 0.13333333083022222}}\n",
            "pair:  obtaining high quality uncertainty estimates essential many applications deep neural networks paper theoretically justify scheme estimating uncertainties based sampling prior distribution crucially uncertainty estimates shown conservative sense never underestimate posterior uncertainty obtained hypothetical bayesian algorithm also show concentration implying uncertainty estimates converge zero get data uncertainty estimates obtained random priors adapted deep network architecture trained using standard supervised learning pipelines provide experimental evaluation random priors calibration distribution detection typical computer vision tasks demonstrating outperform deep ensembles practice\n",
            "output sentence:  provide theoretical support uncertainty estimates deep learning obtained fitting random priors \n",
            "\n",
            "{'rouge-1': {'r': 0.13253012048192772, 'p': 0.6470588235294118, 'f': 0.21999999717800003}, 'rouge-2': {'r': 0.038834951456310676, 'p': 0.21052631578947367, 'f': 0.06557376786213394}, 'rouge-l': {'r': 0.060240963855421686, 'p': 0.29411764705882354, 'f': 0.09999999717800007}}\n",
            "pair:  positive unlabeled pu learning addresses problem learning binary classifier positive unlabeled data often applied situations negative data difficult fully labeled however collecting non representative set contains small portion possible data much easier many practical situations paper studies novel classification framework incorporates biased bn data pu learning fact training data biased also makes work different standard semi supervised learning provide empirical risk minimization based method address pubn classification problem approach regarded variant traditional example reweighting algorithms weight example computed preliminary step draws inspiration pu learning also derive estimation error bound proposed method experimental results demonstrate effectiveness algorithm pubn learning scenarios also ordinary pu leaning scenarios several benchmark datasets\n",
            "output sentence:  paper studied pubn classification problem incorporate biased negative bn data negative data fully representative true underlying negative distribution positive unlabeled \n",
            "\n",
            "{'rouge-1': {'r': 0.18181818181818182, 'p': 0.9333333333333333, 'f': 0.3043478233577505}, 'rouge-2': {'r': 0.12264150943396226, 'p': 0.9285714285714286, 'f': 0.21666666460555556}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.9333333333333333, 'f': 0.3043478233577505}}\n",
            "pair:  one challenges training generative models variational auto encoder vae avoiding posterior collapse generator much capacity prone ignoring latent code problem exacerbated dataset small latent dimension high root problem elbo objective specifically kullback leibler kl divergence term objective function paper proposes new objective function replace kl term one emulates maximum mean discrepancy mmd objective also introduces new technique named latent clipping used control distance samples latent space probabilistic autoencoder model named mu vae designed trained mnist mnist fashion datasets using new objective function shown outperform models trained elbo beta vae objective mu vae less prone posterior collapse generate reconstructions new samples good quality latent representations learned mu vae shown good used downstream tasks classification\n",
            "output sentence:  paper proposes new objective function replace kl term one emulates maximum mean discrepancy mmd objective \n",
            "\n",
            "{'rouge-1': {'r': 0.2037037037037037, 'p': 0.8461538461538461, 'f': 0.3283582058275786}, 'rouge-2': {'r': 0.12121212121212122, 'p': 0.6153846153846154, 'f': 0.2025316428200609}, 'rouge-l': {'r': 0.14814814814814814, 'p': 0.6153846153846154, 'f': 0.23880596702160842}}\n",
            "pair:  wide range defenses proposed harden neural networks adversarial attacks however pattern emerged majority adversarial defenses quickly broken new attacks given lack success generating robust defenses led ask fundamental question adversarial attacks inevitable paper analyzes adversarial examples theoretical perspective identifies fundamental bounds susceptibility classifier adversarial attacks show certain classes problems adversarial examples inescapable using experiments explore implications theoretical guarantees real world problems discuss factors dimensionality image complexity limit classifier robustness adversarial examples\n",
            "output sentence:  paper identifies classes problems adversarial examples inescapable derives fundamental bounds susceptibility classifier adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.04477611940298507, 'p': 0.5, 'f': 0.08219177931319198}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04477611940298507, 'p': 0.5, 'f': 0.08219177931319198}}\n",
            "pair:  robustness verification aims formally certify prediction behavior neural networks become important tool understanding behavior given model obtaining safety guarantees however previous methods usually limited relatively simple neural networks paper consider robustness verification problem transformers transformers complex self attention layers pose many challenges verification including cross nonlinearity cross position dependency discussed previous work resolve challenges develop first verification algorithm transformers certified robustness bounds computed method significantly tighter naive interval bound propagation bounds also shed light interpreting transformers consistently reflect importance words sentiment analysis\n",
            "output sentence:  propose first algorithm verifying robustness transformers \n",
            "\n",
            "{'rouge-1': {'r': 0.18867924528301888, 'p': 0.625, 'f': 0.2898550689014913}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.2, 'f': 0.08333333003472235}, 'rouge-l': {'r': 0.11320754716981132, 'p': 0.375, 'f': 0.17391303991598414}}\n",
            "pair:  work provides theoretical empirical evidence invariance inducing regularizers increase predictive accuracy worst case spatial transformations spatial robustness evaluated adversarially transformed examples demonstrate adding regularization top standard adversarial training reduces relative error cifar without increasing computational cost outperforms handcrafted networks explicitly designed spatial equivariant furthermore observe svhn known inherent variance orientation robust training also improves standard accuracy test set\n",
            "output sentence:  spatial transformations robust minimizer also minimizes standard accuracy invariance inducing regularization leads better robustness specialized architectures \n",
            "\n",
            "{'rouge-1': {'r': 0.0891089108910891, 'p': 0.6, 'f': 0.1551724115413199}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04950495049504951, 'p': 0.3333333333333333, 'f': 0.0862068942999406}}\n",
            "pair:  skip connections increasingly utilized deep neural networks improve accuracy cost efficiency particular recent densenet efficient computation parameters achieves state art predictions directly connecting feature layer previous ones however densenet extreme connectivity pattern may hinder scalability high depths applications like fully convolutional networks full densenet connections prohibitively expensive work first experimentally shows one key advantage skip connections short distances among feature layers backpropagation specifically using fixed number skip connections connection patterns shorter backpropagation distance among layers accurate predictions following insight propose connection template log densenet comparison densenet slightly increases backpropagation distances among layers log uses log total connections instead hence logdenses easier scale densenets longer require careful gpu memory management demonstrate effectiveness design principle showing better performance densenets tabula rasa semantic segmentation competitive results visual recognition\n",
            "output sentence:  show shortcut connections placed patterns minimize layer distances backpropagation design networks achieve log distances using log connections \n",
            "\n",
            "{'rouge-1': {'r': 0.2857142857142857, 'p': 0.9230769230769231, 'f': 0.43636363275371903}, 'rouge-2': {'r': 0.24444444444444444, 'p': 0.9166666666666666, 'f': 0.3859649089566021}, 'rouge-l': {'r': 0.2857142857142857, 'p': 0.9230769230769231, 'f': 0.43636363275371903}}\n",
            "pair:  propose new anytime neural network allows partial evaluation subnetworks different widths well depths compared conventional anytime networks depth controllability increased architectural diversity leads higher resource utilization consequent performance improvement various dynamic resource budgets highlight architectural features make scheme feasible well efficient show effectiveness image classification tasks\n",
            "output sentence:  propose new anytime neural network allows partial evaluation subnetworks different widths well depths \n",
            "\n",
            "{'rouge-1': {'r': 0.10909090909090909, 'p': 0.5, 'f': 0.17910447467141904}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.18181818181818182, 'f': 0.056338025550486136}, 'rouge-l': {'r': 0.05454545454545454, 'p': 0.25, 'f': 0.08955223586544897}}\n",
            "pair:  paper proposes self supervised learning approach video features results significantly improved performance downstream tasks video classification captioning segmentation compared existing methods method extends bert model text sequences case sequences real valued feature vectors replacing softmax loss noise contrastive estimation nce also show learn representations sequences visual features sequences words derived asr automatic speech recognition show cross modal training possible helps even\n",
            "output sentence:  generalized bert continuous cross modal inputs state art self supervised video representations \n",
            "\n",
            "{'rouge-1': {'r': 0.1794871794871795, 'p': 0.6363636363636364, 'f': 0.279999996568}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.23809523809523808, 'f': 0.09433961946422224}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.5909090909090909, 'f': 0.25999999656800005}}\n",
            "pair:  vanilla rnn relu activation simple structure amenable systematic dynamical systems analysis interpretation suffer exploding vs vanishing gradients problem recent attempts retain simplicity alleviating gradient problem based proper initialization schemes orthogonality unitary constraints rnn recurrency matrix however comes limitations expressive power regards dynamical systems phenomena like chaos multi stability instead suggest regularization scheme pushes part rnn latent subspace toward line attractor configuration enables long short term memory arbitrarily slow time scales show approach excels number benchmarks like sequential mnist multiplication problems enables reconstruction dynamical systems harbor widely different time scales\n",
            "output sentence:  develop new optimization approach vanilla relu based rnn enables long short term memory identification arbitrary nonlinear dynamical systems widely differing time scales \n",
            "\n",
            "{'rouge-1': {'r': 0.07462686567164178, 'p': 0.7142857142857143, 'f': 0.13513513342220598}, 'rouge-2': {'r': 0.012987012987012988, 'p': 0.16666666666666666, 'f': 0.024096384200900062}, 'rouge-l': {'r': 0.04477611940298507, 'p': 0.42857142857142855, 'f': 0.08108107936815197}}\n",
            "pair:  misspecified settings posterior distribution bayesian statistics may lead inconsistent estimates fix issue suggested replace likelihood pseudo likelihood exponential loss function enjoying suitable robustness properties paper build pseudo likelihood based maximum mean discrepancy defined via embedding probability distributions reproducing kernel hilbert space show mmd bayes posterior consistent robust model misspecification posterior obtained way might intractable also prove reasonable variational approximations posterior enjoy properties provide details stochastic gradient algorithm compute variational approximations numerical simulations indeed suggest estimator robust misspecification ones based likelihood\n",
            "output sentence:  robust bayesian estimation via maximum mean discrepancy \n",
            "\n",
            "{'rouge-1': {'r': 0.12698412698412698, 'p': 0.8, 'f': 0.21917807982735973}, 'rouge-2': {'r': 0.0410958904109589, 'p': 0.3333333333333333, 'f': 0.07317072975312319}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.6, 'f': 0.16438355927941453}}\n",
            "pair:  success reinforcement learning real world limited instrumented laboratory scenarios often requiring arduous human supervision enable continuous learning work discuss required elements robotic system continually autonomously improve data collected real world propose particular instantiation system subsequently investigate number challenges learning without instrumentation including lack episodic resets state estimation hand engineered rewards propose simple scalable solutions challenges demonstrate efficacy proposed system dexterous robotic manipulation tasks simulation real world also provide insightful analysis ablation study challenges associated learning paradigm\n",
            "output sentence:  system learn robotic tasks real world reinforcement learning without instrumentation \n",
            "\n",
            "{'rouge-1': {'r': 0.04285714285714286, 'p': 0.6, 'f': 0.07999999875555558}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.25, 'f': 0.0235294108678201}, 'rouge-l': {'r': 0.04285714285714286, 'p': 0.6, 'f': 0.07999999875555558}}\n",
            "pair:  recurrent neural network rnn effective neural network solving complex supervised unsupervised tasks significant improvement rnn field natural language processing speech processing computer vision multiple domains paper deals rnn application different use cases like incident detection fraud detection android malware classification best performing neural network architecture chosen conducting different chain experiments different network parameters structures network run epochs learning rate set range obviously rnn performed well compared classical machine learning algorithms mainly possible rnns implicitly extracts underlying features also identifies characteristics data lead better accuracy\n",
            "output sentence:  recurrent neural networks cybersecurity use cases \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5714285714285714, 'f': 0.1951219483878644}, 'rouge-2': {'r': 0.0375, 'p': 0.23076923076923078, 'f': 0.06451612662735585}, 'rouge-l': {'r': 0.08823529411764706, 'p': 0.42857142857142855, 'f': 0.14634146058298636}}\n",
            "pair:  likelihood based generative models promising resource detect distribution ood inputs could compromise robustness reliability machine learning system however likelihoods derived models shown problematic detecting certain types inputs significantly differ training data paper pose problem due excessive influence input complexity generative models likelihoods report set experiments supporting hypothesis use estimate input complexity derive efficient parameter free ood score seen likelihood ratio akin bayesian model comparison find score perform comparably even better existing ood detection approaches wide range data sets models model sizes complexity estimates\n",
            "output sentence:  pose generative models likelihoods excessively influenced input complexity propose way compensate detecting distribution inputs \n",
            "\n",
            "{'rouge-1': {'r': 0.08928571428571429, 'p': 0.5, 'f': 0.15151514894398535}, 'rouge-2': {'r': 0.015873015873015872, 'p': 0.1111111111111111, 'f': 0.027777775590277953}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.4, 'f': 0.12121211864095506}}\n",
            "pair:  great progress made making neural networks effective across wide range tasks many surprisingly vulnerable small carefully chosen perturbations input known adversarial examples paper advocate experimentally investigate use logit regularization techniques adversarial defense used conjunction methods creating adversarial robustness little cost demonstrate much effectiveness one recent adversarial defense mechanism attributed logit regularization show improve defense white box black box attacks process creating stronger black box attacks pgd based models\n",
            "output sentence:  logit regularization methods help explain improve state art adversarial defenses \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 1.0, 'f': 0.33333333055555564}, 'rouge-2': {'r': 0.14925373134328357, 'p': 1.0, 'f': 0.25974025748018215}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.9090909090909091, 'f': 0.3030303002525253}}\n",
            "pair:  neural networks trained backpropagation standard algorithm deep learning uses weight transport easily fooled existing gradient based adversarial attacks class attacks based certain small perturbations inputs make networks misclassify show less biologically implausible deep neural networks trained feedback alignment use weight transport harder fool providing actual robustness tested mnist deep neural networks trained without weight transport adversarial accuracy compared neural networks trained backpropagation generate non transferable adversarial examples however gap decreases cifar still significant particularly small perturbation magnitude less\n",
            "output sentence:  less biologically implausible deep neural networks trained without weight transport harder fool \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.625, 'f': 0.14492753418189458}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03278688524590164, 'p': 0.25, 'f': 0.0579710124427642}}\n",
            "pair:  current literature machine learning holds unaligned self interested agents learn use emergent communication channel introduce new sender receiver game study emergent communication spectrum partially competitive scenarios put special care evaluation find communication indeed emerge partially competitive scenarios discover three things tied improving first selfish communication proportional cooperation naturally occurs situations cooperative competitive second stability performance improved using lola foerster et al especially competitive scenarios third discrete protocols lend better learning cooperative communication continuous ones\n",
            "output sentence:  manage emerge communication selfish agents contrary current view ml \n",
            "\n",
            "{'rouge-1': {'r': 0.08163265306122448, 'p': 0.4444444444444444, 'f': 0.13793103186087993}, 'rouge-2': {'r': 0.03571428571428571, 'p': 0.23529411764705882, 'f': 0.06201550158764506}, 'rouge-l': {'r': 0.061224489795918366, 'p': 0.3333333333333333, 'f': 0.1034482732401903}}\n",
            "pair:  designing rewards reinforcement learning rl challenging needs convey desired task efficient optimize easy compute latter particularly problematic applying rl robotics detecting whether desired configuration reached might require considerable supervision instrumentation furthermore often interested able reach wide range configurations hence setting different reward every time might unpractical methods like hindsight experience replay recently shown promise learn policies able reach many goals without need reward unfortunately without tricks like resetting points along trajectory might take long time discover reach certain areas state space work investigate different approaches incorporate demonstrations drastically speed convergence policy able reach goal also surpassing performance agent trained imitation learning algorithms furthermore method used trajectories without expert actions available leverage kinestetic third person demonstration\n",
            "output sentence:  tackle goal conditioned tasks combining hindsight experience replay imitation learning algorithms showing faster convergence first higher final performance \n",
            "\n",
            "{'rouge-1': {'r': 0.13432835820895522, 'p': 0.6, 'f': 0.21951219213265916}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.2, 'f': 0.06122448720324875}, 'rouge-l': {'r': 0.1044776119402985, 'p': 0.4666666666666667, 'f': 0.17073170432778112}}\n",
            "pair:  generative models variational auto encoders vaes generative adversarial networks gans typically trained fixed prior distribution latent space uniform gaussian trained model obtained one sample generator various forms exploration understanding interpolating two samples sampling vicinity sample exploring differences pair samples applied third sample paper show latent space operations used literature far induce distribution mismatch resulting outputs prior distribution model trained address propose use distribution matching transport maps ensure latent space operations preserve prior distribution minimally modifying original operation experimental results validate proposed operations give higher quality samples compared original operations\n",
            "output sentence:  operations gan latent space induce distribution mismatch compared training distribution address using optimal transport match distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.11827956989247312, 'p': 0.6875, 'f': 0.2018348598804815}, 'rouge-2': {'r': 0.043859649122807015, 'p': 0.3125, 'f': 0.0769230747644971}, 'rouge-l': {'r': 0.07526881720430108, 'p': 0.4375, 'f': 0.12844036446763743}}\n",
            "pair:  typical sequence prediction problems language generation maximum likelihood estimation mle commonly adopted encourages predicted sequence consistent ground truth sequence highest probability occurring however mle focuses matching predicted sequence gold standard consequently treating incorrect predictions equally incorrect refer drawback negative diversity ignorance paper treating incorrect predictions equal unfairly downplays nuance sequences detailed token wise structure counteract augment mle loss introducing extra kullback leibler divergence term derived comparing data dependent gaussian prior detailed training prediction proposed data dependent gaussian prior objective gpo defined prior topological order tokens poles apart data independent gaussian prior regularization commonly adopted smoothing training mle experimental results show proposed method makes effective use detailed prior data improved performance typical language generation tasks including supervised unsupervised machine translation text summarization storytelling image captioning\n",
            "output sentence:  introduce extra data dependent gaussian prior objective augment current mle training designed capture prior knowledge ground truth data \n",
            "\n",
            "{'rouge-1': {'r': 0.10752688172043011, 'p': 0.7692307692307693, 'f': 0.18867924313100748}, 'rouge-2': {'r': 0.04424778761061947, 'p': 0.4166666666666667, 'f': 0.07999999826432003}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.46153846153846156, 'f': 0.11320754501779993}}\n",
            "pair:  generative adversarial nets gans widely used learn data sampling process performance may heavily depend loss functions given limited computational budget study revisits mmd gan uses maximum mean discrepancy mmd loss function gan makes two contributions first argue existing mmd loss function may discourage learning fine details data attempts contract discriminator outputs real data address issue propose repulsive loss function actively learn difference among real data simply rearranging terms mmd second inspired hinge loss propose bounded gaussian kernel stabilize training mmd gan repulsive loss function proposed methods applied unsupervised image generation tasks cifar stl celeba lsun bedroom datasets results show repulsive loss function significantly improves mmd loss additional computational cost outperforms representative loss functions proposed methods achieve fid score cifar dataset using single dcgan network spectral normalization\n",
            "output sentence:  rearranging terms maximum mean discrepancy yields much better loss function discriminator generative adversarial nets \n",
            "\n",
            "{'rouge-1': {'r': 0.13157894736842105, 'p': 0.7142857142857143, 'f': 0.22222221959506175}, 'rouge-2': {'r': 0.06060606060606061, 'p': 0.42857142857142855, 'f': 0.10619468809460417}, 'rouge-l': {'r': 0.10526315789473684, 'p': 0.5714285714285714, 'f': 0.17777777515061732}}\n",
            "pair:  explore idea compositional set embeddings used infer single class set classes associated input data image video audio signal useful example multi object detection images multi speaker diarization one shot learning audio particular devise implement two novel models consisting embedding function trained jointly composite function computes set union opera tions classes encoded two embedding vectors embedding trained jointly query function computes whether classes en coded one embedding subsume classes encoded another embedding contrast prior work models must perceive classes associated input examples also encode relationships different class label sets experiments conducted simulated data omniglot coco datasets proposed composite embedding models outperform baselines based traditional embedding approaches\n",
            "output sentence:  explored novel method compositional set embeddings perceive represent single class entire set classes associated input data \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.875, 'f': 0.16666666494331067}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.42857142857142855, 'f': 0.06185566876394944}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}}\n",
            "pair:  describe kernel rnn learning kernl reduced rank temporal eligibility trace based approximation backpropagation time bptt training recurrent neural networks rnns gives competitive performance bptt long time dependence tasks approximation replaces rank gradient learning tensor describes past hidden unit activations affect current state simple reduced rank product sensitivity weight temporal eligibility trace structured approximation motivated node perturbation sensitivity weights eligibility kernel time scales learned applying perturbations rule represents another step toward biologically plausible neurally inspired ml lower complexity terms relaxed architectural requirements symmetric return weights smaller memory demand unfolding storage states time shorter feedback time\n",
            "output sentence:  biologically plausible learning rule training recurrent neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.11428571428571428, 'p': 0.7272727272727273, 'f': 0.1975308618503277}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.1, 'f': 0.02197802002173668}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.45454545454545453, 'f': 0.12345678777625364}}\n",
            "pair:  introduce convolutional conditional neural process convcnp new member neural process family models translation equivariance data translation equivariance important inductive bias many learning problems including time series modelling spatial data images model embeds data sets infinite dimensional function space opposed finite dimensional vector spaces formalize notion extend theory neural representations sets include functional representations demonstrate translation equivariant embedding represented using convolutional deep set evaluate convcnps several settings demonstrating achieve state art performance compared existing nps demonstrate building translation equivariance enables zero shot generalization challenging domain tasks\n",
            "output sentence:  extend deep sets functional embeddings neural processes include translation equivariant members \n",
            "\n",
            "{'rouge-1': {'r': 0.16363636363636364, 'p': 0.9, 'f': 0.2769230743195267}, 'rouge-2': {'r': 0.11594202898550725, 'p': 0.8888888888888888, 'f': 0.20512820308678503}, 'rouge-l': {'r': 0.16363636363636364, 'p': 0.9, 'f': 0.2769230743195267}}\n",
            "pair:  adversaries neural networks drawn much attention since first debut existing methods aim deceiving image classification models misclassification crafting attacks specific object instances object setection tasks focus creating universal adversaries fool object detectors hide objects detectors adversaries examine universal three ways specific specific object instances image independent transfer different unknown models achieve propose two novel techniques improve transferability adversaries textit piling textit monochromatization techniques prove simplify patterns generated adversaries ultimately result higher transferability\n",
            "output sentence:  focus creating universal adversaries fool object detectors hide objects detectors \n",
            "\n",
            "{'rouge-1': {'r': 0.12643678160919541, 'p': 0.6111111111111112, 'f': 0.20952380668299328}, 'rouge-2': {'r': 0.0625, 'p': 0.3888888888888889, 'f': 0.10769230530650893}, 'rouge-l': {'r': 0.12643678160919541, 'p': 0.6111111111111112, 'f': 0.20952380668299328}}\n",
            "pair:  goal multi label learning mll associate given instance relevant labels set concepts previous works mll mainly focused setting concept set assumed fixed many real world applications require introducing new concepts set meet new demands one common need refine original coarse concepts split finer grained ones refinement process typically begins limited labeled data finer grained concepts address need propose special weakly supervised mll problem focuses situation limited fine grained supervision also leverages hierarchical relationship coarse concepts fine grained ones problem reduced multi label version negative unlabeled learning problem using hierarchical relationship tackle reduced problem meta learning approach learns assign pseudo labels unlabeled entries experimental results demonstrate proposed method able assign accurate pseudo labels turn achieves superior classification performance compared existing methods\n",
            "output sentence:  propose special weakly supervised multi label learning problem along newly tailored algorithm learns underlying classifier learning assign pseudo labels \n",
            "\n",
            "{'rouge-1': {'r': 0.1875, 'p': 0.9473684210526315, 'f': 0.31304347550245754}, 'rouge-2': {'r': 0.1328125, 'p': 0.9444444444444444, 'f': 0.2328767101670107}, 'rouge-l': {'r': 0.1875, 'p': 0.9473684210526315, 'f': 0.31304347550245754}}\n",
            "pair:  propose model able perform physical parameter estimation systems video differential equations governing scene dynamics known labeled states objects available existing physical scene understanding methods require either object state supervision integrate differentiable physics learn interpretable system parameters states address problem textit physics inverse graphics approach brings together vision inverse graphics differentiable physics engines objects explicit state velocity representations discovered model framework allows us perform long term extrapolative video prediction well vision based model predictive control approach significantly outperforms related unsupervised methods long term future frame prediction systems interacting objects ball spring body gravitational systems due ability build dynamics model inductive bias show value tight vision physics integration demonstrating data efficient learning vision actuated model based control pendulum system also show controller interpretability provides unique capabilities goal driven control physical reasoning zero data adaptation\n",
            "output sentence:  propose model able perform physical parameter estimation systems video differential equations governing scene dynamics known labeled states objects available \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.5882352941176471, 'f': 0.19417475452540298}, 'rouge-2': {'r': 0.05309734513274336, 'p': 0.3157894736842105, 'f': 0.09090908844467409}, 'rouge-l': {'r': 0.09302325581395349, 'p': 0.47058823529411764, 'f': 0.1553398030690923}}\n",
            "pair:  variety cooperative multi agent control problems require agents achieve individual goals contributing collective success multi goal multi agent setting poses difficulties recent algorithms primarily target settings single global reward due two new challenges efficient exploration learning individual goal attainment cooperation others success credit assignment interactions actions goals different agents address challenges restructure problem novel two stage curriculum single agent goal attainment learned prior learning multi agent cooperation derive new multi goal multi agent policy gradient credit function localized credit assignment use function augmentation scheme bridge value policy functions across curriculum complete architecture called cm learns significantly faster direct adaptations existing algorithms three challenging multi goal multi agent problems cooperative navigation difficult formations negotiating multi vehicle lane changes sumo traffic simulator strategic cooperation checkers environment\n",
            "output sentence:  modular method fully cooperative multi goal multi agent reinforcement learning based curriculum learning efficient exploration credit assignment action goal interactions \n",
            "\n",
            "{'rouge-1': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.16666666666666666, 'f': 0.04081632438150782}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}}\n",
            "pair:  driving force behind recent success lstms ability learn complex non linear relationships consequently inability describe relationships led lstms characterized black boxes end introduce contextual decomposition cd interpretation algorithm analysing individual predictions made standard lstms without changes underlying model decomposing output lstm cd captures contributions combinations words variables final prediction lstm task sentiment analysis yelp sst data sets show cd able reliably identify words phrases contrasting sentiment combined yield lstm final prediction using phrase level labels sst also demonstrate cd able successfully extract positive negative negations lstm something previously done\n",
            "output sentence:  introduce contextual decompositions interpretation algorithm lstms capable extracting word phrase interaction level importance \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.46153846153846156, 'f': 0.19672130812147276}, 'rouge-2': {'r': 0.0392156862745098, 'p': 0.16666666666666666, 'f': 0.06349206040816341}, 'rouge-l': {'r': 0.0625, 'p': 0.23076923076923078, 'f': 0.09836065238376793}}\n",
            "pair:  propose effective multitask learning setup reducing distant supervision noise leveraging sentence level supervision show sentence level supervision used improve encoding individual sentences learn input sentences likely express relationship pair entities also introduce novel neural architecture collecting signals multiple input sentences combines benefits attention maxpooling proposed method increases auc outperforms recently published results fb nyt dataset\n",
            "output sentence:  new form attention works well distant supervision setting multitask learning approach add level level \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.5555555555555556, 'f': 0.1298701278057008}, 'rouge-2': {'r': 0.02666666666666667, 'p': 0.25, 'f': 0.048192769342430025}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.4444444444444444, 'f': 0.10389610183167484}}\n",
            "pair:  human annotation syntactic parsing expensive large resources available fraction languages question ask whether one leverage abundant unlabeled texts improve syntactic parsers beyond using texts obtain generalisable lexical features beyond word embeddings end propose novel latent variable generative model semi supervised syntactic dependency parsing exact inference intractable introduce differentiable relaxation obtain approximate samples compute gradients respect parser parameters method differentiable perturb parse relies differentiable dynamic programming stochastically perturbed edge scores demonstrate effectiveness approach experiments english french swedish\n",
            "output sentence:  differentiable dynamic programming perturbed input weights application semi supervised vae \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.75, 'f': 0.13483145903800026}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.037037037037037035, 'p': 0.375, 'f': 0.06741572870092163}}\n",
            "pair:  separating mixed distributions long standing challenge machine learning signal processing applications include single channel multi speaker separation cocktail party problem singing voice separation separating reflections images current methods either rely making strong assumptions source distributions sparsity low rank repetitiveness rely training samples source mixture work tackle scenario extracting unobserved distribution additively mixed signal observed arbitrary distribution introduce new method neural egg separation iterative method learns separate known distribution progressively finer estimates unknown distribution settings neural egg separation initialization sensitive therefore introduce glo masking ensures good initialization extensive experiments show method outperforms current methods use level supervision often achieves similar performance full supervision\n",
            "output sentence:  iterative neural method extracting signals observed mixed signals \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.8571428571428571, 'f': 0.12765957308963333}, 'rouge-2': {'r': 0.0297029702970297, 'p': 0.5, 'f': 0.05607476529653246}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.8571428571428571, 'f': 0.12765957308963333}}\n",
            "pair:  deep neural networks achieved outstanding performance many real world applications expense huge computational resources densenet one recently proposed neural network architecture achieved state art performance many visual tasks however great redundancy due dense connections internal structure leads high computational costs training dense networks address issue design reinforcement learning framework search efficient densenet architectures layer wise pruning lwp different tasks retaining original advantages densenet feature reuse short paths etc framework agent evaluates importance connection two block layers prunes redundant connections addition novel reward shaping trick introduced make densenet reach better trade accuracy float point operations flops experiments show densenet lwp compact efficient existing alternatives\n",
            "output sentence:  learning search efficient densenet layer wise pruning \n",
            "\n",
            "{'rouge-1': {'r': 0.04597701149425287, 'p': 0.4, 'f': 0.08247422495483052}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.034482758620689655, 'p': 0.3, 'f': 0.06185566825379961}}\n",
            "pair:  living organisms struggle forces nature carve niches maintain relative stasis propose search order amidst chaos might offer unifying principle emergence useful behaviors artificial agents formalize idea unsupervised reinforcement learning method called surprise minimizing rl smirl smirl trains agent objective maximizing probability observed states model trained previously seen states resulting agents acquire several proactive behaviors seek maintain stable states balancing damage avoidance closely tied affordances environment prevailing sources entropy winds earthquakes agents demonstrate surprise minimizing agents successfully play tetris doom control humanoid avoid falls without task specific reward supervision nfurther show smirl used unsupervised pre training objective substantially accelerates subsequent reward driven learning\n",
            "output sentence:  learning emergent behavior minimizing bayesian surprise rl natural environments entropy \n",
            "\n",
            "{'rouge-1': {'r': 0.09722222222222222, 'p': 0.6363636363636364, 'f': 0.168674696495863}, 'rouge-2': {'r': 0.03488372093023256, 'p': 0.3, 'f': 0.06249999813368061}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.6363636363636364, 'f': 0.168674696495863}}\n",
            "pair:  relational reasoning ability model interactions relations objects valuable robust multi object tracking pivotal trajectory prediction paper propose mohart class agnostic end end multi object tracking trajectory prediction algorithm explicitly accounts permutation invariance relational reasoning explore number permutation invariant architectures show multi headed self attention outperforms provided baselines better accounts complex physical interactions challenging toy experiment show three real world tracking datasets adding relational reasoning capabilities way increases tracking trajectory prediction performance particularly presence ego motion occlusions crowded scenes faulty sensor inputs best knowledge mohart first fully end end multi object tracking vision approach applied real world data reported literature\n",
            "output sentence:  mohart uses self attention mechanism perform relational reasoning multi object tracking \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.3076923076923077, 'f': 0.09876542940405433}, 'rouge-2': {'r': 0.0125, 'p': 0.07692307692307693, 'f': 0.021505373939183992}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.3076923076923077, 'f': 0.09876542940405433}}\n",
            "pair:  present end end trained memory system quickly adapts new data generates samples like inspired kanerva sparse distributed memory robust distributed reading writing mechanism memory analytically tractable enables optimal line compression via bayesian update rule formulate hierarchical conditional generative model memory provides rich data dependent prior distribution consequently top memory bottom perception combined produce code representing observation empirically demonstrate adaptive memory significantly improves generative models trained omniglot cifar datasets compared differentiable neural computer dnc variants memory model greater capacity significantly easier train\n",
            "output sentence:  generative memory model combines slow learning neural networks fast adapting linear gaussian model memory \n",
            "\n",
            "{'rouge-1': {'r': 0.05102040816326531, 'p': 0.45454545454545453, 'f': 0.091743117451393}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.030612244897959183, 'p': 0.2727272727272727, 'f': 0.05504586974497103}}\n",
            "pair:  neural networks trained map one specific dataset another usually learn generalized transformation extrapolate accurately outside space training instance generative adversarial network gan exclusively trained transform images cars light dark might effect images horses neural networks good generation within manifold data trained however generating new samples outside manifold extrapolating sample much harder problem less well studied address introduce technique called neuron editing learns neurons encode edit particular transformation latent space use autoencoder decompose variation within dataset activations different neurons generate transformed data defining editing transformation neurons performing transformation latent trained space encode fairly complex non linear transformations data much simpler distribution shifts neuron activations showcase technique image domain style transfer two biological applications removal batch artifacts representing unwanted noise modeling effect drug treatments predict synergy drugs\n",
            "output sentence:  reframe generation problem one editing existing points result extrapolate better traditional gans \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.8571428571428571, 'f': 0.23529411527873895}, 'rouge-2': {'r': 0.0784313725490196, 'p': 0.6666666666666666, 'f': 0.14035087530932594}, 'rouge-l': {'r': 0.11363636363636363, 'p': 0.7142857142857143, 'f': 0.19607842900422914}}\n",
            "pair:  two main families reinforcement learning algorithms learning policy gradients recently proven equivalent using softmax relaxation one part entropic regularization relate result well known convex duality shannon entropy softmax function result also known donsker varadhan formula provides short proof equivalence interpret duality use ideas convex analysis prove new policy inequality relative soft learning\n",
            "output sentence:  short proof equivalence soft learning policy gradients \n",
            "\n",
            "{'rouge-1': {'r': 0.12244897959183673, 'p': 0.75, 'f': 0.2105263133764235}, 'rouge-2': {'r': 0.05970149253731343, 'p': 0.47058823529411764, 'f': 0.10596026290250432}, 'rouge-l': {'r': 0.12244897959183673, 'p': 0.75, 'f': 0.2105263133764235}}\n",
            "pair:  representation learning graph structured data received significant attention recently due ubiquitous applicability however advancements made static graph settings efforts jointly learning dynamic graph dynamic graph still infant stage two fundamental questions arise learning dynamic graphs elegantly model dynamical processes graphs ii leverage model effectively encode evolving graph information low dimensional representations present dyrep novel modeling framework dynamic graphs posits representation learning latent mediation process bridging two observed processes namely dynamics network realized topological evolution dynamics network realized activities nodes concretely propose two time scale deep temporal point process model captures interleaved dynamics observed processes model parameterized temporal attentive representation network encodes temporally evolving structural information node representations turn drives nonlinear evolution observed graph dynamics unified framework trained using efficient unsupervised procedure capability generalize unseen nodes demonstrate dyrep outperforms state art baselines dynamic link prediction time prediction tasks present extensive qualitative insights framework\n",
            "output sentence:  models representation learning dynamic graphs latent hidden process bridging two observed processes topological evolution interactions dynamic dynamic graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.18518518518518517, 'p': 0.8333333333333334, 'f': 0.3030303000550964}, 'rouge-2': {'r': 0.15942028985507245, 'p': 0.8461538461538461, 'f': 0.26829268025877456}, 'rouge-l': {'r': 0.18518518518518517, 'p': 0.8333333333333334, 'f': 0.3030303000550964}}\n",
            "pair:  generating complex discrete distributions remains one challenging problems machine learning existing techniques generating complex distributions high degrees freedom depend standard generative models like generative adversarial networks gan wasserstein gan associated variations models based optimization involving distance two continuous distributions introduce discrete wasserstein gan dwgan model based dual formulation wasserstein distance two discrete distributions derive novel training algorithm corresponding network architecture based formulation experimental results provided synthetic discrete data real discretized data mnist handwritten digits\n",
            "output sentence:  propose discrete wasserstein gan dwgan model based dual formulation wasserstein distance two discrete distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.16, 'p': 0.6666666666666666, 'f': 0.2580645130072841}, 'rouge-2': {'r': 0.021739130434782608, 'p': 0.09523809523809523, 'f': 0.03539822706241705}, 'rouge-l': {'r': 0.12, 'p': 0.5, 'f': 0.19354838397502605}}\n",
            "pair:  neural architecture search nas task finding neural architectures automatically recently emerged promising approach unveiling better models human designed ones however success stories vision tasks quite limited text except small language modeling setup paper explore nas text sequences scale first focusing task language translation later extending reading comprehension standard sequence sequence models translation conduct extensive searches recurrent cells attention similarity functions across two translation tasks iwslt english vietnamese wmt german english report challenges performing cell searches well demonstrate initial success attention searches translation improvements strong baselines addition show results attention searches transferable reading comprehension squad dataset\n",
            "output sentence:  explore neural architecture search language tasks recurrent cell search challenging nmt attention mechanism search works result attention search translation transferable comprehension comprehension \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.9090909090909091, 'f': 0.2197802176548726}, 'rouge-2': {'r': 0.08737864077669903, 'p': 0.9, 'f': 0.15929203378494794}, 'rouge-l': {'r': 0.125, 'p': 0.9090909090909091, 'f': 0.2197802176548726}}\n",
            "pair:  understanding groundbreaking performance deep neural networks one greatest challenges scientific community today work introduce information theoretic viewpoint behavior deep networks optimization processes generalization abilities studying information plane plane mutual information input variable desired label hidden layer specifically show training network characterized rapid increase mutual information mi layers target label followed longer decrease mi layers input variable explicitly show two fundamental information theoretic quantities correspond generalization error network result introducing new generalization bound exponential representation compression analysis focuses typical patterns large scale problems purpose introduce novel analytic bound mutual information consecutive layers network important consequence analysis super linear boost training time number non degenerate hidden layers demonstrating computational benefit hidden layers\n",
            "output sentence:  introduce information theoretic viewpoint behavior deep networks optimization processes generalization abilities \n",
            "\n",
            "{'rouge-1': {'r': 0.05102040816326531, 'p': 0.625, 'f': 0.09433962124599504}, 'rouge-2': {'r': 0.02654867256637168, 'p': 0.375, 'f': 0.04958677562461584}, 'rouge-l': {'r': 0.04081632653061224, 'p': 0.5, 'f': 0.07547169671769313}}\n",
            "pair:  work present novel upper bound target error address problem unsupervised domain adaptation recent studies reveal deep neural network learn transferable features generalize well novel tasks furthermore ben david et al provide upper bound target error transferring knowledge summarized minimizing source error distance marginal distributions simultaneously however common methods based theory usually ignore joint error samples different classes might mixed together matching marginal distribution case matter minimize marginal discrepancy target error bounded due increasing joint error address problem propose general upper bound taking joint error account undesirable case properly penalized addition utilize constrained hypothesis space formalize tighter bound well novel cross margin discrepancy measure dissimilarity hypotheses alleviates instability adversarial learning extensive empirical evidence shows proposal outperforms related approaches image classification error rates standard domain adaptation benchmarks\n",
            "output sentence:  joint error matters unsupervised domain adaptation especially domain shift huge \n",
            "\n",
            "{'rouge-1': {'r': 0.059322033898305086, 'p': 0.4666666666666667, 'f': 0.10526315589349317}, 'rouge-2': {'r': 0.02112676056338028, 'p': 0.21428571428571427, 'f': 0.038461536827744974}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.4, 'f': 0.09022556190853077}}\n",
            "pair:  large amount interest past particularly recently relative advantage different families universal function approximators instance neural networks polynomials rational functions etc however current research focused almost exclusively understanding problem worst case setting characterizing best infty approximation box sometimes even adversarially constructed data distribution setting many classical tools approximation theory effectively used however typical applications expect data high dimensional structured would important approximate desired function well relevant part domain small manifold real input data actually lies moreover even within domain desired quality approximation may uniform instance classification problems approximation needs accurate near decision boundary issues best knowledge remain unexplored mind analyze performance neural networks polynomial kernels natural regression setting data enjoys sparse latent structure labels depend simple way latent variables give almost tight theoretical analysis performance neural networks polynomials problem well verify theory simulations results involve new complex analytic techniques may independent interest show substantial qualitative differences known worst case setting\n",
            "output sentence:  beyond worst case analysis representational power relu nets polynomial kernels particular presence sparse latent structure \n",
            "\n",
            "{'rouge-1': {'r': 0.12987012987012986, 'p': 0.7142857142857143, 'f': 0.2197802171766695}, 'rouge-2': {'r': 0.06382978723404255, 'p': 0.42857142857142855, 'f': 0.11111110885459538}, 'rouge-l': {'r': 0.1038961038961039, 'p': 0.5714285714285714, 'f': 0.17582417322062555}}\n",
            "pair:  plethora computer vision tasks optical flow image alignment formulated non linear optimization problems resurgence deep learning dominant family solving optimization problems numerical optimization gauss newton gn recently several attempts made formulate learnable gn steps cascade regression architectures paper investigate recent machine learning architectures deep neural networks residual connections perspective end first demonstrate residual blocks considered discretization odes viewed gn steps go step propose new residual block reminiscent newton method numerical optimization exhibits faster convergence thoroughly evaluate proposed newton resnet conducting experiments image speech classification image generation using datasets experiments demonstrate newton resnet requires less parameters achieve performance original resnet\n",
            "output sentence:  demonstrate residual blocks viewed gauss newton steps propose new residual block exploits second order information \n",
            "\n",
            "{'rouge-1': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.039473684210526314, 'p': 0.3333333333333333, 'f': 0.07058823340069209}}\n",
            "pair:  reconstruction view ray computed tomography ct data highly ill posed problem often used applications require low radiation dose clinical ct rapid industrial scanning fixed gantry ct existing analytic iterative algorithms generally produce poorly reconstructed images severely deteriorated artifacts noise especially number ray projections considerably low paper presents deep network driven approach address extreme view ct incorporating convolutional neural network based inference state art iterative reconstruction proposed method interprets view sinogram data using attention based deep networks infer reconstructed image predicted image used prior knowledge iterative algorithm final reconstruction demonstrate effectiveness proposed approach performing reconstruction experiments chest ct dataset\n",
            "output sentence:  present cnn inference based reconstruction algorithm address extremely view ct \n",
            "\n",
            "{'rouge-1': {'r': 0.14102564102564102, 'p': 0.9166666666666666, 'f': 0.24444444213333333}, 'rouge-2': {'r': 0.10576923076923077, 'p': 0.7857142857142857, 'f': 0.18644067587474866}, 'rouge-l': {'r': 0.1282051282051282, 'p': 0.8333333333333334, 'f': 0.2222222199111111}}\n",
            "pair:  tasks could come varying number instances classes realistic settings existing meta learning approaches shot classification assume number instances per task class fixed due restriction learn equally utilize meta knowledge across tasks even number instances per task class largely varies moreover consider distributional difference unseen tasks meta knowledge may less usefulness depending task relatedness overcome limitations propose novel meta learning model adaptively balances effect meta learning task specific learning within task learning balancing variables decide whether obtain solution relying meta knowledge task specific learning formulate objective bayesian inference framework tackle using variational inference validate bayesian task adaptive meta learning bayesian taml two realistic task class imbalanced datasets significantly outperforms existing meta learning approaches ablation study confirms effectiveness balancing component bayesian learning framework\n",
            "output sentence:  novel meta learning model adaptively balances effect meta learning task specific learning also class specific learning within task \n",
            "\n",
            "{'rouge-1': {'r': 0.13513513513513514, 'p': 0.6666666666666666, 'f': 0.22471909832091908}, 'rouge-2': {'r': 0.049019607843137254, 'p': 0.35714285714285715, 'f': 0.08620689442925095}, 'rouge-l': {'r': 0.12162162162162163, 'p': 0.6, 'f': 0.20224718820855958}}\n",
            "pair:  present new latent model natural images learned large scale datasets learning process provides latent embedding every image training dataset well deep convolutional network maps latent space image space training new model provides strong universal image prior variety image restoration tasks large hole inpainting superresolution colorization model high resolution natural images approach uses latent spaces high dimensionality one two orders magnitude higher previous latent image models tackle high dimensionality use latent spaces special manifold structure convolutional manifolds parameterized convnet certain architecture experiments compare learned latent models latent models learned autoencoders advanced variants generative adversarial networks strong baseline system using simpler parameterization latent space model outperforms competing approaches range restoration tasks\n",
            "output sentence:  present new deep latent model natural images trained unlabeled datasets utilized solve various image restoration tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.17857142857142858, 'p': 0.4166666666666667, 'f': 0.24999999580000004}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.10714285714285714, 'p': 0.25, 'f': 0.14999999580000012}}\n",
            "pair:  show ensemble functions leveraged effective exploration deep reinforcement learning build well established algorithms bandit setting adapt learning setting propose exploration strategy based upper confidence bounds ucb experiments show significant gains atari benchmark\n",
            "output sentence:  adapting ucb exploration ensemble learning improves prior methods double dqn atari benchmark \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 0.875, 'f': 0.20895522177767878}, 'rouge-2': {'r': 0.05714285714285714, 'p': 0.5714285714285714, 'f': 0.10389610224321134}, 'rouge-l': {'r': 0.0847457627118644, 'p': 0.625, 'f': 0.14925372924036534}}\n",
            "pair:  introduce cgnn framework learn functional causal models generative neural networks networks trained using backpropagation minimize maximum mean discrepancy observed data unlike previous approaches cgnn leverages conditional independences distributional asymmetries seamlessly discover bivariate multivariate causal structures without hidden variables cgnn estimate causal structure full differentiable generative model data throughout extensive variety experiments illustrate competitive esults cgnn state art alternatives observational causal discovery simulated real data tasks cause effect inference structure identification multivariate causal discovery\n",
            "output sentence:  discover structure functional causal models generative neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.22448979591836735, 'p': 0.9166666666666666, 'f': 0.36065573454447736}, 'rouge-2': {'r': 0.17857142857142858, 'p': 0.9090909090909091, 'f': 0.2985074599420807}, 'rouge-l': {'r': 0.22448979591836735, 'p': 0.9166666666666666, 'f': 0.36065573454447736}}\n",
            "pair:  perform depth investigation suitability self attention models character level neural machine translation test standard transformer model well novel variant encoder block combines information nearby characters using convolution perform extensive experiments wmt un datasets testing bilingual multilingual translation english using three input languages french spanish chinese transformer variant consistently outperforms standard transformer character level converges faster learning robust character level alignments\n",
            "output sentence:  perform depth investigation suitability self attention models character level neural machine translation \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.625, 'f': 0.2197802168820191}, 'rouge-2': {'r': 0.052083333333333336, 'p': 0.3125, 'f': 0.08928571183673477}, 'rouge-l': {'r': 0.10666666666666667, 'p': 0.5, 'f': 0.17582417292597516}}\n",
            "pair:  many approaches causal discovery limited inability discriminate markov equivalent graphs given observational data formulate causal discovery marginal likelihood based bayesian model selection problem adopt parameterization based notion independence causal mechanisms renders markov equivalent graphs distinguishable complement empirical bayesian approach setting priors actual underlying causal graph assigned higher marginal likelihood alternatives adopting bayesian approach also allows straightforward modeling unobserved confounding variables provide variational algorithm approximate marginal likelihood since desirable feat renders computation marginal likelihood intractable believe bayesian approach causal discovery allows rich methodology bayesian inference used various difficult aspects problem provides unifying framework causal discovery research demonstrate promising results experiments conducted real data supporting modeling approach inference methodology\n",
            "output sentence:  cast causal structure discovery bayesian model selection way allows us discriminate markov equivalent graphs identify unique causal graph \n",
            "\n",
            "{'rouge-1': {'r': 0.13513513513513514, 'p': 0.5882352941176471, 'f': 0.21978021674193937}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.25, 'f': 0.07692307431952672}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.4117647058823529, 'f': 0.15384615080787348}}\n",
            "pair:  simultaneously capture syntax semantics text corpus propose new larger context language model extracts recurrent hierarchical semantic structure via dynamic deep topic model guide natural language generation moving beyond conventional language model ignores long range word dependencies sentence order proposed model captures intra sentence word dependencies also temporal transitions sentences inter sentence topic dependences inference develop hybrid stochastic gradient mcmc recurrent autoencoding variational bayes experimental results variety real world text corpora demonstrate proposed model outperforms state art larger context language models also learns interpretable recurrent multilayer topics generates diverse sentences paragraphs syntactically correct semantically coherent\n",
            "output sentence:  introduce novel larger context language model simultaneously captures syntax semantics making capable generating highly interpretable sentences paragraphs \n",
            "\n",
            "{'rouge-1': {'r': 0.11578947368421053, 'p': 0.6111111111111112, 'f': 0.19469026280836402}, 'rouge-2': {'r': 0.049586776859504134, 'p': 0.3333333333333333, 'f': 0.08633093299725694}, 'rouge-l': {'r': 0.07368421052631578, 'p': 0.3888888888888889, 'f': 0.1238938026313729}}\n",
            "pair:  importance weighted autoencoder iwae burda et al popular variational inference method achieves tighter evidence bound hence lower bias standard variational autoencoders optimising multi sample objective objective expressible integral monte carlo samples unfortunately iwae crucially relies availability reparametrisations even exist multi sample objective leads inference network gradients break increased rainforth et al breakdown circumvented removing high variance score function terms either heuristically ignoring yields sticking landing iwae iwae stl gradient roeder et al identity tucker et al yields doubly reparametrised iwae iwae dreg gradient work argue directly optimising proposal distribution importance sampling reweighted wake sleep rws algorithm bornschein bengio preferable optimising iwae type multi sample objectives formalise argument introduce adaptive importance sampling framework termed adaptive importance sampling learning aisle slightly generalises rws algorithm show aisle admits iwae stl iwae dreg iwae gradients avoid breakdown special cases\n",
            "output sentence:  show variants importance weighted autoencoders derived principled manner special cases adaptive importance sampling approaches like reweighted wake sleep algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.5, 'f': 0.12121211908172637}, 'rouge-2': {'r': 0.021897810218978103, 'p': 0.2, 'f': 0.03947368243161366}, 'rouge-l': {'r': 0.04310344827586207, 'p': 0.3125, 'f': 0.07575757362718095}}\n",
            "pair:  owing connection generative adversarial networks gans saddle point problems recently attracted considerable interest machine learning beyond necessity theoretical guarantees revolve around convex concave even linear problems however making theoretical inroads towards efficient gan training depends crucially moving beyond classic framework make piecemeal progress along lines analyze behavior mirror descent md class non monotone problems whose solutions coincide naturally associated variational inequality property call coherence first show ordinary vanilla md converges strict version condition otherwise particular may fail converge even bilinear models unique solution show deficiency mitigated optimism taking extra gradient step optimistic mirror descent omd converges coherent problems analysis generalizes extends results daskalakis et al optimistic gradient descent ogd bilinear problems makes concrete headway provable convergence beyond convex concave games also provide stochastic analogues results validate analysis numerical experiments wide array gan models including gaussian mixture models celeba cifar datasets\n",
            "output sentence:  show inclusion extra gradient step first order gan training methods improve stability lead improved convergence results \n",
            "\n",
            "{'rouge-1': {'r': 0.0875, 'p': 0.7777777777777778, 'f': 0.15730336896856456}, 'rouge-2': {'r': 0.030303030303030304, 'p': 0.375, 'f': 0.05607476497161328}, 'rouge-l': {'r': 0.0875, 'p': 0.7777777777777778, 'f': 0.15730336896856456}}\n",
            "pair:  introduce new routing algorithm capsule networks child capsule routed parent based agreement parent state child vote unlike previously proposed routing algorithms parent ability reconstruct child explicitly taken account update routing probabilities simplifies routing procedure improves performance benchmark datasets cifar cifar new mechanism designs routing via inverted dot product attention imposes layer normalization normalization replaces sequential iterative routing concurrent iterative routing besides outperforming existing capsule networks model performs par powerful cnn resnet using less parameters different task recognizing digits overlayed digit images proposed capsule model performs favorably cnns given number layers neurons per layer believe work raises possibility applying capsule networks complex real world tasks\n",
            "output sentence:  present new routing method capsule networks performs par resnet cifar \n",
            "\n",
            "{'rouge-1': {'r': 0.1323529411764706, 'p': 0.6428571428571429, 'f': 0.21951219229030342}, 'rouge-2': {'r': 0.05, 'p': 0.3076923076923077, 'f': 0.08602150297144186}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.35714285714285715, 'f': 0.12195121668054736}}\n",
            "pair:  unsupervised monocular depth estimation made great progress deep learning involved training binocular stereo images considered good option data easily obtained however depth disparity prediction results show poor performance object boundaries main reason related handling occlusion areas training paper propose novel method overcome issue exploiting disparity maps property generate occlusion mask block back propagation occlusion areas image warping also design new networks flipped stereo images induce networks learn occluded boundaries shows method achieves clearer boundaries better evaluation results kitti driving dataset virtual kitti dataset\n",
            "output sentence:  paper propose mask method solves previous blurred results unsupervised monocular depth estimation caused occlusion \n",
            "\n",
            "{'rouge-1': {'r': 0.06521739130434782, 'p': 0.375, 'f': 0.11111110858710567}, 'rouge-2': {'r': 0.01694915254237288, 'p': 0.14285714285714285, 'f': 0.03030302840679534}, 'rouge-l': {'r': 0.06521739130434782, 'p': 0.375, 'f': 0.11111110858710567}}\n",
            "pair:  clustering fundamental machine learning method quality results dependent data distribution reason deep neural networks used learning better representations data paper propose systematic taxonomy clustering deep learning addition review methods field based taxonomy creating new methods straightforward also propose new approach built taxonomy surpasses limitations previous work experimental evaluation image datasets shows method approaches state art clustering quality performs better cases\n",
            "output sentence:  unifying framework perform clustering using deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.0297029702970297, 'p': 0.3333333333333333, 'f': 0.05454545304297525}, 'rouge-2': {'r': 0.008064516129032258, 'p': 0.125, 'f': 0.015151514012855917}, 'rouge-l': {'r': 0.019801980198019802, 'p': 0.2222222222222222, 'f': 0.03636363486115709}}\n",
            "pair:  rise employment deep learning methods safety critical scenarios interpretability essential ever although many different directions regarding interpretability explored visual modalities time series data neglected handful methods tested due poor intelligibility approach problem interpretability novel way proposing tsinsight attach auto encoder sparsity inducing norm output classifier fine tune based gradients classifier reconstruction penalty auto encoder learns preserve features important prediction classifier suppresses ones irrelevant serves feature attribution method boost interpretability words ask network reconstruct parts useful classifier correlated causal prediction contrast attribution frameworks tsinsight capable generating instance based model based explanations evaluated tsinsight along commonly used attribution methods range different time series datasets validate efficacy furthermore analyzed set properties tsinsight achieves box including adversarial robustness output space contraction obtained results advocate tsinsight effective tool interpretability deep time series models\n",
            "output sentence:  present attribution technique leveraging sparsity inducing norms achieve interpretability \n",
            "\n",
            "{'rouge-1': {'r': 0.07894736842105263, 'p': 0.6666666666666666, 'f': 0.1411764686948097}, 'rouge-2': {'r': 0.021505376344086023, 'p': 0.2222222222222222, 'f': 0.03921568466551333}, 'rouge-l': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}}\n",
            "pair:  proliferation models natural language processing nlp tasks even harder understand differences models relative merits simply looking differences holistic metrics accuracy bleu tell us emph emph particular method better dataset biases influence choices model design paper present general methodology emph interpretable evaluation nlp systems choose task named entity recognition ner case study core task identifying people places organizations text proposed evaluation method enables us interpret textit model biases textit dataset biases emph differences datasets affect design models identifying strengths weaknesses current approaches making analysis tool available make easy future researchers run similar analyses drive progress area\n",
            "output sentence:  propose generalized evaluation methodology interpret model biases dataset biases correlation \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.4166666666666667, 'f': 0.12987012723899483}, 'rouge-2': {'r': 0.012987012987012988, 'p': 0.09090909090909091, 'f': 0.02272727053977294}, 'rouge-l': {'r': 0.06153846153846154, 'p': 0.3333333333333333, 'f': 0.10389610126496887}}\n",
            "pair:  ability generalize quickly observations crucial intelligent systems paper introduce apl algorithm approximates probability distributions remembering surprising observations encountered past observations recalled external memory module processed decoder network combine information different memory slots generalize beyond direct recall show algorithm perform well state art baselines shot classification benchmarks smaller memory footprint addition memory compression allows scale thousands unknown labels finally introduce meta learning reasoning task challenging direct classification setting apl able generalize fewer one example per class via deductive reasoning\n",
            "output sentence:  introduce model generalizes quickly observations storing surprising information attending relevant data time point \n",
            "\n",
            "{'rouge-1': {'r': 0.02127659574468085, 'p': 0.14285714285714285, 'f': 0.037037034780521394}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02127659574468085, 'p': 0.14285714285714285, 'f': 0.037037034780521394}}\n",
            "pair:  deconvnet guided backprop lrp invented better understand deep neural networks show methods produce theoretically correct explanation linear model yet used multi layer networks millions parameters cause concern since linear models simple neural networks argue explanation methods neural nets work reliably limit simplicity linear models based analysis linear models propose generalization yields two explanation techniques patternnet patternattribution theoretically sound linear models produce improved explanations deep networks\n",
            "output sentence:  without learning impossible explain machine learning model decisions \n",
            "\n",
            "{'rouge-1': {'r': 0.12698412698412698, 'p': 0.8, 'f': 0.21917807982735973}, 'rouge-2': {'r': 0.0410958904109589, 'p': 0.3333333333333333, 'f': 0.07317072975312319}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.6, 'f': 0.16438355927941453}}\n",
            "pair:  success reinforcement learning real world limited instrumented laboratory scenarios often requiring arduous human supervision enable continuous learning work discuss required elements robotic system continually autonomously improve data collected real world propose particular instantiation system subsequently investigate number challenges learning without instrumentation including lack episodic resets state estimation hand engineered rewards propose simple scalable solutions challenges demonstrate efficacy proposed system dexterous robotic manipulation tasks simulation real world also provide insightful analysis ablation study challenges associated learning paradigm\n",
            "output sentence:  system learn robotic tasks real world reinforcement learning without instrumentation \n",
            "\n",
            "{'rouge-1': {'r': 0.12222222222222222, 'p': 0.8461538461538461, 'f': 0.2135922308040343}, 'rouge-2': {'r': 0.07142857142857142, 'p': 0.6666666666666666, 'f': 0.12903225631633716}, 'rouge-l': {'r': 0.12222222222222222, 'p': 0.8461538461538461, 'f': 0.2135922308040343}}\n",
            "pair:  applying reinforcement learning rl real world problems require reasoning action reward correlation long time horizons hierarchical reinforcement learning hrl methods handle dividing task hierarchies often hand tuned network structure pre defined subgoals propose novel hrl framework taic learns temporal abstraction past experience expert demonstrations without task specific knowledge formulate temporal abstraction problem learning latent representations action sequences present novel approach regularizing latent space adding information theoretic constraints specifically maximize mutual information latent variables state changes visualization latent space demonstrates algorithm learns effective abstraction long action sequences learned abstraction allows us learn new tasks higher level efficiently convey significant speedup convergence benchmark learning problems results demonstrate learning temporal abstractions effective technique increasing convergence rate sample efficiency rl algorithms\n",
            "output sentence:  propose novel hrl framework formulate temporal abstraction problem learning latent representation action sequence \n",
            "\n",
            "{'rouge-1': {'r': 0.054945054945054944, 'p': 0.38461538461538464, 'f': 0.0961538439663462}, 'rouge-2': {'r': 0.008928571428571428, 'p': 0.08333333333333333, 'f': 0.016129030509885726}, 'rouge-l': {'r': 0.04395604395604396, 'p': 0.3076923076923077, 'f': 0.076923074735577}}\n",
            "pair:  interpretability small labelled datasets key issues practical application deep learning particularly areas medicine paper present semi supervised technique addresses issues simultaneously learn dense representations large unlabelled image datasets use representations learn classifiers small labeled sets generate visual rationales explaining predictions using chest radiography diagnosis motivating application show method good generalization ability learning represent chest radiography dataset training classifier separate set different institution method identifies heart failure thoracic diseases prediction generate visual rationales positive classifications optimizing latent representation minimize probability disease constrained similarity measure image space decoding resultant latent representation produces image without apparent disease difference original altered image forms interpretable visual rationale algorithm prediction method simultaneously produces visual rationales compare favourably previous techniques classifier outperforms current state art\n",
            "output sentence:  propose method using gans generate high quality visual rationales help explain model predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.1978021978021978, 'p': 0.9, 'f': 0.3243243213700187}, 'rouge-2': {'r': 0.14545454545454545, 'p': 0.7619047619047619, 'f': 0.24427480646815453}, 'rouge-l': {'r': 0.1978021978021978, 'p': 0.9, 'f': 0.3243243213700187}}\n",
            "pair:  fine tuning pre trained models achieved exceptional results many language tasks study focused one self attention network model namely bert performed well terms stacking layers across diverse language understanding benchmarks however many downstream tasks information layers ignored bert fine tuning addition although self attention networks well known ability capture global dependencies room improvement remains terms emphasizing importance local contexts light advantages disadvantages paper proposes sesamebert generalized fine tuning method enables extraction global information among layers squeeze excitation enriches local information capturing neighboring contexts via gaussian blurring furthermore demonstrated effectiveness approach hans dataset used determine whether models adopted shallow heuristics instead learning underlying generalizations experiments revealed sesamebert outperformed bert respect glue benchmark hans evaluation set\n",
            "output sentence:  proposed sesamebert generalized fine tuning method enables extraction global information among layers squeeze excitation enriches information information neighboring contexts via gaussian gaussian \n",
            "\n",
            "{'rouge-1': {'r': 0.16883116883116883, 'p': 0.8666666666666667, 'f': 0.2826086929229679}, 'rouge-2': {'r': 0.07142857142857142, 'p': 0.5, 'f': 0.12499999781250003}, 'rouge-l': {'r': 0.12987012987012986, 'p': 0.6666666666666666, 'f': 0.21739130161862005}}\n",
            "pair:  analyze dynamics training deep relu networks implications generalization capability using teacher student setting discovered novel relationship gradient received hidden student nodes activations teacher nodes deep relu networks relationship assumption small overlapping teacher node activations prove student nodes whose weights initialized close teacher nodes converge faster rate parameterized regimes layer case small set lucky nodes converge teacher nodes fan weights nodes converge zero framework provides insight multiple puzzling phenomena deep learning like parameterization implicit regularization lottery tickets etc verify assumption showing majority batchnorm biases pre trained vgg models negative experiments random deep teacher networks gaussian inputs teacher network pre trained cifar extensive ablation studies validate multiple theoretical predictions\n",
            "output sentence:  theoretical framework deep relu network explains multiple puzzling phenomena like parameterization implicit regularization lottery tickets etc \n",
            "\n",
            "{'rouge-1': {'r': 0.2222222222222222, 'p': 0.7619047619047619, 'f': 0.3440860180090184}, 'rouge-2': {'r': 0.15730337078651685, 'p': 0.6666666666666666, 'f': 0.25454545145619834}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.7619047619047619, 'f': 0.3440860180090184}}\n",
            "pair:  understanding object motion one core problems computer vision requires segmenting tracking objects time significant progress made instance segmentation models cannot track objects crucially unable reason space time propose new spatio temporal embedding loss videos generates temporally consistent video instance segmentation model includes temporal network learns model temporal context motion essential produce smooth embeddings time model also estimates monocular depth self supervised loss relative distance object effectively constrains next ensuring time consistent embedding finally show model accurately track segment instances even occlusions missed detections advancing state art kitti multi object tracking dataset\n",
            "output sentence:  introduce new spatio temporal embedding loss videos generates temporally consistent video instance segmentation even occlusions missed detections using appearance geometry temporal context \n",
            "\n",
            "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
            "pair:  random matrix theory rmt applied analyze weight matrices deep neural networks dnns including production quality pre trained models alexnet inception smaller models trained scratch lenet miniature alexnet empirical theoretical results clearly indicate empirical spectral density esd dnn layer matrices displays signatures traditionally regularized statistical models even absence exogenously specifying traditional forms regularization dropout weight norm constraints building recent results rmt notably extension universality classes heavy tailed matrices develop theory identify phases training corresponding increasing amounts implicit self regularization smaller older dnns implicit self regularization like traditional tikhonov regularization size scale separating signal noise state art dnns however identify novel form heavy tailed self regularization similar self organization seen statistical physics disordered systems implicit self regularization depend strongly many knobs training process exploiting generalization gap phenomena demonstrate cause small model exhibit phases training simply changing batch size\n",
            "output sentence:  see abstract revision paper identical except page supplementary material serve stand along technical report version paper \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.5263157894736842, 'f': 0.22471909776543367}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.16666666666666666, 'f': 0.05940593766493495}, 'rouge-l': {'r': 0.1, 'p': 0.3684210526315789, 'f': 0.15730336742835507}}\n",
            "pair:  study problem learning similarity functions large corpora using neural network embedding models models typically trained using sgd random sampling unobserved pairs sample size grows quadratically corpus size making expensive scale propose new efficient methods train models without sample unobserved pairs inspired matrix factorization approach relies adding global quadratic penalty expressing term inner product two generalized gramians show gradient term efficiently computed maintaining estimates gramians develop variance reduction schemes improve quality estimates conduct large scale experiments show significant improvement training time generalization performance compared sampling methods\n",
            "output sentence:  develop efficient methods train neural embedding models dot product structure reformulating objective function terms generalized gram matrices estimates matrices \n",
            "\n",
            "{'rouge-1': {'r': 0.05660377358490566, 'p': 0.375, 'f': 0.09836065345874771}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03773584905660377, 'p': 0.25, 'f': 0.06557376821284609}}\n",
            "pair:  past four years neural networks proven vulnerable adversarial images targeted imperceptible image perturbations lead drastically different predictions show adversarial vulnerability increases gradients training objective viewed function inputs current network architectures prove norm gradients grows square root input size nets therefore become increasingly vulnerable growing image size proofs rely network weight distribution initialization extensive experiments confirm conclusions still hold usual training\n",
            "output sentence:  neural nets large gradients design makes adversarially vulnerable \n",
            "\n",
            "{'rouge-1': {'r': 0.20270270270270271, 'p': 0.9375, 'f': 0.3333333304098766}, 'rouge-2': {'r': 0.16666666666666666, 'p': 0.9375, 'f': 0.2830188653613386}, 'rouge-l': {'r': 0.20270270270270271, 'p': 0.9375, 'f': 0.3333333304098766}}\n",
            "pair:  deep reinforcement learning algorithms learn complex behavioral skills real world application methods requires considerable amount experience collected agent practical settings robotics involves repeatedly attempting task resetting environment attempt however tasks easily automatically reversible practice learning process requires considerable human intervention work propose autonomous method safe efficient reinforcement learning simultaneously learns forward backward policy backward policy resetting environment subsequent attempt learning value function backward policy automatically determine forward policy enter non reversible state providing uncertainty aware safety aborts experiments illustrate proper use backward policy greatly reduce number manual resets required learn task reduce number unsafe actions lead non reversible states\n",
            "output sentence:  propose autonomous method safe efficient reinforcement learning simultaneously learns forward backward policy backward policy resetting environment subsequent attempt \n",
            "\n",
            "{'rouge-1': {'r': 0.14925373134328357, 'p': 0.8333333333333334, 'f': 0.2531645543855151}, 'rouge-2': {'r': 0.07317073170731707, 'p': 0.5454545454545454, 'f': 0.1290322559787259}, 'rouge-l': {'r': 0.11940298507462686, 'p': 0.6666666666666666, 'f': 0.2025316429931101}}\n",
            "pair:  exposure bias problem refers training inference discrepancy caused teacher forcing maximum likelihood estimation mle training auto regressive neural network language models lm regarded central problem natural language generation nlg model training although lot algorithms proposed avoid teacher forcing therefore alleviate exposure bias little work showing serious exposure bias problem work first identify auto recovery ability mle trained lm casts doubt seriousness exposure bias develop precise quantifiable definition exposure bias however according measurements controlled experiments around performance gain training inference discrepancy completely removed results suggest exposure bias problem could much less serious currently assumed\n",
            "output sentence:  show exposure bias could much less serious currently assumed mle lm training \n",
            "\n",
            "{'rouge-1': {'r': 0.16216216216216217, 'p': 0.8, 'f': 0.2696629185456382}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.21428571428571427, 'f': 0.06185566763311733}, 'rouge-l': {'r': 0.12162162162162163, 'p': 0.6, 'f': 0.20224718820855958}}\n",
            "pair:  deep reinforcement learning rl agents often fail generalize unseen environments yet semantically similar trained agents particularly trained high dimensional state spaces images paper propose simple technique improve generalization ability deep rl agents introducing randomized convolutional neural network randomly perturbs input observations enables trained agents adapt new domains learning robust features invariant across varied randomized environments furthermore consider inference method based monte carlo approximation reduce variance induced randomization demonstrate superiority method across coinrun deepmind lab exploration robotics control tasks significantly outperforms various regularization data augmentation methods purpose\n",
            "output sentence:  propose simple randomization technique improving generalization deep reinforcement learning across tasks various unseen visual patterns \n",
            "\n",
            "{'rouge-1': {'r': 0.1262135922330097, 'p': 0.7222222222222222, 'f': 0.21487603052523738}, 'rouge-2': {'r': 0.03676470588235294, 'p': 0.29411764705882354, 'f': 0.06535947514887443}, 'rouge-l': {'r': 0.0970873786407767, 'p': 0.5555555555555556, 'f': 0.16528925366573324}}\n",
            "pair:  locality sensitive hashing schemes simhash provide compact representations multisets similarity estimated however certain applications need estimate similarity dynamically changing sets case need representation homomorphism hash unions differences sets computed directly hashes operands propose two representations property cosine similarity extension simhash angle preserving random projections make substantial progress third representation jaccard similarity extension minhash employ hashes compress sufficient statistics conditional random field crf coreference model study compression affects ability compute similarities entities split merged inference cut study hashes conditional random field crf hierarchical coreference model order compute similarity entities merged split inference also provide novel statistical analysis simhash help justify estimator inside crf showing bias variance reduce quickly number bits problem author coreference find simhash scheme allows scaling hierarchical coreference algorithm order magnitude without degrading statistical performance model coreference accuracy long employ least bits angle preserving random projections improve coreference quality potentially allowing even fewer dimensions used\n",
            "output sentence:  employ linear homomorphic compression schemes represent sufficient statistics conditional random field model coreference allows us scale inference improve speed \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.5, 'f': 0.21428571091836737}, 'rouge-2': {'r': 0.025, 'p': 0.11764705882352941, 'f': 0.04123711051121287}, 'rouge-l': {'r': 0.07575757575757576, 'p': 0.2777777777777778, 'f': 0.11904761568027221}}\n",
            "pair:  gans provide framework training generative models mimic data distribution however many cases wish train generative model optimize auxiliary objective function within data generates making aesthetically pleasing images cases objective functions difficult evaluate may require human interaction develop system efficiently training gan increase generic rate positive user interactions example aesthetic ratings build model human behavior targeted domain relatively small set interactions use behavioral model auxiliary loss function improve generative model proof concept demonstrate system successful improving positive interaction rates simulated variety objectives characterize\n",
            "output sentence:  describe improve image generative model according slow difficult evaluate objective human feedback could many applications like making aesthetic \n",
            "\n",
            "{'rouge-1': {'r': 0.17307692307692307, 'p': 0.6, 'f': 0.26865671294274895}, 'rouge-2': {'r': 0.09836065573770492, 'p': 0.42857142857142855, 'f': 0.15999999696355563}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.26666666666666666, 'f': 0.11940298159946547}}\n",
            "pair:  analyze joint probability distribution lengths vectors hidden variables different layers fully connected deep network weights biases chosen randomly according gaussian distributions input binary valued show activation function satisfies minimal set assumptions satisfied activation functions know used practice width network gets large length process converges probability length map determined simple function variances random weights biases activation function also show convergence may fail activation functions violate assumptions\n",
            "output sentence:  prove activation functions satisfying conditions deep network gets wide lengths vectors hidden variables converge length map \n",
            "\n",
            "{'rouge-1': {'r': 0.11224489795918367, 'p': 0.7333333333333333, 'f': 0.19469026318427443}, 'rouge-2': {'r': 0.01652892561983471, 'p': 0.14285714285714285, 'f': 0.029629627770644835}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.4666666666666667, 'f': 0.1238938030072833}}\n",
            "pair:  information bottleneck ib tuning relative strength compression prediction terms two terms behave relationship dataset learned representation paper set answer questions studying multiple phase transitions ib objective ib defined encoding distribution input target representation sudden jumps di prediction accuracy observed increasing introduce definition ib phase transitions qualitative change ib loss landscape show transitions correspond onset learning new classes using second order calculus variations derive formula provides practical condition ib phase transitions draw connection fisher information matrix parameterized models provide two perspectives understand formula revealing ib phase transition finding component maximum nonlinear correlation orthogonal learned representation close analogy canonical correlation analysis cca linear settings based theory present algorithm discovering phase transition points finally verify theory algorithm accurately predict phase transitions categorical datasets predict onset learning new classes class difficulty mnist predict prominent phase transitions cifar\n",
            "output sentence:  give theoretical analysis information bottleneck objective understand predict observed phase transitions prediction vs compression tradeoff \n",
            "\n",
            "{'rouge-1': {'r': 0.028985507246376812, 'p': 0.2857142857142857, 'f': 0.05263157727493079}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.16666666666666666, 'f': 0.019999998872000067}, 'rouge-l': {'r': 0.028985507246376812, 'p': 0.2857142857142857, 'f': 0.05263157727493079}}\n",
            "pair:  learning rich representations predictive learning without labels longstanding challenge field machine learning generative pre training far successful contrastive methods modeling representations raw images paper propose neural architecture self supervised representation learning raw images called patchformer learns model spatial dependencies across patches raw image method learns model conditional probability distribution missing patches given context surrounding patches evaluate utility learned representations fine tuning pre trained model low data regime classification tasks specifically benchmark model semi supervised imagenet classification become popular benchmark recently semi supervised self supervised learning methods model able achieve top accuracies trained using labels imagenet showing promise generative pre training methods\n",
            "output sentence:  decoding pixels still work representation learning images \n",
            "\n",
            "{'rouge-1': {'r': 0.22413793103448276, 'p': 0.7222222222222222, 'f': 0.3421052595429363}, 'rouge-2': {'r': 0.11594202898550725, 'p': 0.47058823529411764, 'f': 0.18604650845592216}, 'rouge-l': {'r': 0.1896551724137931, 'p': 0.6111111111111112, 'f': 0.2894736805955679}}\n",
            "pair:  describe three approaches enabling extremely computationally limited embedded scheduler consider small number alternative activities based resource availability consider case scheduler computationally limited cannot backtrack search first two approaches precompile resource checks called guards enable selection preferred alternative activity sufficient resources estimated available schedule remaining activities final approach mimics backtracking invoking scheduler multiple times alternative activities present evaluation techniques mission scenarios called sol types nasa next planetary rover techniques evaluated inclusion onboard scheduler\n",
            "output sentence:  paper describes three techniques allow non backtracking computationally limited scheduler consider small number alternative activities based resource availability \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 1.0, 'f': 0.11764705771626299}, 'rouge-2': {'r': 0.031914893617021274, 'p': 0.75, 'f': 0.06122448901291129}, 'rouge-l': {'r': 0.0625, 'p': 1.0, 'f': 0.11764705771626299}}\n",
            "pair:  audio signals sampled high temporal resolutions learning synthesize audio requires capturing structure across range timescales generative adversarial networks gans seen wide success generating images locally globally coherent seen little application audio generation paper introduce wavegan first attempt applying gans unsupervised synthesis raw waveform audio wavegan capable synthesizing one second slices audio waveforms global coherence suitable sound effect generation experiments demonstrate without labels wavegan learns produce intelligible words trained small vocabulary speech dataset also synthesize audio domains drums bird vocalizations piano compare wavegan method applies gans designed image generation image like audio feature representations finding approaches promising\n",
            "output sentence:  learning synthesize raw waveform audio gans \n",
            "\n",
            "{'rouge-1': {'r': 0.06593406593406594, 'p': 0.5454545454545454, 'f': 0.11764705689926953}, 'rouge-2': {'r': 0.015748031496062992, 'p': 0.2, 'f': 0.029197078938675538}, 'rouge-l': {'r': 0.04395604395604396, 'p': 0.36363636363636365, 'f': 0.07843137062475976}}\n",
            "pair:  myriad kinds segmentation ultimately right segmentation given scene eye annotator standard approaches require large amounts labeled data learn one particular kind segmentation first step towards relieving annotation burden propose problem guided segmentation given varying amounts pixel wise labels segment unannotated pixels propagating supervision locally within image non locally across images propose guided networks extract latent task representation guidance variable amounts classes categories instances etc pixel supervision optimize architecture end end fast accurate data efficient segmentation meta learning span shot many shot learning regimes examine guidance little one pixel per concept much images compare full gradient optimization extremes explore generalization analyze guidance bridge different levels supervision segment classes union instances segmentor concentrates different amounts supervision different types classes efficient latent representation non locally propagates supervision across images updated quickly cumulatively given supervision\n",
            "output sentence:  propose meta learning approach guiding visual segmentation tasks varying amounts supervision \n",
            "\n",
            "{'rouge-1': {'r': 0.06481481481481481, 'p': 0.6363636363636364, 'f': 0.11764705714568183}, 'rouge-2': {'r': 0.031496062992125984, 'p': 0.4, 'f': 0.05839415923064631}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5454545454545454, 'f': 0.10084033445660619}}\n",
            "pair:  crafting adversarial examples discrete inputs like text sequences fundamentally different generating examples continuous inputs like images paper tries answer question black box setting create adversarial examples automatically effectively fool deep learning classifiers texts making imperceptible changes answer firm yes previous efforts mostly replied using gradient evidence less effective either due finding nearest neighbor word wrt meaning automatically difficult relying heavily hand crafted linguistic rules instead use monte carlo tree search mcts finding important words perturb perform homoglyph attack replacing one character selected word symbol identical shape novel algorithm call mctsbug black box extremely effective time experimental results indicate mctsbug fool deep learning classifiers success rates seven large scale benchmark datasets perturbing characters surprisingly mctsbug without relying gradient information effective gradient based white box baseline thanks nature homoglyph attack generated adversarial perturbations almost imperceptible human eyes\n",
            "output sentence:  use monte carlo tree search homoglyphs generate indistinguishable adversarial samples text data \n",
            "\n",
            "{'rouge-1': {'r': 0.061224489795918366, 'p': 0.4, 'f': 0.10619468796303554}, 'rouge-2': {'r': 0.007462686567164179, 'p': 0.06666666666666667, 'f': 0.013422816981217308}, 'rouge-l': {'r': 0.05102040816326531, 'p': 0.3333333333333333, 'f': 0.08849557291878775}}\n",
            "pair:  despite considerable advances neural language modeling remains open question best decoding strategy text generation language model generate story counter intuitive empirical observation even though use likelihood training objective leads high quality models broad range language understanding tasks maximization based decoding methods beam search lead degeneration output text bland incoherent gets stuck repetitive loops address propose nucleus sampling simple effective method draw considerably higher quality text neural language models approach avoids text degeneration truncating unreliable tail probability distribution sampling dynamic nucleus tokens containing vast majority probability mass properly examine current maximization based stochastic decoding methods compare generations methods distribution human text along several axes likelihood diversity repetition results show maximization inappropriate decoding objective open ended text generation probability distributions best current language models unreliable tail needs truncated generation nucleus sampling best decoding strategy generating long form text high quality measured human evaluation diverse human written text\n",
            "output sentence:  current language generation systems either aim high likelihood devolve generic repetition miscalibrate stochasticity provide stochasticity propose \n",
            "\n",
            "{'rouge-1': {'r': 0.06, 'p': 0.5454545454545454, 'f': 0.10810810632253876}, 'rouge-2': {'r': 0.024793388429752067, 'p': 0.3, 'f': 0.045801525307383066}, 'rouge-l': {'r': 0.06, 'p': 0.5454545454545454, 'f': 0.10810810632253876}}\n",
            "pair:  recurrent neural networks rnns shown excellent performance processing sequence data however complex memory intensive due recursive nature limitations make rnns difficult embed mobile devices requiring real time processes limited hardware resources address issues introduce method learn binary ternary weights training phase facilitate hardware implementations rnns result using approach replaces multiply accumulate operations simple accumulations bringing significant benefits custom hardware terms silicon area power consumption software side evaluate performance terms accuracy method using long short term memories lstms gated recurrent units grus various sequential models including sequence classification language modeling demonstrate method achieves competitive results aforementioned tasks using binary ternary weights runtime hardware side present custom hardware accelerating recurrent computations lstms binary ternary weights ultimately show lstms binary ternary weights achieve memory saving inference speedup compared full precision hardware implementation design\n",
            "output sentence:  propose high performance lstms binary ternary weights greatly reduce implementation complexity \n",
            "\n",
            "{'rouge-1': {'r': 0.09615384615384616, 'p': 0.5, 'f': 0.16129031987513012}, 'rouge-2': {'r': 0.03571428571428571, 'p': 0.2222222222222222, 'f': 0.06153845915266282}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.4, 'f': 0.1290322553590011}}\n",
            "pair:  present new family objective functions term conditional entropy bottleneck ceb objectives motivated minimum necessary information mni criterion demonstrate application ceb classification tasks show ceb gives well calibrated predictions strong detection challenging distribution examples powerful whitebox adversarial examples substantial robustness adversaries finally report ceb fails learn information free datasets providing possible resolution problem generalization observed zhang et al\n",
            "output sentence:  conditional entropy bottleneck information theoretic objective function learning optimal representations \n",
            "\n",
            "{'rouge-1': {'r': 0.16417910447761194, 'p': 0.55, 'f': 0.2528735596776325}, 'rouge-2': {'r': 0.0759493670886076, 'p': 0.3, 'f': 0.1212121179879605}, 'rouge-l': {'r': 0.08955223880597014, 'p': 0.3, 'f': 0.13793103094200035}}\n",
            "pair:  study emergence cooperative behaviors reinforcement learning agents introducing challenging competitive multi agent soccer environment continuous simulated physics demonstrate decentralized population based training co play lead progression agents behaviors random simple ball chasing finally showing evidence cooperation study highlights several challenges encountered large scale multi agent training continuous control particular demonstrate automatic optimization simple shaping rewards conducive co operative behavior lead long horizon team behavior apply evaluation scheme grounded game theoretic principals assess agent performance absence pre defined evaluation tasks human baselines\n",
            "output sentence:  introduce new mujoco soccer environment continuous multi agent reinforcement learning research show population based training independent reinforcement learners learn cooperative behaviors \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.631578947368421, 'f': 0.21428571146843114}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.2, 'f': 0.06153845893491135}, 'rouge-l': {'r': 0.08602150537634409, 'p': 0.42105263157894735, 'f': 0.14285714003985975}}\n",
            "pair:  recent literature suggests averaged word vectors followed simple post processing outperform many deep learning methods semantic textual similarity tasks furthermore averaged word vectors trained supervised large corpora paraphrases achieve state art results standard sts benchmarks inspired insights push limits word embeddings even propose novel fuzzy bag words fbow representation text contains words vocabulary simultaneously different degrees membership derived similarities word vectors show max pooled word vectors special case fuzzy bow compared via fuzzy jaccard index rather cosine similarity finally propose dynamax completely unsupervised non parametric similarity measure dynamically extracts max pools good features depending sentence pair method efficient easy implement yet outperforms current baselines sts tasks large margin even competitive supervised word vectors trained directly optimise cosine similarity\n",
            "output sentence:  max pooled word vectors fuzzy jaccard set similarity extremely competitive baseline semantic similarity propose simple dynamic variant performs even even better \n",
            "\n",
            "{'rouge-1': {'r': 0.10714285714285714, 'p': 0.6666666666666666, 'f': 0.18461538222958582}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.4444444444444444, 'f': 0.1230769206911243}}\n",
            "pair:  dynamical system models including rnns often lack ability adapt sequence generation prediction given context limiting real world application paper show hierarchical multi task dynamical systems mtdss provide direct user control sequence generation via use latent code specifies customization individual data sequence enables style transfer interpolation morphing within generated sequences show mtds improve predictions via latent code interpolation avoid long term performance degradation standard rnn approaches\n",
            "output sentence:  tailoring predictions sequence models ldss rnns via explicit latent code \n",
            "\n",
            "{'rouge-1': {'r': 0.05128205128205128, 'p': 0.36363636363636365, 'f': 0.08988763828304512}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05128205128205128, 'p': 0.36363636363636365, 'f': 0.08988763828304512}}\n",
            "pair:  recent work shown deep reinforcement learning agents learn follow language like instructions infrequent environment rewards however places environment designers onus designing language conditional reward functions may easily tractably implemented complexity environment language scales overcome limitation present framework within instruction conditional rl agents trained using rewards obtained environment reward models jointly trained expert examples reward models improve learn accurately reward agents completing tasks environment configurations instructions present amongst expert data framework effectively separates representation instructions require executed simple grid world enables agent learn range commands requiring interaction blocks understanding spatial relations underspecified abstract arrangements show method allows agent adapt changes environment without requiring new expert examples\n",
            "output sentence:  propose agile framework training agents perform instructions examples respective goal states \n",
            "\n",
            "{'rouge-1': {'r': 0.04477611940298507, 'p': 0.6, 'f': 0.08333333204089506}, 'rouge-2': {'r': 0.0125, 'p': 0.25, 'f': 0.023809522902494367}, 'rouge-l': {'r': 0.04477611940298507, 'p': 0.6, 'f': 0.08333333204089506}}\n",
            "pair:  reproducibility reinforcement learning research highlighted key challenge area field paper present case study reproducing results one groundbreaking algorithm alphazero reinforcement learning system learns play go superhuman level given rules game describe minigo reproduction alphazero system using publicly available google cloud platform infrastructure google cloud tpus minigo system includes central reinforcement learning loop well auxiliary monitoring evaluation infrastructure ten days training scratch cloud tpus minigo play evenly leelazero elf opengo two strongest publicly available go ais discuss difficulties scaling reinforcement learning system monitoring systems required understand complex interplay hyperparameter configurations\n",
            "output sentence:  reproduced alphazero google cloud platform \n",
            "\n",
            "{'rouge-1': {'r': 0.22448979591836735, 'p': 0.9166666666666666, 'f': 0.36065573454447736}, 'rouge-2': {'r': 0.17857142857142858, 'p': 0.9090909090909091, 'f': 0.2985074599420807}, 'rouge-l': {'r': 0.22448979591836735, 'p': 0.9166666666666666, 'f': 0.36065573454447736}}\n",
            "pair:  perform depth investigation suitability self attention models character level neural machine translation test standard transformer model well novel variant encoder block combines information nearby characters using convolution perform extensive experiments wmt un datasets testing bilingual multilingual translation english using three input languages french spanish chinese transformer variant consistently outperforms standard transformer character level converges faster learning robust character level alignments\n",
            "output sentence:  perform depth investigation suitability self attention models character level neural machine translation \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.8, 'f': 0.1355932187877047}, 'rouge-2': {'r': 0.02459016393442623, 'p': 0.3333333333333333, 'f': 0.04580152543791158}, 'rouge-l': {'r': 0.06481481481481481, 'p': 0.7, 'f': 0.11864406624533182}}\n",
            "pair:  batch normalization bn often used attempt stabilize accelerate training deep neural networks many cases indeed decreases number parameter updates required achieve low training error however also reduces robustness small adversarial input perturbations common corruptions double digit percentages show five standard datasets furthermore find substituting weight decay bn sufficient nullify relationship adversarial vulnerability input dimension recent mean field analysis found bn induces gradient explosion used multiple layers cannot fully explain vulnerability observe given occurs already single bn layer argue actual cause tilting decision boundary respect nearest centroid classifier along input dimensions low variance result constant introduced numerical stability bn step acts important hyperparameter tuned recover robustness cost standard test accuracy explain mechanism explicitly linear toy model show experiments still holds nonlinear real world models\n",
            "output sentence:  batch normalization reduces robustness test time common corruptions adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.08045977011494253, 'p': 0.5384615384615384, 'f': 0.13999999773800004}, 'rouge-2': {'r': 0.01020408163265306, 'p': 0.08333333333333333, 'f': 0.018181816238016735}, 'rouge-l': {'r': 0.05747126436781609, 'p': 0.38461538461538464, 'f': 0.09999999773800006}}\n",
            "pair:  deep neural networks highly successful model class large memory footprint puts considerable strain energy consumption communication bandwidth storage requirements consequently model size reduction become utmost goal deep learning following classical bits back argument encode network weights using random sample requiring number bits corresponding kullback leibler divergence sampled variational distribution encoding distribution imposing constraint kullback leibler divergence able explicitly control compression rate optimizing expected loss training set employed encoding scheme shown close optimal information theoretical lower bound respect employed variational family benchmarks lenet mnist vgg cifar approach yields best test performance fixed memory budget vice versa achieves highest compression rates fixed test performance\n",
            "output sentence:  paper proposes effective coding scheme neural networks encodes random set weights variational distribution \n",
            "\n",
            "{'rouge-1': {'r': 0.14705882352941177, 'p': 0.8333333333333334, 'f': 0.24999999745}, 'rouge-2': {'r': 0.05194805194805195, 'p': 0.36363636363636365, 'f': 0.09090908872159097}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.4166666666666667, 'f': 0.12499999745000005}}\n",
            "pair:  flies mice species separated million years evolution yet evolved olfactory systems share many similarities anatomic functional organization functions shared anatomical functional features serve optimal odor sensing study address optimality evolutionary design olfactory circuits studying artificial neural networks trained sense odors found artificial neural networks quantitatively recapitulate structures inherent olfactory system including formation glomeruli onto compression layer sparse random connectivity onto expansion layer finally offer theoretical justifications result work offers framework explain evolutionary convergence olfactory circuits gives insight logic anatomic functional structure olfactory system\n",
            "output sentence:  artificial neural networks evolved structures present olfactory systems flies mice trained classify odors \n",
            "\n",
            "{'rouge-1': {'r': 0.1267605633802817, 'p': 0.75, 'f': 0.21686746740600957}, 'rouge-2': {'r': 0.011764705882352941, 'p': 0.08333333333333333, 'f': 0.020618554532894268}, 'rouge-l': {'r': 0.07042253521126761, 'p': 0.4166666666666667, 'f': 0.12048192523733495}}\n",
            "pair:  state art machine learning methods exhibit limited compositional generalization time lack realistic benchmarks comprehensively measure ability makes challenging find evaluate improvements introduce novel method systematically construct benchmarks maximizing compound divergence guaranteeing small atom divergence train test sets quantitatively compare method approaches creating compositional generalization benchmarks present large realistic natural language question answering dataset constructed according method use analyze compositional generalization ability three machine learning architectures find fail generalize compositionally surprisingly strong negative correlation compound divergence accuracy also demonstrate method used create new compositionality benchmarks top existing scan dataset confirms findings\n",
            "output sentence:  benchmark method measure compositional generalization maximizing divergence compound frequency small divergence atom frequency \n",
            "\n",
            "{'rouge-1': {'r': 0.14864864864864866, 'p': 0.7333333333333333, 'f': 0.24719100843327865}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.2857142857142857, 'f': 0.07692307459319533}, 'rouge-l': {'r': 0.12162162162162163, 'p': 0.6, 'f': 0.20224718820855958}}\n",
            "pair:  sparsely available data points cause numerical error finite differences hinder modeling dynamics physical systems discretization error becomes even larger sparse data irregularly distributed data defined unstructured grid making hard build deep learning models handle physics governing observations unstructured grid paper propose novel architecture named physics aware difference graph networks pa dgn exploits neighboring information learn finite differences inspired physics equations pa dgn leverages data driven end end learning discover underlying dynamical relations spatial temporal differences given observations demonstrate superiority pa dgn approximation directional derivatives prediction graph signals synthetic data real world climate observations weather stations\n",
            "output sentence:  propose physics aware difference graph networks designed effectively learn spatial differences modeling sparsely observed dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.1506849315068493, 'p': 0.9166666666666666, 'f': 0.2588235269868512}, 'rouge-2': {'r': 0.10989010989010989, 'p': 0.9090909090909091, 'f': 0.19607842944828915}, 'rouge-l': {'r': 0.1506849315068493, 'p': 0.9166666666666666, 'f': 0.2588235269868512}}\n",
            "pair:  introduce systematic framework quantifying robustness classifiers naturally occurring perturbations images found videos part framework construct imagenet vid robust human expert reviewed dataset images grouped sets perceptually similar images derived frames imagenet video object detection dataset evaluate diverse array classifiers trained imagenet including models trained robustness show median classification accuracy drop additionally evaluate faster cnn fcn models detection show natural perturbations induce classification well localization errors leading median drop detection map points analysis shows natural perturbations real world heavily problematic current cnns posing significant challenge deployment safety critical environments require reliable low latency predictions\n",
            "output sentence:  introduce systematic framework quantifying robustness classifiers naturally occurring perturbations images found videos \n",
            "\n",
            "{'rouge-1': {'r': 0.1188118811881188, 'p': 0.75, 'f': 0.20512820276718532}, 'rouge-2': {'r': 0.024, 'p': 0.2, 'f': 0.04285714094387764}, 'rouge-l': {'r': 0.039603960396039604, 'p': 0.25, 'f': 0.06837606601504867}}\n",
            "pair:  open domain question answering qa important problem ai nlp emerging bellwether progress generalizability ai methods techniques much progress open domain qa systems realized advances information retrieval methods corpus construction paper focus recently introduced arc challenge dataset contains multiple choice questions authored grade school science exams questions selected challenging current qa systems current state art performance slightly better random chance present system reformulates given question queries used retrieve supporting text large corpus science related text rewriter able incorporate background knowledge conceptnet tandem generic textual entailment system trained scitail identifies support retrieved results outperforms several strong baselines end end qa task despite trained identify essential terms original source question use generalizable decision methodology retrieved evidence answer candidates select best answer combining query reformulation background knowledge textual entailment system able outperform several strong baselines arc dataset\n",
            "output sentence:  explore using background knowledge query reformulation help retrieve better supporting evidence answering multiple choice science questions \n",
            "\n",
            "{'rouge-1': {'r': 0.07, 'p': 0.7777777777777778, 'f': 0.1284403654574531}, 'rouge-2': {'r': 0.023809523809523808, 'p': 0.375, 'f': 0.04477611828024061}, 'rouge-l': {'r': 0.03, 'p': 0.3333333333333333, 'f': 0.055045870044609084}}\n",
            "pair:  recent image super resolution sr studies leverage deep convolutional neural networks rich hierarchical features offered leads better reconstruction performance conventional methods however small receptive fields sampling reconstruction process models stop take full advantage global contextual information causes problems performance improvement paper inspired image reconstruction principles human visual system propose image super resolution global reasoning network srgrn effectively learn correlations different regions image global reasoning specifically propose global reasoning sampling module grum global reasoning reconstruction block grrb construct graph model perform relation reasoning regions low resolution lr images aim reason interactions different regions sampling reconstruction process thus leverage contextual information generate accurate details proposed srgrn robust handle low resolution images corrupted multiple types degradation extensive experiments different benchmark data sets show model outperforms state art methods also model lightweight consumes less computing power makes suitable real life deployment\n",
            "output sentence:  state art model based global reasoning image super resolution \n",
            "\n",
            "{'rouge-1': {'r': 0.029850746268656716, 'p': 0.4, 'f': 0.05555555426311731}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.014925373134328358, 'p': 0.2, 'f': 0.027777776485339568}}\n",
            "pair:  neural conversational models widely used applications like personal assistants chat bots models seem give better performance operating word level however fusion languages like french russian polish vocabulary size sometimes become infeasible since words lots word forms propose neural network architecture transforming normalized text grammatically correct one model efficiently employs correspondence normalized target words significantly outperforms character level models faster training faster evaluation also propose new pipeline building conversational models first generate normalized answer transform grammatically correct one using network proposed pipeline gives better performance character level conversational models according assessor testing\n",
            "output sentence:  proposed architecture solve morphological agreement task \n",
            "\n",
            "{'rouge-1': {'r': 0.0684931506849315, 'p': 0.3333333333333333, 'f': 0.11363636080836784}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0547945205479452, 'p': 0.26666666666666666, 'f': 0.09090908808109513}}\n",
            "pair:  pruning neural network parameters often viewed means compress models pruning also motivated desire prevent overfitting motivation particularly relevant given perhaps surprising observation wide variety pruning approaches increase test accuracy despite sometimes massive reductions parameter counts better understand phenomenon analyze behavior pruning course training finding pruning effect generalization relies instability generates defined drops test accuracy immediately following pruning final size pruned model demonstrate even pruning unimportant parameters lead instability show similarities pruning regularizing injecting noise suggesting mechanism pruning based generalization improvements compatible strong generalization recently observed parameterized networks\n",
            "output sentence:  demonstrate pruning methods introduce greater instability loss also confer improved generalization explore mechanisms underlying effect \n",
            "\n",
            "{'rouge-1': {'r': 0.06493506493506493, 'p': 0.625, 'f': 0.11764705711833912}, 'rouge-2': {'r': 0.021052631578947368, 'p': 0.2857142857142857, 'f': 0.03921568499615537}, 'rouge-l': {'r': 0.06493506493506493, 'p': 0.625, 'f': 0.11764705711833912}}\n",
            "pair:  present tensor train rnn tt rnn novel family neural sequence architectures multivariate forecasting environments nonlinear dynamics long term forecasting systems highly challenging since exist long term temporal dependencies higher order correlations sensitivity error propagation proposed tensor recurrent architecture addresses issues learning nonlinear dynamics directly using higher order moments high order state transition functions furthermore decompose higher order structure using tensor train tt decomposition reduce number parameters preserving model performance theoretically establish approximation properties tensor train rnns general sequence inputs guarantees available usual rnns also demonstrate significant long term prediction improvements general rnn lstm architectures range simulated environments nonlinear dynamics well real world climate traffic data\n",
            "output sentence:  accurate forecasting long time horizons using tensor train rnns \n",
            "\n",
            "{'rouge-1': {'r': 0.19607843137254902, 'p': 0.8333333333333334, 'f': 0.31746031437641725}, 'rouge-2': {'r': 0.06451612903225806, 'p': 0.36363636363636365, 'f': 0.10958903853631081}, 'rouge-l': {'r': 0.17647058823529413, 'p': 0.75, 'f': 0.2857142826303855}}\n",
            "pair:  federated learning involves jointly learning massively distributed partitions data generated remote devices naively minimizing aggregate loss function network may disproportionately advantage disadvantage devices work propose fair federated learning ffl novel optimization objective inspired resource allocation strategies wireless networks encourages fair accuracy distribution across devices federated networks solve ffl devise scalable method fedavg run federated networks validate improved fairness flexibility ffl efficiency fedavg simulations federated datasets\n",
            "output sentence:  propose novel optimization objective encourages fairness heterogeneous federated networks develop scalable method solve \n",
            "\n",
            "{'rouge-1': {'r': 0.06976744186046512, 'p': 0.5454545454545454, 'f': 0.1237113381953449}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.2, 'f': 0.032786883740929924}, 'rouge-l': {'r': 0.05813953488372093, 'p': 0.45454545454545453, 'f': 0.10309278149431397}}\n",
            "pair:  touch interactions current mobile devices limited expressiveness augmenting devices additional degrees freedom add power interaction several augmentations proposed tested however still little known effects learning multiple sets augmented interactions mapped different applications better understand whether multiple command mappings interfere one another affect transfer retention developed prototype three pushbuttons smartphone case used provide augmented input system buttons chorded provide seven possible shortcuts transient mode switches mapped buttons three different sets actions carried study see multiple mappings affect learning performance transfer retention results show mappings quickly learned reduction performance multiple mappings transfer realistic task successful although slight reduction accuracy retention one week initially poor expert performance quickly restored work provides new information design use augmented input mobile interactions\n",
            "output sentence:  describes study investigating interference transfer retention multiple mappings set chorded buttons \n",
            "\n",
            "{'rouge-1': {'r': 0.0379746835443038, 'p': 0.42857142857142855, 'f': 0.06976744036506224}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0379746835443038, 'p': 0.42857142857142855, 'f': 0.06976744036506224}}\n",
            "pair:  motivated applications unsupervised learning consider problem measuring mutual information recent analysis shown naive knn estimators mutual information serious statistical limitations motivating refined methods paper prove serious statistical limitations inherent measurement method specifically show distribution free high confidence lower bound mutual information cannot larger ln size data sample also analyze donsker varadhan lower bound kl divergence particular show simple statistical considerations taken account bound never produce high confidence value larger ln large high confidence lower bounds impossible practice one use estimators without formal guarantees suggest expressing mutual information difference entropies using cross entropy entropy estimator observe although cross entropy upper bound entropy cross entropy estimates converge true cross entropy rate sqrt\n",
            "output sentence:  give theoretical analysis measurement optimization mutual information \n",
            "\n",
            "{'rouge-1': {'r': 0.23684210526315788, 'p': 0.6428571428571429, 'f': 0.34615384221893497}, 'rouge-2': {'r': 0.09302325581395349, 'p': 0.3076923076923077, 'f': 0.14285713929209193}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.5714285714285714, 'f': 0.3076923037573965}}\n",
            "pair:  deepa deep learning framework explores parallelism parallelizable dimensions accelerate training process convolutional neural networks deepa optimizes parallelism granularity individual layer network present elimination based algorithm finds optimal parallelism configuration every layer evaluation shows deepa achieves speedup compared state art deep learning frameworks reduces data transfers\n",
            "output sentence:  best knowledge deepa first deep learning framework controls optimizes parallelism cnns parallelizable dimensions granularity layer \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.9, 'f': 0.17999999820000004}, 'rouge-2': {'r': 0.05172413793103448, 'p': 0.6666666666666666, 'f': 0.09599999866368002}, 'rouge-l': {'r': 0.07777777777777778, 'p': 0.7, 'f': 0.13999999820000003}}\n",
            "pair:  recent work adversarial machine learning started focus visual perception autonomous driving studied adversarial examples aes object detection models however visual perception pipeline detected objects must also tracked process called multiple object tracking mot build moving trajectories surrounding obstacles since mot designed robust errors object detection poses general challenge existing attack techniques blindly target objection detection find success rate needed actually affect tracking results requirement existing attack technique satisfy paper first study adversarial machine learning attacks complete visual perception pipeline autonomous driving discover novel attack technique tracker hijacking effectively fool mot using aes object detection using technique successful aes one single frame move existing object headway autonomous vehicle cause potential safety hazards perform evaluation using berkeley deep drive dataset find average frames attacked attack nearly success rate attacks blindly target object detection\n",
            "output sentence:  study adversarial machine learning attacks multiple object tracking mechanisms first time \n",
            "\n",
            "{'rouge-1': {'r': 0.07865168539325842, 'p': 0.6363636363636364, 'f': 0.139999998042}, 'rouge-2': {'r': 0.025423728813559324, 'p': 0.3, 'f': 0.04687499855957036}, 'rouge-l': {'r': 0.0449438202247191, 'p': 0.36363636363636365, 'f': 0.07999999804200005}}\n",
            "pair:  designing accurate efficient convolutional neural architectures vast amount hardware challenging hardware designs complex diverse paper addresses hardware diversity challenge neural architecture search nas unlike previous approaches apply search algorithms small human designed search space without considering hardware diversity propose hurricane explores automatic hardware aware search much larger search space multistep search scheme coordinate ascent framework generate tailored models different types hardware extensive experiments imagenet show algorithm consistently achieves much lower inference latency similar better accuracy state art nas methods three types hardware remarkably hurricane achieves top accuracy imagenet inference latency ms dsp higher accuracy inference speedup fbnet iphonex vpu hurricane achieves higher top accuracy proxyless mobile speedup even well studied mobile cpu hurricane achieves higher top accuracy fbnet iphonex comparable inference latency hurricane also reduces training time average compared singlepath oneshot\n",
            "output sentence:  propose hurricane address challenge hardware diversity one shot neural architecture search \n",
            "\n",
            "{'rouge-1': {'r': 0.09411764705882353, 'p': 0.8888888888888888, 'f': 0.17021276422589407}, 'rouge-2': {'r': 0.06862745098039216, 'p': 0.875, 'f': 0.12727272592396696}, 'rouge-l': {'r': 0.09411764705882353, 'p': 0.8888888888888888, 'f': 0.17021276422589407}}\n",
            "pair:  paper formalises problem online algorithm selection context reinforcement learning rl setup follows given episodic task finite number policy rl algorithms meta algorithm decide rl algorithm control next episode maximize expected return article presents novel meta algorithm called epochal stochastic bandit algorithm selection esbas principle freeze policy updates epoch leave rebooted stochastic bandit charge algorithm selection assumptions thorough theoretical analysis demonstrates near optimality considering structural sampling budget limitations esbas first empirically evaluated dialogue task shown outperform individual algorithm configurations esbas adapted true online setting algorithms update policies transition call ssbas ssbas evaluated fruit collection task shown adapt stepsize parameter efficiently classical hyperbolic decay atari game improves performance wide margin\n",
            "output sentence:  paper formalises problem online algorithm selection context reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.4, 'f': 0.13793103162901313}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.4, 'f': 0.13793103162901313}}\n",
            "pair:  order choose neural network architecture effective particular modeling problem one must understand limitations imposed potential options limitations typically described terms information theoretic bounds comparing relative complexity needed approximate example functions different architectures paper examine topological constraints architecture neural network imposes level sets functions able approximate approach novel nature limitations fact independent network depth broad family activation functions\n",
            "output sentence:  paper proves skinny neural networks cannot approximate certain functions matter deep \n",
            "\n",
            "{'rouge-1': {'r': 0.2835820895522388, 'p': 0.95, 'f': 0.436781605654644}, 'rouge-2': {'r': 0.2222222222222222, 'p': 0.8181818181818182, 'f': 0.3495145597473843}, 'rouge-l': {'r': 0.2835820895522388, 'p': 0.95, 'f': 0.436781605654644}}\n",
            "pair:  recent advances deep generative models lead remarkable progress synthesizing high quality images following successful application image processing representation learning important next step consider videos learning generative models video much harder task requiring model capture temporal dynamics scene addition visual presentation objects recent generative models video success current progress hampered lack qualitative metrics consider visual quality temporal coherence diversity samples extent propose fr chet video distance fvd new metric generative models video based fid contribute large scale human study confirms fvd correlates well qualitative human judgment generated videos\n",
            "output sentence:  propose fvd new metric generative models video based fid large scale human study confirms fvd correlates well qualitative human judgment generated generated generated \n",
            "\n",
            "{'rouge-1': {'r': 0.2222222222222222, 'p': 0.9411764705882353, 'f': 0.35955055870723396}, 'rouge-2': {'r': 0.17045454545454544, 'p': 0.8823529411764706, 'f': 0.28571428300045354}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.9411764705882353, 'f': 0.35955055870723396}}\n",
            "pair:  propose warped residual network warpnet using parallelizable warp operator forward backward propagation distant layers trains faster original residual neural network apply perturbation theory residual networks decouple interactions residual units resulting warp operator first order approximation output multiple layers first order perturbation theory exhibits properties binomial path lengths exponential gradient scaling found experimentally veit et al demonstrate extensive performance study proposed network achieves comparable predictive performance original residual network number parameters achieving significant speed total training time warpnet performs model parallelism residual network training weights distributed different gpus offers speed capability train larger networks compared original residual networks\n",
            "output sentence:  propose warped residual network using parallelizable warp operator forward backward propagation distant layers trains faster original residual network \n",
            "\n",
            "{'rouge-1': {'r': 0.1388888888888889, 'p': 0.7692307692307693, 'f': 0.23529411505605538}, 'rouge-2': {'r': 0.04395604395604396, 'p': 0.3333333333333333, 'f': 0.07766990085399195}, 'rouge-l': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}}\n",
            "pair:  information bottleneck method provides information theoretic method representation learning training encoder retain information relevant predicting label minimizing amount superfluous information representation original formulation however requires labeled data order identify information superfluous work extend ability multi view unsupervised setting two views underlying entity provided label unknown enables us identify superfluous information shared views theoretical analysis leads definition new multi view model produces state art results sketchy dataset label limited versions mir flickr dataset also extend theory single view setting taking advantage standard data augmentation techniques empirically showing better generalization capabilities compared traditional unsupervised approaches representation learning\n",
            "output sentence:  extend information bottleneck method unsupervised multiview setting show state art results standard datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.625, 'f': 0.13157894548476456}, 'rouge-2': {'r': 0.03260869565217391, 'p': 0.42857142857142855, 'f': 0.060606059291909015}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.625, 'f': 0.13157894548476456}}\n",
            "pair:  effectively capturing graph node sequences form vector embeddings critical many applications achieve first learning vector embeddings single graph nodes ii composing compactly represent node sequences specifically propose sense semantically enhanced node sequence embedding single nodes skip gram based novel embedding mechanism single graph nodes co learns graph structure well textual descriptions demonstrate sense vectors increase accuracy multi label classification tasks link prediction tasks variety scenarios using real datasets based sense next propose generic sense compute composite vectors represent sequence nodes preserving node order important prove approach efficient embedding node sequences experiments real data confirm high accuracy node order decoding\n",
            "output sentence:  node sequence embedding mechanism captures graph text properties \n",
            "\n",
            "{'rouge-1': {'r': 0.20270270270270271, 'p': 1.0, 'f': 0.3370786488827169}, 'rouge-2': {'r': 0.1320754716981132, 'p': 1.0, 'f': 0.23333333127222225}, 'rouge-l': {'r': 0.20270270270270271, 'p': 1.0, 'f': 0.3370786488827169}}\n",
            "pair:  mixup data augmentation scheme pairs training samples corresponding labels mixed using linear coefficients without label mixing mixup becomes conventional scheme input samples moved original labels retained samples preferentially moved direction classes iffalse typically clustered input space fi refer method directional adversarial training dat show two mild conditions mixup asymptotically convergences subset dat define untied mixup umixup superset mixup wherein training labels mixed different linear coefficients corresponding samples show mild conditions untied mixup converges entire class dat schemes motivated understanding umixup generalization mixup form adversarial training experiment different datasets loss functions show umixup provides improved performance mixup short present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup\n",
            "output sentence:  present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup \n",
            "\n",
            "{'rouge-1': {'r': 0.1323529411764706, 'p': 0.6428571428571429, 'f': 0.21951219229030342}, 'rouge-2': {'r': 0.05, 'p': 0.3076923076923077, 'f': 0.08602150297144186}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.35714285714285715, 'f': 0.12195121668054736}}\n",
            "pair:  unsupervised monocular depth estimation made great progress deep learning involved training binocular stereo images considered good option data easily obtained however depth disparity prediction results show poor performance object boundaries main reason related handling occlusion areas training paper propose novel method overcome issue exploiting disparity maps property generate occlusion mask block back propagation occlusion areas image warping also design new networks flipped stereo images induce networks learn occluded boundaries shows method achieves clearer boundaries better evaluation results kitti driving dataset virtual kitti dataset\n",
            "output sentence:  paper propose mask method solves previous blurred results unsupervised monocular depth estimation caused occlusion \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.7, 'f': 0.2372881327779374}, 'rouge-2': {'r': 0.0603448275862069, 'p': 0.35, 'f': 0.10294117396193779}, 'rouge-l': {'r': 0.12244897959183673, 'p': 0.6, 'f': 0.2033898276931916}}\n",
            "pair:  symbolic logic allows practitioners build systems perform rule based reasoning interpretable easily augmented prior knowledge however systems traditionally difficult apply problems involving natural language due large linguistic variability language currently work natural language processing focuses neural networks learn distributed representations words composition thereby performing well presence large linguistic variability propose reap benefits approaches applying combination neural networks logic programming natural language question answering propose employ external non differentiable prolog prover utilizes similarity function pretrained sentence encoders fine tune representations via evolution strategies goal multi hop reasoning natural language allows us create system apply rule based reasoning natural language induce domain specific natural language rules training data evaluate proposed system two different question answering tasks showing complements two strong baselines bidaf seo et al fastqa weissenborn et al outperforms used ensemble\n",
            "output sentence:  introduce nlprolog system performs rule based reasoning natural language leveraging pretrained sentence embeddings fine tuning evolution strategies apply multi hop hop hop tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.26666666666666666, 'p': 0.9230769230769231, 'f': 0.4137930999702736}, 'rouge-2': {'r': 0.22, 'p': 0.9166666666666666, 'f': 0.3548387065556712}, 'rouge-l': {'r': 0.26666666666666666, 'p': 0.9230769230769231, 'f': 0.4137930999702736}}\n",
            "pair:  introduce attention mechanism improve feature extraction deep active learning al semi supervised setting proposed attention mechanism based recent methods visually explain predictions made dnns apply proposed explanation based attention mnist svhn classification conducted experiments show accuracy improvements original class imbalanced datasets number training examples faster long tail convergence compared uncertainty based methods\n",
            "output sentence:  introduce attention mechanism improve feature extraction deep active learning al semi supervised setting \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.6, 'f': 0.1499999978125}, 'rouge-2': {'r': 0.01282051282051282, 'p': 0.1111111111111111, 'f': 0.022988503892191984}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.5, 'f': 0.12499999781250003}}\n",
            "pair:  open question deep learning community neural networks trained gradient descent generalize well real datasets even though capable fitting random data propose approach answering question based hypothesis dynamics gradient descent call coherent gradients gradients similar examples similar overall gradient stronger certain directions reinforce thus changes network parameters training biased towards locally simultaneously benefit many examples similarity exists support hypothesis heuristic arguments perturbative experiments outline explain several common empirical observations deep learning furthermore analysis descriptive prescriptive suggests natural modification gradient descent greatly reduce overfitting\n",
            "output sentence:  propose hypothesis gradient descent generalizes based per example gradients interact \n",
            "\n",
            "{'rouge-1': {'r': 0.09278350515463918, 'p': 0.47368421052631576, 'f': 0.15517241105380503}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.1111111111111111, 'f': 0.028571426330612427}, 'rouge-l': {'r': 0.07216494845360824, 'p': 0.3684210526315789, 'f': 0.12068965243311539}}\n",
            "pair:  adversarial feature learning afl one promising ways explicitly constrains neural networks learn desired representations example afl could help learn anonymized representations avoid privacy issues afl learn representations training networks deceive adversary predict sensitive information network therefore success afl heavily relies choice adversary paper proposes novel design adversary em multiple adversaries random subspaces mars instantiate concept em volunerableness proposed method motivated assumption deceiving adversary could fail give meaningful information adversary easily fooled adversary rely single classifier suffer issues contrast proposed method designed less vulnerable utilizing ensemble independent classifiers classifier tries predict sensitive variables different em subset representations empirical validations three user anonymization tasks show proposed method achieves state art performances three datasets without significantly harming utility data significant gives new implications designing adversary important improve performance afl\n",
            "output sentence:  paper improves quality recently proposed adversarial feature leaning afl approach incorporating explicit constrains representations introducing concept em vulnerableness adversary \n",
            "\n",
            "{'rouge-1': {'r': 0.14634146341463414, 'p': 0.46153846153846156, 'f': 0.22222221856652952}, 'rouge-2': {'r': 0.0625, 'p': 0.25, 'f': 0.0999999968000001}, 'rouge-l': {'r': 0.14634146341463414, 'p': 0.46153846153846156, 'f': 0.22222221856652952}}\n",
            "pair:  address problem marginal inference exponential family defined set permutation matrices problem known quickly become intractable size permutation increases since involves computation permanent matrix hard problem introduce sinkhorn variational marginal inference scalable alternative method whose validity ultimately justified called sinkhorn approximation permanent demonstrate efectiveness method problem probabilistic identification neurons worm elegans\n",
            "output sentence:  new methodology variational marginal inference permutations based sinkhorn algorithm applied probabilistic identification neurons \n",
            "\n",
            "{'rouge-1': {'r': 0.12087912087912088, 'p': 0.5238095238095238, 'f': 0.19642856838169645}, 'rouge-2': {'r': 0.04424778761061947, 'p': 0.25, 'f': 0.07518796736955179}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.3333333333333333, 'f': 0.12499999695312508}}\n",
            "pair:  high dimensional reinforcement learning settings sparse rewards performing effective exploration even obtain reward signal open challenge model based approaches hold promise better exploration via planning extremely difficult learn reliable enough markov decision process mdp high dimensions states paper propose learning abstract mdp much smaller number states plan effective exploration assume abstraction function maps concrete states raw pixels abstract states agent position ignoring objects approach manager maintains abstract mdp subset abstract states grows monotonically targeted exploration possible due abstract mdp concurrently learn worker policy travel abstract states worker deals messiness concrete states presents clean abstraction manager three hardest games arcade learning environment montezuma pitfall private eye approach outperforms previous state art factor game pitfall approach first achieve superhuman performance without demonstrations\n",
            "output sentence:  automatically construct explore small abstract markov decision process enabling us achieve state art results montezuma revenge pitfall private eye significant margin \n",
            "\n",
            "{'rouge-1': {'r': 0.0684931506849315, 'p': 0.625, 'f': 0.12345678834324036}, 'rouge-2': {'r': 0.036585365853658534, 'p': 0.375, 'f': 0.06666666504691361}, 'rouge-l': {'r': 0.0684931506849315, 'p': 0.625, 'f': 0.12345678834324036}}\n",
            "pair:  temporal point processes dominant paradigm modeling sequences events happening irregular intervals standard way learning models estimating conditional intensity function however parameterizing intensity function usually incurs several trade offs show overcome limitations intensity based approaches directly modeling conditional distribution inter event times draw literature normalizing flows design models flexible efficient additionally propose simple mixture model matches flexibility flow based models also permits sampling computing moments closed form proposed models achieve state art performance standard prediction tasks suitable novel applications learning sequence embeddings imputing missing data\n",
            "output sentence:  learn temporal point processes modeling conditional density conditional intensity \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.5, 'f': 0.13698629900544196}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07936507936507936, 'p': 0.5, 'f': 0.13698629900544196}}\n",
            "pair:  capturing high level structure audio waveforms challenging single second audio spans tens thousands timesteps long range dependencies difficult model directly time domain show tractably modelled two dimensional time frequency representations spectrograms leveraging representational advantage conjunction highly expressive probabilistic model multiscale generation procedure design model capable generating high fidelity audio samples capture structure timescales time domain models yet achieve demonstrate model captures longer range dependencies time domain models wavenet across diverse set unconditional generation tasks including single speaker speech generation multi speaker speech generation music generation\n",
            "output sentence:  introduce autoregressive generative model spectrograms demonstrate applications speech music generation \n",
            "\n",
            "{'rouge-1': {'r': 0.24242424242424243, 'p': 0.8421052631578947, 'f': 0.3764705847640139}, 'rouge-2': {'r': 0.09876543209876543, 'p': 0.36363636363636365, 'f': 0.15533980246583096}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.631578947368421, 'f': 0.2823529377051903}}\n",
            "pair:  model free deep reinforcement learning approaches shown superhuman performance simulated environments atari games go etc training approaches often implicitly construct latent space contains key information decision making paper learn forward model latent space apply model based planning miniature real time strategy game incomplete information minirts first show latent space constructed existing actor critic models contains relevant information game design training procedure learn forward models also show learned forward model predict meaningful future state usable latent space monte carlo tree search mcts terms win rates rule based agents\n",
            "output sentence:  paper analyzes latent space learned model free approaches miniature incomplete information game trains forward model latent space apply monte monte tree search search positive \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.7777777777777778, 'f': 0.2121212097658402}, 'rouge-2': {'r': 0.030303030303030304, 'p': 0.2222222222222222, 'f': 0.05333333122133342}, 'rouge-l': {'r': 0.12280701754385964, 'p': 0.7777777777777778, 'f': 0.2121212097658402}}\n",
            "pair:  introduce adaptive input representations neural language modeling extend adaptive softmax grave et al input representations variable capacity several choices factorize input output layers whether model words characters sub word units perform systematic comparison popular choices self attentional architecture experiments show models equipped adaptive embeddings twice fast train popular character input cnn lower number parameters wikitext benchmark achieve perplexity improvement perplexity compared previously best published result billion word benchmark achieve perplexity\n",
            "output sentence:  variable capacity input word embeddings sota wikitext billion word benchmarks \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.5, 'f': 0.19718309542551085}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.23076923076923078, 'f': 0.06593406348508643}, 'rouge-l': {'r': 0.10526315789473684, 'p': 0.42857142857142855, 'f': 0.16901408134100382}}\n",
            "pair:  graph neural networks gnns class deep models operates data arbitrary topology order invariant structure represented graphs introduce efficient memory layer gnns learn jointly perform graph representation learning graph pooling also introduce two new networks based memory layer memory based graph neural network memgnn graph memory network gmn learn hierarchical graph representations coarsening graph throughout layers memory experimental results demonstrate proposed models achieve state art results six seven graph classification regression benchmarks also show learned representations could correspond chemical features molecule data\n",
            "output sentence:  introduce efficient memory layer learn representation coarsen input graphs simultaneously without relying message passing \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}, 'rouge-2': {'r': 0.08860759493670886, 'p': 0.875, 'f': 0.16091953855991545}, 'rouge-l': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}}\n",
            "pair:  propose new model making generalizable diverse retrosynthetic reaction predictions given target compound task predict likely chemical reactants produce target generative task framed sequence sequence problem using smiles representations molecules building top popular transformer architecture propose two novel pre training methods construct relevant auxiliary tasks plausible reactions problem furthermore incorporate discrete latent variable model architecture encourage model produce diverse set alternative predictions subset reaction examples united states patent literature uspto benchmark dataset model greatly improves performance baseline also generating predictions diverse\n",
            "output sentence:  propose new model making generalizable diverse retrosynthetic reaction predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.5, 'f': 0.20689654844233057}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.18181818181818182, 'f': 0.060606057828282954}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.3333333333333333, 'f': 0.13793103120095132}}\n",
            "pair:  well known neural networks universal approximators deeper networks tend practice powerful shallower ones shed light proving total number neurons required approximate natural classes multivariate polynomials variables grows linearly deep neural networks grows exponentially merely single hidden layer allowed also provide evidence number hidden layers increased neuron requirement grows exponentially suggesting minimum number layers required practical expressibility grows logarithmically\n",
            "output sentence:  prove deep neural networks exponentially efficient shallow ones approximating sparse multivariate polynomials \n",
            "\n",
            "{'rouge-1': {'r': 0.05084745762711865, 'p': 0.5, 'f': 0.0923076906319527}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.3333333333333333, 'f': 0.06153845986272194}}\n",
            "pair:  work offers new method domain translation semantic label maps computer graphic cg simulation edge map images photo realistic im ages train generative adversarial network gan conditional way generate photo realistic version given cg scene existing architectures gans still lack photo realism capabilities needed train dnns computer vision tasks address issue embedding edge maps training adversarial mode also offer extension model uses gan architecture create visually appealing temporally coherent videos\n",
            "output sentence:  simulation real images translation video generation \n",
            "\n",
            "{'rouge-1': {'r': 0.1044776119402985, 'p': 0.5384615384615384, 'f': 0.17499999727812504}, 'rouge-2': {'r': 0.0379746835443038, 'p': 0.25, 'f': 0.06593406364448746}, 'rouge-l': {'r': 0.08955223880597014, 'p': 0.46153846153846156, 'f': 0.14999999727812502}}\n",
            "pair:  recent work cross lingual word embeddings severely anglocentric vast majority lexicon induction evaluation dictionaries english another language english embedding space selected default hub learning multilingual setting work however challenge practices first show choice hub language significantly impact downstream lexicon induction performance second expand current evaluation dictionary collection include language pairs using triangulation also create new dictionaries represented languages evaluating established methods language pairs sheds light suitability presents new challenges field finally analysis identify general guidelines strong cross lingual embeddings baselines based anglocentric experiments\n",
            "output sentence:  choice hub target language affects quality cross lingual embeddings evaluated english centric dictionaries \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.7777777777777778, 'f': 0.15053763266042317}, 'rouge-2': {'r': 0.019801980198019802, 'p': 0.25, 'f': 0.0366972463462672}, 'rouge-l': {'r': 0.05952380952380952, 'p': 0.5555555555555556, 'f': 0.10752687997225113}}\n",
            "pair:  deep generative neural networks proven effective conditional unconditional modeling complex data distributions conditional generation enables interactive control creating new controls often requires expensive retraining paper develop method condition generation without retraining model post hoc learning latent constraints value functions identify regions latent space generate outputs desired attributes conditionally sample regions gradient based optimization amortized actor functions combining attribute constraints universal realism constraint enforces similarity data distribution generate realistic conditional images unconditional variational autoencoder using gradient based optimization demonstrate identity preserving transformations make minimal adjustment latent space modify attributes image finally discrete sequences musical notes demonstrate zero shot conditional generation learning latent constraints absence labeled data differentiable reward function\n",
            "output sentence:  new approach conditional generation constraining latent space unconditional generative model \n",
            "\n",
            "{'rouge-1': {'r': 0.06493506493506493, 'p': 0.5555555555555556, 'f': 0.11627906789345592}, 'rouge-2': {'r': 0.010526315789473684, 'p': 0.125, 'f': 0.019417474295409663}, 'rouge-l': {'r': 0.06493506493506493, 'p': 0.5555555555555556, 'f': 0.11627906789345592}}\n",
            "pair:  field deep reinforcement learning drl recently seen surge popularity maximum entropy reinforcement learning algorithms popularity stems intuitive interpretation maximum entropy objective superior sample efficiency standard benchmarks paper seek understand primary contribution entropy term performance maximum entropy algorithms mujoco benchmark demonstrate entropy term soft actor critic sac principally addresses bounded nature action spaces insight propose simple normalization scheme allows streamlined algorithm without entropy maximization match performance sac experimental results demonstrate need revisit benefits entropy regularization drl also propose simple non uniform sampling method selecting transitions replay buffer training show streamlined algorithm simple non uniform sampling scheme outperforms sac achieves state art performance challenging continuous control tasks\n",
            "output sentence:  propose new drl policy algorithm achieving state art performance \n",
            "\n",
            "{'rouge-1': {'r': 0.07017543859649122, 'p': 0.36363636363636365, 'f': 0.11764705611159176}, 'rouge-2': {'r': 0.034482758620689655, 'p': 0.2, 'f': 0.058823526903114286}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.2727272727272727, 'f': 0.08823529140570943}}\n",
            "pair:  partially observable markov decision processes pomdps natural model scenarios one deal incomplete knowledge random events applications include limited robotics motion planning however many relevant properties pomdps either undecidable expensive compute terms runtime memory consumption work develop game based abstraction method able deliver safe bounds tight approximations important sub classes properties discuss theoretical implications showcase applicability results broad spectrum benchmarks\n",
            "output sentence:  paper provides game based abstraction scheme compute provably sound policies pomdps \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.3333333333333333, 'f': 0.10526315523545714}, 'rouge-2': {'r': 0.012195121951219513, 'p': 0.09090909090909091, 'f': 0.02150537425829596}, 'rouge-l': {'r': 0.0625, 'p': 0.3333333333333333, 'f': 0.10526315523545714}}\n",
            "pair:  vector semantics especially sentence vectors recently used successfully many areas natural language processing however relatively little work explored internal structure properties spaces sentence vectors paper explore properties sentence vectors studying particular real world application automatic summarization particular show cosine similarity sentence vectors document vectors strongly correlated sentence importance vector semantics identify correct gaps sentences chosen far document addition identify specific dimensions linked effective summaries knowledge first time specific dimensions sentence embeddings connected sentence properties also compare features different methods sentence embeddings many insights applications uses sentence embeddings far beyond summarization\n",
            "output sentence:  comparison detailed analysis various sentence embedding models real world task automatic summarization \n",
            "\n",
            "{'rouge-1': {'r': 0.07865168539325842, 'p': 0.7, 'f': 0.14141413959800023}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.4, 'f': 0.06666666513888891}, 'rouge-l': {'r': 0.056179775280898875, 'p': 0.5, 'f': 0.10101009919395981}}\n",
            "pair:  graph neural networks recently achieved great successes predicting quantum mechanical properties molecules models represent molecule graph using distance atoms nodes spatial direction one atom another however directional information plays central role empirical potentials molecules angular potentials alleviate limitation propose directional message passing embed messages passed atoms instead atoms message associated direction coordinate space directional message embeddings rotationally equivariant since associated directions rotate molecule propose message passing scheme analogous belief propagation uses directional information transforming messages based angle additionally use spherical bessel functions construct theoretically well founded orthogonal radial basis achieves better performance currently prevalent gaussian radial basis functions using fewer parameters leverage innovations construct directional message passing neural network dimenet dimenet outperforms previous gnns average md qm\n",
            "output sentence:  directional message passing incorporates spatial directional information improve graph neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.20454545454545456, 'p': 0.8181818181818182, 'f': 0.3272727240727274}, 'rouge-2': {'r': 0.08, 'p': 0.4166666666666667, 'f': 0.13422818521688218}, 'rouge-l': {'r': 0.125, 'p': 0.5, 'f': 0.19999999680000002}}\n",
            "pair:  state art unsupervised domain adaptation uda methods learn transferable features minimizing feature distribution discrepancy source target domains different methods model feature distributions explicitly paper explore explicit feature distribution modeling uda particular propose distribution matching prototypical network dmpn model deep features domain gaussian mixture distributions explicit feature distribution modeling easily measure discrepancy two domains dmpn propose two new domain discrepancy losses probabilistic interpretations first one minimizes distances corresponding gaussian component means source target data second one minimizes pseudo negative log likelihood generating target features source feature distribution learn discriminative domain invariant features dmpn trained minimizing classification loss labeled source data domain discrepancy losses together extensive experiments conducted two uda tasks approach yields large margin digits image transfer task state art approaches remarkably dmpn obtains mean accuracy visda dataset hyper parameter sensitivity analysis shows approach robust hyper parameter changes\n",
            "output sentence:  propose explicitly model deep feature distributions source target data gaussian mixture distributions unsupervised domain adaptation uda achieve superior results multiple uda tasks state methods methods tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.075, 'p': 0.6, 'f': 0.13333333135802472}, 'rouge-2': {'r': 0.019801980198019802, 'p': 0.2, 'f': 0.036036034396558794}, 'rouge-l': {'r': 0.0625, 'p': 0.5, 'f': 0.1111111091358025}}\n",
            "pair:  generative adversarial networks gans evolved one successful unsupervised techniques generating realistic images even though recently shown gan training converges gan models often end local nash equilibria associated mode collapse otherwise fail model target distribution introduce coulomb gans pose gan learning problem potential field generated samples attracted training set samples repel discriminator learns potential field generator decreases energy moving samples along vector force field determined gradient potential field decreasing energy gan model learns generate samples according whole target distribution cover modes prove coulomb gans possess one nash equilibrium optimal sense model distribution equals target distribution show efficacy coulomb gans lsun bedrooms celeba faces cifar google billion word text generation\n",
            "output sentence:  coulomb gans optimally learn distribution posing distribution learning problem optimizing potential field \n",
            "\n",
            "{'rouge-1': {'r': 0.18333333333333332, 'p': 0.7333333333333333, 'f': 0.2933333301333333}, 'rouge-2': {'r': 0.0759493670886076, 'p': 0.42857142857142855, 'f': 0.12903225550699507}, 'rouge-l': {'r': 0.13333333333333333, 'p': 0.5333333333333333, 'f': 0.2133333301333334}}\n",
            "pair:  su boyd candes made connection nesterov method ordinary differential equation ode show hessian damping term added ode su boyd candes nesterov method arises straightforward discretization modified ode analogously strongly convex case hessian damping term added polyak ode discretized yield nesterov method strongly convex functions despite hessian term second order odes represented first order systems established liapunov analysis used recover accelerated rates convergence continuous discrete time moreover liapunov analysis extended case stochastic gradients allows full gradient case considered special case stochastic case result unified approach convex acceleration continuous discrete time stochastic full gradient cases\n",
            "output sentence:  derive nesterov method arises straightforward discretization ode different one su boyd candes prove acceleration stochastic case \n",
            "\n",
            "{'rouge-1': {'r': 0.10638297872340426, 'p': 0.5, 'f': 0.17543859359803018}, 'rouge-2': {'r': 0.03571428571428571, 'p': 0.2, 'f': 0.06060605803489451}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.4, 'f': 0.1403508742997846}}\n",
            "pair:  reinforcement learning learn model future observations rewards use plan agent next actions however jointly modeling future observations computationally expensive even intractable observations high dimensional images reason previous works considered partial models model part observation paper show partial models causally incorrect confounded observations model therefore lead incorrect planning address introduce general family partial models provably causally correct avoid need fully model future observations\n",
            "output sentence:  causally correct partial models generate whole observation remain causally correct stochastic environments \n",
            "\n",
            "{'rouge-1': {'r': 0.21428571428571427, 'p': 0.631578947368421, 'f': 0.3199999962168889}, 'rouge-2': {'r': 0.04838709677419355, 'p': 0.16666666666666666, 'f': 0.07499999651250017}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.42105263157894735, 'f': 0.2133333295502223}}\n",
            "pair:  paper addresses scalability challenge architecture search formulating task differentiable manner unlike conventional approaches applying evolution reinforcement learning discrete non differentiable search space method based continuous relaxation architecture representation allowing efficient search architecture using gradient descent extensive experiments cifar imagenet penn treebank wikitext show algorithm excels discovering high performance convolutional architectures image classification recurrent architectures language modeling orders magnitude faster state art non differentiable techniques\n",
            "output sentence:  propose differentiable architecture search algorithm convolutional recurrent networks achieving competitive performance state art using orders magnitude less computation resources \n",
            "\n",
            "{'rouge-1': {'r': 0.10377358490566038, 'p': 0.8461538461538461, 'f': 0.18487394763364173}, 'rouge-2': {'r': 0.07142857142857142, 'p': 0.7142857142857143, 'f': 0.1298701282172373}, 'rouge-l': {'r': 0.09433962264150944, 'p': 0.7692307692307693, 'f': 0.1680672249445661}}\n",
            "pair:  model pruning seeks induce sparsity deep neural network various connection matrices thereby reducing number nonzero valued parameters model recent reports han et al narang et al prune deep networks cost marginal loss accuracy achieve sizable reduction model size hints possibility baseline models experiments perhaps severely parameterized outset viable alternative model compression might simply reduce number hidden units maintaining model dense connection structure exposing similar trade model size accuracy investigate two distinct paths model compression within context energy efficient inference resource constrained environments propose new gradual pruning technique simple straightforward apply across variety models datasets minimal tuning seamlessly incorporated within training process compare accuracy large pruned models large sparse smaller dense small dense counterparts identical memory footprint across broad range neural network architectures deep cnns stacked lstm seq seq lstm models find large sparse models consistently outperform small dense models achieve reduction number non zero parameters minimal loss accuracy\n",
            "output sentence:  demonstrate large pruned models large sparse outperform smaller dense small dense counterparts identical memory footprint \n",
            "\n",
            "{'rouge-1': {'r': 0.14516129032258066, 'p': 0.5625, 'f': 0.23076922750821835}, 'rouge-2': {'r': 0.04225352112676056, 'p': 0.2, 'f': 0.06976743898053014}, 'rouge-l': {'r': 0.11290322580645161, 'p': 0.4375, 'f': 0.17948717622616703}}\n",
            "pair:  work investigates unsupervised learning representations maximizing mutual information input output deep neural network encoder importantly show structure matters incorporating knowledge locality input objective significantly improve representation suitability downstream tasks control characteristics representation matching prior distribution adversarially method call deep infomax dim outperforms number popular unsupervised learning methods compares favorably fully supervised learning several classification tasks standard architectures dim opens new avenues unsupervised learning representations important step towards flexible formulations representation learning objectives specific end goals\n",
            "output sentence:  learn deep representation maximizing mutual information leveraging structure objective able compute fully supervised classifiers comparable architectures \n",
            "\n",
            "{'rouge-1': {'r': 0.036585365853658534, 'p': 0.2727272727272727, 'f': 0.06451612694646787}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.036585365853658534, 'p': 0.2727272727272727, 'f': 0.06451612694646787}}\n",
            "pair:  parallel developments neuroscience deep learning led mutually productive exchanges pushing understanding real artificial neural networks sensory cognitive systems however interaction fields less developed study motor control work develop virtual rodent platform grounded study motor activity artificial models embodied control use platform study motor activity across contexts training model solve four complex tasks using methods familiar neuroscientists describe behavioral representations algorithms employed different layers network using neuroethological approach characterize motor activity relative rodent behavior goals find model uses two classes representations respectively encode task specific behavioral strategies task invariant behavioral kinematics representations reflected sequential activity population dynamics neural subpopulations overall virtual rodent facilitates grounded collaborations deep reinforcement learning motor neuroscience\n",
            "output sentence:  built physical simulation rodent trained solve set tasks analyzed resulting networks \n",
            "\n",
            "{'rouge-1': {'r': 0.1724137931034483, 'p': 0.5, 'f': 0.25641025259697575}, 'rouge-2': {'r': 0.08064516129032258, 'p': 0.23809523809523808, 'f': 0.12048192393090446}, 'rouge-l': {'r': 0.1724137931034483, 'p': 0.5, 'f': 0.25641025259697575}}\n",
            "pair:  show implicit filter level sparsity manifests convolutional neural networks cnns employ batch normalization relu activation trained using adaptive gradient descent techniques regularization weight decay extensive empirical study anonymous hypothesize mechanism hind sparsification process find interplay various phenomena influences strength weight decay regularizers leading supposedly non sparsity inducing regularizers induce filter sparsity workshop article summarize key findings experiments present additional results modern network architectures resnet\n",
            "output sentence:  filter level sparsity emerges implicitly cnns trained adaptive gradient descent approaches due various phenomena extent sparsity inadvertently affected different seemingly seemingly seemingly \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.875, 'f': 0.21874999781250004}, 'rouge-2': {'r': 0.07246376811594203, 'p': 0.625, 'f': 0.12987012800809583}, 'rouge-l': {'r': 0.125, 'p': 0.875, 'f': 0.21874999781250004}}\n",
            "pair:  despite existing work ensuring generalization neural networks terms scale sensitive complexity measures norms margin sharpness complexity measures offer explanation neural networks generalize better parametrization work suggest novel complexity measure based unit wise capacities resulting tighter generalization bound two layer relu networks capacity bound correlates behavior test error increasing network sizes within range reported experiments could partly explain improvement generalization parametrization present matching lower bound rademacher complexity improves previous capacity lower bounds neural networks\n",
            "output sentence:  suggest generalization bound could partly explain improvement generalization parametrization \n",
            "\n",
            "{'rouge-1': {'r': 0.16470588235294117, 'p': 0.875, 'f': 0.277227720105872}, 'rouge-2': {'r': 0.09615384615384616, 'p': 0.625, 'f': 0.16666666435555558}, 'rouge-l': {'r': 0.15294117647058825, 'p': 0.8125, 'f': 0.25742573990785217}}\n",
            "pair:  important question task transfer learning determine task transferability given common input domain estimating extent representations learned source task help learning target task typically transferability either measured experimentally inferred task relatedness often defined without clear operational meaning paper present novel metric score easily computable evaluation function estimates performance transferred representations one task another classification problems inspired principled information theoretic approach score direct connection asymptotic error probability decision function based transferred feature formulation transferability used select suitable set source tasks task transfer learning problems devise efficient transfer learning policies experiments using synthetic real image data show formulation transferability meaningful practice also generalize inference problems beyond classification recognition tasks indoor scene understanding\n",
            "output sentence:  present provable easily computable evaluation function estimates performance transferred representations one learning task another task transfer learning \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.6, 'f': 0.23076922766272187}, 'rouge-2': {'r': 0.044642857142857144, 'p': 0.23809523809523808, 'f': 0.07518796726553234}, 'rouge-l': {'r': 0.10714285714285714, 'p': 0.45, 'f': 0.17307691997041422}}\n",
            "pair:  model free reinforcement learning rl proven powerful general tool learning complex behaviors however sample efficiency often impractically large solving challenging real world problems even policy algorithms learning limiting factor classic model free rl learning signal consists scalar rewards ignoring much rich information contained state transition tuples model based rl uses information training predictive model often achieve asymptotic performance model free rl due model bias introduce temporal difference models tdms family goal conditioned value functions trained model free learning used model based control tdms combine benefits model free model based rl leverage rich information state transitions learn efficiently still attaining asymptotic performance exceeds direct model based rl methods experimental results show range continuous control tasks tdms provide substantial improvement efficiency compared state art model based model free methods\n",
            "output sentence:  show special goal condition value function trained model free methods used within model based control resulting substantially better sample sample performance performance \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.42857142857142855, 'f': 0.13953488099513256}, 'rouge-2': {'r': 0.03125, 'p': 0.21428571428571427, 'f': 0.054545452323967035}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.2857142857142857, 'f': 0.09302325308815583}}\n",
            "pair:  need large amounts training image data clearly defined features major obstacle applying generative adversarial networks gan image generation training data limited diverse since insufficient latent feature representation already scarce data often leads instability mode collapse gan training overcome hurdle limited data applying gan limited datasets propose paper strategy textit parallel recurrent data augmentation gan model progressively enriches training set sample images constructed gans trained parallel consecutive training epochs experiments variety small yet diverse datasets demonstrate method little model specific considerations produces images better quality compared images generated without strategy source code generated images paper made public review\n",
            "output sentence:  introduced novel simple efficient data augmentation method boosts performances existing gans training data limited diverse \n",
            "\n",
            "{'rouge-1': {'r': 0.12643678160919541, 'p': 0.6470588235294118, 'f': 0.2115384588036243}, 'rouge-2': {'r': 0.04716981132075472, 'p': 0.29411764705882354, 'f': 0.08130081062595025}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.35294117647058826, 'f': 0.11538461264977817}}\n",
            "pair:  recent success neural networks solving difficult decision tasks incentivized incorporating smart decision making edge however work traditionally focused neural network inference rather training due memory compute limitations especially emerging non volatile memory systems writes energetically costly reduce lifespan yet ability train edge becoming increasingly important enables applications real time adaptability device drift environmental variation user customization federated learning across devices work address four key challenges training edge devices non volatile memory low weight update density weight quantization low auxiliary memory online learning present low rank training scheme addresses four challenges maintaining computational efficiency demonstrate technique representative convolutional neural network across several adaptation problems performs standard sgd accuracy number weight updates\n",
            "output sentence:  use kronecker sum approximations low rank training address challenges training neural networks edge devices utilize emerging memory technologies \n",
            "\n",
            "{'rouge-1': {'r': 0.14457831325301204, 'p': 0.6, 'f': 0.2330097056084457}, 'rouge-2': {'r': 0.06363636363636363, 'p': 0.30434782608695654, 'f': 0.10526315503420212}, 'rouge-l': {'r': 0.10843373493975904, 'p': 0.45, 'f': 0.1747572784239797}}\n",
            "pair:  decades research neural code underlying spatial navigation revealed diverse set neural response properties entorhinal cortex ec mammalian brain contains rich set spatial correlates including grid cells encode space using tessellating patterns however mechanisms functional significance spatial representations remain largely mysterious new way understand neural representations trained recurrent neural networks rnns perform navigation tasks arenas based velocity inputs surprisingly find grid like spatial response patterns emerge trained networks along units exhibit spatial correlates including border cells band like cells different functional types neurons observed experimentally order emergence grid like border cells also consistent observations developmental studies together results suggest grid cells border cells others observed ec may natural solution representing space efficiently given predominant recurrent connections neural circuits\n",
            "output sentence:  knowledge first study show neural representations space including grid like cells border cells observed brain could training recurrent recurrent neural network tasks tasks navigation \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.9166666666666666, 'f': 0.21999999788800007}, 'rouge-2': {'r': 0.07216494845360824, 'p': 0.6363636363636364, 'f': 0.1296296278000686}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.5, 'f': 0.11999999788800003}}\n",
            "pair:  many tasks artificial intelligence require collaboration multiple agents exam deep reinforcement learning multi agent domains recent research efforts often take form two seemingly conflicting perspectives decentralized perspective agent supposed controller centralized perspective one assumes larger model controlling agents regard revisit idea master slave architecture incorporating perspectives within one framework hierarchical structure naturally leverages advantages one another idea combining perspective intuitive well motivated many real world systems however variety possible realizations highlights three key ingredients composed action representation learnable communication independent reasoning network designs facilitate explicitly proposal consistently outperforms latest competing methods synthetics experiments applied challenging starcraft micromanagement tasks\n",
            "output sentence:  revisit idea master slave architecture multi agent deep reinforcement learning outperforms state arts \n",
            "\n",
            "{'rouge-1': {'r': 0.17777777777777778, 'p': 0.47058823529411764, 'f': 0.2580645121488034}, 'rouge-2': {'r': 0.1, 'p': 0.2777777777777778, 'f': 0.14705881963667833}, 'rouge-l': {'r': 0.15555555555555556, 'p': 0.4117647058823529, 'f': 0.22580644763267432}}\n",
            "pair:  deep convolutional networks dcns shown sensitive universal adversarial perturbations uaps input agnostic perturbations fool model large portions dataset uaps exhibit interesting visual patterns phenomena yet poorly understood work shows visually similar procedural noise patterns also act uaps particular demonstrate different dcn architectures sensitive gabor noise patterns behaviour causes implications deserve depth study\n",
            "output sentence:  existing deep convolutional networks image classification tasks sensitive gabor noise patterns small structured changes changes large changes large output output \n",
            "\n",
            "{'rouge-1': {'r': 0.16216216216216217, 'p': 0.8, 'f': 0.2696629185456382}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.21428571428571427, 'f': 0.06185566763311733}, 'rouge-l': {'r': 0.12162162162162163, 'p': 0.6, 'f': 0.20224718820855958}}\n",
            "pair:  deep reinforcement learning rl agents often fail generalize unseen environments yet semantically similar trained agents particularly trained high dimensional state spaces images paper propose simple technique improve generalization ability deep rl agents introducing randomized convolutional neural network randomly perturbs input observations enables trained agents adapt new domains learning robust features invariant across varied randomized environments furthermore consider inference method based monte carlo approximation reduce variance induced randomization demonstrate superiority method across coinrun deepmind lab exploration robotics control tasks significantly outperforms various regularization data augmentation methods purpose\n",
            "output sentence:  propose simple randomization technique improving generalization deep reinforcement learning across tasks various unseen visual patterns \n",
            "\n",
            "{'rouge-1': {'r': 0.25333333333333335, 'p': 0.9047619047619048, 'f': 0.3958333299153647}, 'rouge-2': {'r': 0.1956521739130435, 'p': 0.8571428571428571, 'f': 0.31858406777038145}, 'rouge-l': {'r': 0.25333333333333335, 'p': 0.9047619047619048, 'f': 0.3958333299153647}}\n",
            "pair:  order mimic human ability continual acquisition transfer knowledge across various tasks learning system needs capability life long learning effectively utilizing previously acquired skills key challenge transfer generalize knowledge learned one task tasks avoiding interference previous knowledge improving overall performance paper within continual learning paradigm introduce method effectively forgets less useful data samples continuously across different tasks method uses statistical leverage score information measure importance data samples every task adopts frequent directions approach enable life long learning property effectively maintains constant training size across tasks first provide mathematical intuition method demonstrate effectiveness experiments variants mnist cifar datasets\n",
            "output sentence:  new method uses statistical leverage score information measure importance data samples every task adopts frequent directions approach enable life long long learning \n",
            "\n",
            "{'rouge-1': {'r': 0.10416666666666667, 'p': 0.625, 'f': 0.17857142612244903}, 'rouge-2': {'r': 0.034482758620689655, 'p': 0.2857142857142857, 'f': 0.06153845961656811}, 'rouge-l': {'r': 0.10416666666666667, 'p': 0.625, 'f': 0.17857142612244903}}\n",
            "pair:  paper develops variational continual learning vcl simple general framework continual learning fuses online variational inference vi recent advances monte carlo vi neural networks framework successfully train deep discriminative models deep generative models complex continual learning settings existing tasks evolve time entirely new tasks emerge experimental results show vcl outperforms state art continual learning methods variety tasks avoiding catastrophic forgetting fully automatic way\n",
            "output sentence:  paper develops principled method continual learning deep models \n",
            "\n",
            "{'rouge-1': {'r': 0.16494845360824742, 'p': 0.8888888888888888, 'f': 0.2782608669247637}, 'rouge-2': {'r': 0.08181818181818182, 'p': 0.47368421052631576, 'f': 0.139534881209062}, 'rouge-l': {'r': 0.12371134020618557, 'p': 0.6666666666666666, 'f': 0.20869564953345937}}\n",
            "pair:  inspired adaptation phenomenon biological neuronal firing propose regularity normalization reparameterization activation neural network take account statistical regularity implicit space considering neural network optimization process model selection problem implicit space constrained normalizing factor minimum description length optimal universal code introduce incremental version computing universal code normalized maximum likelihood demonstrated flexibility include data prior top attention oracle information compatibility incorporated batch normalization layer normalization preliminary results showed proposed method outperforms existing normalization methods tackling limited imbalanced data non stationary distribution benchmarked computer vision task unsupervised attention mechanism given input data biologically plausible normalization potential deal complicated real world scenarios well reinforcement learning setting rewards sparse non uniform research proposed discover scenarios explore behaviors among different variants\n",
            "output sentence:  considering neural network optimization process model selection problem introduce biological plausible normalization method extracts statistical regularity principle principle imbalanced imbalanced imbalanced data \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.7272727272727273, 'f': 0.21052631331371197}, 'rouge-2': {'r': 0.036585365853658534, 'p': 0.3, 'f': 0.06521738936672974}, 'rouge-l': {'r': 0.1076923076923077, 'p': 0.6363636363636364, 'f': 0.18421052384002776}}\n",
            "pair:  paper propose new loss function performing principal component analysis pca using linear autoencoders laes optimizing standard loss results decoder matrix spans principal subspace sample covariance data fails identify exact eigenvectors downside originates invariance cancels global map prove loss function eliminates issue decoder converges exact ordered unnormalized eigenvectors sample covariance matrix new loss establish local minima global optima also show computing new loss also gradients order complexity classical loss report numerical results synthetic simulations real data pca experiment mnist matrix demonstrating approach practically applicable rectify previous laes downsides\n",
            "output sentence:  new loss function pca linear autoencoders provably yields ordered exact eigenvectors \n",
            "\n",
            "{'rouge-1': {'r': 0.11320754716981132, 'p': 0.5714285714285714, 'f': 0.18897637519251043}, 'rouge-2': {'r': 0.032520325203252036, 'p': 0.2, 'f': 0.05594405353807043}, 'rouge-l': {'r': 0.0660377358490566, 'p': 0.3333333333333333, 'f': 0.1102362177121955}}\n",
            "pair:  ability autonomously explore navigate physical space fundamental requirement virtually mobile autonomous agent household robotic vacuums autonomous vehicles traditional slam based approaches exploration navigation largely focus leveraging scene geometry fail model dynamic objects agents semantic constraints wet floors doorways learning based rl agents attractive alternative incorporate semantic geometric information notoriously sample inefficient difficult generalize novel settings difficult interpret paper combine best worlds modular approach em learns spatial representation scene trained effective coupled traditional geometric planners specifically design agent learns predict spatial affordance map elucidates parts scene navigable active self supervised experience gathering contrast simulation environments assume static world evaluate approach vizdoom simulator using large scale randomly generated maps containing variety dynamic actors hazards show learned affordance maps used augment traditional approaches exploration navigation providing significant improvements performance\n",
            "output sentence:  address task autonomous exploration navigation using spatial affordance maps learned self supervised manner outperform classic geometric baselines sample efficient contemporary contemporary \n",
            "\n",
            "{'rouge-1': {'r': 0.09302325581395349, 'p': 1.0, 'f': 0.1702127644001811}, 'rouge-2': {'r': 0.019230769230769232, 'p': 0.3333333333333333, 'f': 0.036363635332231435}, 'rouge-l': {'r': 0.09302325581395349, 'p': 1.0, 'f': 0.1702127644001811}}\n",
            "pair:  work presents poincar wasserstein autoencoder reformulation recently proposed wasserstein autoencoder framework non euclidean manifold poincar ball model hyperbolic space assuming latent space hyperbolic use intrinsic hierarchy impose structure learned latent space representations show datasets latent hierarchies recover structure low dimensional latent space also demonstrate model visual domain analyze properties show competitive results graph link prediction task\n",
            "output sentence:  wasserstein autoencoder hyperbolic latent space \n",
            "\n",
            "{'rouge-1': {'r': 0.11320754716981132, 'p': 0.6, 'f': 0.1904761878054926}, 'rouge-2': {'r': 0.030303030303030304, 'p': 0.2222222222222222, 'f': 0.05333333122133342}, 'rouge-l': {'r': 0.09433962264150944, 'p': 0.5, 'f': 0.15873015605946086}}\n",
            "pair:  propose wasserstein auto encoder wae new algorithm building generative model data distribution wae minimizes penalized form wasserstein distance model distribution target distribution leads different regularizer one used variational auto encoder vae regularizer encourages encoded training distribution match prior compare algorithm several techniques show generalization adversarial auto encoders aae experiments show wae shares many properties vaes stable training encoder decoder architecture nice latent manifold structure generating samples better quality\n",
            "output sentence:  propose new auto encoder based wasserstein distance improves sampling properties vae \n",
            "\n",
            "{'rouge-1': {'r': 0.16470588235294117, 'p': 0.875, 'f': 0.277227720105872}, 'rouge-2': {'r': 0.09615384615384616, 'p': 0.625, 'f': 0.16666666435555558}, 'rouge-l': {'r': 0.15294117647058825, 'p': 0.8125, 'f': 0.25742573990785217}}\n",
            "pair:  important question task transfer learning determine task transferability given common input domain estimating extent representations learned source task help learning target task typically transferability either measured experimentally inferred task relatedness often defined without clear operational meaning paper present novel metric score easily computable evaluation function estimates performance transferred representations one task another classification problems inspired principled information theoretic approach score direct connection asymptotic error probability decision function based transferred feature formulation transferability used select suitable set source tasks task transfer learning problems devise efficient transfer learning policies experiments using synthetic real image data show formulation transferability meaningful practice also generalize inference problems beyond classification recognition tasks indoor scene understanding\n",
            "output sentence:  present provable easily computable evaluation function estimates performance transferred representations one learning task another task transfer learning \n",
            "\n",
            "{'rouge-1': {'r': 0.1267605633802817, 'p': 0.75, 'f': 0.21686746740600957}, 'rouge-2': {'r': 0.11764705882352941, 'p': 0.6666666666666666, 'f': 0.19999999745000002}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.5, 'f': 0.1445783107795036}}\n",
            "pair:  modern federated networks comprised wearable devices mobile phones autonomous vehicles generate massive amounts data day wealth data help learn models improve user experience device however scale heterogeneity federated data presents new challenges research areas federated learning meta learning multi task learning machine learning community begins tackle challenges critical time ensure developments made areas grounded realistic benchmarks end propose leaf modular benchmarking framework learning federated settings leaf includes suite open source federated datasets rigorous evaluation framework set reference implementations geared towards capturing obstacles intricacies practical federated environments\n",
            "output sentence:  present leaf modular benchmarking framework learning federated data applications learning paradigms federated learning meta learning multi task \n",
            "\n",
            "{'rouge-1': {'r': 0.12222222222222222, 'p': 0.8461538461538461, 'f': 0.2135922308040343}, 'rouge-2': {'r': 0.07142857142857142, 'p': 0.6666666666666666, 'f': 0.12903225631633716}, 'rouge-l': {'r': 0.12222222222222222, 'p': 0.8461538461538461, 'f': 0.2135922308040343}}\n",
            "pair:  applying reinforcement learning rl real world problems require reasoning action reward correlation long time horizons hierarchical reinforcement learning hrl methods handle dividing task hierarchies often hand tuned network structure pre defined subgoals propose novel hrl framework taic learns temporal abstraction past experience expert demonstrations without task specific knowledge formulate temporal abstraction problem learning latent representations action sequences present novel approach regularizing latent space adding information theoretic constraints specifically maximize mutual information latent variables state changes visualization latent space demonstrates algorithm learns effective abstraction long action sequences learned abstraction allows us learn new tasks higher level efficiently convey significant speedup convergence benchmark learning problems results demonstrate learning temporal abstractions effective technique increasing convergence rate sample efficiency rl algorithms\n",
            "output sentence:  propose novel hrl framework formulate temporal abstraction problem learning latent representation action sequence \n",
            "\n",
            "{'rouge-1': {'r': 0.1746031746031746, 'p': 0.9166666666666666, 'f': 0.29333333064533335}, 'rouge-2': {'r': 0.11842105263157894, 'p': 0.75, 'f': 0.20454545219008266}, 'rouge-l': {'r': 0.1746031746031746, 'p': 0.9166666666666666, 'f': 0.29333333064533335}}\n",
            "pair:  propose novel deep network architecture lifelong learning refer dynamically expandable network den dynamically decide network capacity trains sequence tasks learn compact overlapping knowledge sharing structure among tasks den efficiently trained online manner performing selective retraining dynamically expands network capacity upon arrival task necessary number units effectively prevents semantic drift splitting duplicating units timestamping validate den multiple public datasets lifelong learning scenarios multiple public datasets significantly outperforms existing lifelong learning methods deep networks also achieves level performance batch model substantially fewer number parameters\n",
            "output sentence:  propose novel deep network architecture dynamically decide network capacity trains lifelong learning scenario \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.8571428571428571, 'f': 0.12765957308963333}, 'rouge-2': {'r': 0.0297029702970297, 'p': 0.5, 'f': 0.05607476529653246}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.8571428571428571, 'f': 0.12765957308963333}}\n",
            "pair:  deep neural networks achieved outstanding performance many real world applications expense huge computational resources densenet one recently proposed neural network architecture achieved state art performance many visual tasks however great redundancy due dense connections internal structure leads high computational costs training dense networks address issue design reinforcement learning framework search efficient densenet architectures layer wise pruning lwp different tasks retaining original advantages densenet feature reuse short paths etc framework agent evaluates importance connection two block layers prunes redundant connections addition novel reward shaping trick introduced make densenet reach better trade accuracy float point operations flops experiments show densenet lwp compact efficient existing alternatives\n",
            "output sentence:  learning search efficient densenet layer wise pruning \n",
            "\n",
            "{'rouge-1': {'r': 0.08974358974358974, 'p': 0.875, 'f': 0.16279069598702}, 'rouge-2': {'r': 0.06382978723404255, 'p': 0.8571428571428571, 'f': 0.11881187989804921}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.875, 'f': 0.16279069598702}}\n",
            "pair:  work presents scalable solution continuous visual speech recognition achieve constructed largest existing visual speech recognition dataset consisting pairs text video clips faces speaking hours video tandem designed trained integrated lipreading system consisting video processing pipeline maps raw video stable videos lips sequences phonemes scalable deep neural network maps lip videos sequences phoneme distributions production level speech decoder outputs sequences words proposed system achieves word error rate wer measured held set comparison professional lipreaders achieve either wer dataset access additional types contextual information approach significantly improves previous lipreading approaches including variants lipnet watch attend spell capable wer respectively\n",
            "output sentence:  work presents scalable solution continuous visual speech recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.07017543859649122, 'p': 0.4, 'f': 0.11940298253508581}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.1111111111111111, 'f': 0.02439024194824525}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.3, 'f': 0.08955223626642911}}\n",
            "pair:  recent work studied emergence language among deep reinforcement learning agents must collaborate solve task particular interest factors cause language compositional express meaning combining words meaning evolutionary linguists found addition structural priors like already studied deep learning dynamics transmitting language generation generation contribute significantly emergence compositionality paper introduce cultural evolutionary dynamics language emergence periodically replacing agents population create knowledge gap implicitly inducing cultural transmission language show implicit cultural transmission encourages resulting languages exhibit better compositional generalization\n",
            "output sentence:  use cultural transmission encourage compositionality languages emerge interactions neural agents \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.8, 'f': 0.21621621387874362}, 'rouge-2': {'r': 0.0958904109589041, 'p': 0.7, 'f': 0.16867469667586008}, 'rouge-l': {'r': 0.125, 'p': 0.8, 'f': 0.21621621387874362}}\n",
            "pair:  recent efforts combine representation learning formal methods commonly known neuro symbolic methods given rise new trend applying rich neural architectures solve classical combinatorial optimization problems paper propose neural framework learn solve circuit satisfiability problem framework built upon two fundamental contributions rich embedding architecture encodes problem structure end end differentiable training procedure mimics reinforcement learning trains model directly toward solving sat problem experimental results show superior sample generalization performance framework compared recently developed neurosat method\n",
            "output sentence:  propose neural framework learn solve circuit satisfiability problem unlabeled circuit instances \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.5454545454545454, 'f': 0.14814814580094499}, 'rouge-2': {'r': 0.0125, 'p': 0.09090909090909091, 'f': 0.02197801985267501}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.45454545454545453, 'f': 0.12345678777625364}}\n",
            "pair:  designing architectures deep neural networks requires expert knowledge substantial computation time propose technique accelerate architecture selection learning auxiliary hypernet generates weights main model conditioned model architecture comparing relative validation performance networks hypernet generated weights effectively search wide range architectures cost single training run facilitate search develop flexible mechanism based memory read writes allows us define wide range network connectivity patterns resnet densenet fractalnet blocks special cases validate method smash cifar cifar stl modelnet imagenet achieving competitive performance similarly sized hand designed networks\n",
            "output sentence:  technique accelerating neural architecture selection approximating weights candidate architecture instead training individually \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}, 'rouge-2': {'r': 0.028037383177570093, 'p': 0.375, 'f': 0.052173911748960335}, 'rouge-l': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}}\n",
            "pair:  learning communication via deep reinforcement learning recently shown effective way solve cooperative multi agent tasks however learning communicated information beneficial agent decision making remains challenging task order address problem introduce fully differentiable framework communication reasoning enabling agents solve cooperative tasks partially observable environments framework designed facilitate explicit reasoning agents novel memory based attention network learn selectively past memories model communicates series reasoning steps decompose agent intentions learned representations used first compute relevance communicated information second extract information memories given newly received information selectively interacting new information model effectively learns communication protocol directly end end manner empirically demonstrate strength model cooperative multi agent tasks inter agent communication reasoning prior information substantially improves performance compared baselines\n",
            "output sentence:  novel architecture memory based attention mechanism multi agent communication \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.875, 'f': 0.16666666494331067}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.42857142857142855, 'f': 0.06185566876394944}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}}\n",
            "pair:  describe kernel rnn learning kernl reduced rank temporal eligibility trace based approximation backpropagation time bptt training recurrent neural networks rnns gives competitive performance bptt long time dependence tasks approximation replaces rank gradient learning tensor describes past hidden unit activations affect current state simple reduced rank product sensitivity weight temporal eligibility trace structured approximation motivated node perturbation sensitivity weights eligibility kernel time scales learned applying perturbations rule represents another step toward biologically plausible neurally inspired ml lower complexity terms relaxed architectural requirements symmetric return weights smaller memory demand unfolding storage states time shorter feedback time\n",
            "output sentence:  biologically plausible learning rule training recurrent neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.23214285714285715, 'p': 0.9285714285714286, 'f': 0.37142856822857145}, 'rouge-2': {'r': 0.11940298507462686, 'p': 0.6153846153846154, 'f': 0.199999997278125}, 'rouge-l': {'r': 0.17857142857142858, 'p': 0.7142857142857143, 'f': 0.2857142825142857}}\n",
            "pair:  derive unbiased estimator expectations discrete random variables based sampling without replacement reduces variance avoids duplicate samples show estimator derived rao blackwellization three different estimators combining estimator reinforce obtain policy gradient estimator reduce variance using built control variate obtained without additional model evaluations resulting estimator closely related gradient estimators experiments toy problem categorical variational auto encoder structured prediction problem show estimator estimator consistently among best estimators high low entropy settings\n",
            "output sentence:  derive low variance unbiased gradient estimator expectations discrete random variables based sampling without replacement \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.7058823529411765, 'f': 0.2376237595765121}, 'rouge-2': {'r': 0.04716981132075472, 'p': 0.2631578947368421, 'f': 0.07999999742208008}, 'rouge-l': {'r': 0.11904761904761904, 'p': 0.5882352941176471, 'f': 0.19801979918047252}}\n",
            "pair:  despite ability memorize large datasets deep neural networks often achieve good generalization performance however differences learned solutions networks generalize remain unclear additionally tuning properties single directions defined activation single unit linear combination units response input highlighted importance evaluated connect lines inquiry demonstrate network reliance single directions good predictor generalization performance across networks trained datasets different fractions corrupted labels across ensembles networks trained datasets unmodified labels across different hyper parameters course training dropout regularizes quantity point batch normalization implicitly discourages single direction reliance part decreasing class selectivity individual units finally find class selectivity poor predictor task importance suggesting networks generalize well minimize dependence individual units reducing selectivity also individually selective units may necessary strong network performance\n",
            "output sentence:  find deep networks generalize poorly reliant single directions generalize well evaluate impact dropout batch normalization well class selectivity single direction \n",
            "\n",
            "{'rouge-1': {'r': 0.18518518518518517, 'p': 0.8333333333333334, 'f': 0.3030303000550964}, 'rouge-2': {'r': 0.125, 'p': 0.6363636363636364, 'f': 0.2089552211361105}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.75, 'f': 0.2727272697520661}}\n",
            "pair:  communicating humans rely internally consistent language representations speakers expect listeners behave way listen work proposes several methods encouraging internal consistency dialog agents emergent communication setting consider two hypotheses effect internal consistency constraints improve agents ability refer unseen referents improve agents ability generalize across communicative roles performing speaker de spite trained listener find evidence favor former results show significant support latter\n",
            "output sentence:  internal consistency constraints improve agents ability develop emergent protocols generalize across communicative roles \n",
            "\n",
            "{'rouge-1': {'r': 0.04819277108433735, 'p': 0.6666666666666666, 'f': 0.08988763919202121}, 'rouge-2': {'r': 0.01, 'p': 0.2, 'f': 0.01904761814058961}, 'rouge-l': {'r': 0.03614457831325301, 'p': 0.5, 'f': 0.06741572907966167}}\n",
            "pair:  recent studies highlighted adversarial examples ubiquitous threat different neural network models many downstream applications nonetheless unique data properties inspired distinct powerful learning principles paper aims explore potentials towards mitigating adversarial inputs particular results reveal importance using temporal dependency audio data gain discriminate power adversarial examples tested automatic speech recognition asr tasks three recent audio adversarial attacks find input transformation developed image adversarial defense provides limited robustness improvement subtle advanced attacks ii temporal dependency exploited gain discriminative power audio adversarial examples resistant adaptive attacks considered experiments results show promising means improving robustness asr systems also offer novel insights exploiting domain specific data properties mitigate negative effects adversarial examples\n",
            "output sentence:  adversarial audio discrimination using temporal dependency \n",
            "\n",
            "{'rouge-1': {'r': 0.27848101265822783, 'p': 0.9565217391304348, 'f': 0.43137254552672055}, 'rouge-2': {'r': 0.1891891891891892, 'p': 0.875, 'f': 0.31111110818765436}, 'rouge-l': {'r': 0.27848101265822783, 'p': 0.9565217391304348, 'f': 0.43137254552672055}}\n",
            "pair:  imitation learning il appealing approach learn desirable autonomous behavior however directing il achieve arbitrary goals difficult contrast planning based algorithms use dynamics models reward functions achieve goals yet reward functions evoke desirable behavior often difficult specify paper propose imitative models combine benefits il goal directed planning imitative models probabilistic predictive models desirable behavior able plan interpretable expert like trajectories achieve specified goals derive families flexible goal objectives including constrained goal regions unconstrained goal sets energy based goals show method use objectives successfully direct behavior method substantially outperforms six il approaches planning based approach dynamic simulated autonomous driving task efficiently learned expert demonstrations without online data collection also show approach robust poorly specified goals goals wrong side road\n",
            "output sentence:  paper propose imitative models combine benefits il goal directed planning probabilistic predictive models desirable behavior able plan interpretable expert like trajectories achieve achieve specified specified \n",
            "\n",
            "{'rouge-1': {'r': 0.03614457831325301, 'p': 0.3333333333333333, 'f': 0.065217389539225}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03614457831325301, 'p': 0.3333333333333333, 'f': 0.065217389539225}}\n",
            "pair:  binarized neural networks bnns shown effective improving network efficiency inference phase network trained however bnns binarize model parameters activations propagations therefore bnns offer significant efficiency improvements training since gradients still propagated used high precision show inherent difficulty training bnns using binarized backpropagation bbp also binarize gradients avoid significant degradation test accuracy simply increase number filter maps convolution layer using bbp dedicated hardware potentially significantly improve execution efficiency emph reduce dynamic memory footprint memory bandwidth computational energy speed training process appropriate hardware support even increase network size moreover method ideal distributed learning reduces communication costs significantly using method demonstrate minimal loss classification accuracy several datasets topologies\n",
            "output sentence:  binarized back propagation need completely binarized training inflate size network \n",
            "\n",
            "{'rouge-1': {'r': 0.13157894736842105, 'p': 0.7142857142857143, 'f': 0.22222221959506175}, 'rouge-2': {'r': 0.06060606060606061, 'p': 0.42857142857142855, 'f': 0.10619468809460417}, 'rouge-l': {'r': 0.10526315789473684, 'p': 0.5714285714285714, 'f': 0.17777777515061732}}\n",
            "pair:  explore idea compositional set embeddings used infer single class set classes associated input data image video audio signal useful example multi object detection images multi speaker diarization one shot learning audio particular devise implement two novel models consisting embedding function trained jointly composite function computes set union opera tions classes encoded two embedding vectors embedding trained jointly query function computes whether classes en coded one embedding subsume classes encoded another embedding contrast prior work models must perceive classes associated input examples also encode relationships different class label sets experiments conducted simulated data omniglot coco datasets proposed composite embedding models outperform baselines based traditional embedding approaches\n",
            "output sentence:  explored novel method compositional set embeddings perceive represent single class entire set classes associated input data \n",
            "\n",
            "{'rouge-1': {'r': 0.23214285714285715, 'p': 0.8666666666666667, 'f': 0.3661971797659195}, 'rouge-2': {'r': 0.14285714285714285, 'p': 0.6428571428571429, 'f': 0.23376623079102715}, 'rouge-l': {'r': 0.23214285714285715, 'p': 0.8666666666666667, 'f': 0.3661971797659195}}\n",
            "pair:  distributed approaches natural language semantics developed diversified embedders linguistic units larger words sentences come play increasingly important role date embedders evaluated using benchmark tasks glue linguistic probes propose comparative approach nearest neighbor overlap quantifies similarity embedders task agnostic manner requires collection examples simple understand two embedders similar set inputs greater overlap inputs nearest neighbors use compare sentence embedders show effects different design choices architectures\n",
            "output sentence:  propose nearest neighbor overlap procedure quantifies similarity embedders task agnostic manner use compare sentence embedders \n",
            "\n",
            "{'rouge-1': {'r': 0.11290322580645161, 'p': 0.7777777777777778, 'f': 0.19718309637770284}, 'rouge-2': {'r': 0.05555555555555555, 'p': 0.5555555555555556, 'f': 0.10101009935720845}, 'rouge-l': {'r': 0.0967741935483871, 'p': 0.6666666666666666, 'f': 0.1690140822931958}}\n",
            "pair:  actor critic methods solve reinforcement learning problems updating parameterized policy known actor direction increases estimate expected return known critic however existing actor critic methods use values gradients critic update policy parameter paper propose novel actor critic method called guide actor critic gac gac firstly learns guide actor locally maximizes critic updates policy parameter based guide actor supervised learning main theoretical contributions two folds first show gac updates guide actor performing second order optimization action space curvature matrix based hessians critic second show deterministic policy gradient method special case gac hessians ignored experiments show method promising reinforcement learning method continuous controls\n",
            "output sentence:  paper proposes novel actor critic method uses hessians critic update actor \n",
            "\n",
            "{'rouge-1': {'r': 0.08860759493670886, 'p': 0.6363636363636364, 'f': 0.15555555340987656}, 'rouge-2': {'r': 0.04081632653061224, 'p': 0.36363636363636365, 'f': 0.07339449359818201}, 'rouge-l': {'r': 0.0759493670886076, 'p': 0.5454545454545454, 'f': 0.13333333118765436}}\n",
            "pair:  named entity recognition ner relation extraction two important tasks information extraction retrieval ie ir recent work demonstrated beneficial learn tasks jointly avoids propagation error inherent pipeline based systems improves performance however state art joint models typically rely external natural language processing nlp tools dependency parsers limiting usefulness domains news tools perform well neural end end models proposed trained almost completely scratch paper propose neural end end model jointly extracting entities relations rely external nlp tools integrates large pre trained language model bulk model parameters pre trained eschew recurrence self attention model fast train datasets across domains model matches exceeds state art performance sometimes large margin\n",
            "output sentence:  novel high performing architecture end end named entity recognition relation extraction fast train \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.375, 'f': 0.11009174061442645}, 'rouge-2': {'r': 0.00819672131147541, 'p': 0.06666666666666667, 'f': 0.014598538195961689}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.375, 'f': 0.11009174061442645}}\n",
            "pair:  several studies recently showing strong natural language understanding nlu models prone relying unwanted dataset biases without learning underlying task resulting models fail generalize domain datasets likely perform poorly real world scenarios propose several learning strategies train neural models robust biases transfer better domain datasets introduce additional lightweight bias model learns dataset biases uses prediction adjust loss base model reduce biases words methods weight importance biased examples focus training hard examples examples cannot correctly classified relying biases approaches model agnostic simple implement experiment large scale natural language inference fact verification datasets domain datasets show debiased models significantly improve robustness settings including gaining points fever symmetric evaluation dataset hans dataset points snli hard set datasets specifically designed assess robustness models domain setting typical biases training data exist evaluation set\n",
            "output sentence:  propose several general debiasing strategies address common biases seen different datasets obtain substantial improved domain performance \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.8888888888888888, 'f': 0.19753086222222224}, 'rouge-2': {'r': 0.06818181818181818, 'p': 0.75, 'f': 0.12499999847222223}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.8888888888888888, 'f': 0.19753086222222224}}\n",
            "pair:  detecting anomalies growing importance various industrial applications mission critical infrastructures including satellite systems although several studies detecting anomalies based rule based machine learning based approaches satellite systems tensor based decomposition method extensively explored anomaly detection work introduce integrative tensor based anomaly detection itad framework detect anomalies satellite system high risk cost detecting anomalies satellite system crucial construct rd order tensors telemetry data collected korea multi purpose satellite kompsat calculate anomaly score using one component matrices obtained applying candecomp parafac decomposition detect anomalies result shows tensor based approach effective achieving higher accuracy reducing false positives detecting anomalies compared existing approaches\n",
            "output sentence:  integrative tensor based anomaly detection itad framework satellite system \n",
            "\n",
            "{'rouge-1': {'r': 0.10891089108910891, 'p': 0.7333333333333333, 'f': 0.18965517016200953}, 'rouge-2': {'r': 0.023622047244094488, 'p': 0.2, 'f': 0.042253519237254594}, 'rouge-l': {'r': 0.07920792079207921, 'p': 0.5333333333333333, 'f': 0.13793103223097508}}\n",
            "pair:  human brain function measured functional magnetic resonance imaging fmri exhibits rich diversity response understanding individual variability brain function association behavior become one major concerns modern cognitive neuroscience work motivated view generative models provide useful tool understanding variability end manuscript presents two novel generative models trained real neuroimaging data synthesize task dependent functional brain images brain images high dimensional tensors exhibit structured spatial correlations thus models conditional generative adversarial networks gans apply convolutional neural networks cnns learn abstraction brain image representations results show generated brain images diverse yet task dependent addition qualitative evaluation utilize generated synthetic brain volumes additional training data improve downstream fmri classifiers also known decoding brain reading approach achieves significant improvements variety datasets classifi cation tasks evaluation scores classification results provide quantitative evaluation quality generated images also serve additional contribution manuscript\n",
            "output sentence:  two novel gans constructed generate high quality fmri brain images synthetic brain images greatly help improve classification tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.1864406779661017, 'p': 0.5789473684210527, 'f': 0.2820512783662065}, 'rouge-2': {'r': 0.1, 'p': 0.3684210526315789, 'f': 0.15730336742835507}, 'rouge-l': {'r': 0.1864406779661017, 'p': 0.5789473684210527, 'f': 0.2820512783662065}}\n",
            "pair:  goal survival clustering map subjects users social network patients medical study clusters ranging low risk high risk existing survival methods assume presence clear textit end life signals introduce artificially using pre defined timeout paper forego assumption introduce loss function differentiates empirical lifetime distributions clusters using modified kuiper statistic learn deep neural network optimizing loss performs soft clustering users survival groups apply method social network dataset subjects show significant improvement index compared alternatives\n",
            "output sentence:  goal survival clustering map subjects clusters without end life signals challenging task address task propose new loss function modifying kuiper \n",
            "\n",
            "{'rouge-1': {'r': 0.06944444444444445, 'p': 0.7142857142857143, 'f': 0.1265822768658869}, 'rouge-2': {'r': 0.037037037037037035, 'p': 0.5, 'f': 0.06896551595719384}, 'rouge-l': {'r': 0.06944444444444445, 'p': 0.7142857142857143, 'f': 0.1265822768658869}}\n",
            "pair:  inferring likely configuration subset variables joint distribution given remaining ones refer co generation important challenge computationally demanding simplest settings task received considerable amount attention particularly classical ways modeling distributions like structured prediction contrast almost nothing known task considering recently proposed techniques modeling high dimensional distributions particularly generative adversarial nets gans therefore paper study occurring challenges co generation gans address challenges develop annealed importance sampling ais based hamiltonian monte carlo hmc co generation algorithm presented approach significantly outperforms classical gradient based methods synthetic data celeba\n",
            "output sentence:  using annealed importance sampling co generation problem \n",
            "\n",
            "{'rouge-1': {'r': 0.1686746987951807, 'p': 0.9333333333333333, 'f': 0.285714283121616}, 'rouge-2': {'r': 0.11764705882352941, 'p': 0.8571428571428571, 'f': 0.20689654960166468}, 'rouge-l': {'r': 0.10843373493975904, 'p': 0.6, 'f': 0.1836734667950854}}\n",
            "pair:  topic modeling discovers latent topic probability given text documents generate meaningful topic better represents given document proposed universal method used data preprocessing stage method consists three steps first generates word word pair every single document second applies two way parallel tf idf algorithm word word pair semantic filtering third uses means algorithm merge word pairs similar semantic meaning experiments carried open movie database omdb reuters dataset newsgroup dataset use mean average precision score evaluation metric comparing results state art topic models latent dirichlet allocation traditional restricted boltzmann machines proposed data preprocessing improve generated topic accuracy number clusters number word pairs adjusted different type text document also discussed\n",
            "output sentence:  proposed universal method used data preprocessing stage generate meaningful topic better represents given document document \n",
            "\n",
            "{'rouge-1': {'r': 0.13513513513513514, 'p': 0.7692307692307693, 'f': 0.22988505492931696}, 'rouge-2': {'r': 0.05813953488372093, 'p': 0.4166666666666667, 'f': 0.10204081417742612}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}}\n",
            "pair:  leverage recent insights second order optimisation neural networks construct kronecker factored laplace approximation posterior weights trained network approximation requires modification training procedure enabling practitioners estimate uncertainty models currently used production without retrain extensively compare method using dropout diagonal laplace approximation estimating uncertainty network demonstrate kronecker factored method leads better uncertainty estimates distribution data robust simple adversarial attacks approach requires calculating two square curvature factor matrices layer size equal respective square input output size layer making method efficient computationally terms memory usage illustrate scalability applying state art convolutional network architecture\n",
            "output sentence:  construct kronecker factored laplace approximation neural networks leads efficient matrix normal distribution weights \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.5, 'f': 0.11428571226122453}, 'rouge-2': {'r': 0.014492753623188406, 'p': 0.14285714285714285, 'f': 0.026315787801246642}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.375, 'f': 0.08571428368979596}}\n",
            "pair:  intuitively image classification profit using spatial information recent work however suggests might overrated standard cnns paper pushing envelope aim investigate reliance necessity spatial information propose analyze three methods namely shuffle conv gap fc conv destroy spatial information training testing phases extensively evaluate methods several object recognition datasets cifar small imagenet imagenet wide range cnn architectures vgg resnet resnet mobilenet squeezenet interestingly consistently observe spatial information completely deleted significant number layers small performance drops\n",
            "output sentence:  spatial information last layers necessary good classification accuracy \n",
            "\n",
            "{'rouge-1': {'r': 0.19230769230769232, 'p': 0.5555555555555556, 'f': 0.2857142818938776}, 'rouge-2': {'r': 0.10909090909090909, 'p': 0.3157894736842105, 'f': 0.16216215834550776}, 'rouge-l': {'r': 0.19230769230769232, 'p': 0.5555555555555556, 'f': 0.2857142818938776}}\n",
            "pair:  present end end trainable approach optical character recognition ocr printed documents based predicting two dimensional character grid chargrid representation document image semantic segmentation task identify individual character instances chargrid regard characters objects use object detection techniques computer vision demonstrate experimentally method outperforms previous state art approaches accuracy easily parallelizable gpu thereby significantly faster well easier train\n",
            "output sentence:  end end trainable optical character recognition printed documents achieve state art results beating tesseract benchmark datasets accuracy using accuracy using algorithm based \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.5555555555555556, 'f': 0.1388888867013889}, 'rouge-2': {'r': 0.025, 'p': 0.25, 'f': 0.04545454380165295}, 'rouge-l': {'r': 0.07936507936507936, 'p': 0.5555555555555556, 'f': 0.1388888867013889}}\n",
            "pair:  flow based models real nvp extremely powerful approach density estimation however existing flow based models restricted transforming continuous densities continuous input space similarly continuous distributions continuous latent variables makes poorly suited modeling representing discrete structures data distributions example class membership discrete symmetries address difficulty present normalizing flow architecture relies domain partitioning using locally invertible functions possesses real discrete valued latent variables real discrete rad approach retains desirable normalizing flow properties exact sampling exact inference analytically computable probabilities time allowing simultaneous modeling continuous discrete structure data distribution\n",
            "output sentence:  flow based models non invertible also learn discrete variables \n",
            "\n",
            "{'rouge-1': {'r': 0.1323529411764706, 'p': 0.8181818181818182, 'f': 0.22784809886877105}, 'rouge-2': {'r': 0.08139534883720931, 'p': 0.7, 'f': 0.1458333314670139}, 'rouge-l': {'r': 0.1323529411764706, 'p': 0.8181818181818182, 'f': 0.22784809886877105}}\n",
            "pair:  system identification process building mathematical model unknown system measurements inputs outputs key step model based control estimator design output prediction work presents algorithm non linear offline system identification partial observations situations system full state directly observable algorithm presented called sisl iteratively infers system full state non linear optimization updates model parameters test algorithm simulated system coupled lorenz attractors showing algorithm ability identify high dimensional systems prove intractable particle based approaches also use sisl identify dynamics aerobatic helicopter augmenting state unobserved fluid states learn model predicts acceleration helicopter better state art approaches\n",
            "output sentence:  work presents scalable algorithm non linear offline system identification partial observations \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}, 'rouge-2': {'r': 0.08860759493670886, 'p': 0.875, 'f': 0.16091953855991545}, 'rouge-l': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}}\n",
            "pair:  propose new model making generalizable diverse retrosynthetic reaction predictions given target compound task predict likely chemical reactants produce target generative task framed sequence sequence problem using smiles representations molecules building top popular transformer architecture propose two novel pre training methods construct relevant auxiliary tasks plausible reactions problem furthermore incorporate discrete latent variable model architecture encourage model produce diverse set alternative predictions subset reaction examples united states patent literature uspto benchmark dataset model greatly improves performance baseline also generating predictions diverse\n",
            "output sentence:  propose new model making generalizable diverse retrosynthetic reaction predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.9285714285714286, 'f': 0.22033898095949442}, 'rouge-2': {'r': 0.10218978102189781, 'p': 0.875, 'f': 0.18300653407492845}, 'rouge-l': {'r': 0.11538461538461539, 'p': 0.8571428571428571, 'f': 0.20338982841712153}}\n",
            "pair:  training neural network desired task one may prefer adapt pretrained network rather start randomly initialized one due lacking enough training data performing lifelong learning system learn new task previously trained tasks wishing encode priors network via preset weights commonly employed approaches network adaptation fine tuning using pre trained network fixed feature extractor among others paper propose straightforward alternative side tuning side tuning adapts pretrained network training lightweight side network fused unchanged pre rained network using simple additive process simple method works well better existing solutions resolves basic issues fine tuning fixed features several common baselines particular side tuning less prone overfitting little training data available yields better results using fixed feature extractor suffer catastrophic forgetting lifelong learning demonstrate performance side tuning diverse set scenarios including lifelong learning icifar taskonomy reinforcement learning imitation learning visual navigation habitat nlp question answering squad single task transfer learning taskonomy consistently promising results\n",
            "output sentence:  side tuning adapts pre trained network training lightweight side network fused unchanged pre trained network using simple additive process \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.875, 'f': 0.14141413992857876}, 'rouge-2': {'r': 0.059322033898305086, 'p': 0.875, 'f': 0.1111111099218947}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.875, 'f': 0.14141413992857876}}\n",
            "pair:  present end end design methodology efficient deep learning deployment unlike previous methods separately optimize neural network architecture pruning policy quantization policy jointly optimize end end manner deal larger design space brings train quantization aware accuracy predictor fed evolutionary search select best fit first generate large dataset nn architecture imagenet accuracy pairs without training architecture sampling unified supernet use data train accuracy predictor without quantization using predictor transfer technique get quantization aware predictor reduces amount post quantization fine tuning time extensive experiments imagenet show benefits end end methodology maintains accuracy resnet float model saving bitops comparing bit model obtain level accuracy mobilenetv haq achieving latency energy saving end end optimization outperforms separate optimizations using proxylessnas amc haq accuracy reducing orders magnitude gpu hours co emission\n",
            "output sentence:  present end end design methodology efficient deep learning deployment \n",
            "\n",
            "{'rouge-1': {'r': 0.08737864077669903, 'p': 0.5625, 'f': 0.15126050187416146}, 'rouge-2': {'r': 0.032, 'p': 0.25, 'f': 0.05673758664051111}, 'rouge-l': {'r': 0.06796116504854369, 'p': 0.4375, 'f': 0.11764705649601022}}\n",
            "pair:  hamiltonian formalism plays central role classical quantum physics hamiltonians main tool modelling continuous time evolution systems conserved quantities come equipped many useful properties like time reversibility smooth interpolation time properties important many machine learning problems sequence prediction reinforcement learning density modelling typically provided box standard tools recurrent neural networks paper introduce hamiltonian generative network hgn first approach capable consistently learning hamiltonian dynamics high dimensional observations images without restrictive domain assumptions trained use hgn sample new trajectories perform rollouts forward backward time even speed slow learned dynamics demonstrate simple modification network architecture turns hgn powerful normalising flow model called neural hamiltonian flow nhf uses hamiltonian dynamics model expressive densities hence hope work serves first practical demonstration value hamiltonian formalism bring machine learning results video evaluations available http tiny cc hgn\n",
            "output sentence:  introduce class generative models reliably learn hamiltonian dynamics high dimensional observations learnt hamiltonian applied sequence modeling normalising flow \n",
            "\n",
            "{'rouge-1': {'r': 0.04938271604938271, 'p': 0.3333333333333333, 'f': 0.08602150312868545}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.037037037037037035, 'p': 0.25, 'f': 0.06451612678459945}}\n",
            "pair:  deep neural networks dnns require complex models achieve high performance parameter quantization widely used reducing implementation complexities previous studies quantization mostly based extensive simulation using training data choose different approach attempt measure per parameter capacity dnn models interpret results obtain insights optimum quantization parameters research uses artificially generated data generic forms fully connected dnns convolutional neural networks recurrent neural networks conduct memorization classification tests study effects number precision parameters performance model per parameter capacities assessed measuring mutual information input classified output also extend memorization capacity measurement results image classification language modeling tasks get insight parameter quantization performing real tasks training test performances compared\n",
            "output sentence:  suggest sufficient number bits representing weights dnns optimum bits conservative solving real problems \n",
            "\n",
            "{'rouge-1': {'r': 0.11224489795918367, 'p': 0.7333333333333333, 'f': 0.19469026318427443}, 'rouge-2': {'r': 0.01652892561983471, 'p': 0.14285714285714285, 'f': 0.029629627770644835}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.4666666666666667, 'f': 0.1238938030072833}}\n",
            "pair:  information bottleneck ib tuning relative strength compression prediction terms two terms behave relationship dataset learned representation paper set answer questions studying multiple phase transitions ib objective ib defined encoding distribution input target representation sudden jumps di prediction accuracy observed increasing introduce definition ib phase transitions qualitative change ib loss landscape show transitions correspond onset learning new classes using second order calculus variations derive formula provides practical condition ib phase transitions draw connection fisher information matrix parameterized models provide two perspectives understand formula revealing ib phase transition finding component maximum nonlinear correlation orthogonal learned representation close analogy canonical correlation analysis cca linear settings based theory present algorithm discovering phase transition points finally verify theory algorithm accurately predict phase transitions categorical datasets predict onset learning new classes class difficulty mnist predict prominent phase transitions cifar\n",
            "output sentence:  give theoretical analysis information bottleneck objective understand predict observed phase transitions prediction vs compression tradeoff \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.45454545454545453, 'f': 0.12658227608396094}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.45454545454545453, 'f': 0.12658227608396094}}\n",
            "pair:  automatic piano fingering hard task computers learn using data data collection hard expensive propose automate process automatically extracting fingerings public videos midi files using computer vision techniques running process videos results largest dataset piano fingering notes show running previously proposed model automatic piano fingering dataset fine tuning manually labeled piano fingering data achieve state art results addition fingering extraction method also introduce novel method transferring deep learning computer vision models work domain data fine tuning domain augmentation proposed generative adversarial network gan demonstration anonymously release visualization output process single video https youtu gfs uwqhr\n",
            "output sentence:  automatically extract fingering information videos piano performances used automatic fingering prediction models \n",
            "\n",
            "{'rouge-1': {'r': 0.0975609756097561, 'p': 0.6666666666666666, 'f': 0.17021276373019467}, 'rouge-2': {'r': 0.05309734513274336, 'p': 0.5454545454545454, 'f': 0.09677419193158171}, 'rouge-l': {'r': 0.0975609756097561, 'p': 0.6666666666666666, 'f': 0.17021276373019467}}\n",
            "pair:  deep neural networks achieved state art performance various fields scaled used real world applications means reduce size neural network preserving performance knowledge transfer brought lot attention one popular method knowledge transfer knowledge distillation kd softened outputs pre trained teacher network help train student networks since kd transfer methods proposed mainly focus loss functions activations hidden layers additional modules transfer knowledge well teacher networks student networks work focus structure teacher network get effect multiple teacher networks without additional resources propose changing structure teacher network stochastic blocks skip connections teacher network becomes aggregate huge number paths training phase sub network generated dropping stochastic blocks randomly used teacher network allows training student network multiple teacher networks enhances student network resources single teacher network verify proposed structure brings improvement student networks benchmark datasets\n",
            "output sentence:  goal paper get effect multiple teacher networks exploiting stochastic blocks skip connections \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.6666666666666666, 'f': 0.24999999695312503}, 'rouge-2': {'r': 0.09195402298850575, 'p': 0.5, 'f': 0.15533980320105573}, 'rouge-l': {'r': 0.13846153846153847, 'p': 0.6, 'f': 0.224999996953125}}\n",
            "pair:  paper describe implicit autoencoder iae generative autoencoder generative path recognition path parametrized implicit distributions use two generative adversarial networks define reconstruction regularization cost functions implicit autoencoder derive learning rules based maximum likelihood learning using implicit distributions allows us learn expressive posterior conditional likelihood distributions autoencoder learning expressive conditional likelihood distribution enables latent code capture abstract high level information data remaining information captured implicit conditional likelihood distribution example show implicit autoencoders disentangle global local information perform deterministic stochastic reconstructions images show implicit autoencoders disentangle discrete underlying factors variation continuous factors unsupervised fashion perform clustering semi supervised learning\n",
            "output sentence:  propose generative autoencoder learn expressive posterior conditional likelihood distributions using implicit distributions train model using new formulation elbo \n",
            "\n",
            "{'rouge-1': {'r': 0.05128205128205128, 'p': 0.5, 'f': 0.0930232541265549}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.038461538461538464, 'p': 0.375, 'f': 0.06976744017306657}}\n",
            "pair:  present method policy learning navigate indoor environments adopt hierarchical policy approach two agents trained work cohesion one another perform complex navigation task planner agent operates higher level proposes sub goals executor agent executor reports embedding summary back planner additional side information end series operations planner next sub goal proposal end goal generated environment exposed planner decides set sub goals propose executor show planner executor setup drastically increases sample efficiency method traditional single agent approaches effectively mitigating difficulty accompanying long series actions sparse reward signal challenging habitat environment requires navigating various realistic indoor environments demonstrate approach offers significant improvement prior work navigation\n",
            "output sentence:  present hierarchical learning framework navigation within embodied learning setting \n",
            "\n",
            "{'rouge-1': {'r': 0.021505376344086023, 'p': 0.3333333333333333, 'f': 0.04040403926538112}, 'rouge-2': {'r': 0.00909090909090909, 'p': 0.2, 'f': 0.01739130351606809}, 'rouge-l': {'r': 0.021505376344086023, 'p': 0.3333333333333333, 'f': 0.04040403926538112}}\n",
            "pair:  passage time unmanned autonomous vehicles uavs especially autonomous flying drones grabbed lot attention artificial intelligence since electronic technology getting smaller cheaper efficient huge advancement study uavs observed recently monitoring floods discerning spread algae water bodies detecting forest trail application far wide work mainly focused autonomous flying drones establish case study towards efficiency robustness accuracy uavs showed results well supported experiments provide details software hardware architecture used study discuss implementation algorithms present experiments provide comparison three different state art algorithms namely trailnet inceptionresnet mobilenet terms accuracy robustness power consumption inference time study shown mobilenet produced better results less computational requirement power consumption also reported challenges faced work well brief discussion future work improve safety features performance\n",
            "output sentence:  case study optimal deep learning model uavs \n",
            "\n",
            "{'rouge-1': {'r': 0.1038961038961039, 'p': 0.5333333333333333, 'f': 0.17391304074905484}, 'rouge-2': {'r': 0.050505050505050504, 'p': 0.3125, 'f': 0.08695651934366737}, 'rouge-l': {'r': 0.1038961038961039, 'p': 0.5333333333333333, 'f': 0.17391304074905484}}\n",
            "pair:  central goal unsupervised learning acquire representations unlabeled data experience used effective learning downstream tasks modest amounts labeled data many prior unsupervised learning works aim developing proxy objectives based reconstruction disentanglement prediction metrics instead develop unsupervised meta learning method explicitly optimizes ability learn variety tasks small amounts data construct tasks unlabeled data automatic way run meta learning constructed tasks surprisingly find integrated meta learning relatively simple task construction mechanisms clustering embeddings lead good performance variety downstream human specified tasks experiments across four image datasets indicate unsupervised meta learning approach acquires learning algorithm without labeled data applicable wide range downstream classification tasks improving upon embedding learned four prior unsupervised learning methods\n",
            "output sentence:  unsupervised learning method uses meta learning enable efficient learning downstream image classification tasks outperforming state art methods \n",
            "\n",
            "{'rouge-1': {'r': 0.16216216216216217, 'p': 0.8571428571428571, 'f': 0.2727272700516529}, 'rouge-2': {'r': 0.12087912087912088, 'p': 0.8461538461538461, 'f': 0.21153845935096155}, 'rouge-l': {'r': 0.16216216216216217, 'p': 0.8571428571428571, 'f': 0.2727272700516529}}\n",
            "pair:  study problem alleviating instability issue gan training procedure via new architecture design discrepancy minimax maximin objective values could serve proxy difficulties alternating gradient descent encounters optimization gans work give new results benefits multi generator architecture gans show minimax gap shrinks epsilon number generators increases rate epsilon improves best known result epsilon core techniques novel application shapley folkman lemma generic minimax problem literature technique known work objective function restricted lagrangian function constraint optimization problem proposed stackelberg gan performs well experimentally synthetic real world datasets improving frechet inception distance previous multi generator gans benchmark datasets\n",
            "output sentence:  study problem alleviating instability issue gan training procedure via new architecture design theoretical guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.6666666666666666, 'f': 0.1791044752862553}, 'rouge-2': {'r': 0.04225352112676056, 'p': 0.3, 'f': 0.07407407190976992}, 'rouge-l': {'r': 0.08620689655172414, 'p': 0.5555555555555556, 'f': 0.14925372901759862}}\n",
            "pair:  search space key consideration neural architecture search recently xie et al found randomly generated networks distribution perform similarly suggest search random graph distributions instead graphs propose graphon new search space graphon limit cauchy sequence graphs scale free probabilistic distribution graphs different number vertices drawn property enables us perform nas using fast low capacity models scale found models necessary develop algorithm nas space graphons empirically demonstrate find stage wise graphs outperform densenet baselines imagenet\n",
            "output sentence:  graphon good search space neural architecture search empirically produces good networks \n",
            "\n",
            "{'rouge-1': {'r': 0.037383177570093455, 'p': 0.2857142857142857, 'f': 0.06611570043303058}, 'rouge-2': {'r': 0.00819672131147541, 'p': 0.07692307692307693, 'f': 0.014814813074348631}, 'rouge-l': {'r': 0.028037383177570093, 'p': 0.21428571428571427, 'f': 0.0495867748131959}}\n",
            "pair:  model pruning become useful technique improves computational efficiency deep learning making possible deploy solutions resource limited scenarios widely used practice relevant work assumes smaller norm parameter feature plays less informative role inference time paper propose channel pruning technique accelerating computations deep convolutional neural networks cnns critically rely assumption instead focuses direct simplification channel channel computation graph cnn without need performing computationally difficult always useful task making high dimensional tensors cnn structured sparse approach takes two stages first adopt end end stochastic training method eventually forces outputs channels constant prune constant channels original neural network adjusting biases impacting layers resulting compact model quickly fine tuned approach mathematically appealing optimization perspective easy reproduce experimented approach several image learning benchmarks demonstrate interest ing aspects competitive performance\n",
            "output sentence:  cnn model pruning method using ista rescaling trick enforce sparsity scaling parameters batch normalization \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.7777777777777778, 'f': 0.1647058804595156}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.25, 'f': 0.04123711188861734}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.4444444444444444, 'f': 0.09411764516539796}}\n",
            "pair:  carbon footprint natural language processing nlp research increasing recent years due reliance large inefficient neural network implementations distillation network compression technique attempts impart knowledge large model smaller one use teacher student distillation improve efficiency biaffine dependency parser obtains state art performance respect accuracy parsing speed dozat manning distilling original model trainable parameters observe average decrease point uas las across number diverse universal dependency treebanks faster baseline model cpu gpu inference time also observe small increase performance compressing treebanks finally distillation attain parser faster also accurate fastest modern parser penn treebank\n",
            "output sentence:  increase efficiency neural network dependency parsers teacher student distillation \n",
            "\n",
            "{'rouge-1': {'r': 0.17073170731707318, 'p': 0.8235294117647058, 'f': 0.28282827998367516}, 'rouge-2': {'r': 0.10476190476190476, 'p': 0.6111111111111112, 'f': 0.17886178611937345}, 'rouge-l': {'r': 0.17073170731707318, 'p': 0.8235294117647058, 'f': 0.28282827998367516}}\n",
            "pair:  effective performance neural networks depends critically effective tuning optimization hyperparameters especially learning rates schedules thereof present amortized proximal optimization apo takes perspective optimization step approximately minimize proximal objective similar ones used motivate natural gradient trust region policy optimization optimization hyperparameters adapted best minimize proximal objective one weight update show idealized version apo oracle minimizes proximal objective exactly achieves global convergence stationary point locally second order convergence global optimum neural networks apo incurs minimal computational overhead experiment using apo adapt variety optimization hyperparameters online training including possibly layer specific learning rates damping coefficients gradient variance exponents variety network architectures optimization algorithms including sgd rmsprop fac show minimal tuning apo performs competitively carefully tuned optimizers\n",
            "output sentence:  introduce amortized proximal optimization apo method adapt variety optimization hyperparameters online training including learning rates damping gradient gradient variance \n",
            "\n",
            "{'rouge-1': {'r': 0.06349206349206349, 'p': 0.3333333333333333, 'f': 0.10666666397866674}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.07692307692307693, 'f': 0.023255811387236626}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.25, 'f': 0.07999999731200008}}\n",
            "pair:  modern deep neural networks achieve high accuracy training distribution test distribution identically distributed assumption frequently violated practice train test distributions mismatched accuracy plummet currently techniques improve robustness unforeseen data shifts encountered deployment work propose technique improve robustness uncertainty estimates image classifiers propose augmix data processing technique simple implement adds limited computational overhead helps models withstand unforeseen corruptions augmix significantly improves robustness uncertainty measures challenging image classification benchmarks closing gap previous methods best possible performance cases half\n",
            "output sentence:  obtain state art robustness data shifts maintain calibration data shift even though even accuracy drops \n",
            "\n",
            "{'rouge-1': {'r': 0.21052631578947367, 'p': 0.9411764705882353, 'f': 0.3440860185177477}, 'rouge-2': {'r': 0.16279069767441862, 'p': 0.875, 'f': 0.27450980127643215}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.9411764705882353, 'f': 0.3440860185177477}}\n",
            "pair:  deep neural networks shown incredible performance inference tasks variety domains unfortunately current deep networks enormous cloud based structures require significant storage space limits scaling deep learning service dlaas use device augmented intelligence paper finds algorithms directly use lossless compressed representations deep feedforward networks synaptic weights drawn discrete sets perform inference without full decompression basic insight allows less rate naive approaches recognition bipartite graph layers feedforward networks kind permutation invariance labeling nodes terms inferential operation inference operation depends locally edges directly connected also provide experimental results approach mnist dataset\n",
            "output sentence:  paper finds algorithms directly use lossless compressed representations deep feedforward networks perform inference without full decompression decompression \n",
            "\n",
            "{'rouge-1': {'r': 0.03389830508474576, 'p': 0.3333333333333333, 'f': 0.06153845986272194}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.3333333333333333, 'f': 0.06153845986272194}}\n",
            "pair:  reinforcement learning algorithms though successful tend fit training environments thereby hampering application real world paper proposes text text text robust reinforcement learning algorithm significant robust performance low high dimensional control tasks method formalises robust reinforcement learning novel min max game wasserstein constraint correct convergent solver apart formulation also propose efficient scalable solver following novel zero order optimisation method believe useful numerical optimisation general empirically demonstrate significant gains compared standard robust state art algorithms high dimensional mujuco environments\n",
            "output sentence:  rl algorithm learns robust changes dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.14084507042253522, 'p': 0.8333333333333334, 'f': 0.2409638529481783}, 'rouge-2': {'r': 0.060240963855421686, 'p': 0.4166666666666667, 'f': 0.10526315568753467}, 'rouge-l': {'r': 0.1267605633802817, 'p': 0.75, 'f': 0.21686746740600957}}\n",
            "pair:  analyze speed convergence global optimum gradient descent training deep linear neural network minimizing loss whitened data convergence linear rate guaranteed following hold dimensions hidden layers least minimum input output dimensions ii weight matrices initialization approximately balanced iii initial loss smaller loss rank deficient solution assumptions initialization conditions ii iii necessary sense violating one may lead convergence failure moreover important case output dimension scalar regression met thus convergence global optimum holds constant probability random initialization scheme results significantly extend previous analyses deep linear residual networks bartlett et al\n",
            "output sentence:  analyze gradient descent deep linear neural networks providing guarantee convergence global optimum linear rate \n",
            "\n",
            "{'rouge-1': {'r': 0.08823529411764706, 'p': 0.75, 'f': 0.15789473495844877}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.125, 'f': 0.022471908476202618}, 'rouge-l': {'r': 0.08823529411764706, 'p': 0.75, 'f': 0.15789473495844877}}\n",
            "pair:  regularizers critical tools machine learning due ability simplify solutions however imposing strong regularization gradient descent method easily fails limits generalization ability underlying neural networks understand phenomenon investigate training fails strong regularization specifically examine gradients change time different regularization strengths provide analysis gradients diminish fast find exists tolerance level regularization strength learning completely fails regularization strength goes beyond propose simple novel method delayed strong regularization order moderate tolerance level experiment results show proposed approach indeed achieves strong regularization regularizers improves accuracy sparsity public data sets source code published\n",
            "output sentence:  investigate strong regularization fails propose method achieve strong regularization \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.4, 'f': 0.13793103162901313}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.2, 'f': 0.06153845893491135}, 'rouge-l': {'r': 0.0625, 'p': 0.3, 'f': 0.1034482730083235}}\n",
            "pair:  propose method automatically compute importance features every observation time series simulating counterfactual trajectories given previous observations define importance observation change model output caused replacing observation generated one method applied arbitrarily complex time series models compare generated feature importance existing methods like sensitivity analyses feature occlusion explanation baselines show approach generates precise explanations less sensitive noise input signals\n",
            "output sentence:  explaining multivariate time series models finding important observations time using counterfactuals \n",
            "\n",
            "{'rouge-1': {'r': 0.06666666666666667, 'p': 0.4444444444444444, 'f': 0.11594202671707628}, 'rouge-2': {'r': 0.014492753623188406, 'p': 0.125, 'f': 0.02597402411199204}, 'rouge-l': {'r': 0.05, 'p': 0.3333333333333333, 'f': 0.0869565194706995}}\n",
            "pair:  significant recent evidence supervised learning parametrized setting wider networks achieve better test error words bias variance tradeoff directly observable increasing network width arbitrarily investigate whether corresponding phenomenon present reinforcement learning experiment four openai gym environments increasing width value policy networks beyond prescribed values empirical results lend support hypothesis however tuning hyperparameters network width separately remains important future work environments algorithms optimal hyperparameters vary noticably across widths confounding results hyperparameters used widths\n",
            "output sentence:  parametrization width seems help deep reinforcement learning supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.14925373134328357, 'p': 0.9090909090909091, 'f': 0.2564102539875082}, 'rouge-2': {'r': 0.1, 'p': 0.8, 'f': 0.17777777580246917}, 'rouge-l': {'r': 0.13432835820895522, 'p': 0.8181818181818182, 'f': 0.23076922834648256}}\n",
            "pair:  learnability different neural architectures characterized directly computable measures data complexity paper reframe problem architecture selection understanding data determines expressive generalizable architectures suited data beyond inductive bias suggesting algebraic topology measure data complexity show power network express topological complexity dataset decision boundary strictly limiting factor ability generalize provide first empirical characterization topological capacity neural networks empirical analysis shows every level dataset complexity neural networks exhibit topological phase transitions stratification observation allowed us connect existing theory empirically driven conjectures choice architectures single hidden layer neural networks\n",
            "output sentence:  show learnability different neural architectures characterized directly computable measures data complexity \n",
            "\n",
            "{'rouge-1': {'r': 0.1206896551724138, 'p': 0.5833333333333334, 'f': 0.1999999971591837}, 'rouge-2': {'r': 0.02702702702702703, 'p': 0.18181818181818182, 'f': 0.04705882127612468}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.3333333333333333, 'f': 0.11428571144489805}}\n",
            "pair:  uncertainty important feature intelligence helps brain become flexible creative powerful intelligent system crossbar based neuromorphic computing chips computing mainly performed analog circuits uncertainty used imitate brain however current deep neural networks taken uncertainty neuromorphic computing chip consideration therefore performances neuromorphic computing chips good original platforms cpus gpus work proposed uncertainty adaptation training scheme uats tells uncertainty neural network training process experimental results show neural networks achieve comparable inference performances uncertain neuromorphic computing chip compared results original platforms much better performances without training scheme\n",
            "output sentence:  training method make deep learning algorithms work better neuromorphic computing chips uncertainty \n",
            "\n",
            "{'rouge-1': {'r': 0.06930693069306931, 'p': 0.7, 'f': 0.12612612448664884}, 'rouge-2': {'r': 0.025, 'p': 0.3333333333333333, 'f': 0.04651162660897786}, 'rouge-l': {'r': 0.06930693069306931, 'p': 0.7, 'f': 0.12612612448664884}}\n",
            "pair:  graph neural networks shown promising results representing analyzing diverse graph structured data social citation protein interaction networks existing approaches commonly suffer oversmoothing issue regardless whether policies edge based node based neighborhood aggregation methods also focus transductive scenarios fixed graphs leading poor generalization performance unseen graphs address issues propose new graph neural network model considers edge based neighborhood relationships node based entity features graph entities step mixture via random walk gesm gesm employs mixture various steps random walk alleviate oversmoothing problem attention use node information explicitly two mechanisms allow weighted neighborhood aggregation considers properties entities relations intensive experiments show proposed gesm achieves state art comparable performances four benchmark graph datasets comprising transductive inductive learning tasks furthermore empirically demonstrate significance considering global information source code publicly available near future\n",
            "output sentence:  simple effective graph neural network mixture random walk steps attention \n",
            "\n",
            "{'rouge-1': {'r': 0.12048192771084337, 'p': 0.625, 'f': 0.20202019931027448}, 'rouge-2': {'r': 0.0392156862745098, 'p': 0.26666666666666666, 'f': 0.06837606614069698}, 'rouge-l': {'r': 0.07228915662650602, 'p': 0.375, 'f': 0.1212121185021937}}\n",
            "pair:  leverage crowd sourced data train multi speaker text speech tts models synthesize clean speech speakers essential learn disentangled representations independently control speaker identity background noise generated signals however learning representations challenging due lack labels describing recording conditions training example fact speakers recording conditions often correlated since users often make many recordings using equipment paper proposes three components address problem formulating conditional generative model factorized latent variables using data augmentation add noise correlated speaker identity whose label known training using adversarial factorization improve disentanglement experimental results demonstrate proposed method disentangle speaker noise attributes even correlated training data used consistently synthesize clean speech speakers ablation studies verify importance proposed component\n",
            "output sentence:  data augmentation adversarial training effective disentangling correlated speaker noise enabling independent control attribute text speech synthesis \n",
            "\n",
            "{'rouge-1': {'r': 0.09722222222222222, 'p': 0.5, 'f': 0.16279069494862092}, 'rouge-2': {'r': 0.011627906976744186, 'p': 0.07692307692307693, 'f': 0.020202017920620603}, 'rouge-l': {'r': 0.06944444444444445, 'p': 0.35714285714285715, 'f': 0.1162790670416442}}\n",
            "pair:  simulation useful tool situations training data machine learning models costly annotate even hard acquire work propose reinforcement learning based method automatically adjusting parameters non differentiable simulator thereby controlling distribution synthesized data order maximize accuracy model trained data contrast prior art hand crafts simulation parameters adjusts parts available parameters approach fully controls simulator actual underlying goal maximizing accuracy rather mimicking real data distribution randomly generating large volume data find approach quickly converges optimal simulation parameters controlled experiments ii indeed discover good sets parameters image rendering simulator actual computer vision applications\n",
            "output sentence:  propose algorithm automatically adjusts parameters simulation engine generate training data neural network validation accuracy \n",
            "\n",
            "{'rouge-1': {'r': 0.12037037037037036, 'p': 0.6842105263157895, 'f': 0.20472440690433383}, 'rouge-2': {'r': 0.014705882352941176, 'p': 0.1111111111111111, 'f': 0.02597402390959706}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.3157894736842105, 'f': 0.09448818643189293}}\n",
            "pair:  study problem safe adaptation given model trained variety past experiences task model learn perform task new situation avoiding catastrophic failure problem setting occurs frequently real world reinforcement learning scenarios vehicle adapting drive new city robotic drone adapting policy trained simulation learning without catastrophic failures exceptionally difficult prior experience allow us learn models make much easier models might directly transfer new settings enable cautious adaptation substantially safer na adaptation well learning scratch building intuition propose risk averse domain adaptation rada rada works two steps first trains probabilistic model based rl agents population source domains gain experience capture epistemic uncertainty environment dynamics dropped new environment employs pessimistic exploration policy selecting actions best worst case performance forecasted probabilistic model show simple maximin policy accelerates domain adaptation safety critical driving environment varying vehicle sizes compare approach approaches adapting new environments including meta reinforcement learning\n",
            "output sentence:  adaptation rl agent target environment unknown dynamics fast safe transfer prior experience variety environments select risk averse actions adaptation \n",
            "\n",
            "{'rouge-1': {'r': 0.14666666666666667, 'p': 0.8461538461538461, 'f': 0.24999999748192153}, 'rouge-2': {'r': 0.06542056074766354, 'p': 0.5384615384615384, 'f': 0.11666666473472223}, 'rouge-l': {'r': 0.14666666666666667, 'p': 0.8461538461538461, 'f': 0.24999999748192153}}\n",
            "pair:  injecting adversarial examples training known adversarial training improve robustness one step attacks unknown iterative attacks address challenge first show iteratively generated adversarial images easily transfer networks trained strategy inspired observation propose cascade adversarial training transfers knowledge end results adversarial training train network scratch injecting iteratively generated adversarial images crafted already defended networks addition one step adversarial images network trained also propose utilize embedding space classification low level pixel level similarity learning ignore unknown pixel level perturbation training inject adversarial images without replacing corresponding clean images penalize distance two embeddings clean adversarial experimental results show cascade adversarial training together proposed low level similarity learning efficiently enhances robustness iterative attacks expense decreased robustness one step attacks show combining two techniques also improve robustness worst case black box attack scenario\n",
            "output sentence:  cascade adversarial training low level similarity learning improve robustness white box black box attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.056818181818181816, 'p': 0.5, 'f': 0.10204081449396088}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03409090909090909, 'p': 0.3, 'f': 0.061224487963348645}}\n",
            "pair:  paper propose improved quantitative evaluation framework generative adversarial networks gans generating domain specific images improve conventional evaluation methods two levels feature representation evaluation metric unlike existing evaluation frameworks transfer representation imagenet inception model map images onto feature space framework uses specialized encoder acquire fine grained domain specific representation moreover datasets multiple classes propose class aware frechet distance cafd employs gaussian mixture model feature space better fit multi manifold feature distribution experiments analysis feature level image level conducted demonstrate improvements proposed framework recently proposed state art fid method best knowledge first provide counter examples fid gives inconsistent results human judgments shown experiments framework able overcome shortness fid improves robustness code made available\n",
            "output sentence:  paper improves existing sample based evaluation gans contains insightful experiments \n",
            "\n",
            "{'rouge-1': {'r': 0.053763440860215055, 'p': 0.625, 'f': 0.0990098995314185}, 'rouge-2': {'r': 0.008333333333333333, 'p': 0.14285714285714285, 'f': 0.01574803045446098}, 'rouge-l': {'r': 0.03225806451612903, 'p': 0.375, 'f': 0.05940593913537892}}\n",
            "pair:  consider dictionary learning problem aim model given data linear combination columns matrix known dictionary sparse weights forming linear combination known coefficients since dictionary coefficients parameterizing linear model unknown corresponding optimization inherently non convex major challenge recently provable algorithms dictionary learning proposed yet provide guarantees recovery dictionary without explicit recovery guarantees coefficients moreover estimation error dictionary adversely impacts ability successfully localize estimate coefficients potentially limits utility existing provable dictionary learning methods applications coefficient recovery interest end develop noodl simple neurally plausible alternating optimization based online dictionary learning algorithm recovers dictionary coefficients exactly geometric rate initialized appropriately algorithm noodl also scalable amenable large scale distributed implementations neural architectures mean involves simple linear non linear operations finally corroborate theoretical results via experimental evaluation proposed algorithm current state art techniques\n",
            "output sentence:  present provable algorithm exactly recovering factors dictionary learning model \n",
            "\n",
            "{'rouge-1': {'r': 0.1978021978021978, 'p': 0.9, 'f': 0.3243243213700187}, 'rouge-2': {'r': 0.15454545454545454, 'p': 0.85, 'f': 0.2615384589349113}, 'rouge-l': {'r': 0.1978021978021978, 'p': 0.9, 'f': 0.3243243213700187}}\n",
            "pair:  fine tuning pre trained models achieved exceptional results many language tasks study focused one self attention network model namely bert performed well terms stacking layers across diverse language understanding benchmarks however many downstream tasks information layers ignored bert fine tuning addition although self attention networks well known ability capture global dependencies room improvement remains terms emphasizing importance local contexts light advantages disadvantages paper proposes sesamebert generalized fine tuning method enables extraction global information among layers squeeze excitation enriches local information capturing neighboring contexts via gaussian blurring furthermore demonstrated effectiveness approach hans dataset used determine whether models adopted shallow heuristics instead learning underlying generalizations experiments revealed sesamebert outperformed bert respect glue benchmark hans evaluation set\n",
            "output sentence:  proposed sesamebert generalized fine tuning method enables extraction global information among layers squeeze excitation enriches local information neighboring contexts via gaussian \n",
            "\n",
            "{'rouge-1': {'r': 0.25, 'p': 0.8333333333333334, 'f': 0.3846153810650888}, 'rouge-2': {'r': 0.1282051282051282, 'p': 0.5263157894736842, 'f': 0.20618556386013392}, 'rouge-l': {'r': 0.23333333333333334, 'p': 0.7777777777777778, 'f': 0.3589743554240632}}\n",
            "pair:  sequence sequence seq seq neural models actively investigated abstractive summarization nevertheless existing neural abstractive systems frequently generate factually incorrect summaries vulnerable adversarial information suggesting crucial lack semantic understanding paper propose novel semantic aware neural abstractive summarization model learns generate high quality summaries semantic interpretation salient content novel evaluation scheme adversarial samples introduced measure well model identifies topic information model yields significantly better performance popular pointer generator summarizer human evaluation also confirms system summaries uniformly informative faithful well less redundant seq seq model\n",
            "output sentence:  propose semantic aware neural abstractive summarization model novel automatic summarization evaluation scheme measures well model identifies topic information adversarial samples \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 1.0, 'f': 0.11764705771626299}, 'rouge-2': {'r': 0.031914893617021274, 'p': 0.75, 'f': 0.06122448901291129}, 'rouge-l': {'r': 0.0625, 'p': 1.0, 'f': 0.11764705771626299}}\n",
            "pair:  audio signals sampled high temporal resolutions learning synthesize audio requires capturing structure across range timescales generative adversarial networks gans seen wide success generating images locally globally coherent seen little application audio generation paper introduce wavegan first attempt applying gans unsupervised synthesis raw waveform audio wavegan capable synthesizing one second slices audio waveforms global coherence suitable sound effect generation experiments demonstrate without labels wavegan learns produce intelligible words trained small vocabulary speech dataset also synthesize audio domains drums bird vocalizations piano compare wavegan method applies gans designed image generation image like audio feature representations finding approaches promising\n",
            "output sentence:  learning synthesize raw waveform audio gans \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.5, 'f': 0.20689654844233057}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.18181818181818182, 'f': 0.060606057828282954}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.3333333333333333, 'f': 0.13793103120095132}}\n",
            "pair:  well known neural networks universal approximators deeper networks tend practice powerful shallower ones shed light proving total number neurons required approximate natural classes multivariate polynomials variables grows linearly deep neural networks grows exponentially merely single hidden layer allowed also provide evidence number hidden layers increased neuron requirement grows exponentially suggesting minimum number layers required practical expressibility grows logarithmically\n",
            "output sentence:  prove deep neural networks exponentially efficient shallow ones approximating sparse multivariate polynomials \n",
            "\n",
            "{'rouge-1': {'r': 0.18333333333333332, 'p': 0.7333333333333333, 'f': 0.2933333301333333}, 'rouge-2': {'r': 0.07042253521126761, 'p': 0.35714285714285715, 'f': 0.11764705607197239}, 'rouge-l': {'r': 0.1, 'p': 0.4, 'f': 0.15999999680000007}}\n",
            "pair:  temporal logics useful describing dynamic system behavior successfully used language goal definitions task planning prior works inferring temporal logic specifications focused summarizing input dataset finding specifications satisfied plan traces belonging given set paper examine problem inferring specifications describe temporal differences two sets plan traces formalize concept providing contrastive explanations present bayesian probabilistic model inferring contrastive explanations linear temporal logic specifications demonstrate efficacy scalability robustness model inferring correct specifications across various benchmark planning domains simulated air combat mission\n",
            "output sentence:  present bayesian inference model infer contrastive explanations ltl specifications describing two sets plan traces differ \n",
            "\n",
            "{'rouge-1': {'r': 0.04040404040404041, 'p': 0.5, 'f': 0.07476635375665999}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.030303030303030304, 'p': 0.375, 'f': 0.05607476497161328}}\n",
            "pair:  backpropagation error algorithm bp often said impossible implement real brain recent success deep networks machine learning ai however inspired number proposals understanding brain might learn across multiple layers hence might implement approximate bp yet none proposals rigorously evaluated tasks bp guided deep learning proved critical architectures structured simple fully connected networks present first results scaling biologically motivated model deep learning datasets need deep networks appropriate architectures achieve good performance present results cifar imagenet cifar show algorithm straightforward weight transport free variant difference target propagation dtp modified remove backpropagation penultimate layer competitive bp training deep networks locally defined receptive fields untied weights imagenet find dtp algorithm perform significantly worse bp opening questions whether different architectures algorithms required scale approaches results implementation details help establish baselines biologically motivated deep learning schemes going forward\n",
            "output sentence:  benchmarks biologically plausible learning algorithms complex datasets architectures \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 1.0, 'f': 0.25316455475084126}, 'rouge-2': {'r': 0.05952380952380952, 'p': 0.5555555555555556, 'f': 0.10752687997225113}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.8, 'f': 0.20253164335843615}}\n",
            "pair:  paper focused investigating demystifying intriguing robustness phenomena parameterized neural network training particular provide empirical theoretical evidence first order methods gradient descent provably robust noise corruption constant fraction labels despite parameterization rich dataset model particular first show first iterations updates still vicinity initialization algorithms fit correct labels essentially ignoring noisy labels ii secondly prove start overfit noisy labels algorithms must stray rather far initial model occur many iterations together show gradient descent early stopping provably robust label noise shed light empirical robustness deep networks well commonly adopted early stopping heuristics\n",
            "output sentence:  prove gradient descent robust label corruption despite parameterization rich dataset model \n",
            "\n",
            "{'rouge-1': {'r': 0.2159090909090909, 'p': 0.9047619047619048, 'f': 0.34862385010015995}, 'rouge-2': {'r': 0.1523809523809524, 'p': 0.8, 'f': 0.255999997312}, 'rouge-l': {'r': 0.17045454545454544, 'p': 0.7142857142857143, 'f': 0.2752293546873159}}\n",
            "pair:  stochastic neural net weights used variety contexts including regularization bayesian neural nets exploration reinforcement learning evolution strategies unfortunately due large number weights examples mini batch typically share weight perturbation thereby limiting variance reduction effect large mini batches introduce flipout efficient method decorrelating gradients within mini batch implicitly sampling pseudo independent weight perturbations example empirically flipout achieves ideal linear variance reduction fully connected networks convolutional networks rnns find significant speedups training neural networks multiplicative gaussian perturbations show flipout effective regularizing lstms outperforms previous methods flipout also enables us vectorize evolution strategies experiments single gpu flipout handle throughput least cpu cores using existing methods equivalent factor cost reduction amazon web services\n",
            "output sentence:  introduce flipout efficient method decorrelating gradients computed stochastic neural net weights within mini batch implicitly sampling pseudo independent weight perturbations example \n",
            "\n",
            "{'rouge-1': {'r': 0.24444444444444444, 'p': 0.6470588235294118, 'f': 0.3548387056971904}, 'rouge-2': {'r': 0.12, 'p': 0.375, 'f': 0.1818181781450873}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.5882352941176471, 'f': 0.32258064118106144}}\n",
            "pair:  paper presents method autonomously find periodicities signal based idea using fourier transform autocorrelation function presented vlachos et al showing interesting results method perform well noisy signals signals multiple periodicities thus method adds several new extra steps hints clustering filtering detrending fix issues experimental results show proposed method outperforms state art algorithms\n",
            "output sentence:  paper presents method autonomously find multiple periodicities signal using fft acf add three news steps clustering filtering \n",
            "\n",
            "{'rouge-1': {'r': 0.07142857142857142, 'p': 0.2727272727272727, 'f': 0.11320754388038456}, 'rouge-2': {'r': 0.009345794392523364, 'p': 0.047619047619047616, 'f': 0.015624997257080558}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.2727272727272727, 'f': 0.11320754388038456}}\n",
            "pair:  use deep learning models priors compressive sensing tasks presents new potential inexpensive seismic data acquisition appropriately designed wasserstein generative adversarial network designed based generative adversarial network architecture trained several historical surveys capable learning statistical properties seismic wavelets usage validating performance testing compressive sensing three steps first existence sparse representation different compression rates seismic surveys studied non uniform samplings studied using proposed methodology finally recommendations non uniform seismic survey grid based evaluation reconstructed seismic images metrics proposed primary goal proposed deep learning model provide foundations optimal design seismic acquisition less loss imaging quality along lines compressive sensing design non uniform grid asset gulf mexico versus traditional seismic survey grid collects data uniformly every feet suggested leveraging proposed method\n",
            "output sentence:  improved gan based pixel inpainting network compressed seismic image recovery andproposed xa non uniform sampling survey recommendatio easily applied medical technique domains \n",
            "\n",
            "{'rouge-1': {'r': 0.18181818181818182, 'p': 0.9411764705882353, 'f': 0.3047619020480726}, 'rouge-2': {'r': 0.1568627450980392, 'p': 0.9411764705882353, 'f': 0.2689075605762305}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.9411764705882353, 'f': 0.3047619020480726}}\n",
            "pair:  compressed forms deep neural networks essential deploying large scale computational models resource constrained devices contrary analogous domains large scale systems build hierarchical repetition small scale units current practice machine learning largely relies models non repetitive components spirit molecular composition repeating atoms advance state art model compression proposing atomic compression networks acns novel architecture constructed recursive repetition small set neurons words neurons weights stochastically positioned subsequent layers network empirical evidence suggests acns achieve compression rates three orders magnitudes compared fine tuned fully connected neural networks reduction fractional deterioration classification accuracy moreover method yield sub linear model complexities permits learning deep acns less parameters logistic regression decline classification accuracy\n",
            "output sentence:  advance state art model compression proposing atomic compression networks acns novel architecture constructed recursive repetition small set neurons \n",
            "\n",
            "{'rouge-1': {'r': 0.04225352112676056, 'p': 0.6, 'f': 0.07894736719182825}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.25, 'f': 0.021505375520869494}, 'rouge-l': {'r': 0.04225352112676056, 'p': 0.6, 'f': 0.07894736719182825}}\n",
            "pair:  paper explores simplicity learned neural networks various settings learned real vs random data varying size architecture using large minibatch size vs small minibatch size notion simplicity used learnability accurately prediction function neural network learned labeled samples learnability different fact often higher test accuracy results herein suggest strong correlation small generalization errors high learnability work also shows exist significant qualitative differences shallow networks compared popular deep networks broadly paper extends new direction previous work understanding properties learned neural networks hope empirical study understanding learned neural networks might shed light right assumptions made theoretical study deep learning\n",
            "output sentence:  exploring learnability learned neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.1388888888888889, 'p': 0.8333333333333334, 'f': 0.23809523564625854}, 'rouge-2': {'r': 0.06896551724137931, 'p': 0.5454545454545454, 'f': 0.12244897759891712}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5833333333333334, 'f': 0.1666666642176871}}\n",
            "pair:  knowledge graph embedding research overlooked problem probability calibration show popular embedding models indeed uncalibrated means probability estimates associated predicted triples unreliable present novel method calibrate model ground truth negatives available usual case knowledge graphs propose use platt scaling isotonic regression alongside method experiments three datasets ground truth negatives show contribution leads well calibrated models compared gold standard using negatives get significantly better results uncalibrated models calibration methods show isotonic regression offers best performance overall without trade offs also show calibrated models reach state art accuracy without need define relation specific decision thresholds\n",
            "output sentence:  propose novel method calibrate knowledge graph embedding models without need negative examples \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.6666666666666666, 'f': 0.11764705721453288}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.2, 'f': 0.02564102444115719}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.5, 'f': 0.08823529250865055}}\n",
            "pair:  state art sequence sequence models large scale tasks perform fixed number computations input sequence regardless whether easy hard process paper train transformer models make output predictions different stages network investigate different ways predict much computation required particular sequence unlike dynamic computation universal transformers applies set layers iteratively apply different layers every step adjust amount computation well model capacity iwslt german english translation approach matches accuracy well tuned baseline transformer using less quarter decoder layers\n",
            "output sentence:  sequence model dynamically adjusts amount computation input \n",
            "\n",
            "{'rouge-1': {'r': 0.1320754716981132, 'p': 0.5384615384615384, 'f': 0.21212120895775943}, 'rouge-2': {'r': 0.046153846153846156, 'p': 0.25, 'f': 0.07792207529094292}, 'rouge-l': {'r': 0.11320754716981132, 'p': 0.46153846153846156, 'f': 0.18181817865472916}}\n",
            "pair:  show output residual cnn appropriate prior weights biases gp limit infinitely many convolutional filters extending similar results dense networks cnn equivalent kernel computed exactly unlike deep kernels parameters hyperparameters original cnn show kernel two properties allow computed efficiently cost evaluating kernel pair images similar single forward pass original cnn one filter per layer kernel equivalent layer resnet obtains classification error mnist new record gp comparable number parameters\n",
            "output sentence:  show cnns resnets appropriate priors parameters gaussian processes limit infinitely many convolutional filters \n",
            "\n",
            "{'rouge-1': {'r': 0.08823529411764706, 'p': 0.75, 'f': 0.15789473495844877}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.125, 'f': 0.022471908476202618}, 'rouge-l': {'r': 0.08823529411764706, 'p': 0.75, 'f': 0.15789473495844877}}\n",
            "pair:  regularizers critical tools machine learning due ability simplify solutions however imposing strong regularization gradient descent method easily fails limits generalization ability underlying neural networks understand phenomenon investigate training fails strong regularization specifically examine gradients change time different regularization strengths provide analysis gradients diminish fast find exists tolerance level regularization strength learning completely fails regularization strength goes beyond propose simple novel method delayed strong regularization order moderate tolerance level experiment results show proposed approach indeed achieves strong regularization regularizers improves accuracy sparsity public data sets source code published\n",
            "output sentence:  investigate strong regularization fails propose method achieve strong regularization \n",
            "\n",
            "{'rouge-1': {'r': 0.18032786885245902, 'p': 0.6111111111111112, 'f': 0.27848100913956103}, 'rouge-2': {'r': 0.07042253521126761, 'p': 0.29411764705882354, 'f': 0.11363636051911165}, 'rouge-l': {'r': 0.13114754098360656, 'p': 0.4444444444444444, 'f': 0.20253164205095342}}\n",
            "pair:  reinforcement learning multi agent scenarios important real world applications presents challenges beyond seen single agent settings present actor critic algorithm trains decentralized policies multi agent settings using centrally computed critics share attention mechanism selects relevant information agent every timestep attention mechanism enables effective scalable learning complex multi agent environments compared recent approaches approach applicable cooperative settings shared rewards also individualized reward settings including adversarial settings makes assumptions action spaces agents flexible enough applied multi agent learning problems\n",
            "output sentence:  propose approach learn decentralized policies multi agent settings using attention based critics demonstrate promising results environments complex interactions \n",
            "\n",
            "{'rouge-1': {'r': 0.07865168539325842, 'p': 0.6363636363636364, 'f': 0.139999998042}, 'rouge-2': {'r': 0.025423728813559324, 'p': 0.3, 'f': 0.04687499855957036}, 'rouge-l': {'r': 0.0449438202247191, 'p': 0.36363636363636365, 'f': 0.07999999804200005}}\n",
            "pair:  designing accurate efficient convolutional neural architectures vast amount hardware challenging hardware designs complex diverse paper addresses hardware diversity challenge neural architecture search nas unlike previous approaches apply search algorithms small human designed search space without considering hardware diversity propose hurricane explores automatic hardware aware search much larger search space multistep search scheme coordinate ascent framework generate tailored models different types hardware extensive experiments imagenet show algorithm consistently achieves much lower inference latency similar better accuracy state art nas methods three types hardware remarkably hurricane achieves top accuracy imagenet inference latency ms dsp higher accuracy inference speedup fbnet iphonex vpu hurricane achieves higher top accuracy proxyless mobile speedup even well studied mobile cpu hurricane achieves higher top accuracy fbnet iphonex comparable inference latency hurricane also reduces training time average compared singlepath oneshot\n",
            "output sentence:  propose hurricane address challenge hardware diversity one shot neural architecture search \n",
            "\n",
            "{'rouge-1': {'r': 0.28888888888888886, 'p': 0.7647058823529411, 'f': 0.41935483472944857}, 'rouge-2': {'r': 0.125, 'p': 0.3684210526315789, 'f': 0.18666666288355563}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.5882352941176471, 'f': 0.32258064118106144}}\n",
            "pair:  handheld virtual panel hvp virtual panel attached non dominant hand controller virtual reality vr hvp go technique enabling menus toolboxes vr devices paper investigate target acquisition performance hvp function four factors target width target distance direction approach respect gravity angle approach results show four factors significant effects user performance based results propose guidelines towards ergonomic performant design hvp interfaces\n",
            "output sentence:  paper investigates target acquisition handheld virtual panels vr shows target width distance direction direction respect respect gravity angle approach impact user \n",
            "\n",
            "{'rouge-1': {'r': 0.14772727272727273, 'p': 0.7647058823529411, 'f': 0.24761904490521544}, 'rouge-2': {'r': 0.05357142857142857, 'p': 0.375, 'f': 0.09374999781250005}, 'rouge-l': {'r': 0.10227272727272728, 'p': 0.5294117647058824, 'f': 0.17142856871473924}}\n",
            "pair:  neural networks could misclassify inputs slightly different training data indicates small margin decision boundaries training dataset work study binary classification linearly separable datasets show linear classifiers could also decision boundaries lie close training dataset cross entropy loss used training particular show features training dataset lie low dimensional affine subspace cross entropy loss minimized using gradient method margin training points decision boundary could much smaller optimal value result contrary conclusions recent related works soudry et al identify reason contradiction order improve margin introduce differential training training paradigm uses loss function defined pairs points class show decision boundary linear classifier trained differential training indeed achieves maximum margin results reveal use cross entropy loss one hidden culprits adversarial examples introduces new direction make neural networks robust\n",
            "output sentence:  show minimizing cross entropy loss using gradient method could lead poor margin features dataset lie low dimensional \n",
            "\n",
            "{'rouge-1': {'r': 0.09782608695652174, 'p': 0.75, 'f': 0.17307692103550298}, 'rouge-2': {'r': 0.0594059405940594, 'p': 0.5454545454545454, 'f': 0.10714285537149235}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.6666666666666666, 'f': 0.15384615180473374}}\n",
            "pair:  available resolution visual world extremely high infinite existing cnns applied fully convolutional way images arbitrary resolution size input increases capture contextual information addition computational requirements scale linearly number input pixels resources allocated uniformly across input matter informative different image regions attempt address problems proposing novel architecture traverses image pyramid top fashion uses hard attention mechanism selectively process informative image parts conduct experiments mnist imagenet datasets show models significantly outperform fully convolutional counterparts resolution input big receptive field baselines adequately cover objects interest gains performance come less flops selective processing follow furthermore attention mechanism makes predictions interpretable creates trade accuracy complexity tuned training testing time\n",
            "output sentence:  propose novel architecture traverses image pyramid top fashion visits informative regions along way \n",
            "\n",
            "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
            "pair:  knowledge graph embedding kge attracted attention recent years kge models learn time unaware triples however inclusion temporal information beside triples would improve performance kge model regard propose litse temporal kge model incorporates time information entity relation representations using linear time series decomposition moreover considering temporal uncertainty evolution entity relation representations time map representations temporal kgs space multi dimensional gaussian distributions mean entity relation embedding time step shows current expected position whereas covariance stationary time represents temporal uncertainty experiments show litse achieves state art link prediction temporal kgs also ability predict occurrence time facts missing time annotations well existence future events best knowledge model capable perform tasks\n",
            "output sentence:  submitted emnlp \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.42857142857142855, 'f': 0.1818181784756658}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.3333333333333333, 'f': 0.1111111083333334}, 'rouge-l': {'r': 0.11538461538461539, 'p': 0.42857142857142855, 'f': 0.1818181784756658}}\n",
            "pair:  area explainable ai xai explainable ai planning xaip matures ability agents generate curate explanations likewise grow propose new challenge area form rebellious deceptive explanations discuss explanations might generated briefly discuss evaluation criteria\n",
            "output sentence:  position paper proposing rebellious deceptive explanations agents \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.5714285714285714, 'f': 0.20253164265342097}, 'rouge-2': {'r': 0.0125, 'p': 0.07692307692307693, 'f': 0.021505373939183992}, 'rouge-l': {'r': 0.09230769230769231, 'p': 0.42857142857142855, 'f': 0.1518987312610159}}\n",
            "pair:  augmented reality ar assist physical tasks object assembly use situated instructions instructions form videos pictures text guiding animations helpful media among highly dependent user nature task work supports authoring ar tutorials assembly tasks little overhead beyond simply performing task presented system authar reduces time effort required build interactive ar tutorials automatically generating key components ar tutorial author assembling physical pieces system guides authors process adding videos pictures text animations tutorial concurrent assembly tutorial generation approach allows authoring portable tutorials fit preferences different end users\n",
            "output sentence:  present mixed media assembly tutorial authoring system streamlines creation videos images text dynamic instructions situ \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.7692307692307693, 'f': 0.24390243635633554}, 'rouge-2': {'r': 0.0449438202247191, 'p': 0.3333333333333333, 'f': 0.0792079186981669}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.6153846153846154, 'f': 0.19512194855145748}}\n",
            "pair:  graphs fundamental data structures required model many important real world data knowledge graphs physical social interactions molecules proteins paper study problem learning generative models graphs dataset graphs interest learning models used generate samples similar properties ones dataset models useful lot applications drug discovery knowledge graph construction task learning generative models graphs however unique challenges particular handle symmetries graphs ordering elements generation process important issues propose generic graph neural net based model capable generating arbitrary graph study performance graph generation tasks compared baselines exploit domain knowledge discuss potential issues open problems generative models going forward\n",
            "output sentence:  study graph generation problem propose powerful deep generative model capable generating arbitrary graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.16981132075471697, 'p': 0.5, 'f': 0.25352112297560014}, 'rouge-2': {'r': 0.047619047619047616, 'p': 0.16666666666666666, 'f': 0.07407407061728412}, 'rouge-l': {'r': 0.1320754716981132, 'p': 0.3888888888888889, 'f': 0.19718309480658602}}\n",
            "pair:  compressed sensing primary problem solve reconstruct high dimensional sparse signal small number observations work develop new sparse signal recovery algorithm using reinforcement learning rl monte carlotree search mcts similarly orthogonal matching pursuit omp rl mcts algorithm chooses support signal sequentially key novelty proposed algorithm learns choose next support opposed following pre designed rule omp empirical results provided demonstrate superior performance proposed rl mcts algorithm existing sparse signal recovery algorithms\n",
            "output sentence:  formulating sparse signal recovery sequential decision making problem develop method based rl mcts learns policy discover support sparse signal \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 0.8181818181818182, 'f': 0.32142856827168376}, 'rouge-2': {'r': 0.057692307692307696, 'p': 0.3, 'f': 0.09677419084287209}, 'rouge-l': {'r': 0.15555555555555556, 'p': 0.6363636363636364, 'f': 0.24999999684311228}}\n",
            "pair:  work propose novel formulation planning views probabilistic inference problem future optimal trajectories enables us use sampling methods thus tackle planning continuous domains using fixed computational budget design new algorithm sequential monte carlo planning leveraging classical methods sequential monte carlo bayesian smoothing context control inference furthermore show sequential monte carlo planning capture multimodal policies quickly learn continuous control tasks\n",
            "output sentence:  leveraging control inference sequential monte carlo methods proposed probabilistic planning algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.1518987341772152, 'p': 0.75, 'f': 0.25263157614626036}, 'rouge-2': {'r': 0.07865168539325842, 'p': 0.4666666666666667, 'f': 0.13461538214681956}, 'rouge-l': {'r': 0.1518987341772152, 'p': 0.75, 'f': 0.25263157614626036}}\n",
            "pair:  study use wave net architecture speech enhancement model introduced stoller et al separation music vocals accompaniment end end learning method audio source separation operates directly time domain permitting integrated modelling phase information able take large temporal contexts account experiments show proposed method improves several metrics namely pesq csig cbak covl ssnr state art respect speech enhancement task voice bank corpus vctk dataset find reduced number hidden layers sufficient speech enhancement comparison original system designed singing voice separation music see initial result encouraging signal explore speech enhancement time domain end pre processing step speech recognition systems\n",
            "output sentence:  wave net architecture recently introduced stoller et al music source separation highly effective speech enhancement beating \n",
            "\n",
            "{'rouge-1': {'r': 0.21428571428571427, 'p': 0.631578947368421, 'f': 0.3199999962168889}, 'rouge-2': {'r': 0.04838709677419355, 'p': 0.16666666666666666, 'f': 0.07499999651250017}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.42105263157894735, 'f': 0.2133333295502223}}\n",
            "pair:  paper addresses scalability challenge architecture search formulating task differentiable manner unlike conventional approaches applying evolution reinforcement learning discrete non differentiable search space method based continuous relaxation architecture representation allowing efficient search architecture using gradient descent extensive experiments cifar imagenet penn treebank wikitext show algorithm excels discovering high performance convolutional architectures image classification recurrent architectures language modeling orders magnitude faster state art non differentiable techniques\n",
            "output sentence:  propose differentiable architecture search algorithm convolutional recurrent networks achieving competitive performance state art using orders magnitude less computation resources \n",
            "\n",
            "{'rouge-1': {'r': 0.14084507042253522, 'p': 0.8333333333333334, 'f': 0.2409638529481783}, 'rouge-2': {'r': 0.060240963855421686, 'p': 0.4166666666666667, 'f': 0.10526315568753467}, 'rouge-l': {'r': 0.1267605633802817, 'p': 0.75, 'f': 0.21686746740600957}}\n",
            "pair:  analyze speed convergence global optimum gradient descent training deep linear neural network minimizing loss whitened data convergence linear rate guaranteed following hold dimensions hidden layers least minimum input output dimensions ii weight matrices initialization approximately balanced iii initial loss smaller loss rank deficient solution assumptions initialization conditions ii iii necessary sense violating one may lead convergence failure moreover important case output dimension scalar regression met thus convergence global optimum holds constant probability random initialization scheme results significantly extend previous analyses deep linear residual networks bartlett et al\n",
            "output sentence:  analyze gradient descent deep linear neural networks providing guarantee convergence global optimum linear rate \n",
            "\n",
            "{'rouge-1': {'r': 0.11235955056179775, 'p': 0.6666666666666666, 'f': 0.19230768983912724}, 'rouge-2': {'r': 0.056074766355140186, 'p': 0.4, 'f': 0.09836065358102665}, 'rouge-l': {'r': 0.0898876404494382, 'p': 0.5333333333333333, 'f': 0.15384615137758878}}\n",
            "pair:  conditional generative adversarial networks cgans finding increasingly widespread use many application domains despite outstanding progress quantitative evaluation models often involves multiple distinct metrics assess different desirable properties image quality conditional consistency intra conditioning diversity setting model benchmarking becomes challenge metric may indicate different best model paper propose frechet joint distance fjd defined frechet distance joint distributions images conditioning allowing implicitly capture aforementioned properties single metric conduct proof concept experiments controllable synthetic dataset consistently highlight benefits fjd compared currently established metrics moreover use newly introduced metric compare existing cgan based models variety conditioning modalities class labels object masks bounding boxes images text captions show fjd used promising single metric model benchmarking\n",
            "output sentence:  propose new metric evaluating conditional gans captures image quality conditional consistency intra conditioning diversity single measure \n",
            "\n",
            "{'rouge-1': {'r': 0.16393442622950818, 'p': 0.7142857142857143, 'f': 0.26666666363022223}, 'rouge-2': {'r': 0.09090909090909091, 'p': 0.5384615384615384, 'f': 0.15555555308395064}, 'rouge-l': {'r': 0.11475409836065574, 'p': 0.5, 'f': 0.18666666363022225}}\n",
            "pair:  resnet batch normalization bn achieved high performance even labeled data available however reasons high performance unclear clear reasons analyzed effect skip connection resnet bn data separation ability important ability classification problem results show multilayer perceptron randomly initialized weights angle two input vectors converges zero exponential order depth skip connection makes exponential decrease sub exponential decrease bn relaxes sub exponential decrease reciprocal decrease moreover analysis shows preservation angle initialization encourages trained neural networks separate points different classes imply skip connection bn improve data separation ability achieve high performance even labeled data available\n",
            "output sentence:  skip connection resnet batch normalization improve data separation ability help train deep neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.03260869565217391, 'p': 0.3333333333333333, 'f': 0.0594059389706892}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03260869565217391, 'p': 0.3333333333333333, 'f': 0.0594059389706892}}\n",
            "pair:  generative adversarial networks gans trained large datasets diverse modes known produce conflated images distinctly belong modes hypothesize problem occurs due interaction two facts datasets large variety likely modes lie separate manifolds generator formulated continuous function input noise derived connected set due output connected set covers modes must portion output connects corresponds undesirable conflated images develop theoretical arguments support intuitions propose novel method break second assumption via learnable discontinuities latent noise space equivalently viewed training several generators thus creating discontinuities function also augment gan formulation classifier predicts noise partition generator produced output images encouraging diversity partition generator experiment mnist celeba stl difficult dataset clearly distinct modes show noise partitions correspond different modes data distribution produce images superior quality\n",
            "output sentence:  introduce theory explain failure gans complex datasets propose solution fix \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.5, 'f': 0.19718309542551085}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.23076923076923078, 'f': 0.06593406348508643}, 'rouge-l': {'r': 0.10526315789473684, 'p': 0.42857142857142855, 'f': 0.16901408134100382}}\n",
            "pair:  graph neural networks gnns class deep models operates data arbitrary topology order invariant structure represented graphs introduce efficient memory layer gnns learn jointly perform graph representation learning graph pooling also introduce two new networks based memory layer memory based graph neural network memgnn graph memory network gmn learn hierarchical graph representations coarsening graph throughout layers memory experimental results demonstrate proposed models achieve state art results six seven graph classification regression benchmarks also show learned representations could correspond chemical features molecule data\n",
            "output sentence:  introduce efficient memory layer learn representation coarsen input graphs simultaneously without relying message passing \n",
            "\n",
            "{'rouge-1': {'r': 0.13157894736842105, 'p': 0.8333333333333334, 'f': 0.22727272491735537}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.2727272727272727, 'f': 0.059405938653073295}, 'rouge-l': {'r': 0.06578947368421052, 'p': 0.4166666666666667, 'f': 0.11363636128099178}}\n",
            "pair:  origin destination od flow data important instrument transportation studies precise prediction customer demands original location destination given series previous snapshots helps ride sharing platforms better understand market mechanism however existing prediction methods ignore network structure od flow data fail utilize topological dependencies among related od pairs paper propose latent spatial temporal origin destination lstod model novel convolutional neural network cnn filter learn spatial features od pairs graph perspective attention structure capture long term periodicity experiments real customer request dataset available od information ride sharing platform demonstrate advantage lstod achieving least improvement prediction accuracy second best model\n",
            "output sentence:  propose purely convolutional cnn model attention mechanism predict spatial temporal origin destination flows \n",
            "\n",
            "{'rouge-1': {'r': 0.12222222222222222, 'p': 0.7857142857142857, 'f': 0.2115384592085799}, 'rouge-2': {'r': 0.04132231404958678, 'p': 0.3333333333333333, 'f': 0.07352940980211943}, 'rouge-l': {'r': 0.12222222222222222, 'p': 0.7857142857142857, 'f': 0.2115384592085799}}\n",
            "pair:  paper propose generalization bn algorithm diminishing batch normalization dbn update bn parameters diminishing moving average way batch normalization bn effective accelerating convergence neural network training phase become common practice proposed dbn algorithm remains overall structure original bn algorithm introduces weighted averaging update trainable parameters provide analysis convergence dbn algorithm converges stationary point respect trainable parameters analysis easily generalized original bn algorithm setting parameters constant best knowledge authors analysis first kind convergence batch normalization introduced analyze two layer model arbitrary activation function primary challenge analysis fact parameters updated gradient others convergence analysis applies activation function satisfies common assumptions analysis also show sufficient necessary conditions stepsizes diminishing weights ensure convergence numerical experiments use complex models layers relu activation observe dbn outperforms original bn algorithm imagenet mnist ni cifar datasets reasonable complex fnn cnn models\n",
            "output sentence:  propose extension batch normalization show first kind convergence analysis extension show numerical experiments better performance original batch \n",
            "\n",
            "{'rouge-1': {'r': 0.06060606060606061, 'p': 0.5, 'f': 0.10810810617969324}, 'rouge-2': {'r': 0.011764705882352941, 'p': 0.14285714285714285, 'f': 0.021739129028828068}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.5, 'f': 0.10810810617969324}}\n",
            "pair:  paper propose differentiable adversarial grammar model future prediction objective model formal grammar terms differentiable functions latent representations learning possible standard backpropagation learning formal grammar represented latent terminals non terminals productions rules allows capturing sequential structures multiple possibilities data adversarial grammar designed learn stochastic production rules data distribution able select multiple production rules leads different predicted outcomes thus efficiently modeling many plausible futures confirm benefit adversarial grammar two diverse tasks future human pose prediction future activity prediction settings proposed adversarial grammar outperforms state art approaches able predict much accurately future prior work\n",
            "output sentence:  design grammar learned adversarial setting apply future prediction video \n",
            "\n",
            "{'rouge-1': {'r': 0.07894736842105263, 'p': 0.6666666666666666, 'f': 0.1411764686948097}, 'rouge-2': {'r': 0.021505376344086023, 'p': 0.2222222222222222, 'f': 0.03921568466551333}, 'rouge-l': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}}\n",
            "pair:  proliferation models natural language processing nlp tasks even harder understand differences models relative merits simply looking differences holistic metrics accuracy bleu tell us emph emph particular method better dataset biases influence choices model design paper present general methodology emph interpretable evaluation nlp systems choose task named entity recognition ner case study core task identifying people places organizations text proposed evaluation method enables us interpret textit model biases textit dataset biases emph differences datasets affect design models identifying strengths weaknesses current approaches making analysis tool available make easy future researchers run similar analyses drive progress area\n",
            "output sentence:  propose generalized evaluation methodology interpret model biases dataset biases correlation \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.5, 'f': 0.19718309542551085}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.23076923076923078, 'f': 0.06593406348508643}, 'rouge-l': {'r': 0.10526315789473684, 'p': 0.42857142857142855, 'f': 0.16901408134100382}}\n",
            "pair:  graph neural networks gnns class deep models operates data arbitrary topology order invariant structure represented graphs introduce efficient memory layer gnns learn jointly perform graph representation learning graph pooling also introduce two new networks based memory layer memory based graph neural network memgnn graph memory network gmn learn hierarchical graph representations coarsening graph throughout layers memory experimental results demonstrate proposed models achieve state art results six seven graph classification regression benchmarks also show learned representations could correspond chemical features molecule data\n",
            "output sentence:  introduce efficient memory layer learn representation coarsen input graphs simultaneously without relying message passing \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.6363636363636364, 'f': 0.16091953802087464}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.2727272727272727, 'f': 0.05769230580066575}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.6363636363636364, 'f': 0.16091953802087464}}\n",
            "pair:  relate minimax game generative adversarial networks gans finding saddle points lagrangian function convex optimization problem discriminator outputs distribution generator outputs play roles primal variables dual variables respectively formulation shows connection standard gan training process primal dual subgradient methods convex optimization inherent connection provide theoretical convergence proof training gans function space also inspires novel objective function training modified objective function forces distribution generator outputs updated along direction according primal dual subgradient methods toy example shows proposed method able resolve mode collapse case cannot avoided standard gan wasserstein gan experiments gaussian mixture synthetic data real world image datasets demonstrate performance proposed method generating diverse samples\n",
            "output sentence:  propose primal dual subgradient method training gans method effectively alleviates mode collapse \n",
            "\n",
            "{'rouge-1': {'r': 0.08888888888888889, 'p': 0.7272727272727273, 'f': 0.15841583964317224}, 'rouge-2': {'r': 0.02727272727272727, 'p': 0.3, 'f': 0.049999998472222265}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.45454545454545453, 'f': 0.09900989904911285}}\n",
            "pair:  paper consider specific problem word level language modeling investigate strategies regularizing optimizing lstm based models propose weight dropped lstm uses dropconnect hidden hidden weights form recurrent regularization introduce nt asgd non monotonically triggered nt variant averaged stochastic gradient method asgd wherein averaging trigger determined using nt condition opposed tuned user using regularization strategies asgd weight dropped lstm awd lstm achieves state art word level perplexities two data sets penn treebank wikitext exploring effectiveness neural cache conjunction proposed model achieve even lower state art perplexity penn treebank wikitext also explore viability proposed regularization optimization strategies context quasi recurrent neural network qrnn demonstrate comparable performance awd lstm counterpart code reproducing results open sourced available https github com salesforce awd lstm lm\n",
            "output sentence:  effective regularization optimization strategies lstm based language models achieves sota ptb wt \n",
            "\n",
            "{'rouge-1': {'r': 0.027777777777777776, 'p': 0.2222222222222222, 'f': 0.04938271407407415}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.027777777777777776, 'p': 0.2222222222222222, 'f': 0.04938271407407415}}\n",
            "pair:  keyword spotting wakeword detection essential feature hands free operation modern voice controlled devices devices becoming ubiquitous users might want choose personalized custom wakeword work present donut ctc based algorithm online query example keyword spotting enables custom wakeword detection algorithm works recording small number training examples user generating set label sequence hypotheses training examples detecting wakeword aggregating scores hypotheses given new audio recording method combines generalization interpretability ctc based keyword spotting user adaptation convenience conventional query example system donut low computational requirements well suited learning inference embedded systems without requiring private user data uploaded cloud\n",
            "output sentence:  propose interpretable model detecting user chosen wakewords learns user examples \n",
            "\n",
            "{'rouge-1': {'r': 0.3230769230769231, 'p': 0.9545454545454546, 'f': 0.48275861691108474}, 'rouge-2': {'r': 0.24358974358974358, 'p': 0.7916666666666666, 'f': 0.3725490160092273}, 'rouge-l': {'r': 0.3076923076923077, 'p': 0.9090909090909091, 'f': 0.4597701111639584}}\n",
            "pair:  propose new architecture termed dual adversarial transfer network datnet addressing low resource named entity recognition ner specifically two variants datnet datnet datnet proposed explore effective feature fusion high low resource address noisy imbalanced training data propose novel generalized resource adversarial discriminator grad additionally adversarial training adopted boost model generalization examine effects different components datnet across domains languages show significant improvement obtained especially low resource data without augmenting additional hand crafted features achieve new state art performances conll twitter ner spanish wnut wnut\n",
            "output sentence:  propose new architecture termed dual adversarial transfer network datnet addressing low resource named entity recognition ner achieve new state art performances ner twitter twitter twitter \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.6923076923076923, 'f': 0.19780219535321825}, 'rouge-2': {'r': 0.04395604395604396, 'p': 0.3333333333333333, 'f': 0.07766990085399195}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.38461538461538464, 'f': 0.10989010744113034}}\n",
            "pair:  deep generative models advanced state art semi supervised classification however capacity deriving useful discriminative features completely unsupervised fashion classification difficult real world data sets adequate manifold separation required adequately explored methods rely defining pipeline deriving features via generative modeling applying clustering algorithms separating modeling discriminative processes propose deep hierarchical generative model uses mixture discrete continuous distributions learn effectively separate different data manifolds trainable end end show specifying form discrete variable distribution imposing specific structure model latent representations test model discriminative performance task cll diagnosis baselines field computational fc well variational autoencoder literature\n",
            "output sentence:  unsupervised classification via deep generative modeling controllable feature learning evaluated difficult real world task \n",
            "\n",
            "{'rouge-1': {'r': 0.08974358974358974, 'p': 0.7, 'f': 0.1590909070764463}, 'rouge-2': {'r': 0.03488372093023256, 'p': 0.3333333333333333, 'f': 0.0631578930216067}, 'rouge-l': {'r': 0.05128205128205128, 'p': 0.4, 'f': 0.09090908889462815}}\n",
            "pair:  learning mahalanobis metric spaces important problem found numerous applications several algorithms designed problem including information theoretic metric learning itml davis et al large margin nearest neighbor lmnn classification weinberger saul consider formulation mahalanobis metric learning optimization problem objective minimize number violated similarity dissimilarity constraints show fixed ambient dimension exists fully polynomial time approximation scheme fptas nearly linear running time result obtained using tools theory linear programming low dimensions also discuss improvements algorithm practice present experimental results synthetic real world data sets algorithm fully parallelizable performs favorably presence adversarial noise\n",
            "output sentence:  fully parallelizable adversarial noise resistant metric learning algorithm theoretical guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.037037037037037035, 'p': 0.375, 'f': 0.06741572870092163}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.037037037037037035, 'p': 0.375, 'f': 0.06741572870092163}}\n",
            "pair:  optimal transport ot naturally arises many machine learning applications need handle cross modality data multiple sources yet heavy computational burden limits wide spread uses address scalability issue propose implicit generative learning based framework called spot scalable push forward optimal transport specifically approximate optimal transport plan pushforward reference distribution cast optimal transport problem minimax problem solve ot problems efficiently using primal dual stochastic gradient type algorithms also show recover density optimal transport plan using neural ordinary differential equations numerical experiments synthetic real datasets illustrate spot robust favorable convergence behavior spot also allows us efficiently sample optimal transport plan benefits downstream applications domain adaptation\n",
            "output sentence:  use gan based method scalably solve optimal transport \n",
            "\n",
            "{'rouge-1': {'r': 0.047619047619047616, 'p': 0.7142857142857143, 'f': 0.08928571311383929}, 'rouge-2': {'r': 0.007633587786259542, 'p': 0.16666666666666666, 'f': 0.014598539308434168}, 'rouge-l': {'r': 0.0380952380952381, 'p': 0.5714285714285714, 'f': 0.07142857025669645}}\n",
            "pair:  recent studies shown vulnerability reinforcement learning rl models noisy settings sources noises differ across scenarios instance practice observed reward channel often subject noise observed rewards collected sensors thus observed rewards may credible result also applications robotics deep reinforcement learning drl algorithm manipulated produce arbitrary errors paper consider noisy rl problems observed rewards rl agents generated reward confusion matrix call observed rewards perturbed rewards develop unbiased reward estimator aided robust rl framework enables rl agents learn noisy environments observing perturbed rewards framework draws upon approaches supervised learning noisy data core ideas solution include estimating reward confusion matrix defining set unbiased surrogate rewards prove convergence sample complexity approach extensive experiments different drl platforms show policies based estimated surrogate reward achieve higher expected rewards converge faster existing baselines instance state art ppo algorithm able obtain improvements average five atari games error rates respectively\n",
            "output sentence:  new approach learning noisy rewards reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5384615384615384, 'f': 0.18421052347991693}, 'rouge-2': {'r': 0.013513513513513514, 'p': 0.07692307692307693, 'f': 0.0229885032051793}, 'rouge-l': {'r': 0.07936507936507936, 'p': 0.38461538461538464, 'f': 0.13157894453254854}}\n",
            "pair:  learning disentangled representations data one central themes unsupervised learning general generative modelling particular work tackle slightly intricate scenario observations generated conditional distribution known control variate latent noise variate end present hierarchical model training method cz gem leverages recent developments likelihood based likelihood free generative models show formulation cz gem introduces right inductive biases ensure disentanglement control noise variables also keeping components control variate disentangled achieved without compromising quality generated samples approach simple general applied supervised unsupervised settings\n",
            "output sentence:  hierarchical generative model hybrid vae gan learns disentangled representation data without compromising generative quality \n",
            "\n",
            "{'rouge-1': {'r': 0.136986301369863, 'p': 0.7142857142857143, 'f': 0.22988505477077553}, 'rouge-2': {'r': 0.04, 'p': 0.2857142857142857, 'f': 0.07017543644198222}, 'rouge-l': {'r': 0.1232876712328767, 'p': 0.6428571428571429, 'f': 0.2068965490236491}}\n",
            "pair:  orthogonal recurrent neural networks address vanishing gradient problem parameterizing recurrent connections using orthogonal matrix class models particularly effective solve tasks require memorization long sequences propose alternative solution based explicit memorization using linear autoencoders sequences show recently proposed recurrent architecture linear memory network composed nonlinear feedforward layer separate linear recurrence used solve hard memorization tasks propose initialization schema sets weights recurrent architecture approximate linear autoencoder input sequences found closed form solution initialization schema easily adapted recurrent architecture argue approach superior random orthogonal initialization due autoencoder allows memorization long sequences even training empirical analysis show approach achieves competitive results alternative orthogonal models lstm sequential mnist permuted mnist timit\n",
            "output sentence:  show initialize recurrent architectures closed form solution linear autoencoder sequences show advantages approach compared orthogonal rnns \n",
            "\n",
            "{'rouge-1': {'r': 0.11688311688311688, 'p': 0.75, 'f': 0.20224718867819722}, 'rouge-2': {'r': 0.05747126436781609, 'p': 0.45454545454545453, 'f': 0.10204081433361102}, 'rouge-l': {'r': 0.1038961038961039, 'p': 0.6666666666666666, 'f': 0.17977527856583767}}\n",
            "pair:  neural networks trained stochastic gradient descent sgd around years still escape understanding paper takes experimental approach divide conquer strategy mind start studying happens single neurons core building block deep neural networks way encode information inputs encodings emerge still unknown report experiments providing strong evidence hidden neurons behave like binary classifiers training testing training analysis gradients reveals neuron separates two categories inputs impressively constant across training testing show fuzzy binary partition described embeds core information used network prediction observations bring light core internal mechanics deep neural networks potential guide next theoretical practical developments\n",
            "output sentence:  report experiments providing strong evidence neuron behaves like binary classifier training testing \n",
            "\n",
            "{'rouge-1': {'r': 0.07317073170731707, 'p': 0.6, 'f': 0.13043478067107753}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.18181818181818182, 'f': 0.036363634563636456}, 'rouge-l': {'r': 0.036585365853658534, 'p': 0.3, 'f': 0.06521738936672974}}\n",
            "pair:  chemical reactions described stepwise redistribution electrons molecules reactions often depicted using arrow pushing diagrams show movement sequence arrows propose electron path prediction model electro learn sequences directly raw reaction data instead predicting product molecules directly reactant molecules one shot learning model electron movement benefits easy chemists interpret incorporating constraints chemistry balanced atom counts reaction naturally encoding sparsity chemical reactions usually involve changes small number atoms reactants design method extract approximate reaction paths dataset atom mapped reaction smiles strings model achieves excellent performance important subset uspto reaction dataset comparing favorably strongest baselines furthermore show model recovers basic knowledge chemistry without explicitly trained\n",
            "output sentence:  generative model reaction prediction learns mechanistic electron steps reaction directly raw reaction data \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.5333333333333333, 'f': 0.1860465087479719}, 'rouge-2': {'r': 0.024096385542168676, 'p': 0.14285714285714285, 'f': 0.04123711093208646}, 'rouge-l': {'r': 0.09859154929577464, 'p': 0.4666666666666667, 'f': 0.16279069479448352}}\n",
            "pair:  paper improves upon line research formulates named entity recognition ner sequence labeling problem use called black box long short term memory lstm encoders achieve state art results providing insightful understanding auto regressive model learns parallel self attention mechanism specifically decouple sequence labeling problem ner entity chunking barack obama elected entity typing barack person obama person none elected none analyze model learns difficulties capturing text patterns subtasks insights gain lead us explore sophisticated deep cross bi lstm encoder proves better capturing global interactions given empirical results theoretical justification\n",
            "output sentence:  provide insightful understanding sequence labeling ner propose use two types cross structures bring theoretical improvements \n",
            "\n",
            "{'rouge-1': {'r': 0.09195402298850575, 'p': 0.8, 'f': 0.1649484517589542}, 'rouge-2': {'r': 0.03669724770642202, 'p': 0.4444444444444444, 'f': 0.06779660876041371}, 'rouge-l': {'r': 0.09195402298850575, 'p': 0.8, 'f': 0.1649484517589542}}\n",
            "pair:  twe present new approach novel architecture termed wsnet learning compact efficient deep neural networks existing approaches conventionally learn full model parameters independently compress via emph ad hoc processing model pruning filter factorization alternatively wsnet proposes learning model parameters sampling compact set learnable parameters naturally enforces parameter sharing throughout learning process demonstrate novel weight sampling approach induced wsnet promotes weights computation sharing favorably employing method efficiently learn much smaller networks competitive performance compared baseline networks equal numbers convolution filters specifically consider learning compact efficient convolutional neural networks audio classification extensive experiments multiple audio classification datasets verify effectiveness wsnet combined weight quantization resulted models textbf times smaller theoretically textbf times faster well established baselines without noticeable performance drop\n",
            "output sentence:  present novel network architecture learning compact efficient deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.1794871794871795, 'p': 0.6363636363636364, 'f': 0.279999996568}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.23809523809523808, 'f': 0.09433961946422224}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.5909090909090909, 'f': 0.25999999656800005}}\n",
            "pair:  vanilla rnn relu activation simple structure amenable systematic dynamical systems analysis interpretation suffer exploding vs vanishing gradients problem recent attempts retain simplicity alleviating gradient problem based proper initialization schemes orthogonality unitary constraints rnn recurrency matrix however comes limitations expressive power regards dynamical systems phenomena like chaos multi stability instead suggest regularization scheme pushes part rnn latent subspace toward line attractor configuration enables long short term memory arbitrarily slow time scales show approach excels number benchmarks like sequential mnist multiplication problems enables reconstruction dynamical systems harbor widely different time scales\n",
            "output sentence:  develop new optimization approach vanilla relu based rnn enables long short term memory identification arbitrary nonlinear dynamical systems widely differing time scales \n",
            "\n",
            "{'rouge-1': {'r': 0.10256410256410256, 'p': 0.8, 'f': 0.181818179803719}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.4444444444444444, 'f': 0.07619047462312928}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.6, 'f': 0.13636363434917356}}\n",
            "pair:  modelling statistical relationships beyond conditional mean crucial many settings conditional density estimation cde aims learn full conditional probability density data though highly expressive neural network based cde models suffer severe fitting trained maximum likelihood objective due inherent structure models classical regularization approaches parameter space rendered ineffective address issue develop model agnostic noise regularization method cde adds random perturbations data training demonstrate proposed approach corresponds smoothness regularization prove asymptotic consistency experiments noise regularization significantly consistently outperforms regularization methods across seven data sets three cde models effectiveness noise regularization makes neural network based cde preferable method previous non semi parametric approaches even training data scarce\n",
            "output sentence:  model agnostic regularization scheme neural network based conditional density estimation \n",
            "\n",
            "{'rouge-1': {'r': 0.203125, 'p': 0.65, 'f': 0.3095238058956917}, 'rouge-2': {'r': 0.05333333333333334, 'p': 0.21052631578947367, 'f': 0.08510637975328215}, 'rouge-l': {'r': 0.15625, 'p': 0.5, 'f': 0.23809523446712022}}\n",
            "pair:  transforming one probability distribution another powerful tool bayesian inference machine learning prominent examples constrained unconstrained transformations distributions use hamiltonian monte carlo constructing flexible learnable densities normalizing flows present bijectors jl software package transforming distributions implemented julia available github com turinglang bijectors jl package provides flexible composable way implementing transformations distributions without tied computational framework demonstrate use bijectors jl improving variational inference encoding known statistical dependencies variational posterior using normalizing flows providing general approach relaxing mean field assumption usually made variational inference\n",
            "output sentence:  present software framework transforming distributions demonstrate flexibility relaxing mean field assumptions variational inference use coupling flows replicate structure target generative model \n",
            "\n",
            "{'rouge-1': {'r': 0.13953488372093023, 'p': 0.6666666666666666, 'f': 0.23076922790680476}, 'rouge-2': {'r': 0.08163265306122448, 'p': 0.5, 'f': 0.1403508747799323}, 'rouge-l': {'r': 0.13953488372093023, 'p': 0.6666666666666666, 'f': 0.23076922790680476}}\n",
            "pair:  paper present novel optimization algorithm called advanced neuroevolution aim algorithm train deep neural networks eventually act alternative stochastic gradient descent sgd variants needed evaluated algorithm mnist dataset well several global optimization problems ackley function find algorithm performing relatively well cases overtaking global optimization algorithms particle swarm optimization pso evolution strategies es\n",
            "output sentence:  new algorithm train deep neural networks tested optimization functions mnist \n",
            "\n",
            "{'rouge-1': {'r': 0.10784313725490197, 'p': 0.7857142857142857, 'f': 0.1896551702913199}, 'rouge-2': {'r': 0.04032258064516129, 'p': 0.38461538461538464, 'f': 0.07299269901220101}, 'rouge-l': {'r': 0.08823529411764706, 'p': 0.6428571428571429, 'f': 0.15517241167063023}}\n",
            "pair:  deep reinforcement learning drl led many recent breakthroughs complex control tasks defeating best human player game go however decisions made drl agent explainable hindering applicability safety critical settings viper recently proposed technique constructs decision tree policy mimicking drl agent decision trees interpretable action made traced back decision rule path lead however one global decision tree approximating drl policy significant limitations respect geometry decision boundaries propose moet expressive yet still interpretable model based mixture experts consisting gating function partitions state space multiple decision tree experts specialize different partitions propose training procedure support non differentiable decision tree experts integrate imitation learning procedure viper evaluate algorithm four openai gym environments show policy constructed way performant better mimics drl agent lowering mispredictions increasing reward also show moet policies amenable verification using shelf automated theorem provers\n",
            "output sentence:  explainable reinforcement learning model using novel combination mixture experts non differentiable decision tree experts \n",
            "\n",
            "{'rouge-1': {'r': 0.08888888888888889, 'p': 0.6666666666666666, 'f': 0.15686274302191466}, 'rouge-2': {'r': 0.020833333333333332, 'p': 0.2, 'f': 0.037735847347810686}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.5, 'f': 0.11764705674740487}}\n",
            "pair:  propose extend existing deep reinforcement learning deep rl algorithms allowing additionally choose sequences actions part policy modification forces network anticipate reward action sequences show improves exploration leading better convergence proposal simple flexible easily incorporated deep rl framework show power scheme consistently outperforming state art ga algorithm several popular atari games\n",
            "output sentence:  anticipation improves convergence deep reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}, 'rouge-2': {'r': 0.028037383177570093, 'p': 0.375, 'f': 0.052173911748960335}, 'rouge-l': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}}\n",
            "pair:  learning communication via deep reinforcement learning recently shown effective way solve cooperative multi agent tasks however learning communicated information beneficial agent decision making remains challenging task order address problem introduce fully differentiable framework communication reasoning enabling agents solve cooperative tasks partially observable environments framework designed facilitate explicit reasoning agents novel memory based attention network learn selectively past memories model communicates series reasoning steps decompose agent intentions learned representations used first compute relevance communicated information second extract information memories given newly received information selectively interacting new information model effectively learns communication protocol directly end end manner empirically demonstrate strength model cooperative multi agent tasks inter agent communication reasoning prior information substantially improves performance compared baselines\n",
            "output sentence:  novel architecture memory based attention mechanism multi agent communication \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.8235294117647058, 'f': 0.2772277199725517}, 'rouge-2': {'r': 0.05172413793103448, 'p': 0.375, 'f': 0.0909090887786961}, 'rouge-l': {'r': 0.13095238095238096, 'p': 0.6470588235294118, 'f': 0.21782177937849234}}\n",
            "pair:  algorithm introduced learning predictive state representation policy temporal difference td learning used learn steer vehicle reinforcement learning three components learned simultaneously policy predictions compact representation state behavior policy distribution estimating policy predictions deterministic policy gradient learning act behavior policy discriminator learned used estimating important sampling ratios needed learn predictive representation policy general value functions gvfs linear deterministic policy gradient method used train agent predictive representations predictions learned three components combined demonstrated evaluated problem steering vehicle images torcs racing simulator environment steering images challenging problem evaluation completed held set tracks never seen training order measure generalization predictions controller experiments show proposed method able steer smoothly navigate many tracks available torcs performance exceeds ddpg using images input approaches performance ideal non vision based kinematics model\n",
            "output sentence:  algorithm learn predictive state representation general value functions policy learning applied problem vision based steering autonomous driving \n",
            "\n",
            "{'rouge-1': {'r': 0.10975609756097561, 'p': 0.5625, 'f': 0.18367346665556022}, 'rouge-2': {'r': 0.042105263157894736, 'p': 0.26666666666666666, 'f': 0.0727272703719009}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.3125, 'f': 0.10204081359433577}}\n",
            "pair:  implicit models allow generation samples point wise evaluation probabilities omnipresent real world problems tackled machine learning hot topic current research examples include data simulators widely used engineering scientific research generative adversarial networks gans image synthesis hot press approximate inference techniques relying implicit distributions majority existing approaches learning implicit models rely approximating intractable distribution optimisation objective gradient based optimisation liable produce inaccurate updates thus poor models paper alleviates need approximations proposing emph stein gradient estimator directly estimates score function implicitly defined distribution efficacy proposed estimator empirically demonstrated examples include meta learning approximate inference entropy regularised gans provide improved sample diversity\n",
            "output sentence:  introduced novel gradient estimator using stein method compared methods learning implicit models approximate inference image generation \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.8333333333333334, 'f': 0.14492753464398234}, 'rouge-2': {'r': 0.02666666666666667, 'p': 0.4, 'f': 0.04999999882812503}, 'rouge-l': {'r': 0.031746031746031744, 'p': 0.3333333333333333, 'f': 0.057971012904851967}}\n",
            "pair:  paper propose framework leverages semi supervised models improve unsupervised clustering performance leverage semi supervised models first need automatically generate labels called pseudo labels find prior approaches generating pseudo labels hurt clustering performance low accuracy instead use ensemble deep networks construct similarity graph extract high accuracy pseudo labels approach finding high quality pseudo labels using ensembles training semi supervised model iterated yielding continued improvement show approach outperforms state art clustering results multiple image text datasets example achieve accuracy cifar news outperforming state art absolute terms\n",
            "output sentence:  using ensembles pseudo labels unsupervised clustering \n",
            "\n",
            "{'rouge-1': {'r': 0.08, 'p': 0.5, 'f': 0.1379310321046374}, 'rouge-2': {'r': 0.03508771929824561, 'p': 0.2857142857142857, 'f': 0.06249999805175788}, 'rouge-l': {'r': 0.08, 'p': 0.5, 'f': 0.1379310321046374}}\n",
            "pair:  success modern machine learning becoming increasingly important understand control learning algorithms interact unfortunately negative results game theory show little hope understanding controlling general player games therefore introduce smooth markets sm games class player games pairwise zero sum interactions sm games codify common design pattern machine learning includes gans adversarial training recent algorithms show sm games amenable analysis optimization using first order methods\n",
            "output sentence:  introduce class player games suited gradient based methods \n",
            "\n",
            "{'rouge-1': {'r': 0.14457831325301204, 'p': 0.631578947368421, 'f': 0.23529411461553248}, 'rouge-2': {'r': 0.06, 'p': 0.3157894736842105, 'f': 0.10084033345102754}, 'rouge-l': {'r': 0.14457831325301204, 'p': 0.631578947368421, 'f': 0.23529411461553248}}\n",
            "pair:  transfer adaptation new unknown environmental dynamics key challenge reinforcement learning rl even greater challenge performing near optimally single attempt test time possibly without access dense rewards addressed current methods require multiple experience rollouts adaptation achieve single episode transfer family environments related dynamics propose general algorithm optimizes probe inference model rapidly estimate underlying latent variables test dynamics immediately used input universal control policy modular approach enables integration state art algorithms variational inference rl moreover approach require access rewards test time allowing perform settings existing adaptive approaches cannot diverse experimental domains single episode test constraint method significantly outperforms existing adaptive approaches shows favorable performance baselines robust transfer\n",
            "output sentence:  single episode policy transfer family environments related dynamics via optimized probing rapid inference latent variables immediate execution universal policy policy \n",
            "\n",
            "{'rouge-1': {'r': 0.06593406593406594, 'p': 0.5454545454545454, 'f': 0.11764705689926953}, 'rouge-2': {'r': 0.01652892561983471, 'p': 0.2, 'f': 0.030534349734864002}, 'rouge-l': {'r': 0.054945054945054944, 'p': 0.45454545454545453, 'f': 0.09803921376201463}}\n",
            "pair:  classical models describe primary visual cortex filter bank orientation selective linear nonlinear ln energy models models fail predict neural responses natural stimuli accurately recent work shows convolutional neural networks cnns trained predict activity accurately remains unclear features extracted neurons beyond orientation selectivity phase invariance work towards systematically studying computations categorizing neurons groups perform similar computations present framework identifying common features independent individual neurons orientation selectivity using rotation equivariant convolutional neural network automatically extracts every feature multiple different orientations fit rotation equivariant cnn responses population neurons natural images recorded mouse primary visual cortex using two photon imaging show rotation equivariant network outperforms regular cnn number feature maps reveals number common features shared many neurons pooled sparsely predict neural activity findings first step towards powerful new tool study nonlinear functional organization visual cortex\n",
            "output sentence:  rotation equivariant cnn model outperforms previous models suggest functional groupings neurons \n",
            "\n",
            "{'rouge-1': {'r': 0.1875, 'p': 0.9230769230769231, 'f': 0.31168830888176763}, 'rouge-2': {'r': 0.0625, 'p': 0.4166666666666667, 'f': 0.1086956499054821}, 'rouge-l': {'r': 0.140625, 'p': 0.6923076923076923, 'f': 0.23376623095968968}}\n",
            "pair:  federated learning involves training effectively combining machine learning models distributed partitions data tasks edge devices naturally viewed multi task learning problem federated averaging fedavg leading optimization method training non convex models setting behavior well understood realistic federated settings devices tasks statistically heterogeneous device collects data non identical fashion work introduce framework called fedprox tackle statistical heterogeneity fedprox encompasses fedavg special case provide convergence guarantees fedprox device dissimilarity assumption empirical evaluation validates theoretical analysis demonstrates improved robustness stability fedprox learning heterogeneous networks\n",
            "output sentence:  introduce fedprox framework tackle statistical heterogeneity federated settings convergence guarantees improved robustness stability \n",
            "\n",
            "{'rouge-1': {'r': 0.09302325581395349, 'p': 0.5333333333333333, 'f': 0.15841583905499462}, 'rouge-2': {'r': 0.03389830508474576, 'p': 0.26666666666666666, 'f': 0.06015037393860598}, 'rouge-l': {'r': 0.08139534883720931, 'p': 0.4666666666666667, 'f': 0.13861385885697486}}\n",
            "pair:  real brain networks exhibit functional modularity investigate whether functional mod ularity also exists deep neural networks dnn trained back propagation hypothesis dnn also organized task specific modules paper seek dissect hidden layer disjoint groups task specific hidden neurons help relatively well studied neuron attribution methods saying task specific mean hidden neurons group functionally related predicting set similar data samples samples similar feature patterns argue groups neurons call functional modules serve basic functional unit dnn propose preliminary method identify functional modules via bi clustering attribution scores hidden neurons find first unsurprisingly functional neurons highly sparse small sub set neurons important predicting small subset data samples use label supervision samples corresponding group bicluster show surprisingly coherent feature patterns also show functional modules perform critical role discriminating data samples ablation experiment\n",
            "output sentence:  develop approach parcellate hidden layer dnn functionally related groups applying spectral coclustering attribution scores hidden neurons \n",
            "\n",
            "{'rouge-1': {'r': 0.08433734939759036, 'p': 0.7, 'f': 0.15053763248930513}, 'rouge-2': {'r': 0.04854368932038835, 'p': 0.5555555555555556, 'f': 0.08928571280771685}, 'rouge-l': {'r': 0.07228915662650602, 'p': 0.6, 'f': 0.12903225614521913}}\n",
            "pair:  convolutional architectures recently shown competitive many sequence modelling tasks compared de facto standard recurrent neural networks rnns providing computational modelling advantages due inherent parallelism however currently remains performance gap expressive stochastic rnn variants especially several layers dependent random variables work propose stochastic temporal convolutional networks stcns novel architecture combines computational advantages temporal convolutional networks tcn representational power robustness stochastic latent spaces particular propose hierarchy stochastic latent variables captures temporal dependencies different time scales architecture modular flexible due decoupling deterministic stochastic layers show proposed architecture achieves state art log likelihoods across several tasks finally model capable predicting high quality synthetic samples long range temporal horizon modelling handwritten text\n",
            "output sentence:  combine computational advantages temporal convolutional architectures expressiveness stochastic latent variables \n",
            "\n",
            "{'rouge-1': {'r': 0.05952380952380952, 'p': 0.625, 'f': 0.10869565058601136}, 'rouge-2': {'r': 0.02727272727272727, 'p': 0.375, 'f': 0.05084745636311408}, 'rouge-l': {'r': 0.05952380952380952, 'p': 0.625, 'f': 0.10869565058601136}}\n",
            "pair:  order efficiently learn small amount data new tasks meta learning transfers knowledge learned previous tasks new ones however critical challenge meta learning task heterogeneity cannot well handled traditional globally shared meta learning methods addition current task specific meta learning methods may either suffer hand crafted structure design lack capability capture complex relations tasks paper motivated way knowledge organization knowledge bases propose automated relational meta learning arml framework automatically extracts cross task relations constructs meta knowledge graph new task arrives quickly find relevant structure tailor learned structure knowledge meta learner result proposed framework addresses challenge task heterogeneity learned meta knowledge graph also increases model interpretability conduct extensive experiments toy regression shot image classification results demonstrate superiority arml state art baselines\n",
            "output sentence:  addressing task heterogeneity problem meta learning introducing meta knowledge graph \n",
            "\n",
            "{'rouge-1': {'r': 0.11224489795918367, 'p': 0.7857142857142857, 'f': 0.19642856924107144}, 'rouge-2': {'r': 0.05172413793103448, 'p': 0.46153846153846156, 'f': 0.09302325400156243}, 'rouge-l': {'r': 0.09183673469387756, 'p': 0.6428571428571429, 'f': 0.16071428352678574}}\n",
            "pair:  reduced precision computation one key areas addressing widening compute gap driven exponential growth deep learning applications recent years deep neural network training largely migrated bit precision significant gains performance energy efficiency however attempts train dnns bit precision met significant challenges higher precision dynamic range requirements back propagation paper propose method train deep neural networks using bit floating point representation weights activations errors gradients demonstrate state art accuracy across multiple data sets imagenet wmt broader set workloads resnet gnmt transformer previously reported propose enhanced loss scaling method augment reduced subnormal range bit floating point improve error propagation also examine impact quantization noise generalization propose stochastic rounding technique address gradient noise result applying techniques report slightly higher validation accuracy compared full precision baseline\n",
            "output sentence:  demonstrated state art training results using bit floating point representation across resnet gnmt transformer \n",
            "\n",
            "{'rouge-1': {'r': 0.05813953488372093, 'p': 0.4166666666666667, 'f': 0.10204081417742612}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.15384615384615385, 'f': 0.029629627889163346}, 'rouge-l': {'r': 0.05813953488372093, 'p': 0.4166666666666667, 'f': 0.10204081417742612}}\n",
            "pair:  meta learning algorithms learn acquire new tasks quickly past experience context reinforcement learning meta learning algorithms acquire reinforcement learning procedures solve new problems efficiently utilizing experience prior tasks performance meta learning algorithms depends tasks available meta training way supervised learning generalizes best test points drawn distribution training points meta learning methods generalize best tasks distribution meta training tasks effect meta reinforcement learning offloads design burden algorithm design task design automate process task design well devise meta learning algorithm truly automated work take step direction proposing family unsupervised meta learning algorithms reinforcement learning motivate describe general recipe unsupervised meta reinforcement learning present instantiation approach conceptual theoretical contributions consist formulating unsupervised meta reinforcement learning problem describing task proposals based mutual information principle used train optimal meta learners experimental results indicate unsupervised meta reinforcement learning effectively acquires accelerated reinforcement learning procedures without need manual task design significantly exceeds performance learning scratch\n",
            "output sentence:  meta learning self proposed task distributions speed reinforcement learning without human specified task distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.057971014492753624, 'p': 0.5714285714285714, 'f': 0.10526315622229918}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.42857142857142855, 'f': 0.07894736674861498}}\n",
            "pair:  existing neural networks learning graphs deal issue permutation invariance conceiving network message passing scheme node sums feature vectors coming neighbors argue imposes limitation representation power instead propose new general architecture representing objects consisting hierarchy parts call covariant compositional networks ccns covariance means activation neuron must transform specific way permutations similarly steerability cnns achieve covariance making activation transform according tensor representation permutation group derive corresponding tensor aggregation rules neuron must implement experiments show ccns outperform competing methods standard graph learning benchmarks\n",
            "output sentence:  general framework creating covariant graph neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5333333333333333, 'f': 0.19277108137610685}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.26666666666666666, 'f': 0.07766990042416824}, 'rouge-l': {'r': 0.10294117647058823, 'p': 0.4666666666666667, 'f': 0.16867469583393818}}\n",
            "pair:  policy learning task evaluating improving policies using historic data collected logging policy important policy evaluation usually expensive adverse impacts one major challenge policy learning derive counterfactual estimators also low variance thus low generalization error work inspired learning bounds importance sampling problems present new counterfactual learning principle policy learning bandit feedbacks method regularizes generalization error minimizing distribution divergence logging policy new policy removes need iterating training samples compute sample variance regularization prior work neural network policies end end training algorithms using variational divergence minimization showed significant improvement conventional baseline algorithms also consistent theoretical results\n",
            "output sentence:  policy learning bandit feedbacks propose new variance regularized counterfactual learning algorithm theoretical foundations superior empirical performance \n",
            "\n",
            "{'rouge-1': {'r': 0.15476190476190477, 'p': 0.7222222222222222, 'f': 0.25490195787773934}, 'rouge-2': {'r': 0.05660377358490566, 'p': 0.3, 'f': 0.09523809256739742}, 'rouge-l': {'r': 0.11904761904761904, 'p': 0.5555555555555556, 'f': 0.19607842846597462}}\n",
            "pair:  despite ability memorize large datasets deep neural networks often achieve good generalization performance however differences learned solutions networks generalize remain unclear additionally tuning properties single directions defined activation single unit linear combination units response input highlighted importance evaluated connect lines inquiry demonstrate network reliance single directions good predictor generalization performance across networks trained datasets different fractions corrupted labels across ensembles networks trained datasets unmodified labels across different hyper parameters course training dropout regularizes quantity point batch normalization implicitly discourages single direction reliance part decreasing class selectivity individual units finally find class selectivity poor predictor task importance suggesting networks generalize well minimize dependence individual units reducing selectivity also individually selective units may necessary strong network performance\n",
            "output sentence:  find deep networks generalize poorly reliant single directions generalize well evaluate impact dropout batch normalization well class selectivity single direction reliance \n",
            "\n",
            "{'rouge-1': {'r': 0.08974358974358974, 'p': 0.5384615384615384, 'f': 0.15384615139717428}, 'rouge-2': {'r': 0.01020408163265306, 'p': 0.06666666666666667, 'f': 0.01769911274179684}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.38461538461538464, 'f': 0.10989010744113034}}\n",
            "pair:  deep neural networks demonstrated unprecedented success various knowledge management applications however networks created often complex large numbers trainable edges require extensive computational resources note many successful networks nevertheless often contain large numbers redundant edges moreover many edges may negligible contributions towards overall network performance paper propose novel isparse framework experimentally show sparsify network without impacting network performance isparse leverages novel edge significance score determine importance edge respect final network output furthermore isparse applied training model top pre trained model making retraining free approach leading minimal computational overhead comparisons isparse pfec nisp dropconnect retraining free benchmark datasets show isparse leads effective network sparsifications\n",
            "output sentence:  isparse eliminates irrelevant insignificant network edges minimal impact network network determining edge importance final network output \n",
            "\n",
            "{'rouge-1': {'r': 0.04597701149425287, 'p': 0.26666666666666666, 'f': 0.07843137004036918}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.034482758620689655, 'p': 0.2, 'f': 0.058823526903114286}}\n",
            "pair:  computer vision applications prior works shown efficacy reducing numeric precision model parameters network weights deep neural networks activation maps however occupy large memory footprint training inference step using mini batches inputs one way reduce large memory footprint reduce precision activations however past works shown reducing precision activations hurts model accuracy study schemes train networks scratch using reduced precision activations without hurting accuracy reduce precision activation maps along model parameters increase number filter maps layer find scheme matches surpasses accuracy baseline full precision network result one significantly improve execution efficiency reduce dynamic memory footprint memory band width computational energy speed training inference process appropriate hardware support call scheme wrpn wide reduced precision networks report results show wrpn scheme better previously reported accuracies ilsvrc dataset computationally less expensive compared previously reported reduced precision networks\n",
            "output sentence:  lowering precision bits bits even binary widening filter banks gives accurate network obtained fp weights activations \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.8, 'f': 0.2758620661117717}, 'rouge-2': {'r': 0.08888888888888889, 'p': 0.5333333333333333, 'f': 0.15238094993197282}, 'rouge-l': {'r': 0.1527777777777778, 'p': 0.7333333333333333, 'f': 0.25287356036464526}}\n",
            "pair:  answering questions require multi hop reasoning web scale necessitates retrieving multiple evidence documents one often little lexical semantic relationship question paper introduces new graph based recurrent retrieval approach learns retrieve reasoning paths wikipedia graph answer multi hop open domain questions retriever model trains recurrent neural network learns sequentially retrieve evidence paragraphs reasoning path conditioning previously retrieved documents reader model ranks reasoning paths extracts answer span included best reasoning path experimental results show state art results three open domain qa datasets showcasing effectiveness robustness method notably method achieves significant improvement hotpotqa outperforming previous best model points\n",
            "output sentence:  graph based recurrent retriever learns retrieve reasoning paths wikipedia graph outperforms recent state art hotpotqa points \n",
            "\n",
            "{'rouge-1': {'r': 0.15254237288135594, 'p': 0.47368421052631576, 'f': 0.2307692270841552}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.05263157894736842, 'f': 0.023809520308957433}, 'rouge-l': {'r': 0.06779661016949153, 'p': 0.21052631578947367, 'f': 0.1025640988790271}}\n",
            "pair:  deep reinforcement learning rl methods generally engage exploratory behavior noise injection action space alternative add noise directly agent parameters lead consistent exploration richer set behaviors methods evolutionary strategies use parameter perturbations discard temporal structure process require significantly samples combining parameter noise traditional rl methods allows combine best worlds demonstrate policy methods benefit approach experimental comparison dqn ddpg trpo high dimensional discrete action environments well continuous control tasks\n",
            "output sentence:  parameter space noise allows reinforcement learning algorithms explore perturbing parameters instead actions often leading significantly improved exploration exploration performance performance \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.35294117647058826, 'f': 0.1428571396286849}, 'rouge-2': {'r': 0.012658227848101266, 'p': 0.058823529411764705, 'f': 0.020833330418837216}, 'rouge-l': {'r': 0.07462686567164178, 'p': 0.29411764705882354, 'f': 0.11904761581916108}}\n",
            "pair:  paper propose neural network framework called neuron hierarchical network nhn evolves beyond hierarchy layers concentrates hierarchy neurons observe mass redundancy weights handcrafted randomly searched architectures inspired development human brains prune low sensitivity neurons model add new neurons graph relation individual neurons emphasized existence layers weakened propose process discover best base model random architecture search discover best locations connections added neurons evolutionary search experiment results show nhn achieves higher test accuracy cifar state art handcrafted randomly searched architectures requiring much fewer parameters less searching time\n",
            "output sentence:  breaking layer hierarchy propose step approach construction neuron hierarchy networks outperform nas smash hierarchical representation fewer parameters shorter \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.8571428571428571, 'f': 0.162162160449233}, 'rouge-2': {'r': 0.022988505747126436, 'p': 0.3333333333333333, 'f': 0.043010751481096114}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.5714285714285714, 'f': 0.108108106395179}}\n",
            "pair:  explore collaborative multi agent setting team deep reinforcement learning agents attempt solve shared task partially observable environments scenario learning effective communication protocol key propose communication protocol allows targeted communication agents learn emph messages send emph send additionally introduce multi stage communication approach agents co ordinate via several rounds communication taking action environment evaluate approach several cooperative multi agent tasks varying difficulties varying number agents variety environments ranging grid layouts shapes simulated traffic junctions complex indoor environments demonstrate benefits targeted well multi stage communication moreover show targeted communication strategies learned agents quite interpretable intuitive\n",
            "output sentence:  targeted communication multi agent cooperative reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.23684210526315788, 'p': 0.6428571428571429, 'f': 0.34615384221893497}, 'rouge-2': {'r': 0.09302325581395349, 'p': 0.3076923076923077, 'f': 0.14285713929209193}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.5714285714285714, 'f': 0.3076923037573965}}\n",
            "pair:  deepa deep learning framework explores parallelism parallelizable dimensions accelerate training process convolutional neural networks deepa optimizes parallelism granularity individual layer network present elimination based algorithm finds optimal parallelism configuration every layer evaluation shows deepa achieves speedup compared state art deep learning frameworks reduces data transfers\n",
            "output sentence:  best knowledge deepa first deep learning framework controls optimizes parallelism cnns parallelizable dimensions granularity layer \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-123d9fb647b7>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateRandomlyprint_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-efe5772afd9a>\u001b[0m in \u001b[0;36mevaluateRandomlyprint_1\u001b[0;34m(encoder, decoder, n)\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mtokenized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0;31m#print(len(tokenized_input))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m           \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-229d13fc485a>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             decoder_output, decoder_hidden, decoder_attention = decoder(\n\u001b[0m\u001b[1;32m     24\u001b[0m                 decoder_input, decoder_hidden, encoder_outputs)\n\u001b[1;32m     25\u001b[0m             \u001b[0mdecoder_attentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-34-aaff5599279e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m#print(temp.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m#print(\"after getting attn weights softmax\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#print(attn_weights.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from rouge import Rouge\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "\n",
        "evaluateRandomlyprint_1(encoder1, attn_decoder1,15000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6UzQwGPRJnuU",
        "outputId": "6a4b92cd-c4b0-4c8e-fe1a-bd13f473cf8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "{'rouge-1': {'r': 0.16, 'p': 0.6666666666666666, 'f': 0.2580645130072841}, 'rouge-2': {'r': 0.10344827586206896, 'p': 0.42857142857142855, 'f': 0.16666666353395063}, 'rouge-l': {'r': 0.08, 'p': 0.3333333333333333, 'f': 0.12903225494276802}}\n",
            "pair:  propose support guided adversarial imitation learning sail generic imitation learning framework unifies support estimation expert policy family adversarial imitation learning ail algorithms sail addresses two important challenges ail including implicit reward bias potential training instability also show sail least efficient standard ail extensive evaluation demonstrate proposed method effectively handles reward bias achieves better performance training stability baseline methods wide range benchmark control tasks\n",
            "output sentence:  unify support estimation family adversarial imitation learning algorithms support guided adversarial imitation learning robust stable imitation learning framework \n",
            "\n",
            "{'rouge-1': {'r': 0.08064516129032258, 'p': 0.3333333333333333, 'f': 0.12987012673300732}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.07142857142857142, 'f': 0.02298850304663792}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.26666666666666666, 'f': 0.10389610075898138}}\n",
            "pair:  despite empirical success theoretical underpinnings stability convergence acceleration properties batch normalization bn remain elusive paper attack problem modelling approach perform thorough theoretical analysis bn applied simplified model ordinary least squares ols discover gradient descent ols bn interesting properties including scaling law convergence arbitrary learning rates weights asymptotic acceleration effects well insensitivity choice learning rates demonstrate numerically findings specific ols problem hold qualitatively complex supervised learning problems points new direction towards uncovering mathematical principles underlies batch normalization\n",
            "output sentence:  mathematically analyze effect batch normalization simple model obtain key new insights applies general supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.4166666666666667, 'f': 0.1818181784066116}, 'rouge-2': {'r': 0.0392156862745098, 'p': 0.16666666666666666, 'f': 0.06349206040816341}, 'rouge-l': {'r': 0.09302325581395349, 'p': 0.3333333333333333, 'f': 0.14545454204297528}}\n",
            "pair:  introduce simple efficient algorithms computing minhash probability distribution suitable sparse dense data equivalent running times state art cases collision probability algorithms new measure similarity positive vectors investigate detail describe sense collision probability optimal locality sensitive hash based sampling argue similarity measure useful probability distributions similarity pursued algorithms weighted minhash natural generalization jaccard index\n",
            "output sentence:  minimum set exponentially distributed hashes useful collision probability generalizes jaccard index probability distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.875, 'f': 0.21538461322603553}, 'rouge-2': {'r': 0.08955223880597014, 'p': 0.8571428571428571, 'f': 0.162162160449233}, 'rouge-l': {'r': 0.12280701754385964, 'p': 0.875, 'f': 0.21538461322603553}}\n",
            "pair:  consider tackling single agent rl problem distributing learners learners called advisors endeavour solve problem different focus advice taking form action values communicated aggregator control system show local planning method advisors critical none ones found literature flawless textit egocentric planning overestimates values states advisors disagree textit agnostic planning inefficient around danger zones introduce novel approach called textit empathic discuss theoretical aspects empirically examine validate theoretical findings fruit collection task\n",
            "output sentence:  consider tackling single agent rl problem distributing learners \n",
            "\n",
            "{'rouge-1': {'r': 0.11214953271028037, 'p': 0.6666666666666666, 'f': 0.19199999753472002}, 'rouge-2': {'r': 0.04878048780487805, 'p': 0.35294117647058826, 'f': 0.08571428358061231}, 'rouge-l': {'r': 0.08411214953271028, 'p': 0.5, 'f': 0.14399999753472}}\n",
            "pair:  continual learning problem sequentially learning new tasks knowledge protecting previously acquired knowledge however catastrophic forgetting poses grand challenge neural networks performing learning process thus neural networks deployed real world often struggle scenarios data distribution non stationary concept drift imbalanced always fully available rare edge cases propose differentiable hebbian consolidation model composed differentiable hebbian plasticity dhp softmax layer adds rapid learning plastic component compressed episodic memory fixed slow changing parameters softmax output layer enabling learned representations retained longer timescale demonstrate flexibility method integrating well known task specific synaptic consolidation methods penalize changes slow weights important target task evaluate approach permuted mnist split mnist vision datasets mixture benchmarks introduce imbalanced variant permuted mnist dataset combines challenges class imbalance concept drift proposed model requires additional hyperparameters outperforms comparable baselines reducing forgetting\n",
            "output sentence:  hebbian plastic weights behave compressed episodic memory storage neural networks combination task specific synaptic consolidation improve ability learning \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.5714285714285714, 'f': 0.20512820218277453}, 'rouge-2': {'r': 0.08860759493670886, 'p': 0.5, 'f': 0.15053763185108107}, 'rouge-l': {'r': 0.125, 'p': 0.5714285714285714, 'f': 0.20512820218277453}}\n",
            "pair:  study problem multiset prediction goal multiset prediction train predictor maps input multiset consisting multiple items unlike existing problems supervised learning classification ranking sequence generation known order among items target multiset item multiset may appear making problem extremely challenging paper propose novel multiset loss function viewing problem perspective sequential decision making proposed multiset loss function empirically evaluated two families datasets one synthetic real varying levels difficulty various baseline loss functions including reinforcement learning sequence aggregated distribution matching loss functions experiments reveal effectiveness proposed loss function others\n",
            "output sentence:  study problem multiset prediction propose novel multiset loss function providing analysis empirical evidence demonstrates effectiveness \n",
            "\n",
            "{'rouge-1': {'r': 0.08536585365853659, 'p': 0.5384615384615384, 'f': 0.14736841869030476}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04878048780487805, 'p': 0.3076923076923077, 'f': 0.08421052395346268}}\n",
            "pair:  energy based models ebms un normalized models recent successes continuous spaces however successfully applied model text sequences decreasing energy training samples straightforward mining negative samples energy increased difficult part standard gradient based methods readily applicable input high dimensional discrete side step issue generating negatives using pre trained auto regressive language models ebm works nin em residual language model trained discriminate real text text generated auto regressive models investigate generalization ability residual ebms pre requisite using applications extensively analyze generalization task classifying whether input machine human generated natural task given training loss mine negatives overall observe ebms generalize remarkably well changes architecture generators producing negatives however ebms exhibit sensitivity training set used generators\n",
            "output sentence:  residual ebm text whose formulation equivalent discriminating human machine generated text study generalization behavior \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.5833333333333334, 'f': 0.25925925580246917}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.18181818181818182, 'f': 0.07142856827168381}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.5833333333333334, 'f': 0.25925925580246917}}\n",
            "pair:  propose single neural probabilistic model based variational autoencoder conditioned arbitrary subset observed features sample remaining features one shot features may real valued categorical training model performed stochastic variational bayes experimental evaluation synthetic data well feature imputation image inpainting problems shows effectiveness proposed approach diversity generated samples\n",
            "output sentence:  propose extension conditional variational autoencoder allows conditioning arbitrary subset features sampling remaining ones \n",
            "\n",
            "{'rouge-1': {'r': 0.11842105263157894, 'p': 0.5294117647058824, 'f': 0.1935483841091456}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.1875, 'f': 0.060606057896133166}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.4117647058823529, 'f': 0.15053763142097357}}\n",
            "pair:  domain time series forecasting extensively studied fundamental importance many real life applications weather prediction traffic flow forecasting sales compelling examples sequential phenomena predictive models generally make use relations past future values however case stationary time series observed values also drastically depend number exogenous features used improve forecasting quality work propose change paradigm consists learning features embeddings vectors within recurrent neural networks apply framework forecast smart cards tap logs parisian subway network results show context embedded models perform quantitatively better one step ahead multi step ahead forecasting\n",
            "output sentence:  order forecast multivariate stationary time series learn embeddings containing contextual features within rnn apply framework public transportation data \n",
            "\n",
            "{'rouge-1': {'r': 0.08247422680412371, 'p': 0.6666666666666666, 'f': 0.14678898886625708}, 'rouge-2': {'r': 0.04838709677419355, 'p': 0.5, 'f': 0.08823529250865055}, 'rouge-l': {'r': 0.07216494845360824, 'p': 0.5833333333333334, 'f': 0.12844036501304604}}\n",
            "pair:  convolutional neural networks cnns generally acknowledged one driving forces advancement computer vision despite promising performances many tasks cnns still face major obstacles road achieving ideal machine intelligence one cnns complex hard interpret another standard cnns require large amounts annotated data sometimes hard obtain desirable able learn examples work address limitations cnns developing novel simple interpretable models shot learn ing models based idea encoding objects terms visual concepts interpretable visual cues represented feature vectors within cnns first adapt learning visual concepts shot setting uncover two key properties feature encoding using visual concepts call category sensitivity spatial pattern motivated properties present two intuitive models problem shot learning experiments show models achieve competitive performances much flexible interpretable alternative state art shot learning methods conclude using visual concepts helps expose natural capability cnns shot learning\n",
            "output sentence:  enable ordinary cnns shot learning exploiting visual concepts interpretable visual cues learnt within cnns \n",
            "\n",
            "{'rouge-1': {'r': 0.07446808510638298, 'p': 0.5384615384615384, 'f': 0.13084111936064288}, 'rouge-2': {'r': 0.01680672268907563, 'p': 0.16666666666666666, 'f': 0.030534349480799576}, 'rouge-l': {'r': 0.06382978723404255, 'p': 0.46153846153846156, 'f': 0.11214953057559615}}\n",
            "pair:  large scale pre trained language model bert recently achieved great success wide range language understanding tasks however remains open question utilize bert text generation tasks paper present novel approach addressing challenge generic sequence sequence seq seq setting first propose new task conditional masked language modeling mlm enable fine tuning bert target text generation dataset fine tuned bert teacher exploited extra supervision improve conventional seq seq models student text generation leveraging bert idiosyncratic bidirectional nature distilling knowledge learned bert encourage auto regressive seq seq models plan ahead imposing global sequence level supervision coherent text generation experiments show proposed approach significantly outperforms strong baselines transformer multiple text generation tasks including machine translation mt text summarization proposed model also achieves new state art results iwslt german english english vietnamese mt datasets\n",
            "output sentence:  propose model agnostic way leverage bert text generation achieve improvements transformer tasks datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.08064516129032258, 'p': 0.5555555555555556, 'f': 0.14084506820868875}, 'rouge-2': {'r': 0.013333333333333334, 'p': 0.1, 'f': 0.023529409688581502}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.3333333333333333, 'f': 0.08450704003967473}}\n",
            "pair:  describe simple general neural network weight compression approach network parameters weights biases represented latent space amounting reparameterization space equipped learned probability model used impose entropy penalty parameter representation training compress representation using simple arithmetic coder training classification accuracy model compressibility maximized jointly bitrate accuracy trade specified hyperparameter evaluate method mnist cifar imagenet classification benchmarks using six distinct model architectures results show state art model compression achieved scalable general way without requiring complex procedures multi stage training\n",
            "output sentence:  end end trainable model compression method optimizing accuracy jointly expected model size \n",
            "\n",
            "{'rouge-1': {'r': 0.10526315789473684, 'p': 0.6153846153846154, 'f': 0.1797752784042419}, 'rouge-2': {'r': 0.011111111111111112, 'p': 0.07692307692307693, 'f': 0.019417473522481164}, 'rouge-l': {'r': 0.07894736842105263, 'p': 0.46153846153846156, 'f': 0.1348314581795228}}\n",
            "pair:  recent years deep neural network approaches widely adopted machine learning tasks including classification however shown vulnerable adversarial perturbations carefully crafted small perturbations cause misclassification legitimate images propose defense gan new framework leveraging expressive capability generative models defend deep neural networks attacks defense gan trained model distribution unperturbed images inference time finds close output given image contain adversarial changes output fed classifier proposed method used classification model modify classifier structure training procedure also used defense attack assume knowledge process generating adversarial examples empirically show defense gan consistently effective different attack methods improves existing defense strategies\n",
            "output sentence:  defense gan uses generative adversarial network defend white box black box attacks classification models \n",
            "\n",
            "{'rouge-1': {'r': 0.05970149253731343, 'p': 0.2857142857142857, 'f': 0.0987654292394453}, 'rouge-2': {'r': 0.02531645569620253, 'p': 0.14285714285714285, 'f': 0.043010750130651094}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.2857142857142857, 'f': 0.0987654292394453}}\n",
            "pair:  many deployed learned models black boxes given input returns output internal information model architecture optimisation procedure training data disclosed explicitly might contain proprietary information make system vulnerable work shows attributes neural networks exposed sequence queries multiple implications one hand work exposes vulnerability black box neural networks different types attacks show revealed internal information helps generate effective adversarial examples black box model hand technique used better protection private content automatic recognition models using adversarial examples paper suggests actually hard draw line white box black box models\n",
            "output sentence:  querying black box neural network reveals lot information propose novel metamodels effectively extracting extracting black box \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.30434782608695654, 'f': 0.13084111812035995}, 'rouge-2': {'r': 0.009345794392523364, 'p': 0.045454545454545456, 'f': 0.01550387313983586}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.30434782608695654, 'f': 0.13084111812035995}}\n",
            "pair:  use deep learning models priors compressive sensing tasks presents new potential inexpensive seismic data acquisition appropriately designed wasserstein generative adversarial network designed based generative adversarial network architecture trained several historical surveys capable learning statistical properties seismic wavelets usage validating performance testing compressive sensing three steps first existence sparse representation different compression rates seismic surveys studied non uniform samplings studied using proposed methodology finally recommendations non uniform seismic survey grid based evaluation reconstructed seismic images metrics proposed primary goal proposed deep learning model provide foundations optimal design seismic acquisition less loss imaging quality along lines compressive sensing design non uniform grid asset gulf mexico versus traditional seismic survey grid collects data uniformly every feet suggested leveraging proposed method\n",
            "output sentence:  improved gan based pixel inpainting network compressed seismic image recovery andproposed xa non uniform sampling survey recommendatio easily applied medical compressive domains technique \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.6666666666666666, 'f': 0.11764705721453288}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.2, 'f': 0.02564102444115719}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.5, 'f': 0.08823529250865055}}\n",
            "pair:  state art sequence sequence models large scale tasks perform fixed number computations input sequence regardless whether easy hard process paper train transformer models make output predictions different stages network investigate different ways predict much computation required particular sequence unlike dynamic computation universal transformers applies set layers iteratively apply different layers every step adjust amount computation well model capacity iwslt german english translation approach matches accuracy well tuned baseline transformer using less quarter decoder layers\n",
            "output sentence:  sequence model dynamically adjusts amount computation input \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.9411764705882353, 'f': 0.2935779790186011}, 'rouge-2': {'r': 0.12173913043478261, 'p': 0.875, 'f': 0.21374045587087004}, 'rouge-l': {'r': 0.13043478260869565, 'p': 0.7058823529411765, 'f': 0.2201834836057571}}\n",
            "pair:  recent evidence shows convolutional neural networks cnns biased towards textures cnns non robust adversarial perturbations textures traditional robust visual features like sift scale invariant feature transforms designed robust across substantial range affine distortion addition noise etc mimic human perception nature paper aims leverage good properties sift renovate cnn architectures towards better accuracy robustness borrow scale space extreme value idea sift propose evpnet extreme value preserving network contains three novel components model extreme values parametric differences gaussian dog extract extrema truncated relu suppress non stable extrema projected normalization layer pnl mimic pca sift like feature normalization experiments demonstrate evpnets achieve similar better accuracy conventional cnns achieving much better robustness set adversarial attacks fgsm pgd etc even without adversarial training\n",
            "output sentence:  paper aims leverage good properties robust visual features like sift renovate cnn architectures towards better accuracy robustness \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.7058823529411765, 'f': 0.26966291825779576}, 'rouge-2': {'r': 0.12195121951219512, 'p': 0.5555555555555556, 'f': 0.19999999704800003}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.7058823529411765, 'f': 0.26966291825779576}}\n",
            "pair:  dramatic advances generative models resulted near photographic quality artificially rendered faces animals objects natural world spite advances higher level understanding vision imagery arise exhaustively modeling object instead identifying higher level attributes best summarize aspects object work attempt model drawing process fonts building sequential generative models vector graphics model benefit providing scale invariant representation imagery whose latent representation may systematically manipulated exploited perform style propagation demonstrate results large dataset fonts highlight model captures statistical dependencies richness dataset envision model find use tool designers facilitate font design\n",
            "output sentence:  attempt model drawing process fonts building sequential generative models vector graphics svgs highly structured structured font font characters characters \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.42857142857142855, 'f': 0.12631578696066487}, 'rouge-2': {'r': 0.03125, 'p': 0.21428571428571427, 'f': 0.054545452323967035}, 'rouge-l': {'r': 0.04938271604938271, 'p': 0.2857142857142857, 'f': 0.08421052380277017}}\n",
            "pair:  many real world tasks exhibit rich structure repeated across different parts state space time work study possibility leveraging repeated structure speed regularize learning start kl regularized expected reward objective introduces additional component default policy instead relying fixed default policy learn data crucially restrict amount information default policy receives forcing learn reusable behaviors help policy learn faster formalize strategy discuss connections information bottleneck approaches variational em algorithm present empirical results discrete continuous action domains demonstrate certain tasks learning default policy alongside policy significantly speed improve learning please watch video demonstrating learned experts default policies several continuous control tasks https youtu qa llzus\n",
            "output sentence:  limiting state information default policy improvement performance kl regularized rl framework agent default policy optimized together \n",
            "\n",
            "{'rouge-1': {'r': 0.03571428571428571, 'p': 0.42857142857142855, 'f': 0.06593406451394762}, 'rouge-2': {'r': 0.009900990099009901, 'p': 0.16666666666666666, 'f': 0.01869158772643905}, 'rouge-l': {'r': 0.03571428571428571, 'p': 0.42857142857142855, 'f': 0.06593406451394762}}\n",
            "pair:  paper propose novel kind kernel random forest kernel enhance empirical performance mmd gan different common forests deterministic routings probabilistic routing variant used innovated random forest kernel possible merge cnn frameworks proposed random forest kernel following advantages perspective random forest output gan discriminator viewed feature inputs forest tree gets access merely fraction features thus entire forest benefits ensemble learning aspect kernel method random forest kernel proved characteristic therefore suitable mmd structure besides asymmetric kernel random forest kernel much flexible terms capturing differences distributions sharing advantages cnn kernel method ensemble learning random forest kernel based mmd gan obtains desirable empirical performances cifar celeba lsun bedroom data sets furthermore sake completeness also put forward comprehensive theoretical analysis support experimental results\n",
            "output sentence:  equip mmd gans new random forest kernel \n",
            "\n",
            "{'rouge-1': {'r': 0.1728395061728395, 'p': 0.9333333333333333, 'f': 0.2916666640299479}, 'rouge-2': {'r': 0.13186813186813187, 'p': 0.8, 'f': 0.2264150919099324}, 'rouge-l': {'r': 0.1728395061728395, 'p': 0.9333333333333333, 'f': 0.2916666640299479}}\n",
            "pair:  increasing model size pretraining natural language representations often results improved performance downstream tasks however point model increases become harder due gpu tpu memory limitations longer training times unexpected model degradation address problems present two parameter reduction techniques lower memory consumption increase training speed bert comprehensive empirical evidence shows proposed methods lead models scale much better compared original bert also use self supervised loss focuses modeling inter sentence coherence show consistently helps downstream tasks multi sentence inputs result best model establishes new state art results glue race squad benchmarks fewer parameters compared bert large\n",
            "output sentence:  new pretraining method establishes new state art results glue race squad benchmarks fewer parameters compared bert large \n",
            "\n",
            "{'rouge-1': {'r': 0.1125, 'p': 0.45, 'f': 0.17999999680000006}, 'rouge-2': {'r': 0.02857142857142857, 'p': 0.15, 'f': 0.047999997312000144}, 'rouge-l': {'r': 0.0625, 'p': 0.25, 'f': 0.0999999968000001}}\n",
            "pair:  state art methods learning cross lingual word embeddings relied bilingual dictionaries parallel corpora recent studies showed need parallel data supervision alleviated character level information methods showed encouraging results par supervised counterparts limited pairs languages sharing common alphabet work show build bilingual dictionary two languages without using parallel corpora aligning monolingual word embedding spaces unsupervised way without using character information model even outperforms existing supervised methods cross lingual tasks language pairs experiments demonstrate method works well also distant language pairs like english russian english chinese finally describe experiments english esperanto low resource language pair exists limited amount parallel data show potential impact method fully unsupervised machine translation code embeddings dictionaries publicly available\n",
            "output sentence:  aligning languages without rosetta stone parallel data construct bilingual dictionaries using adversarial training cross domain local scaling accurate proxy cross criterion \n",
            "\n",
            "{'rouge-1': {'r': 0.10810810810810811, 'p': 0.5, 'f': 0.17777777485432103}, 'rouge-2': {'r': 0.03409090909090909, 'p': 0.1875, 'f': 0.057692305088757515}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.4375, 'f': 0.1555555526320988}}\n",
            "pair:  many machine learning algorithms represent input data vector embeddings discrete codes inputs exhibit compositional structure objects built parts procedures subroutines natural ask whether compositional structure reflected inputs learned representations assessment compositionality languages received significant attention linguistics adjacent fields machine learning literature lacks general purpose tools producing graded measurements compositional structure general vector valued representation spaces describe procedure evaluating compositionality measuring well true representation producing model approximated model explicitly composes collection inferred representational primitives use procedure provide formal empirical characterizations compositional structure variety settings exploring relationship compositionality learning dynamics human judgments representational similarity generalization\n",
            "output sentence:  paper proposes simple procedure evaluating compositional structure learned representations uses procedure explore role compositionality four learning problems \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.875, 'f': 0.14141413992857876}, 'rouge-2': {'r': 0.059322033898305086, 'p': 0.875, 'f': 0.1111111099218947}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.875, 'f': 0.14141413992857876}}\n",
            "pair:  present end end design methodology efficient deep learning deployment unlike previous methods separately optimize neural network architecture pruning policy quantization policy jointly optimize end end manner deal larger design space brings train quantization aware accuracy predictor fed evolutionary search select best fit first generate large dataset nn architecture imagenet accuracy pairs without training architecture sampling unified supernet use data train accuracy predictor without quantization using predictor transfer technique get quantization aware predictor reduces amount post quantization fine tuning time extensive experiments imagenet show benefits end end methodology maintains accuracy resnet float model saving bitops comparing bit model obtain level accuracy mobilenetv haq achieving latency energy saving end end optimization outperforms separate optimizations using proxylessnas amc haq accuracy reducing orders magnitude gpu hours co emission\n",
            "output sentence:  present end end design methodology efficient deep learning deployment \n",
            "\n",
            "{'rouge-1': {'r': 0.14814814814814814, 'p': 0.7272727272727273, 'f': 0.24615384334201185}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.1, 'f': 0.026315787188365854}, 'rouge-l': {'r': 0.09259259259259259, 'p': 0.45454545454545453, 'f': 0.15384615103431953}}\n",
            "pair:  data augmentation da fundamental overfitting large convolutional neural networks especially limited training dataset images da usually based heuristic transformations like geometric color transformations instead using predefined transformations work learns data augmentation directly training data learning transform images encoder decoder architecture combined spatial transformer network transformed images still belong class new complex samples classifier experiments show approach better previous generative data augmentation methods comparable predefined transformation methods training image classifier\n",
            "output sentence:  automatic learning data augmentation using gan based architecture improve image classifier \n",
            "\n",
            "{'rouge-1': {'r': 0.1044776119402985, 'p': 0.7, 'f': 0.18181817955810425}, 'rouge-2': {'r': 0.0375, 'p': 0.3333333333333333, 'f': 0.06741572851912643}, 'rouge-l': {'r': 0.08955223880597014, 'p': 0.6, 'f': 0.15584415358407824}}\n",
            "pair:  multi relational graph embedding aims achieving effective representations reduced low dimensional parameters widely used knowledge base completion although knowledge base data usually contains tree like cyclic structure none existing approaches embed data compatible space line structure overcome problem novel framework called riemannian transe proposed paper embed entities riemannian manifold riemannian transe models relation move point defines specific novel distance dissimilarity relation relations naturally embedded correspondence structure data experiments several knowledge base completion tasks shown based appropriate choice manifold riemannian transe achieves good performance even significantly reduced parameters\n",
            "output sentence:  multi relational graph embedding riemannian manifolds transe like loss function \n",
            "\n",
            "{'rouge-1': {'r': 0.18518518518518517, 'p': 0.8333333333333334, 'f': 0.3030303000550964}, 'rouge-2': {'r': 0.15942028985507245, 'p': 0.8461538461538461, 'f': 0.26829268025877456}, 'rouge-l': {'r': 0.18518518518518517, 'p': 0.8333333333333334, 'f': 0.3030303000550964}}\n",
            "pair:  generating complex discrete distributions remains one challenging problems machine learning existing techniques generating complex distributions high degrees freedom depend standard generative models like generative adversarial networks gan wasserstein gan associated variations models based optimization involving distance two continuous distributions introduce discrete wasserstein gan dwgan model based dual formulation wasserstein distance two discrete distributions derive novel training algorithm corresponding network architecture based formulation experimental results provided synthetic discrete data real discretized data mnist handwritten digits\n",
            "output sentence:  propose discrete wasserstein gan dwgan model based dual formulation wasserstein distance two discrete distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 0.7692307692307693, 'f': 0.31746031418493326}, 'rouge-2': {'r': 0.14754098360655737, 'p': 0.75, 'f': 0.24657533971852133}, 'rouge-l': {'r': 0.2, 'p': 0.7692307692307693, 'f': 0.31746031418493326}}\n",
            "pair:  present optimal transport gan ot gan variant generative adversarial nets minimizing new metric measuring distance generator distribution data distribution metric call mini batch energy distance combines optimal transport primal form energy distance defined adversarially learned feature space resulting highly discriminative distance function unbiased mini batch gradients experimentally show ot gan highly stable trained large mini batches present state art results several popular benchmark problems image generation\n",
            "output sentence:  extension gans combining optimal transport primal form energy distance defined adversarially learned feature space \n",
            "\n",
            "{'rouge-1': {'r': 0.0684931506849315, 'p': 0.625, 'f': 0.12345678834324036}, 'rouge-2': {'r': 0.036585365853658534, 'p': 0.375, 'f': 0.06666666504691361}, 'rouge-l': {'r': 0.0684931506849315, 'p': 0.625, 'f': 0.12345678834324036}}\n",
            "pair:  temporal point processes dominant paradigm modeling sequences events happening irregular intervals standard way learning models estimating conditional intensity function however parameterizing intensity function usually incurs several trade offs show overcome limitations intensity based approaches directly modeling conditional distribution inter event times draw literature normalizing flows design models flexible efficient additionally propose simple mixture model matches flexibility flow based models also permits sampling computing moments closed form proposed models achieve state art performance standard prediction tasks suitable novel applications learning sequence embeddings imputing missing data\n",
            "output sentence:  learn temporal point processes modeling conditional density conditional intensity \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.5555555555555556, 'f': 0.11235954874384549}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.125, 'f': 0.01999999852800011}, 'rouge-l': {'r': 0.05, 'p': 0.4444444444444444, 'f': 0.08988763863148597}}\n",
            "pair:  point clouds form lagrangian representation allow powerful flexible applications large number computational disciplines propose novel deep learning method learn stable temporally coherent feature spaces points clouds change time identify set inherent problems approaches without knowledge time dimension inferred solutions exhibit strong flickering easy solutions suppress flickering result undesirable local minima manifest halo structures propose novel temporal loss function takes account higher time derivatives point positions encourages mingling prevent aforementioned halos combine techniques super resolution method truncation approach flexibly adapt size generated positions show method works large deforming point sets different sources demonstrate flexibility approach\n",
            "output sentence:  propose generative neural network approach temporally coherent point clouds \n",
            "\n",
            "{'rouge-1': {'r': 0.1875, 'p': 0.8, 'f': 0.3037974652780004}, 'rouge-2': {'r': 0.04054054054054054, 'p': 0.2, 'f': 0.06741572753440234}, 'rouge-l': {'r': 0.140625, 'p': 0.6, 'f': 0.22784809818939272}}\n",
            "pair:  previous work bowman et al yang et al found difficulty developing generative models based variational autoencoders vaes text address problem decoder ignoring information encoder posterior collapse previous models weaken capacity decoder force model use information latent variables however strategy ideal degrades quality generated text increases hyper parameters paper propose new vae text utilizing multimodal prior distribution modified encoder multi task learning show model generate well conditioned sentences without weakening capacity decoder also multimodal prior distribution improves interpretability acquired representations\n",
            "output sentence:  propose model variational autoencoders text modeling without weakening decoder improves quality text generation interpretability acquired representations \n",
            "\n",
            "{'rouge-1': {'r': 0.037037037037037035, 'p': 0.375, 'f': 0.06741572870092163}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.037037037037037035, 'p': 0.375, 'f': 0.06741572870092163}}\n",
            "pair:  optimal transport ot naturally arises many machine learning applications need handle cross modality data multiple sources yet heavy computational burden limits wide spread uses address scalability issue propose implicit generative learning based framework called spot scalable push forward optimal transport specifically approximate optimal transport plan pushforward reference distribution cast optimal transport problem minimax problem solve ot problems efficiently using primal dual stochastic gradient type algorithms also show recover density optimal transport plan using neural ordinary differential equations numerical experiments synthetic real datasets illustrate spot robust favorable convergence behavior spot also allows us efficiently sample optimal transport plan benefits downstream applications domain adaptation\n",
            "output sentence:  use gan based method scalably solve optimal transport \n",
            "\n",
            "{'rouge-1': {'r': 0.08450704225352113, 'p': 0.75, 'f': 0.15189873235699408}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.42857142857142855, 'f': 0.0666666652320988}, 'rouge-l': {'r': 0.056338028169014086, 'p': 0.5, 'f': 0.10126582096458903}}\n",
            "pair:  recent trends incorporating attention mechanisms vision led researchers reconsider supremacy convolutional layers primary building block beyond helping cnns handle long range dependencies ramachandran et al showed attention completely replace convolution achieve state art performance vision tasks raises question learned attention layers operate similarly convolutional layers work provides evidence attention layers perform convolution indeed often learn practice specifically prove multi head self attention layer sufficient number heads least expressive convolutional layer numerical experiments show self attention layers attend pixel grid patterns similarly cnn layers corroborating analysis code publicly available\n",
            "output sentence:  self attention layer perform convolution often learns practice \n",
            "\n",
            "{'rouge-1': {'r': 0.08823529411764706, 'p': 0.6666666666666666, 'f': 0.1558441537797268}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.25, 'f': 0.04123711188861734}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.4444444444444444, 'f': 0.10389610183167484}}\n",
            "pair:  generative adversarial networks gans learn map samples noise distribution chosen data distribution recent work demonstrated gans consequently sensitive limited shape noise distribution example single generator struggles map continuous noise uniform distribution discontinuous output separate gaussians complex output intersecting parabolas address problem learning generate multiple models generator output actually combination several distinct networks contribute novel formulation multi generator models learn prior generators conditioned noise parameterized neural network thus network learns optimal rate sample generator also optimally shapes noise received generator resulting noise prior gan npgan achieves expressivity flexibility surpasses single generator models previous multi generator models\n",
            "output sentence:  multi generator gan framework additional network learn prior input noise \n",
            "\n",
            "{'rouge-1': {'r': 0.12345679012345678, 'p': 0.5, 'f': 0.19801979880403883}, 'rouge-2': {'r': 0.028037383177570093, 'p': 0.15, 'f': 0.04724409183458382}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.45, 'f': 0.17821781860601904}}\n",
            "pair:  capsule group neurons whose outputs represent different properties entity layer capsule network contains many capsules describe version capsules capsule logistic unit represent presence entity matrix could learn represent relationship entity viewer pose capsule one layer votes pose matrix many different capsules layer multiplying pose matrix trainable viewpoint invariant transformation matrices could learn represent part whole relationships votes weighted assignment coefficient coefficients iteratively updated image using expectation maximization algorithm output capsule routed capsule layer receives cluster similar votes transformation matrices trained discriminatively backpropagating unrolled iterations em pair adjacent capsule layers smallnorb benchmark capsules reduce number test errors compared state art capsules also show far resistance white box adversarial attacks baseline convolutional neural network\n",
            "output sentence:  capsule networks learned pose matrices em routing improves state art classification smallnorb improves generalizability new view points white box adversarial robustness \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.8, 'f': 0.2758620661117717}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.3333333333333333, 'f': 0.09090908855371907}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.4, 'f': 0.13793103162901313}}\n",
            "pair:  improved generative adversarial network improved gan successful method using generative adversarial models solve problem semi supervised learning however suffers problem unstable training paper found instability mostly due vanishing gradients generator remedy issue propose new method use collaborative training improve stability semi supervised gan combination wasserstein gan experiments shown proposed method stable original improved gan achieves comparable classification accuracy different data sets\n",
            "output sentence:  improve training stability semi supervised generative adversarial networks collaborative training \n",
            "\n",
            "{'rouge-1': {'r': 0.031914893617021274, 'p': 0.3333333333333333, 'f': 0.05825242558959378}, 'rouge-2': {'r': 0.007518796992481203, 'p': 0.1111111111111111, 'f': 0.01408450585498919}, 'rouge-l': {'r': 0.02127659574468085, 'p': 0.2222222222222222, 'f': 0.03883494986143847}}\n",
            "pair:  long known single layer fully connected neural network prior parameters equivalent gaussian process gp limit infinite network width correspondence enables exact bayesian inference infinite width neural networks regression tasks means evaluating corresponding gp recently kernel functions mimic multi layer random neural networks developed outside bayesian framework previous work identified kernels used covariance functions gps allow fully bayesian prediction deep neural network work derive exact equivalence infinitely wide deep networks gps particular covariance function develop computationally efficient pipeline compute covariance function use resulting gp perform bayesian inference deep neural networks mnist cifar observe trained neural network accuracy approaches corresponding gp increasing layer width gp uncertainty strongly correlated trained network prediction error find test performance increases finite width trained networks made wider similar gp gp based predictions typically outperform finite width networks finally connect prior distribution weights variances gp formulation recent development signal propagation random neural networks\n",
            "output sentence:  show make predictions using deep networks without training deep networks \n",
            "\n",
            "{'rouge-1': {'r': 0.3548387096774194, 'p': 0.9166666666666666, 'f': 0.5116279029529476}, 'rouge-2': {'r': 0.19736842105263158, 'p': 0.625, 'f': 0.29999999635200003}, 'rouge-l': {'r': 0.25806451612903225, 'p': 0.6666666666666666, 'f': 0.3720930192320173}}\n",
            "pair:  interactive fiction games text based simulations agent interacts world purely natural language ideal environments studying extend reinforcement learning agents meet challenges natural language understanding partial observability action generation combinatorially large text based action spaces present kg agent builds dynamic knowledge graph exploring generates actions using template based action space contend dual uses knowledge graph reason game state constrain natural language generation keys scalable exploration combinatorially large natural language actions results across wide variety games show kg outperforms current agents despite exponential increase action space size\n",
            "output sentence:  present kg reinforcement learning agent builds dynamic knowledge graph exploring generates natural language using template based action space outperforming current agents wide set text based games \n",
            "\n",
            "{'rouge-1': {'r': 0.0641025641025641, 'p': 0.7142857142857143, 'f': 0.11764705731211073}, 'rouge-2': {'r': 0.028846153846153848, 'p': 0.5, 'f': 0.0545454535140496}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.7142857142857143, 'f': 0.11764705731211073}}\n",
            "pair:  neural embeddings used great success natural language processing nlp provide compact representations encapsulate word similarity attain state art performance range linguistic tasks success neural embeddings prompted significant amounts research applications domains language one domain graph structured data embeddings vertices learned encapsulate vertex similarity improve performance tasks including edge prediction vertex labelling nlp graph based tasks embeddings high dimensional euclidean spaces learned however recent work shown appropriate isometric space embedding complex networks flat euclidean space negatively curved hyperbolic space present new concept exploits recent insights propose learning neural embeddings graphs hyperbolic space provide experimental evidence hyperbolic embeddings significantly outperform euclidean embeddings vertex classification tasks several real world public datasets\n",
            "output sentence:  learn neural embeddings graphs hyperbolic instead euclidean space \n",
            "\n",
            "{'rouge-1': {'r': 0.1232876712328767, 'p': 0.6428571428571429, 'f': 0.2068965490236491}, 'rouge-2': {'r': 0.043478260869565216, 'p': 0.26666666666666666, 'f': 0.0747663527294961}, 'rouge-l': {'r': 0.1095890410958904, 'p': 0.5714285714285714, 'f': 0.1839080432765227}}\n",
            "pair:  rate medical questions asked online significantly exceeds capacity qualified people answer leaving many questions unanswered inadequately answered many questions unique reliable identification similar questions would enable efficient effective question answering schema many research efforts focused problem general question similarity approaches generalize well medical domain medical expertise often required determine semantic similarity paper show semi supervised approach pre training neural network medical question answer pairs particularly useful intermediate task ultimate goal determining medical question similarity pre training tasks yield accuracy task model achieves accuracy number training examples accuracy much smaller training set accuracy full corpus medical question answer data used\n",
            "output sentence:  show question answer matching particularly good pre training task question similarity release dataset medical question similarity \n",
            "\n",
            "{'rouge-1': {'r': 0.042105263157894736, 'p': 0.5, 'f': 0.0776699014798756}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.031578947368421054, 'p': 0.375, 'f': 0.05825242575172028}}\n",
            "pair:  make deep neural networks feasible resource constrained environments mobile devices beneficial quantize models using low precision weights one common technique quantizing neural networks straight gradient method enables back propagation quantization mapping despite empirical success little understood straight gradient method works building upon novel observation straight gradient method fact identical well known nesterov dual averaging algorithm quantization constrained optimization problem propose principled alternative approach called proxquant formulates quantized network training regularized learning problem instead optimizes via prox gradient method proxquant back propagation underlying full precision vector applies efficient prox operator stochastic gradient steps encourage quantizedness quantizing resnets lstms proxquant outperforms state art results binary quantization par state art multi bit quantization binary quantization analysis shows theoretically experimentally proxquant stable straight gradient method binaryconnect challenging indispensability straight gradient method providing powerful alternative\n",
            "output sentence:  principled framework model quantization using proximal gradient method \n",
            "\n",
            "{'rouge-1': {'r': 0.04477611940298507, 'p': 0.42857142857142855, 'f': 0.08108107936815197}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.029850746268656716, 'p': 0.2857142857142857, 'f': 0.05405405234112497}}\n",
            "pair:  task visually grounded dialog involves learning goal oriented cooperative dialog autonomous agents exchange information scene several rounds questions answers posit requiring agents adhere rules human language also maximizing information exchange ill posed problem observe humans stray common language social creatures communicate many people everyday far easier stick common language even cost efficiency loss using inspiration propose evaluate multi agent dialog framework agent interacts learns multiple agents show results relevant coherent dialog judged human evaluators without sacrificing task performance judged quantitative metrics\n",
            "output sentence:  social agents learn talk natural language towards goal \n",
            "\n",
            "{'rouge-1': {'r': 0.06329113924050633, 'p': 0.625, 'f': 0.11494252706566259}, 'rouge-2': {'r': 0.021739130434782608, 'p': 0.2857142857142857, 'f': 0.04040403908988883}, 'rouge-l': {'r': 0.05063291139240506, 'p': 0.5, 'f': 0.09195402131853615}}\n",
            "pair:  driven need parallelizable hyperparameter optimization methods paper studies open loop search methods sequences predetermined generated single configuration evaluated examples include grid search uniform random search low discrepancy sequences sampling distributions particular propose use determinantal point processes hyperparameter optimization via random search compared conventional uniform random search hyperparameter settings sampled independently dpp promotes diversity describe approach transforms hyperparameter search spaces efficient use dpp addition introduce novel metropolis hastings algorithm sample dpps defined space uniform samples drawn including spaces mixture discrete continuous dimensions tree structure experiments show significant benefits realistic scenarios limited budget training supervised learners whether serial parallel\n",
            "output sentence:  address fully parallel hyperparameter optimization determinantal point processes \n",
            "\n",
            "{'rouge-1': {'r': 0.16393442622950818, 'p': 0.9090909090909091, 'f': 0.27777777518904323}, 'rouge-2': {'r': 0.10294117647058823, 'p': 0.7, 'f': 0.17948717725180804}, 'rouge-l': {'r': 0.16393442622950818, 'p': 0.9090909090909091, 'f': 0.27777777518904323}}\n",
            "pair:  paper presents two methods disentangle interpret contextual effects encoded pre trained deep neural network unlike convolutional studies visualize image appearances corresponding network output neural activation global perspective research aims clarify certain input unit dimension collaborates units dimensions constitute inference patterns neural network thus contribute network output analysis local contextual effects certain input units special values real applications particular used methods explain gaming strategy alphago zero model experiments method successfully disentangled rationale move game\n",
            "output sentence:  paper presents methods disentangle interpret contextual effects encoded deep neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.07894736842105263, 'p': 0.6, 'f': 0.13953488166576528}, 'rouge-2': {'r': 0.033707865168539325, 'p': 0.3333333333333333, 'f': 0.061224488127863436}, 'rouge-l': {'r': 0.07894736842105263, 'p': 0.6, 'f': 0.13953488166576528}}\n",
            "pair:  convolutional neural networks cnns achieved state art performance recognizing representing audio images videos volumes domains input characterized regular graph structure however generalizing cnns irregular domains like meshes challenging additionally training data meshes often limited work generalize convolutional autoencoders mesh surfaces perform spectral decomposition meshes apply convolutions directly frequency space addition use max pooling introduce upsampling within network represent meshes low dimensional space construct complex dataset high resolution meshes extreme facial expressions encode using convolutional mesh autoencoder despite limited training data method outperforms state art pca models faces lower error using fewer parameters\n",
            "output sentence:  convolutional autoencoders generalized mesh surfaces encoding reconstructing extreme facial expressions \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.5, 'f': 0.19718309542551085}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.23076923076923078, 'f': 0.06593406348508643}, 'rouge-l': {'r': 0.10526315789473684, 'p': 0.42857142857142855, 'f': 0.16901408134100382}}\n",
            "pair:  graph neural networks gnns class deep models operates data arbitrary topology order invariant structure represented graphs introduce efficient memory layer gnns learn jointly perform graph representation learning graph pooling also introduce two new networks based memory layer memory based graph neural network memgnn graph memory network gmn learn hierarchical graph representations coarsening graph throughout layers memory experimental results demonstrate proposed models achieve state art results six seven graph classification regression benchmarks also show learned representations could correspond chemical features molecule data\n",
            "output sentence:  introduce efficient memory layer learn representation coarsen input graphs simultaneously without relying message passing \n",
            "\n",
            "{'rouge-1': {'r': 0.17857142857142858, 'p': 0.4166666666666667, 'f': 0.24999999580000004}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.10714285714285714, 'p': 0.25, 'f': 0.14999999580000012}}\n",
            "pair:  show ensemble functions leveraged effective exploration deep reinforcement learning build well established algorithms bandit setting adapt learning setting propose exploration strategy based upper confidence bounds ucb experiments show significant gains atari benchmark\n",
            "output sentence:  adapting ucb exploration ensemble learning improves prior methods double dqn atari benchmark \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.36363636363636365, 'f': 0.09195402077949535}, 'rouge-2': {'r': 0.01904761904761905, 'p': 0.2, 'f': 0.03478260710775055}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.36363636363636365, 'f': 0.09195402077949535}}\n",
            "pair:  interpreting generative adversarial network gan training approximate divergence minimization theoretically insightful spurred discussion lead theoretically practically interesting extensions gans wasserstein gans classic gans gans original variant training non saturating variant uses alternative form generator update original variant theoretically easier study alternative variant frequently performs better recommended use practice alternative generator update often regarded simple modification deal optimization issues appears common misconception two variants minimize divergence short note derive divergences approximately minimized original alternative variants gan gan training highlights important differences two variants example show alternative variant kl gan training actually minimizes reverse kl divergence alternative variant conventional gan training minimizes softened version reverse kl hope results may help clarify theoretical discussion surrounding divergence minimization view gan training\n",
            "output sentence:  typical gan training optimize jensen shannon something like reverse kl divergence \n",
            "\n",
            "{'rouge-1': {'r': 0.18518518518518517, 'p': 0.8333333333333334, 'f': 0.3030303000550964}, 'rouge-2': {'r': 0.15942028985507245, 'p': 0.8461538461538461, 'f': 0.26829268025877456}, 'rouge-l': {'r': 0.18518518518518517, 'p': 0.8333333333333334, 'f': 0.3030303000550964}}\n",
            "pair:  generating complex discrete distributions remains one challenging problems machine learning existing techniques generating complex distributions high degrees freedom depend standard generative models like generative adversarial networks gan wasserstein gan associated variations models based optimization involving distance two continuous distributions introduce discrete wasserstein gan dwgan model based dual formulation wasserstein distance two discrete distributions derive novel training algorithm corresponding network architecture based formulation experimental results provided synthetic discrete data real discretized data mnist handwritten digits\n",
            "output sentence:  propose discrete wasserstein gan dwgan model based dual formulation wasserstein distance two discrete distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.09433962264150944, 'p': 0.4166666666666667, 'f': 0.15384615083550301}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05660377358490566, 'p': 0.25, 'f': 0.09230768929704153}}\n",
            "pair:  present novel approach training neural abstract architectures corporates partial supervision machine interpretable components cleanly capture set neural architectures method applies introduce concept differential neural computational machine ncm show several existing architectures ntms nrams instantiated ncm thus benefit amount additional supervision interpretable components based method performed detailed experimental evaluation ntm nram architectures showed approach leads significantly better convergence generalization capabilities learning phase training using input output examples\n",
            "output sentence:  increase amount trace supervision possible utilize training fully differentiable neural machine architectures \n",
            "\n",
            "{'rouge-1': {'r': 0.14754098360655737, 'p': 0.6923076923076923, 'f': 0.24324324034696862}, 'rouge-2': {'r': 0.025974025974025976, 'p': 0.16666666666666666, 'f': 0.04494381789168046}, 'rouge-l': {'r': 0.08196721311475409, 'p': 0.38461538461538464, 'f': 0.13513513223886056}}\n",
            "pair:  paper introduce random path generative adversarial network rpgan alternative scheme gans serve tool generative model analysis latent space typical gan consists input vectors randomly sampled standard gaussian distribution latent space rpgan consists random paths generator network show design allows associate different layers generator different regions latent space providing natural interpretability experiments standard benchmarks demonstrate rpgan reveals several interesting insights roles different layers play image generation process aside interpretability rpgan model also provides competitive generation quality allows efficient incremental learning new data\n",
            "output sentence:  introduce alternative gan design based random routes generator serve tool generative models interpretability \n",
            "\n",
            "{'rouge-1': {'r': 0.0847457627118644, 'p': 0.47619047619047616, 'f': 0.14388488952124634}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.2857142857142857, 'f': 0.08633093268671403}}\n",
            "pair:  similar humans animals deep artificial neural networks exhibit critical periods temporary stimulus deficit impair development skill extent impairment depends onset length deficit window animal models size neural network deficits affect low level statistics vertical flipping images lasting effect performance overcome training better understand phenomenon use fisher information weights measure effective connectivity layers network training counterintuitively information rises rapidly early phases training decreases preventing redistribution information resources phenomenon refer loss information plasticity analysis suggests first epochs critical creation strong connections optimal relative input data distribution strong connections created appear change additional training findings suggest initial learning transient scrutinized compared asymptotic behavior plays key role determining outcome training process findings combined recent theoretical results literature also suggest forgetting decrease information weights critical achieving invariance disentanglement representation learning finally critical periods restricted biological systems emerge naturally learning systems whether biological artificial due fundamental constrains arising learning dynamics information processing\n",
            "output sentence:  sensory deficits early training phases lead irreversible performance loss artificial neuronal networks suggesting information phenomena common cause point importance initial forgetting \n",
            "\n",
            "{'rouge-1': {'r': 0.0392156862745098, 'p': 0.6666666666666666, 'f': 0.07407407302469136}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.4, 'f': 0.029197079588683488}, 'rouge-l': {'r': 0.029411764705882353, 'p': 0.5, 'f': 0.055555554506172854}}\n",
            "pair:  training activation quantized neural networks involves minimizing piecewise constant training loss whose gradient vanishes almost everywhere undesirable standard back propagation chain rule empirical way around issue use straight estimator ste bengio et al backward pass gradient modified chain rule becomes non trivial since unusual gradient certainly gradient loss function following question arises searching negative direction minimizes training loss paper provide theoretical justification concept ste answering question consider problem learning two linear layer network binarized relu activation gaussian input data shall refer unusual gradient given ste modifed chain rule coarse gradient choice ste unique prove ste properly chosen expected coarse gradient correlates positively population gradient available training negation descent direction minimizing population loss show associated coarse gradient descent algorithm converges critical point population loss minimization problem moreover show poor choice ste leads instability training algorithm near certain local minima verified cifar experiments\n",
            "output sentence:  make theoretical justification concept straight estimator \n",
            "\n",
            "{'rouge-1': {'r': 0.12121212121212122, 'p': 0.5333333333333333, 'f': 0.19753086117969826}, 'rouge-2': {'r': 0.0273972602739726, 'p': 0.14285714285714285, 'f': 0.0459770087937642}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.26666666666666666, 'f': 0.09876542908093287}}\n",
            "pair:  propose tackle time series regression problem computing temporal evolution probability density function provide probabilistic forecast recurrent neural network rnn based model employed learn nonlinear operator temporal evolution probability density function use softmax layer numerical discretization smooth probability density functions transforms function approximation problem classification task explicit implicit regularization strategies introduced impose smoothness condition estimated probability distribution monte carlo procedure compute temporal evolution distribution multiple step forecast presented evaluation proposed algorithm three synthetic two real data sets shows advantage compared baselines\n",
            "output sentence:  proposed rnn based algorithm estimate predictive distribution one multi step forecasts time series prediction problems \n",
            "\n",
            "{'rouge-1': {'r': 0.20270270270270271, 'p': 0.9375, 'f': 0.3333333304098766}, 'rouge-2': {'r': 0.16666666666666666, 'p': 0.9375, 'f': 0.2830188653613386}, 'rouge-l': {'r': 0.20270270270270271, 'p': 0.9375, 'f': 0.3333333304098766}}\n",
            "pair:  deep reinforcement learning algorithms learn complex behavioral skills real world application methods requires considerable amount experience collected agent practical settings robotics involves repeatedly attempting task resetting environment attempt however tasks easily automatically reversible practice learning process requires considerable human intervention work propose autonomous method safe efficient reinforcement learning simultaneously learns forward backward policy backward policy resetting environment subsequent attempt learning value function backward policy automatically determine forward policy enter non reversible state providing uncertainty aware safety aborts experiments illustrate proper use backward policy greatly reduce number manual resets required learn task reduce number unsafe actions lead non reversible states\n",
            "output sentence:  propose autonomous method safe efficient reinforcement learning simultaneously learns forward backward policy backward policy resetting environment attempt attempt \n",
            "\n",
            "{'rouge-1': {'r': 0.22641509433962265, 'p': 0.75, 'f': 0.347826083394245}, 'rouge-2': {'r': 0.1, 'p': 0.375, 'f': 0.15789473351800565}, 'rouge-l': {'r': 0.18867924528301888, 'p': 0.625, 'f': 0.2898550689014913}}\n",
            "pair:  many applications training data machine learning task partitioned across multiple nodes aggregating data may infeasible due storage communication privacy constraints work present good enough model spaces gems novel framework learning global satisficing good enough model within communication rounds carefully combining space local nodes satisficing models experiments benchmark medical datasets approach outperforms baseline aggregation techniques ensembling model averaging performs comparably ideal non distributed models\n",
            "output sentence:  present good enough model spaces gems framework learning aggregate model distributed nodes within small number communication rounds \n",
            "\n",
            "{'rouge-1': {'r': 0.11392405063291139, 'p': 0.6, 'f': 0.1914893590199185}, 'rouge-2': {'r': 0.019230769230769232, 'p': 0.13333333333333333, 'f': 0.03361344317491717}, 'rouge-l': {'r': 0.0759493670886076, 'p': 0.4, 'f': 0.12765957178587603}}\n",
            "pair:  investigate training performance generative adversarial networks using maximum mean discrepancy mmd critic termed mmd gans main theoretical contribution clarify situation bias gan loss functions raised recent work show gradient estimators used optimization process mmd gans wasserstein gans unbiased learning discriminator based samples leads biased gradients generator parameters also discuss issue kernel choice mmd critic characterize kernel corresponding energy distance used cram gan critic integral probability metric mmd benefits training strategies recently developed wasserstein gans experiments mmd gan able employ smaller critic network wasserstein gan resulting simpler faster training algorithm matching performance also propose improved measure gan convergence kernel inception distance show use dynamically adapt learning rates gan training\n",
            "output sentence:  explain bias situation mmd gans mmd gans work smaller critic networks wgan gps new gan evaluation metric \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.375, 'f': 0.1237113374513764}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.125, 'f': 0.03478260630018921}, 'rouge-l': {'r': 0.04938271604938271, 'p': 0.25, 'f': 0.08247422404931458}}\n",
            "pair:  deep learning training accesses vast amounts data high velocity posing challenges datasets retrieved commodity networks storage devices introduce way dynamically reduce overhead fetching transporting training data method term progressive compressed records pcrs pcrs deviate previous formats leveraging progressive compression split training example multiple examples increasingly higher fidelity without adding total data size training examples similar fidelity grouped together reduces system overhead data bandwidth needed train model show models trained aggressively compressed representations training data still retain high accuracy pcrs enable speedup average baseline formats using jpeg compression results hold across deep learning architectures wide range datasets imagenet ham stanford cars celeba hq\n",
            "output sentence:  propose simple general space efficient data format accelerate deep learning training allowing sample fidelity dynamically selected training time \n",
            "\n",
            "{'rouge-1': {'r': 0.17333333333333334, 'p': 0.7222222222222222, 'f': 0.2795698893513701}, 'rouge-2': {'r': 0.06593406593406594, 'p': 0.35294117647058826, 'f': 0.11111110845850486}, 'rouge-l': {'r': 0.09333333333333334, 'p': 0.3888888888888889, 'f': 0.15053763128685405}}\n",
            "pair:  estimating importance atom molecule one appealing challenging problems chemistry physics material engineering common way estimate atomic importance compute electronic structure using density functional theory dft interpret using domain knowledge human experts however conventional approach impractical large molecular database dft calculation requires huge computation specifically time complexity number electrons molecule furthermore calculation results interpreted human experts estimate atomic importance terms target molecular property tackle problem first exploit machine learning based approach atomic importance estimation end propose reverse self attention graph neural networks integrate graph based molecular description method provides efficiently automated target directed way estimate atomic importance without domain knowledge chemistry physics\n",
            "output sentence:  first propose fully automated target directed atomic importance estimator based graph neural networks new concept reverse self attention \n",
            "\n",
            "{'rouge-1': {'r': 0.10638297872340426, 'p': 1.0, 'f': 0.19230769056952665}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.5555555555555556, 'f': 0.08403361204717184}, 'rouge-l': {'r': 0.09574468085106383, 'p': 0.9, 'f': 0.1730769213387574}}\n",
            "pair:  partially observable markov decision processes pomdps widely used framework model decision making uncertainty environment stochastic outcome conventional pomdp models observations agent receives originate fixed known distribution however variety real world scenarios agent active role perception selecting observations receive due combinatorial nature selection process computationally intractable integrate perception decision planning decision prevent expansion action space propose greedy strategy observation selection aims minimize uncertainty state develop novel point based value iteration algorithm incorporates greedy strategy achieve near optimal uncertainty reduction sampled belief points turn enables solver efficiently approximate reachable subspace belief simplex essentially separating computations related perception planning lastly implement proposed solver demonstrate performance computational advantage range robotic scenarios robot simultaneously performs active perception planning\n",
            "output sentence:  develop point based value iteration solver pomdps active perception planning tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.14864864864864866, 'p': 0.7333333333333333, 'f': 0.24719100843327865}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.2857142857142857, 'f': 0.07692307459319533}, 'rouge-l': {'r': 0.12162162162162163, 'p': 0.6, 'f': 0.20224718820855958}}\n",
            "pair:  sparsely available data points cause numerical error finite differences hinder modeling dynamics physical systems discretization error becomes even larger sparse data irregularly distributed data defined unstructured grid making hard build deep learning models handle physics governing observations unstructured grid paper propose novel architecture named physics aware difference graph networks pa dgn exploits neighboring information learn finite differences inspired physics equations pa dgn leverages data driven end end learning discover underlying dynamical relations spatial temporal differences given observations demonstrate superiority pa dgn approximation directional derivatives prediction graph signals synthetic data real world climate observations weather stations\n",
            "output sentence:  propose physics aware difference graph networks designed effectively learn spatial differences modeling sparsely observed dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.061224489795918366, 'p': 0.42857142857142855, 'f': 0.10714285495535718}, 'rouge-2': {'r': 0.016666666666666666, 'p': 0.16666666666666666, 'f': 0.03030302865013783}, 'rouge-l': {'r': 0.04081632653061224, 'p': 0.2857142857142857, 'f': 0.0714285692410715}}\n",
            "pair:  field deep learning craving optimization method shows outstanding property optimization generalization propose method mathematical optimization based flows along geodesics shortest paths two points respect riemannian metric induced non linear function method flows refer exponentially decaying flows edf designed converge local solutions exponentially paper conduct experiments show high performance optimization benchmarks convergence properties well potential producing good machine learning benchmarks generalization properties\n",
            "output sentence:  introduction new optimization method application deep learning \n",
            "\n",
            "{'rouge-1': {'r': 0.07766990291262135, 'p': 0.5, 'f': 0.13445377918508583}, 'rouge-2': {'r': 0.015748031496062992, 'p': 0.125, 'f': 0.027972025984644867}, 'rouge-l': {'r': 0.05825242718446602, 'p': 0.375, 'f': 0.1008403338069346}}\n",
            "pair:  deep learning algorithms achieve high classification accuracy expense significant computation cost address cost number quantization schemeshave proposed techniques focused quantizing weights relatively smaller size compared activations paper proposes novel quantization scheme activations training enables neural networks work well ultra low precision weights activations without significant accuracy degradation technique parameterized clipping acti vation pact uses activation clipping parameter optimized duringtraining find right quantization scale pact allows quantizing activations toarbitrary bit precisions achieving much better accuracy relative publishedstate art quantization schemes show first time weights activations quantized bits precision still achieving accuracy comparable full precision networks across range popular models datasets also show exploiting reduced precision computational units hardware enable super linear improvement inferencing performance dueto significant reduction area accelerator compute engines coupled ability retain quantized model activation data chip memories\n",
            "output sentence:  new way quantizing activation deep neural network via parameterized clipping optimizes quantization scale via stochastic gradient descent \n",
            "\n",
            "{'rouge-1': {'r': 0.04477611940298507, 'p': 0.2727272727272727, 'f': 0.0769230745003288}, 'rouge-2': {'r': 0.012658227848101266, 'p': 0.1, 'f': 0.02247190811766209}, 'rouge-l': {'r': 0.04477611940298507, 'p': 0.2727272727272727, 'f': 0.0769230745003288}}\n",
            "pair:  address following question redundant parameterisation relu networks specifically consider transformations weight space leave function implemented network intact two transformations known feed forward architectures permutation neurons within layer positive scaling incoming weights neuron coupled inverse scaling outgoing weights work show architectures non increasing widths permutation scaling fact function preserving weight transformations eligible architecture give explicit construction neural network network implements function obtained original one application permutations rescaling proof relies geometric understanding boundaries linear regions relu networks hope developed mathematical tools independent interest\n",
            "output sentence:  prove exist relu networks whose parameters almost uniquely determined function implement \n",
            "\n",
            "{'rouge-1': {'r': 0.08235294117647059, 'p': 0.6363636363636364, 'f': 0.1458333313042535}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.2, 'f': 0.036697246039895715}, 'rouge-l': {'r': 0.07058823529411765, 'p': 0.5454545454545454, 'f': 0.12499999797092017}}\n",
            "pair:  deep learning demonstrated abilities learn complex structures restricted available data recently consensus networks cns proposed alleviate data sparsity utilizing features multiple modalities limited size labeled data paper extend cn transductive consensus networks tcns suitable semi supervised learning tcns different modalities input compressed latent representations encourage become indistinguishable iterative adversarial training understand tcns two mechanisms consensus classification put forward three variants ablation studies mechanisms investigate tcn models treat latent representations probability distributions measure similarities negative relative jensen shannon divergences show consensus state beneficial classification desires stable imperfect similarity representations overall tcns outperform align best benchmark algorithms given labeled samples bank marketing dementiabank datasets\n",
            "output sentence:  tcn multimodal semi supervised learning ablation study mechanisms interpretations latent representations \n",
            "\n",
            "{'rouge-1': {'r': 0.0958904109589041, 'p': 0.4666666666666667, 'f': 0.15909090626291325}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.2, 'f': 0.06122448720324875}, 'rouge-l': {'r': 0.0684931506849315, 'p': 0.3333333333333333, 'f': 0.11363636080836784}}\n",
            "pair:  develop novel efficient algorithm optimizing neural networks inspired recently proposed geodesic optimization algorithm algorithm call stochastic geodesic optimization sgeo utilizes adaptive coefficient top polyak heavy ball method effectively controlling amount weight put previous update parameters based change direction optimization path experimental results strongly convex functions lipschitz gradients deep autoencoder benchmarks show sgeo reaches lower errors established first order methods competes well lower similar errors recent second order method called fac kronecker factored approximate curvature also incorporate nesterov style lookahead gradient algorithm sgeo observe notable improvements\n",
            "output sentence:  utilize adaptive coefficient top regular momentum inspired geodesic optimization significantly speeds training convex non convex functions \n",
            "\n",
            "{'rouge-1': {'r': 0.23333333333333334, 'p': 0.9333333333333333, 'f': 0.37333333013333336}, 'rouge-2': {'r': 0.14925373134328357, 'p': 0.7142857142857143, 'f': 0.24691357738759337}, 'rouge-l': {'r': 0.21666666666666667, 'p': 0.8666666666666667, 'f': 0.3466666634666667}}\n",
            "pair:  reinforce used train models structured prediction settings directly optimize test time objective however common case sampling one prediction per datapoint input data inefficient show drawing multiple samples predictions per datapoint learn significantly less data freely obtain reinforce baseline reduce variance additionally derive reinforce estimator baseline based sampling without replacement combined recent technique sample sequences without replacement using stochastic beam search improves training procedure sequence model predicts solution travelling salesman problem\n",
            "output sentence:  show drawing multiple samples predictions per input datapoint learn less data freely obtain reinforce baseline \n",
            "\n",
            "{'rouge-1': {'r': 0.1511627906976744, 'p': 0.9285714285714286, 'f': 0.25999999759200004}, 'rouge-2': {'r': 0.10810810810810811, 'p': 0.9230769230769231, 'f': 0.19354838521982312}, 'rouge-l': {'r': 0.1511627906976744, 'p': 0.9285714285714286, 'f': 0.25999999759200004}}\n",
            "pair:  present adversarial exploration strategy simple yet effective imitation learning scheme incentivizes exploration environment without extrinsic reward human demonstration framework consists deep reinforcement learning drl agent inverse dynamics model contesting former collects training samples latter objective maximize error latter latter trained samples collected former generates rewards former fails predict actual action taken former competitive setting drl agent learns generate samples inverse dynamics model fails predict correctly inverse dynamics model learns adapt challenging samples propose reward structure ensures drl agent collects moderately hard samples overly hard ones prevent inverse model imitating effectively evaluate effectiveness method several openai gym robotic arm hand manipulation tasks number baseline models experimental results show method comparable directly trained expert demonstrations superior baselines even without human priors\n",
            "output sentence:  simple yet effective imitation learning scheme incentivizes exploration environment without extrinsic reward human demonstration \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}, 'rouge-2': {'r': 0.00980392156862745, 'p': 0.125, 'f': 0.018181816833057952}, 'rouge-l': {'r': 0.024096385542168676, 'p': 0.2222222222222222, 'f': 0.04347825910444242}}\n",
            "pair:  counterfactual regret minimization cfr successful algorithm finding approximate nash equilibria imperfect information games however cfr reliance full game tree traversals limits scalability generality therefore game state action space often abstracted simplified cfr resulting strategy mapped back full game requires extensive expert knowledge practical many games outside poker often converges highly exploitable policies recently proposed method deep cfr applies deep learning directly cfr allowing agent intrinsically abstract generalize state space samples without requiring expert knowledge paper introduce single deep cfr sd cfr variant deep cfr lower overall approximation error avoiding training average strategy network show sd cfr attractive theoretical perspective empirically outperforms deep cfr respect exploitability one one play poker\n",
            "output sentence:  better deep reinforcement learning algorithm approximate counterfactual regret minimization \n",
            "\n",
            "{'rouge-1': {'r': 0.26, 'p': 0.9285714285714286, 'f': 0.4062499965820313}, 'rouge-2': {'r': 0.140625, 'p': 0.6428571428571429, 'f': 0.23076922782380016}, 'rouge-l': {'r': 0.22, 'p': 0.7857142857142857, 'f': 0.34374999658203126}}\n",
            "pair:  catastrophic forgetting neural networks one well known problems continual learning previous attempts addressing problem focus preventing important weights changing methods often require task boundaries learn effectively support backward transfer learning paper propose meta learning algorithm learns reconstruct gradients old tasks current parameters combines reconstructed gradients current gradient enable continual learning backward transfer learning current task previous tasks experiments standard continual learning benchmarks show algorithm effectively prevent catastrophic forgetting supports backward transfer learning\n",
            "output sentence:  propose meta learning algorithm continual learning effectively prevent catastrophic forgetting problem support backward transfer learning \n",
            "\n",
            "{'rouge-1': {'r': 0.34782608695652173, 'p': 1.0, 'f': 0.5161290284287201}, 'rouge-2': {'r': 0.2727272727272727, 'p': 0.8823529411764706, 'f': 0.4166666630594136}, 'rouge-l': {'r': 0.34782608695652173, 'p': 1.0, 'f': 0.5161290284287201}}\n",
            "pair:  variational auto encoders vaes designed capture compressible information dataset consequence information stored latent space seldom sufficient reconstruct particular image help understand type information stored latent space train gan style decoder constrained produce images vae encoder map region latent space allows us imagine information captured latent space argue necessary make vae truly generative model use gan visualise latent space standard vae beta vae\n",
            "output sentence:  understand information stored latent space train gan style decoder constrained produce images vae encoder map region latent space space space \n",
            "\n",
            "{'rouge-1': {'r': 0.1625, 'p': 0.6842105263157895, 'f': 0.2626262595245384}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.3157894736842105, 'f': 0.09917355107164819}, 'rouge-l': {'r': 0.1125, 'p': 0.47368421052631576, 'f': 0.18181817871645756}}\n",
            "pair:  deep generative models achieved remarkable progress recent years despite progress quantitative evaluation comparison generative models remains one important challenges one popular metrics evaluating generative models log likelihood direct computation log likelihood intractable recently shown log likelihood interesting generative models variational autoencoders vae generative adversarial networks gan efficiently estimated using annealed importance sampling ais work argue log likelihood metric cannot represent different performance characteristics generative models propose use rate distortion curves evaluate compare deep generative models show approximate entire rate distortion curve using one single run ais roughly computational cost single log likelihood estimate evaluate lossy compression rates different deep generative models vaes gans variants adversarial autoencoders aae mnist cifar arrive number insights obtainable log likelihoods alone\n",
            "output sentence:  study rate distortion approximations evaluating deep generative models show rate distortion curves provide insights model log likelihood alone requiring roughly roughly \n",
            "\n",
            "{'rouge-1': {'r': 0.136986301369863, 'p': 0.5555555555555556, 'f': 0.21978021660669003}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.23529411764705882, 'f': 0.0747663524674645}, 'rouge-l': {'r': 0.0958904109589041, 'p': 0.3888888888888889, 'f': 0.15384615067262414}}\n",
            "pair:  semantic structure extraction spreadsheets includes detecting table regions recognizing structural components classifying cell types automatic semantic structure extraction key automatic data transformation various table structures canonical schema enable data analysis knowledge discovery however challenged diverse table structures spatial correlated semantics cell grids learn spatial correlations capture semantics spreadsheets developed novel learning based framework spreadsheet semantic structure extraction first propose multi task framework learns table region structural components cell types jointly second leverage advances recent language model capture semantics cell value third build large human labeled dataset broad coverage table structures evaluation shows proposed multi task framework highly effective outperforms results training task separately\n",
            "output sentence:  propose novel multi task framework learns table detection semantic component recognition cell type classification spreadsheet tables promising results \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.46153846153846156, 'f': 0.15189873142765586}, 'rouge-2': {'r': 0.04, 'p': 0.25, 'f': 0.06896551486325811}, 'rouge-l': {'r': 0.07575757575757576, 'p': 0.38461538461538464, 'f': 0.12658227573145336}}\n",
            "pair:  engineered proteins offer potential solve many problems biomedicine energy materials science creating designs succeed difficult practice significant aspect challenge complex coupling protein sequence structure task finding viable design often referred inverse protein folding problem develop generative models protein sequences conditioned graph structured specification design target approach efficiently captures complex dependencies proteins focusing long range sequence local space framework significantly improves upon prior parametric models protein sequences given structure takes step toward rapid targeted biomolecular design aid deep generative models\n",
            "output sentence:  learn conditionally generate protein sequences given structures model captures sparse long range dependencies \n",
            "\n",
            "{'rouge-1': {'r': 0.017391304347826087, 'p': 0.2857142857142857, 'f': 0.03278688416420321}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.017391304347826087, 'p': 0.2857142857142857, 'f': 0.03278688416420321}}\n",
            "pair:  historically pursuit efficient inference one driving forces hind research new deep learning architectures building blocks recent examples include squeeze excitation module hu et al depthwise separable convolutions xception chollet inverted bottleneck mobilenet sandler et al notably cases resulting building blocks enabled higher efficiency also higher accuracy found wide adoption field work expand arsenal efficient building blocks neural network architectures instead combining standard primitives convolution advocate replacement dense primitives sparse counterparts idea using sparsity decrease parameter count new mozer smolensky conventional wisdom reduction theoretical flops translate real world efficiency gains aim correct misconception introducing family efficient sparse kernels several hardware platforms plan open source benefit community equipped efficient implementation sparse primitives show sparse versions mobilenet mobilenet architectures substantially outperform strong dense baselines efficiency accuracy curve snapdragon sparse networks outperform dense equivalents equivalent approximately one entire generation improvement hope findings facilitate wider adoption sparsity tool creating efficient accurate deep learning architectures\n",
            "output sentence:  sparse mobilenets faster dense ones appropriate kernels \n",
            "\n",
            "{'rouge-1': {'r': 0.15476190476190477, 'p': 0.6190476190476191, 'f': 0.24761904441904764}, 'rouge-2': {'r': 0.05357142857142857, 'p': 0.2857142857142857, 'f': 0.09022556125049473}, 'rouge-l': {'r': 0.10714285714285714, 'p': 0.42857142857142855, 'f': 0.17142856822857147}}\n",
            "pair:  model free reinforcement learning rl proven powerful general tool learning complex behaviors however sample efficiency often impractically large solving challenging real world problems even policy algorithms learning limiting factor classic model free rl learning signal consists scalar rewards ignoring much rich information contained state transition tuples model based rl uses information training predictive model often achieve asymptotic performance model free rl due model bias introduce temporal difference models tdms family goal conditioned value functions trained model free learning used model based control tdms combine benefits model free model based rl leverage rich information state transitions learn efficiently still attaining asymptotic performance exceeds direct model based rl methods experimental results show range continuous control tasks tdms provide substantial improvement efficiency compared state art model based model free methods\n",
            "output sentence:  show special goal condition value function trained model free methods used within model based control resulting substantially better sample efficiency performance performance \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.6, 'f': 0.1499999978125}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.1111111111111111, 'f': 0.019417474133283194}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.5, 'f': 0.12499999781250003}}\n",
            "pair:  deep learning incredibly successful modeling tasks large carefully curated labeled datasets application problems limited labeled data remains challenge aim present work improve label efficiency large neural networks operating audio data combination multitask learning self supervised learning unlabeled data trained end end audio feature extractor based wavenet feeds simple yet versatile task specific neural networks describe several easily implemented self supervised learning tasks operate large unlabeled audio corpus demonstrate scenarios limited labeled training data one significantly improve performance three different supervised classification tasks individually simultaneous training additional self supervised tasks also show incorporating data augmentation multitask setting leads even gains performance\n",
            "output sentence:  label efficient audio classification via multi task learning self supervision \n",
            "\n",
            "{'rouge-1': {'r': 0.1044776119402985, 'p': 0.7, 'f': 0.18181817955810425}, 'rouge-2': {'r': 0.0375, 'p': 0.3333333333333333, 'f': 0.06741572851912643}, 'rouge-l': {'r': 0.08955223880597014, 'p': 0.6, 'f': 0.15584415358407824}}\n",
            "pair:  multi relational graph embedding aims achieving effective representations reduced low dimensional parameters widely used knowledge base completion although knowledge base data usually contains tree like cyclic structure none existing approaches embed data compatible space line structure overcome problem novel framework called riemannian transe proposed paper embed entities riemannian manifold riemannian transe models relation move point defines specific novel distance dissimilarity relation relations naturally embedded correspondence structure data experiments several knowledge base completion tasks shown based appropriate choice manifold riemannian transe achieves good performance even significantly reduced parameters\n",
            "output sentence:  multi relational graph embedding riemannian manifolds transe like loss function \n",
            "\n",
            "{'rouge-1': {'r': 0.12222222222222222, 'p': 0.7333333333333333, 'f': 0.20952380707482993}, 'rouge-2': {'r': 0.02727272727272727, 'p': 0.2, 'f': 0.04799999788800009}, 'rouge-l': {'r': 0.1, 'p': 0.6, 'f': 0.17142856897959186}}\n",
            "pair:  network quantization model compression acceleration technique become essential neural network deployment quantization methods per form fine tuning pretrained network sometimes results large loss accuracy compared original network introduce new technique train quantization friendly networks directly converted accurate quantized network without need additional fine tuning technique allows quantizing weights activations network layers bits achieving high efficiency facilitating deployment practical settings com pared fully quantized networks operating bits show substantial improvements accuracy example top accuracy imagenet using resnet compared previous state art accuracy louizos et al full precision reference accuracy performed thorough set experiments test efficacy method also conducted ablation studies different aspects method techniques improve training stability accuracy codebase trained models available github\n",
            "output sentence:  train accurate fully quantized networks using loss function maximizing full precision model accuracy minimizing full precision networks \n",
            "\n",
            "{'rouge-1': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.16666666666666666, 'f': 0.04081632438150782}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}}\n",
            "pair:  driving force behind recent success lstms ability learn complex non linear relationships consequently inability describe relationships led lstms characterized black boxes end introduce contextual decomposition cd interpretation algorithm analysing individual predictions made standard lstms without changes underlying model decomposing output lstm cd captures contributions combinations words variables final prediction lstm task sentiment analysis yelp sst data sets show cd able reliably identify words phrases contrasting sentiment combined yield lstm final prediction using phrase level labels sst also demonstrate cd able successfully extract positive negative negations lstm something previously done\n",
            "output sentence:  introduce contextual decompositions interpretation algorithm lstms capable extracting word phrase interaction level importance score \n",
            "\n",
            "{'rouge-1': {'r': 0.23728813559322035, 'p': 0.9333333333333333, 'f': 0.37837837514609207}, 'rouge-2': {'r': 0.1267605633802817, 'p': 0.6, 'f': 0.2093023227014603}, 'rouge-l': {'r': 0.22033898305084745, 'p': 0.8666666666666667, 'f': 0.35135134811906504}}\n",
            "pair:  work first conduct mathematical analysis memory defined function maps element sequence current output three rnn cells namely simple recurrent neural network srn long short term memory lstm gated recurrent unit gru based analysis propose new design called extended long short term memory elstm extend memory length cell next present multi task rnn model robust previous erroneous predictions called dependent bidirectional recurrent neural network dbrnn sequence sequenceout siso problem finally performance dbrnn model elstm cell demonstrated experimental results\n",
            "output sentence:  recurrent neural network cell extended long short term memory multi task rnn model sequence sequence problems \n",
            "\n",
            "{'rouge-1': {'r': 0.16923076923076924, 'p': 0.9166666666666666, 'f': 0.28571428308315067}, 'rouge-2': {'r': 0.0975609756097561, 'p': 0.7272727272727273, 'f': 0.17204300866689792}, 'rouge-l': {'r': 0.16923076923076924, 'p': 0.9166666666666666, 'f': 0.28571428308315067}}\n",
            "pair:  study implicit bias gradient descent methods solving binary classification problem linearly separable dataset classifier described nonlinear relu model objective function adopts exponential loss function first characterize landscape loss function show exist spurious asymptotic local minima besides asymptotic global minima show gradient descent gd converge either global local max margin direction may diverge desired max margin direction general context stochastic gradient descent sgd show converges expectation either global local max margin direction sgd converges explore implicit bias algorithms learning multi neuron network certain stationary conditions show learned classifier maximizes margins sample pattern partition relu activation\n",
            "output sentence:  study implicit bias gradient methods solving binary classification problem nonlinear relu models \n",
            "\n",
            "{'rouge-1': {'r': 0.03296703296703297, 'p': 0.21428571428571427, 'f': 0.05714285483174612}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03296703296703297, 'p': 0.21428571428571427, 'f': 0.05714285483174612}}\n",
            "pair:  study problem model extraction natural language processing adversary query access victim model attempts reconstruct local copy model assuming adversary victim model fine tune large pretrained language model bert devlin et al show adversary need real training data successfully mount attack fact attacker need even use grammatical semantically meaningful queries show random sequences words coupled task specific heuristics form effective queries model extraction diverse set nlp tasks including natural language inference question answering work thus highlights exploit made feasible shift towards transfer learning methods within nlp community query budget hundred dollars attacker extract model performs slightly worse victim model finally study two defense strategies model extraction membership classification api watermarking successful adversaries also circumvented clever ones\n",
            "output sentence:  outputs modern nlp apis nonsensical text provide strong signals model internals allowing adversaries steal apis \n",
            "\n",
            "{'rouge-1': {'r': 0.07894736842105263, 'p': 0.6666666666666666, 'f': 0.1411764686948097}, 'rouge-2': {'r': 0.021505376344086023, 'p': 0.2222222222222222, 'f': 0.03921568466551333}, 'rouge-l': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}}\n",
            "pair:  proliferation models natural language processing nlp tasks even harder understand differences models relative merits simply looking differences holistic metrics accuracy bleu tell us emph emph particular method better dataset biases influence choices model design paper present general methodology emph interpretable evaluation nlp systems choose task named entity recognition ner case study core task identifying people places organizations text proposed evaluation method enables us interpret textit model biases textit dataset biases emph differences datasets affect design models identifying strengths weaknesses current approaches making analysis tool available make easy future researchers run similar analyses drive progress area\n",
            "output sentence:  propose generalized evaluation methodology interpret model biases dataset biases correlation \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.625, 'f': 0.13157894548476456}, 'rouge-2': {'r': 0.03260869565217391, 'p': 0.42857142857142855, 'f': 0.060606059291909015}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.625, 'f': 0.13157894548476456}}\n",
            "pair:  effectively capturing graph node sequences form vector embeddings critical many applications achieve first learning vector embeddings single graph nodes ii composing compactly represent node sequences specifically propose sense semantically enhanced node sequence embedding single nodes skip gram based novel embedding mechanism single graph nodes co learns graph structure well textual descriptions demonstrate sense vectors increase accuracy multi label classification tasks link prediction tasks variety scenarios using real datasets based sense next propose generic sense compute composite vectors represent sequence nodes preserving node order important prove approach efficient embedding node sequences experiments real data confirm high accuracy node order decoding\n",
            "output sentence:  node sequence embedding mechanism captures graph text properties \n",
            "\n",
            "{'rouge-1': {'r': 0.0875, 'p': 0.7777777777777778, 'f': 0.15730336896856456}, 'rouge-2': {'r': 0.02040816326530612, 'p': 0.2222222222222222, 'f': 0.037383176029347605}, 'rouge-l': {'r': 0.05, 'p': 0.4444444444444444, 'f': 0.08988763863148597}}\n",
            "pair:  paper present new generative model learning latent embeddings compared classical generative process observed data point generated individual latent variable approach assumes global latent variable generate whole set observed data points propose learning objective derived approximation lower bound data log likelihood leading algorithm wise ale compared standard elbo objective variational posterior data point encouraged match prior distribution wise ale objective matches averaged posterior samples prior allowing sample wise posterior distributions wider range acceptable embedding mean variance leading better reconstruction quality auto encoding process various examples comparison state art vae models demonstrate wise ale excellent information embedding properties whilst still retaining ability learn smooth compact representation\n",
            "output sentence:  propose new latent variable model learn latent embeddings high dimensional data \n",
            "\n",
            "{'rouge-1': {'r': 0.14864864864864866, 'p': 0.7333333333333333, 'f': 0.24719100843327865}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.2857142857142857, 'f': 0.07692307459319533}, 'rouge-l': {'r': 0.12162162162162163, 'p': 0.6, 'f': 0.20224718820855958}}\n",
            "pair:  sparsely available data points cause numerical error finite differences hinder modeling dynamics physical systems discretization error becomes even larger sparse data irregularly distributed data defined unstructured grid making hard build deep learning models handle physics governing observations unstructured grid paper propose novel architecture named physics aware difference graph networks pa dgn exploits neighboring information learn finite differences inspired physics equations pa dgn leverages data driven end end learning discover underlying dynamical relations spatial temporal differences given observations demonstrate superiority pa dgn approximation directional derivatives prediction graph signals synthetic data real world climate observations weather stations\n",
            "output sentence:  propose physics aware difference graph networks designed effectively learn spatial differences modeling sparsely observed dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.5555555555555556, 'f': 0.14285714061632654}, 'rouge-2': {'r': 0.028985507246376812, 'p': 0.25, 'f': 0.051948050086017945}, 'rouge-l': {'r': 0.08196721311475409, 'p': 0.5555555555555556, 'f': 0.14285714061632654}}\n",
            "pair:  size complexity models datasets grow need communication efficient variants stochastic gradient descent deployed clusters perform model fitting parallel alistarh et al describe two variants data parallel sgd quantize encode gradients lessen communication costs first variant qsgd provide strong theoretical guarantees second variant call qsgdinf demonstrate impressive empirical gains distributed training large neural networks building work propose alternative scheme quantizing gradients show yields stronger theoretical guarantees exist qsgd matching empirical performance qsgdinf\n",
            "output sentence:  nuqsgd closes gap theoretical guarantees qsgd empirical performance qsgdinf \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.5454545454545454, 'f': 0.10619468850810558}, 'rouge-2': {'r': 0.007407407407407408, 'p': 0.09090909090909091, 'f': 0.013698628743666872}, 'rouge-l': {'r': 0.0392156862745098, 'p': 0.36363636363636365, 'f': 0.07079645841961005}}\n",
            "pair:  many tasks natural language understanding require learning relationships two sequences various tasks natural language inference paraphrasing entailment aforementioned tasks similar nature yet often modeled individually knowledge transfer effective closely related tasks usually carried using parameter transfer neural networks however transferring parameters irrelevant target task lead sub optimal results negative effect performance referred textit negative transfer hence paper focuses transferability instances parameters across natural language understanding tasks proposing ensemble based transfer learning method context shot learning main contribution method mitigating negative transfer across tasks using neural networks involves dynamically bagging small recurrent neural networks trained different subsets source task present straightforward yet novel approach incorporating networks target task shot learning using decaying parameter chosen according slope changes smoothed spline error curve sub intervals training proposed method show improvements hard soft parameter sharing transfer methods shot learning case shows competitive performance models trained given full supervision target task examples\n",
            "output sentence:  dynamic bagging methods approach avoiding negatve transfer neural network shot transfer learning \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.8235294117647058, 'f': 0.2772277199725517}, 'rouge-2': {'r': 0.05172413793103448, 'p': 0.375, 'f': 0.0909090887786961}, 'rouge-l': {'r': 0.13095238095238096, 'p': 0.6470588235294118, 'f': 0.21782177937849234}}\n",
            "pair:  algorithm introduced learning predictive state representation policy temporal difference td learning used learn steer vehicle reinforcement learning three components learned simultaneously policy predictions compact representation state behavior policy distribution estimating policy predictions deterministic policy gradient learning act behavior policy discriminator learned used estimating important sampling ratios needed learn predictive representation policy general value functions gvfs linear deterministic policy gradient method used train agent predictive representations predictions learned three components combined demonstrated evaluated problem steering vehicle images torcs racing simulator environment steering images challenging problem evaluation completed held set tracks never seen training order measure generalization predictions controller experiments show proposed method able steer smoothly navigate many tracks available torcs performance exceeds ddpg using images input approaches performance ideal non vision based kinematics model\n",
            "output sentence:  algorithm learn predictive state representation general value functions policy learning applied problem vision based steering autonomous driving \n",
            "\n",
            "{'rouge-1': {'r': 0.0784313725490196, 'p': 0.5333333333333333, 'f': 0.1367521345167653}, 'rouge-2': {'r': 0.024793388429752067, 'p': 0.2, 'f': 0.04411764509623711}, 'rouge-l': {'r': 0.049019607843137254, 'p': 0.3333333333333333, 'f': 0.08547008323471406}}\n",
            "pair:  present simple nearest neighbor nn approach synthesizes high frequency photorealistic images incomplete signal low resolution image surface normal map edges current state art deep generative models designed conditional image synthesis lack two important things unable generate large set diverse outputs due mode collapse problem interpretable making difficult control synthesized output demonstrate nn approaches potentially address limitations suffer accuracy small datasets design simple pipeline combines best worlds first stage uses convolutional neural network cnn map input overly smoothed image second stage uses pixel wise nearest neighbor method map smoothed output multiple high quality high frequency outputs controllable manner importantly pixel wise matching allows method compose novel high frequency content cutting pasting pixels different training exemplars demonstrate approach various input modalities various domains ranging human faces pets shoes handbags\n",
            "output sentence:  pixel wise nearest neighbors used generating multiple images incomplete priors low res images surface normals edges etc \n",
            "\n",
            "{'rouge-1': {'r': 0.08737864077669903, 'p': 0.5625, 'f': 0.15126050187416146}, 'rouge-2': {'r': 0.032, 'p': 0.25, 'f': 0.05673758664051111}, 'rouge-l': {'r': 0.06796116504854369, 'p': 0.4375, 'f': 0.11764705649601022}}\n",
            "pair:  hamiltonian formalism plays central role classical quantum physics hamiltonians main tool modelling continuous time evolution systems conserved quantities come equipped many useful properties like time reversibility smooth interpolation time properties important many machine learning problems sequence prediction reinforcement learning density modelling typically provided box standard tools recurrent neural networks paper introduce hamiltonian generative network hgn first approach capable consistently learning hamiltonian dynamics high dimensional observations images without restrictive domain assumptions trained use hgn sample new trajectories perform rollouts forward backward time even speed slow learned dynamics demonstrate simple modification network architecture turns hgn powerful normalising flow model called neural hamiltonian flow nhf uses hamiltonian dynamics model expressive densities hence hope work serves first practical demonstration value hamiltonian formalism bring machine learning results video evaluations available http tiny cc hgn\n",
            "output sentence:  introduce class generative models reliably learn hamiltonian dynamics high dimensional observations learnt hamiltonian applied sequence modeling normalising flow \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.5, 'f': 0.1408450680023805}, 'rouge-2': {'r': 0.039473684210526314, 'p': 0.3, 'f': 0.06976743980530022}, 'rouge-l': {'r': 0.08196721311475409, 'p': 0.5, 'f': 0.1408450680023805}}\n",
            "pair:  propose novel unsupervised generative model elastic infogan learns disentangle object identity low level aspects class imbalanced datasets first investigate issues surrounding assumptions uniformity made infogan demonstrate ineffectiveness properly disentangle object identity imbalanced data key idea make discovery discrete latent factor variation invariant identity preserving transformations real images use signal learn latent distribution parameters experiments artificial mnist real world youtube faces datasets demonstrate effectiveness approach imbalanced data better disentanglement object identity latent factor variation ii better approximation class imbalance data reflected learned parameters latent distribution\n",
            "output sentence:  elastic infogan modification infogan learns without supervision disentangled representations class imbalanced data \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.625, 'f': 0.13157894548476456}, 'rouge-2': {'r': 0.03260869565217391, 'p': 0.42857142857142855, 'f': 0.060606059291909015}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.625, 'f': 0.13157894548476456}}\n",
            "pair:  effectively capturing graph node sequences form vector embeddings critical many applications achieve first learning vector embeddings single graph nodes ii composing compactly represent node sequences specifically propose sense semantically enhanced node sequence embedding single nodes skip gram based novel embedding mechanism single graph nodes co learns graph structure well textual descriptions demonstrate sense vectors increase accuracy multi label classification tasks link prediction tasks variety scenarios using real datasets based sense next propose generic sense compute composite vectors represent sequence nodes preserving node order important prove approach efficient embedding node sequences experiments real data confirm high accuracy node order decoding\n",
            "output sentence:  node sequence embedding mechanism captures graph text properties \n",
            "\n",
            "{'rouge-1': {'r': 0.15, 'p': 0.8, 'f': 0.25263157628808863}, 'rouge-2': {'r': 0.08163265306122448, 'p': 0.5714285714285714, 'f': 0.14285714066964286}, 'rouge-l': {'r': 0.1375, 'p': 0.7333333333333333, 'f': 0.23157894470914128}}\n",
            "pair:  deep infomax dim unsupervised representation learning framework maximizing mutual information inputs outputs encoder probabilistic constraints imposed outputs paper propose supervised deep infomax sdim introduces supervised probabilistic constraints encoder outputs supervised probabilistic constraints equivalent generative classifier high level data representations class conditional log likelihoods samples evaluated unlike works building generative classifiers conditional generative models sdims scale complex datasets achieve comparable performance discriminative counterparts sdim could perform emph classification rejection ninstead always reporting class label sdim makes predictions test samples largest logits surpass pre chosen thresholds otherwise deemed data distributions rejected experiments show sdim rejection policy effectively reject illegal inputs including distribution samples adversarial examples\n",
            "output sentence:  scale generative classifiers complex datasets evaluate effectiveness reject illegal inputs including distribution samples adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.18181818181818182, 'p': 0.7142857142857143, 'f': 0.2898550692291536}, 'rouge-2': {'r': 0.0625, 'p': 0.38461538461538464, 'f': 0.10752687931552786}, 'rouge-l': {'r': 0.14545454545454545, 'p': 0.5714285714285714, 'f': 0.2318840547363999}}\n",
            "pair:  glove skip gram word embedding methods learn word vectors decomposing denoised matrix word co occurrences product low rank matrices work propose iterative algorithm computing word vectors based modeling word co occurrence matrices generalized low rank models algorithm generalizes skip gram glove well giving rise embedding methods based specified co occurrence matrix distribution co occurences number iterations iterative algorithm example using tweedie distribution one iteration results glove using multinomial distribution full convergence mode results skip gram experimental results demonstrate multiple iterations algorithm improves results glove method google word analogy similarity task\n",
            "output sentence:  present novel iterative algorithm based generalized low rank models computing interpreting word embedding models \n",
            "\n",
            "{'rouge-1': {'r': 0.16923076923076924, 'p': 0.7333333333333333, 'f': 0.27499999695312505}, 'rouge-2': {'r': 0.06329113924050633, 'p': 0.35714285714285715, 'f': 0.10752687916290908}, 'rouge-l': {'r': 0.15384615384615385, 'p': 0.6666666666666666, 'f': 0.24999999695312503}}\n",
            "pair:  propose new learning based approach solve ill posed inverse problems imaging address case ground truth training samples rare problem severely ill posed underlying physics get measurements setting common geophysical imaging remote sensing show case common approach directly learn mapping measured data reconstruction becomes unstable instead propose first learn ensemble simpler mappings data projections unknown image random piecewise constant subspaces combine projections form final reconstruction solving deconvolution like problem show experimentally proposed method robust measurement noise corruptions seen training directly learned inverse\n",
            "output sentence:  solve ill posed inverse problems scarce ground truth examples estimating ensemble random projections model instead model \n",
            "\n",
            "{'rouge-1': {'r': 0.1044776119402985, 'p': 0.5384615384615384, 'f': 0.17499999727812504}, 'rouge-2': {'r': 0.0379746835443038, 'p': 0.25, 'f': 0.06593406364448746}, 'rouge-l': {'r': 0.08955223880597014, 'p': 0.46153846153846156, 'f': 0.14999999727812502}}\n",
            "pair:  recent work cross lingual word embeddings severely anglocentric vast majority lexicon induction evaluation dictionaries english another language english embedding space selected default hub learning multilingual setting work however challenge practices first show choice hub language significantly impact downstream lexicon induction performance second expand current evaluation dictionary collection include language pairs using triangulation also create new dictionaries represented languages evaluating established methods language pairs sheds light suitability presents new challenges field finally analysis identify general guidelines strong cross lingual embeddings baselines based anglocentric experiments\n",
            "output sentence:  choice hub target language affects quality cross lingual embeddings evaluated english centric dictionaries \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.625, 'f': 0.11363636198347109}, 'rouge-2': {'r': 0.010752688172043012, 'p': 0.14285714285714285, 'f': 0.019999998698000086}, 'rouge-l': {'r': 0.05, 'p': 0.5, 'f': 0.09090908925619837}}\n",
            "pair:  click rate ctr prediction critical task industrial applications especially online social commerce applications challenging find proper way automatically discover effective cross features ctr tasks propose novel model ctr tasks called deep neural networks encoder enhanced factorization machine deepenfm instead learning cross features directly deepenfm adopts transformer encoder backbone align feature embeddings clues fields embeddings generated encoder beneficial feature interactions particularly deepenfm utilizes bilinear approach generate different similarity functions respect different field pairs furthermore max pooling method makes deepenfm feasible capture supplementary suppressing information among different attention heads model validated criteo avazu datasets achieves state art performance\n",
            "output sentence:  dnn encoder enhanced fm bilinear attention max pooling ctr \n",
            "\n",
            "{'rouge-1': {'r': 0.08695652173913043, 'p': 0.5, 'f': 0.1481481456241427}, 'rouge-2': {'r': 0.007936507936507936, 'p': 0.06666666666666667, 'f': 0.01418439526180801}, 'rouge-l': {'r': 0.05434782608695652, 'p': 0.3125, 'f': 0.09259259006858717}}\n",
            "pair:  ensembles multiple neural networks trained individually predictions averaged shown widely successful improving accuracy predictive uncertainty single neural networks however ensemble cost training testing increases linearly number networks paper propose batchensemble ensemble method whose computational memory costs significantly lower typical ensembles batchensemble achieves defining weight matrix hadamard product shared weight among ensemble members rank one matrix per member unlike ensembles batchensemble parallelizable across devices one device trains one member also parallelizable within device multiple ensemble members updated simultaneously given mini batch across cifar cifar wmt en de en fr translation contextual bandits tasks batchensemble yields competitive accuracy uncertainties typical ensembles speedup test time memory reduction ensemble size also apply batchensemble lifelong learning split cifar batchensemble yields comparable performance progressive neural networks much lower computational memory costs show batchensemble easily scale lifelong learning split imagenet involves sequential learning tasks\n",
            "output sentence:  introduced batchensemble efficient method ensembling lifelong learning used improve accuracy uncertainty neural network like typical ensemble \n",
            "\n",
            "{'rouge-1': {'r': 0.12727272727272726, 'p': 0.5833333333333334, 'f': 0.2089552209400757}, 'rouge-2': {'r': 0.04411764705882353, 'p': 0.2727272727272727, 'f': 0.07594936469155592}, 'rouge-l': {'r': 0.12727272727272726, 'p': 0.5833333333333334, 'f': 0.2089552209400757}}\n",
            "pair:  holistically exploring perceptual neural representations underlying animal communication traditionally difficult complexity underlying signal present novel set techniques project entire communicative repertoires low dimensional spaces systematically sampled exploring relationship perceptual representations neural representations latent representational spaces learned machine learning algorithms showcase method one ongoing experiment studying sequential temporal maintenance context songbird neural perceptual representations syllables discuss studying neural mechanisms underlying maintenance long range information content present birdsong inform informed machine sequence modeling\n",
            "output sentence:  compare perceptual neural modeled representations animal communication using machine learning behavior physiology \n",
            "\n",
            "{'rouge-1': {'r': 0.027777777777777776, 'p': 0.2, 'f': 0.04878048566329575}, 'rouge-2': {'r': 0.01098901098901099, 'p': 0.1, 'f': 0.019801978413881156}, 'rouge-l': {'r': 0.027777777777777776, 'p': 0.2, 'f': 0.04878048566329575}}\n",
            "pair:  convolutional neural network cnn successfully applied many fields recent decades however lacks ability utilize prior domain knowledge dealing many realistic problems present framework called geometric operator convolutional neural network go cnn uses domain knowledge wherein kernel first convolutional layer replaced kernel generated geometric operator function framework integrates many conventional geometric operators allows adapt diverse range problems certain conditions theoretically analyze convergence bound generalization errors go cnns common cnns although geometric operator convolution kernels fewer trainable parameters common convolution kernels experimental results indicate go cnn performs accurately common cnn cifar furthermore go cnn reduces dependence amount training examples enhances adversarial stability\n",
            "output sentence:  traditional image processing algorithms combined convolutional neural networks new neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.11956521739130435, 'p': 0.8461538461538461, 'f': 0.20952380735419504}, 'rouge-2': {'r': 0.04504504504504504, 'p': 0.4166666666666667, 'f': 0.0813008112472735}, 'rouge-l': {'r': 0.09782608695652174, 'p': 0.6923076923076923, 'f': 0.17142856925895694}}\n",
            "pair:  hierarchical reinforcement learning methods offer powerful means planning flexible behavior complicated domains however learning appropriate hierarchical decomposition domain subtasks remains substantial challenge present novel algorithm subtask discovery based recently introduced multitask linearly solvable markov decision process mlmdp framework mlmdp perform never seen tasks representing linear combination previously learned basis set tasks setting subtask discovery problem naturally posed finding optimal low rank approximation set tasks agent face domain use non negative matrix factorization discover minimal basis set tasks show technique learns intuitive decompositions variety domains method several qualitatively desirable features limited learning subtasks single goal states instead learning distributed patterns preferred states learns qualitatively different hierarchical decompositions domain depending ensemble tasks agent face may straightforwardly iterated obtain deeper hierarchical decompositions\n",
            "output sentence:  present novel algorithm hierarchical subtask discovery leverages multitask linear markov decision process framework \n",
            "\n",
            "{'rouge-1': {'r': 0.05128205128205128, 'p': 0.5, 'f': 0.0930232541265549}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.038461538461538464, 'p': 0.375, 'f': 0.06976744017306657}}\n",
            "pair:  present method policy learning navigate indoor environments adopt hierarchical policy approach two agents trained work cohesion one another perform complex navigation task planner agent operates higher level proposes sub goals executor agent executor reports embedding summary back planner additional side information end series operations planner next sub goal proposal end goal generated environment exposed planner decides set sub goals propose executor show planner executor setup drastically increases sample efficiency method traditional single agent approaches effectively mitigating difficulty accompanying long series actions sparse reward signal challenging habitat environment requires navigating various realistic indoor environments demonstrate approach offers significant improvement prior work navigation\n",
            "output sentence:  present hierarchical learning framework navigation within embodied learning setting \n",
            "\n",
            "{'rouge-1': {'r': 0.2545454545454545, 'p': 1.0, 'f': 0.4057970982146608}, 'rouge-2': {'r': 0.17105263157894737, 'p': 1.0, 'f': 0.2921348289660397}, 'rouge-l': {'r': 0.2545454545454545, 'p': 1.0, 'f': 0.4057970982146608}}\n",
            "pair:  present first verification neural network perception tasks produces correct output within specified tolerance every input interest define correctness relative specification identifies state space consisting relevant states world observation process produces neural network inputs states world tiling state input spaces finite number tiles obtaining ground truth bounds state tiles network output bounds input tiles comparing ground truth network output bounds delivers upper bound network output error input interest results two case studies highlight ability technique deliver tight error bounds inputs interest show error bounds vary state input spaces\n",
            "output sentence:  present first verification neural network perception tasks produces correct output within specified tolerance every input \n",
            "\n",
            "{'rouge-1': {'r': 0.1232876712328767, 'p': 0.6428571428571429, 'f': 0.2068965490236491}, 'rouge-2': {'r': 0.022727272727272728, 'p': 0.14285714285714285, 'f': 0.03921568390619007}, 'rouge-l': {'r': 0.0821917808219178, 'p': 0.42857142857142855, 'f': 0.13793103178226984}}\n",
            "pair:  gaussian processes leading class distributions random functions suffer well known issues including difficulty scaling inflexibility respect certain shape constraints nonnegativity propose deep random splines flexible class random functions obtained transforming gaussian noise deep neural network whose output parameters spline unlike gaussian processes deep random splines allow us readily enforce shape constraints inheriting richness tractability deep generative models also present observational model point process data uses deep random splines model intensity function point process apply neuroscience data obtain low dimensional representation spiking activity inference performed via variational autoencoder uses novel recurrent encoder architecture handle multiple point processes input\n",
            "output sentence:  combine splines neural networks obtain novel distribution functions use model intensity functions point processes processes \n",
            "\n",
            "{'rouge-1': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.16666666666666666, 'f': 0.04081632438150782}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}}\n",
            "pair:  driving force behind recent success lstms ability learn complex non linear relationships consequently inability describe relationships led lstms characterized black boxes end introduce contextual decomposition cd interpretation algorithm analysing individual predictions made standard lstms without changes underlying model decomposing output lstm cd captures contributions combinations words variables final prediction lstm task sentiment analysis yelp sst data sets show cd able reliably identify words phrases contrasting sentiment combined yield lstm final prediction using phrase level labels sst also demonstrate cd able successfully extract positive negative negations lstm something previously done\n",
            "output sentence:  introduce contextual decompositions interpretation algorithm lstms capable extracting word phrase interaction level importance \n",
            "\n",
            "{'rouge-1': {'r': 0.11702127659574468, 'p': 0.6875, 'f': 0.1999999975140496}, 'rouge-2': {'r': 0.027522935779816515, 'p': 0.1875, 'f': 0.04799999776768011}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.5, 'f': 0.14545454296859506}}\n",
            "pair:  exploration key component successful reinforcement learning optimal approaches computationally intractable researchers focused hand designing mechanisms based exploration bonuses intrinsic reward inspired curious behavior natural systems work propose strategy encoding curiosity algorithms programs domain specific language searching meta learning phase algorithms enable rl agents perform well new domains rich language programs combine neural networks building blocks including nearest neighbor modules choose loss functions enables expression highly generalizable programs perform well domains disparate grid navigation image input acrobot lunar lander ant hopper make approach feasible develop several pruning techniques including learning predict program success based syntactic properties demonstrate effectiveness approach empirically finding curiosity strategies similar published literature well novel strategies competitive generalize well\n",
            "output sentence:  meta learning curiosity algorithms searching rich space programs yields novel mechanisms generalize across different reinforcement learning domains \n",
            "\n",
            "{'rouge-1': {'r': 0.03389830508474576, 'p': 0.4, 'f': 0.06249999855957035}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.4, 'f': 0.06249999855957035}}\n",
            "pair:  simultaneous machine translation models start generating target sequence encoded read source sequence recent approach task either apply fixed policy transformer learnable monotonic attention weaker recurrent neural network based structure paper propose new attention mechanism monotonic multihead attention mma introduced monotonic attention mechanism multihead attention also introduced two novel interpretable approaches latency control specifically designed multiple attentions apply mma simultaneous machine translation task demonstrate better latency quality tradeoffs compared milk previous state art approach code released upon publication\n",
            "output sentence:  make transformer streamable monotonic attention \n",
            "\n",
            "{'rouge-1': {'r': 0.140625, 'p': 0.5, 'f': 0.21951219169541944}, 'rouge-2': {'r': 0.0547945205479452, 'p': 0.23529411764705882, 'f': 0.08888888582469147}, 'rouge-l': {'r': 0.140625, 'p': 0.5, 'f': 0.21951219169541944}}\n",
            "pair:  focus problem black box adversarial attacks aim generate adversarial examples using information limited loss function evaluations input output pairs use bayesian optimization bo specifically cater scenarios involving low query budgets develop query efficient adversarial attacks alleviate issues surrounding bo regards optimizing high dimensional deep learning models effective dimension upsampling techniques proposed approach achieves performance comparable state art black box adversarial attacks albeit much lower average query count particular low query budget regimes proposed method reduces query count respect state art methods\n",
            "output sentence:  show relatively simple black box adversarial attack scheme using bayesian optimization dimension upsampling preferable existing methods number number low \n",
            "\n",
            "{'rouge-1': {'r': 0.03571428571428571, 'p': 0.42857142857142855, 'f': 0.06593406451394762}, 'rouge-2': {'r': 0.009900990099009901, 'p': 0.16666666666666666, 'f': 0.01869158772643905}, 'rouge-l': {'r': 0.03571428571428571, 'p': 0.42857142857142855, 'f': 0.06593406451394762}}\n",
            "pair:  paper propose novel kind kernel random forest kernel enhance empirical performance mmd gan different common forests deterministic routings probabilistic routing variant used innovated random forest kernel possible merge cnn frameworks proposed random forest kernel following advantages perspective random forest output gan discriminator viewed feature inputs forest tree gets access merely fraction features thus entire forest benefits ensemble learning aspect kernel method random forest kernel proved characteristic therefore suitable mmd structure besides asymmetric kernel random forest kernel much flexible terms capturing differences distributions sharing advantages cnn kernel method ensemble learning random forest kernel based mmd gan obtains desirable empirical performances cifar celeba lsun bedroom data sets furthermore sake completeness also put forward comprehensive theoretical analysis support experimental results\n",
            "output sentence:  equip mmd gans new random forest kernel \n",
            "\n",
            "{'rouge-1': {'r': 0.27848101265822783, 'p': 0.9565217391304348, 'f': 0.43137254552672055}, 'rouge-2': {'r': 0.1891891891891892, 'p': 0.9130434782608695, 'f': 0.31343283297727786}, 'rouge-l': {'r': 0.27848101265822783, 'p': 0.9565217391304348, 'f': 0.43137254552672055}}\n",
            "pair:  imitation learning il appealing approach learn desirable autonomous behavior however directing il achieve arbitrary goals difficult contrast planning based algorithms use dynamics models reward functions achieve goals yet reward functions evoke desirable behavior often difficult specify paper propose imitative models combine benefits il goal directed planning imitative models probabilistic predictive models desirable behavior able plan interpretable expert like trajectories achieve specified goals derive families flexible goal objectives including constrained goal regions unconstrained goal sets energy based goals show method use objectives successfully direct behavior method substantially outperforms six il approaches planning based approach dynamic simulated autonomous driving task efficiently learned expert demonstrations without online data collection also show approach robust poorly specified goals goals wrong side road\n",
            "output sentence:  paper propose imitative models combine benefits il goal directed planning probabilistic predictive models desirable behavior able plan interpretable expert like trajectories achieve specified specified \n",
            "\n",
            "{'rouge-1': {'r': 0.11320754716981132, 'p': 0.6, 'f': 0.1904761878054926}, 'rouge-2': {'r': 0.030303030303030304, 'p': 0.2222222222222222, 'f': 0.05333333122133342}, 'rouge-l': {'r': 0.09433962264150944, 'p': 0.5, 'f': 0.15873015605946086}}\n",
            "pair:  propose wasserstein auto encoder wae new algorithm building generative model data distribution wae minimizes penalized form wasserstein distance model distribution target distribution leads different regularizer one used variational auto encoder vae regularizer encourages encoded training distribution match prior compare algorithm several techniques show generalization adversarial auto encoders aae experiments show wae shares many properties vaes stable training encoder decoder architecture nice latent manifold structure generating samples better quality\n",
            "output sentence:  propose new auto encoder based wasserstein distance improves sampling properties vae \n",
            "\n",
            "{'rouge-1': {'r': 0.25, 'p': 0.8333333333333334, 'f': 0.3846153810650888}, 'rouge-2': {'r': 0.1282051282051282, 'p': 0.5263157894736842, 'f': 0.20618556386013392}, 'rouge-l': {'r': 0.23333333333333334, 'p': 0.7777777777777778, 'f': 0.3589743554240632}}\n",
            "pair:  sequence sequence seq seq neural models actively investigated abstractive summarization nevertheless existing neural abstractive systems frequently generate factually incorrect summaries vulnerable adversarial information suggesting crucial lack semantic understanding paper propose novel semantic aware neural abstractive summarization model learns generate high quality summaries semantic interpretation salient content novel evaluation scheme adversarial samples introduced measure well model identifies topic information model yields significantly better performance popular pointer generator summarizer human evaluation also confirms system summaries uniformly informative faithful well less redundant seq seq model\n",
            "output sentence:  propose semantic aware neural abstractive summarization model novel automatic summarization evaluation scheme measures well model identifies topic information adversarial samples \n",
            "\n",
            "{'rouge-1': {'r': 0.11428571428571428, 'p': 0.8888888888888888, 'f': 0.20253164355071304}, 'rouge-2': {'r': 0.08045977011494253, 'p': 0.875, 'f': 0.14736841951024932}, 'rouge-l': {'r': 0.11428571428571428, 'p': 0.8888888888888888, 'f': 0.20253164355071304}}\n",
            "pair:  paper consider problem detecting object occlusion object detectors formulate bounding box regression unimodal task regressing single set bounding box coordinates independently however observe bounding box borders occluded object multiple plausible configurations also occluded bounding box borders correlations visible ones motivated two observations propose deep multivariate mixture gaussians model bounding box regression occlusion mixture components potentially learn different configurations occluded part covariances variates help learn relationship occluded parts visible ones quantitatively model improves ap baselines crowdhuman ms coco respectively almost computational memory overhead qualitatively model enjoys explainability since interpret resulting bounding boxes via covariance matrices mixture components\n",
            "output sentence:  deep multivariate mixture gaussians model bounding box regression occlusion \n",
            "\n",
            "{'rouge-1': {'r': 0.19736842105263158, 'p': 0.9375, 'f': 0.32608695364839324}, 'rouge-2': {'r': 0.12631578947368421, 'p': 0.8, 'f': 0.21818181582644633}, 'rouge-l': {'r': 0.19736842105263158, 'p': 0.9375, 'f': 0.32608695364839324}}\n",
            "pair:  consider problem learning reward policy expert examples unknown dynamics proposed method builds framework generative adversarial networks introduces empowerment regularized maximum entropy inverse reinforcement learning learn near optimal rewards policies empowerment based regularization prevents policy overfitting expert demonstrations advantageously leads generalized behaviors result learning near optimal rewards method simultaneously learns empowerment variational information maximization along reward policy adversarial learning formulation evaluate approach various high dimensional complex control tasks also test learned rewards challenging transfer learning problems training testing environments made different terms dynamics structure results show proposed method learns near optimal rewards policies matching expert behavior also performs significantly better state art inverse reinforcement learning algorithms\n",
            "output sentence:  method introduces empowerment regularized maximum entropy inverse reinforcement learning learn near optimal rewards policies expert demonstrations \n",
            "\n",
            "{'rouge-1': {'r': 0.1864406779661017, 'p': 0.6875, 'f': 0.29333332997688893}, 'rouge-2': {'r': 0.11688311688311688, 'p': 0.5625, 'f': 0.19354838424788995}, 'rouge-l': {'r': 0.1864406779661017, 'p': 0.6875, 'f': 0.29333332997688893}}\n",
            "pair:  network pruning widely used reducing heavy computational cost deep models typical pruning algorithm three stage pipeline training large model pruning fine tuning work make rather surprising observation fine tuning pruned model gives comparable even worse performance training model randomly initialized weights results several implications training large parameterized model necessary obtain efficient final model learned important weights large model necessarily useful small pruned model pruned architecture rather set inherited weights leads efficiency benefit final model suggests pruning algorithms could seen performing network architecture search\n",
            "output sentence:  network pruning fine tuning pruned model gives comparable worse performance training scratch advocate rethinking existing pruning algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.0410958904109589, 'p': 0.375, 'f': 0.07407407229385768}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0410958904109589, 'p': 0.375, 'f': 0.07407407229385768}}\n",
            "pair:  federated learning recent advance privacy protection context trusted curator aggregates parameters optimized decentralized fashion multiple clients resulting model distributed back clients ultimately converging joint representative model without explicitly share data however protocol vulnerable differential attacks could originate party contributing federated optimization attack client contribution training information data set revealed analyzing distributed model tackle problem propose algorithm client sided differential privacy preserving federated optimization aim hide clients contributions training balancing trade privacy loss model performance empirical studies suggest given sufficiently large number participating clients proposed procedure maintain client level differential privacy minor cost model performance\n",
            "output sentence:  ensuring models learned federated fashion reveal client participation \n",
            "\n",
            "{'rouge-1': {'r': 0.17333333333333334, 'p': 0.7222222222222222, 'f': 0.2795698893513701}, 'rouge-2': {'r': 0.06593406593406594, 'p': 0.35294117647058826, 'f': 0.11111110845850486}, 'rouge-l': {'r': 0.09333333333333334, 'p': 0.3888888888888889, 'f': 0.15053763128685405}}\n",
            "pair:  estimating importance atom molecule one appealing challenging problems chemistry physics material engineering common way estimate atomic importance compute electronic structure using density functional theory dft interpret using domain knowledge human experts however conventional approach impractical large molecular database dft calculation requires huge computation specifically time complexity number electrons molecule furthermore calculation results interpreted human experts estimate atomic importance terms target molecular property tackle problem first exploit machine learning based approach atomic importance estimation end propose reverse self attention graph neural networks integrate graph based molecular description method provides efficiently automated target directed way estimate atomic importance without domain knowledge chemistry physics\n",
            "output sentence:  first propose fully automated target directed atomic importance estimator based graph neural networks new concept reverse self attention \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.6666666666666666, 'f': 0.09302325451595458}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0375, 'p': 0.5, 'f': 0.06976744056246621}}\n",
            "pair:  several first order stochastic optimization methods commonly used euclidean domain stochastic gradient descent sgd accelerated gradient descent variance reduced methods already adapted certain riemannian settings however popular optimization tools namely adam adagrad recent amsgrad remain generalized riemannian manifolds discuss difficulty generalizing adaptive schemes agnostic riemannian setting provide algorithms convergence proofs geodesically convex objectives particular case product riemannian manifolds adaptivity implemented across manifolds cartesian product generalization tight sense choosing euclidean space riemannian manifold yields algorithms regret bounds already known standard algorithms experimentally show faster convergence lower train loss value riemannian adaptive methods corresponding baselines realistic task embedding wordnet taxonomy poincare ball\n",
            "output sentence:  adapting adam amsgrad adagrad riemannian manifolds \n",
            "\n",
            "{'rouge-1': {'r': 0.07142857142857142, 'p': 0.5833333333333334, 'f': 0.12727272532892564}, 'rouge-2': {'r': 0.00847457627118644, 'p': 0.09090909090909091, 'f': 0.01550387440899}, 'rouge-l': {'r': 0.05102040816326531, 'p': 0.4166666666666667, 'f': 0.0909090889652893}}\n",
            "pair:  hierarchical bayesian methods potential unify many related tasks shot classification conditional unconditional generation framing inference within single generative model show existing approaches learning models fail expressive generative networks pixelcnns describing global distribution little reliance latent variables address develop modification variational autoencoder encoded observations decoded new elements class result call variational homoencoder vhe may understood training hierarchical latent variable model better utilises latent variables cases using framework enables us train hierarchical pixelcnn omniglot dataset outperforming existing models test set likelihood single model achieve strong one shot generation near human level classification competitive state art discriminative classifiers vhe objective extends naturally richer dataset structures factorial hierarchical categories illustrate training models separate character content simple variations drawing style generalise style alphabet new characters\n",
            "output sentence:  technique learning deep generative models shared latent variables applied omniglot pixelcnn decoder \n",
            "\n",
            "{'rouge-1': {'r': 0.049019607843137254, 'p': 0.5, 'f': 0.08928571265943878}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.029411764705882353, 'p': 0.3, 'f': 0.053571426945153114}}\n",
            "pair:  knowledge bases kb automatically manually constructed often incomplete many valid facts inferred kb synthesizing existing information popular approach kb completion infer new relations combinatory reasoning information found along paths connecting pair entities given enormous size kbs exponential number paths previous path based models considered problem predicting missing relation given two entities evaluating truth proposed triple additionally methods traditionally used random paths fixed entity pairs recently learned pick paths propose new algorithm minerva addresses much difficult practical task answering questions relation known one entity since random walks impractical setting unknown destination combinatorially many paths start node present neural reinforcement learning approach learns navigate graph conditioned input query find predictive paths comprehensive evaluation seven knowledge base datasets found minerva competitive many current state art methods\n",
            "output sentence:  present rl agent minerva learns walk knowledge graph answer queries \n",
            "\n",
            "{'rouge-1': {'r': 0.12, 'p': 0.6666666666666666, 'f': 0.20338982792301064}, 'rouge-2': {'r': 0.04838709677419355, 'p': 0.375, 'f': 0.08571428368979596}, 'rouge-l': {'r': 0.08, 'p': 0.4444444444444444, 'f': 0.13559321775351915}}\n",
            "pair:  present hybrid framework leverages trade temporal frequency precision audio representations improve performance speech enhancement task first show conventional approaches using specific representations raw audio spectrograms effective targeting different types noise integrating approaches model learn multi scale multi domain features effectively removing noise existing different regions time frequency space complementary way experimental results show proposed hybrid model yields better performance robustness using model individually\n",
            "output sentence:  hybrid model utilizing raw audio spectrogram information speech enhancement tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.21052631578947367, 'p': 0.9411764705882353, 'f': 0.3440860185177477}, 'rouge-2': {'r': 0.16279069767441862, 'p': 0.875, 'f': 0.27450980127643215}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.9411764705882353, 'f': 0.3440860185177477}}\n",
            "pair:  deep neural networks shown incredible performance inference tasks variety domains unfortunately current deep networks enormous cloud based structures require significant storage space limits scaling deep learning service dlaas use device augmented intelligence paper finds algorithms directly use lossless compressed representations deep feedforward networks synaptic weights drawn discrete sets perform inference without full decompression basic insight allows less rate naive approaches recognition bipartite graph layers feedforward networks kind permutation invariance labeling nodes terms inferential operation inference operation depends locally edges directly connected also provide experimental results approach mnist dataset\n",
            "output sentence:  paper finds algorithms directly use lossless compressed representations deep feedforward networks perform inference without full decompression decompression \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}, 'rouge-2': {'r': 0.03409090909090909, 'p': 0.25, 'f': 0.059999997888000076}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5384615384615384, 'f': 0.16470587976193776}}\n",
            "pair:  goal imitation learning il learn good policy high quality demonstrations however quality demonstrations reality diverse since easier cheaper collect demonstrations mix experts amateurs il situations challenging especially level demonstrators expertise unknown propose new il paradigm called variational imitation learning diverse quality demonstrations vild explicitly model level demonstrators expertise probabilistic graphical model estimate along reward function show naive estimation approach suitable large state action spaces fix issue using variational approach easily implemented using existing reinforcement learning methods experiments continuous control benchmarks demonstrate vild outperforms state art methods work enables scalable data efficient il realistic settings\n",
            "output sentence:  propose imitation learning method learn diverse quality demonstrations collected demonstrators different level expertise \n",
            "\n",
            "{'rouge-1': {'r': 0.06349206349206349, 'p': 0.3333333333333333, 'f': 0.10666666397866674}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.07692307692307693, 'f': 0.023255811387236626}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.25, 'f': 0.07999999731200008}}\n",
            "pair:  modern deep neural networks achieve high accuracy training distribution test distribution identically distributed assumption frequently violated practice train test distributions mismatched accuracy plummet currently techniques improve robustness unforeseen data shifts encountered deployment work propose technique improve robustness uncertainty estimates image classifiers propose augmix data processing technique simple implement adds limited computational overhead helps models withstand unforeseen corruptions augmix significantly improves robustness uncertainty measures challenging image classification benchmarks closing gap previous methods best possible performance cases half\n",
            "output sentence:  obtain state art robustness data shifts maintain calibration data shift even though even accuracy drops \n",
            "\n",
            "{'rouge-1': {'r': 0.140625, 'p': 0.5294117647058824, 'f': 0.22222221890565466}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.10526315789473684, 'f': 0.038095235131065996}, 'rouge-l': {'r': 0.125, 'p': 0.47058823529411764, 'f': 0.1975308608809633}}\n",
            "pair:  domain adaptation open problem deep reinforcement learning rl often agents asked perform environments data difficult obtain settings agents trained similar environments simulators transferred original environment gap visual observations source target environments often causes agent fail target environment present new rl agent sadala soft attention disentangled representation learning agent sadala first learns compressed state representation jointly learns ignore distracting features solve task presented sadala separation important unimportant visual features leads robust domain transfer sadala outperforms prior disentangled representation based rl domain randomization approaches across rl environments visual cartpole deepmind lab\n",
            "output sentence:  present agent uses beta vae extract visual features attention mechanism ignore irrelevant features visual observations enable robust transfer visual domains \n",
            "\n",
            "{'rouge-1': {'r': 0.11428571428571428, 'p': 0.8888888888888888, 'f': 0.20253164355071304}, 'rouge-2': {'r': 0.08045977011494253, 'p': 0.875, 'f': 0.14736841951024932}, 'rouge-l': {'r': 0.11428571428571428, 'p': 0.8888888888888888, 'f': 0.20253164355071304}}\n",
            "pair:  paper consider problem detecting object occlusion object detectors formulate bounding box regression unimodal task regressing single set bounding box coordinates independently however observe bounding box borders occluded object multiple plausible configurations also occluded bounding box borders correlations visible ones motivated two observations propose deep multivariate mixture gaussians model bounding box regression occlusion mixture components potentially learn different configurations occluded part covariances variates help learn relationship occluded parts visible ones quantitatively model improves ap baselines crowdhuman ms coco respectively almost computational memory overhead qualitatively model enjoys explainability since interpret resulting bounding boxes via covariance matrices mixture components\n",
            "output sentence:  deep multivariate mixture gaussians model bounding box regression occlusion \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.9090909090909091, 'f': 0.20618556499946858}, 'rouge-2': {'r': 0.08653846153846154, 'p': 0.6923076923076923, 'f': 0.1538461518708452}, 'rouge-l': {'r': 0.10465116279069768, 'p': 0.8181818181818182, 'f': 0.1855670082984377}}\n",
            "pair:  long term video prediction highly challenging since entails simultaneously capturing spatial temporal information across long range image frames standard recurrent models ineffective since prone error propagation cannot effectively capture higher order correlations potential solution extend higher order spatio temporal recurrent models however model requires large number parameters operations making intractable learn practice prone overfitting work propose convolutional tensor train lstm conv tt lstm learns higher orderconvolutional lstm convlstm efficiently using convolutional tensor train decomposition cttd proposed model naturally incorporates higher order spatio temporal information small cost memory computation using efficient low rank tensor representations evaluate model moving mnist kth datasets show improvements standard convlstm better comparable results convlstm based approaches much fewer parameters\n",
            "output sentence:  propose convolutional tensor train lstm learns higher order convolutional lstm efficiently using convolutional tensor train decomposition \n",
            "\n",
            "{'rouge-1': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}}\n",
            "pair:  introduce concept channel aggregation convnet architecture novel compact representation cnn features useful explicitly modeling nonlinear channels encoding especially new unit embedded inside deep architectures action recognition channel aggregation based multiple channels features convnet aims spot finding optical convergence path fast speed name proposed convolutional architecture nonlinear channels aggregation networks ncan new layer nonlinear channels aggregation layer ncal theoretically motivate channels aggregation functions empirically study effect convergence speed classification accuracy another contribution work efficient effective implementation ncal speeding orders magnitude evaluate performance standard benchmarks ucf hmdb experimental results demonstrate formulation obtains fast convergence stronger generalization capability without sacrificing performance\n",
            "output sentence:  architecture enables cnn trained video sequences converging rapidly \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}, 'rouge-2': {'r': 0.028037383177570093, 'p': 0.375, 'f': 0.052173911748960335}, 'rouge-l': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}}\n",
            "pair:  learning communication via deep reinforcement learning recently shown effective way solve cooperative multi agent tasks however learning communicated information beneficial agent decision making remains challenging task order address problem introduce fully differentiable framework communication reasoning enabling agents solve cooperative tasks partially observable environments framework designed facilitate explicit reasoning agents novel memory based attention network learn selectively past memories model communicates series reasoning steps decompose agent intentions learned representations used first compute relevance communicated information second extract information memories given newly received information selectively interacting new information model effectively learns communication protocol directly end end manner empirically demonstrate strength model cooperative multi agent tasks inter agent communication reasoning prior information substantially improves performance compared baselines\n",
            "output sentence:  novel architecture memory based attention mechanism multi agent communication \n",
            "\n",
            "{'rouge-1': {'r': 0.12698412698412698, 'p': 0.7272727272727273, 'f': 0.21621621368517166}, 'rouge-2': {'r': 0.01282051282051282, 'p': 0.1, 'f': 0.022727270712810096}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.6363636363636364, 'f': 0.18918918665814466}}\n",
            "pair:  recent shot learning algorithms enabled models quickly adapt new tasks based training samples previous shot learning works mainly focused classification reinforcement learning paper propose shot meta learning system focuses exclusively regression tasks model based idea degree freedom unknown function significantly reduced represented linear combination set sparsifying basis functions enables labeled samples approximate function design basis function learner network encode basis functions task distribution weights generator network generate weight vector novel task show model outperforms current state art meta learning methods various regression tasks\n",
            "output sentence:  propose method shot regression learning set basis functions represent function distribution \n",
            "\n",
            "{'rouge-1': {'r': 0.02, 'p': 0.125, 'f': 0.03448275624256854}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02, 'p': 0.125, 'f': 0.03448275624256854}}\n",
            "pair:  generative models forsource code interesting structured prediction problem requiring reason hard syntactic semantic constraints well natural likely programs present novel model problem uses graph represent intermediate state generated output model generates code interleaving grammar driven expansion steps graph augmentation neural message passing steps experimental evaluation shows new model generate semantically meaningful expressions outperforming range strong baselines\n",
            "output sentence:  representing programs graphs including semantics helps generating programs \n",
            "\n",
            "{'rouge-1': {'r': 0.1414141414141414, 'p': 0.7368421052631579, 'f': 0.2372881328914105}, 'rouge-2': {'r': 0.07874015748031496, 'p': 0.5263157894736842, 'f': 0.136986299105836}, 'rouge-l': {'r': 0.10101010101010101, 'p': 0.5263157894736842, 'f': 0.16949152272191903}}\n",
            "pair:  prior work multi agent reinforcement learning marl achieves optimal collaboration directly learning policy agent maximize common reward paper aim address different angle particular consider scenarios self interested agents worker agents minds preferences intentions skills etc dictated perform tasks want achieving optimal coordination among agents train super agent manager manage first inferring minds based current past observations initiating contracts assign suitable tasks workers promise reward corresponding bonuses agree work together objective manager maximize overall productivity well minimize payments made workers ad hoc worker teaming train manager propose mind aware multi agent management reinforcement learning rl consists agent modeling policy learning evaluated approach two environments resource collection crafting simulate multi agent management problems various task settings multiple designs worker agents experimental results validated effectiveness approach modeling worker agents minds online achieving optimal ad hoc teaming good generalization fast adaptation\n",
            "output sentence:  propose mind aware multi agent management reinforcement learning rl training manager motivate self interested achieve achieve optimal collaboration assigning assigning \n",
            "\n",
            "{'rouge-1': {'r': 0.05747126436781609, 'p': 0.5, 'f': 0.10309278165586143}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.034482758620689655, 'p': 0.3, 'f': 0.06185566825379961}}\n",
            "pair:  stochastic neural networks discrete random variables important class models expressivity interpretability since direct differentiation backpropagation possible monte carlo gradient estimation techniques widely employed training models efficient stochastic gradient estimators straight gumbel softmax work well shallow models one two stochastic layers performance however suffers increasing model complexity work focus stochastic networks multiple layers boolean latent variables analyze networks employ framework harmonic analysis boolean functions use derive analytic formulation source bias biased straight estimator based analysis propose emph foust simple gradient estimation algorithm relies three simple bias reduction steps extensive experiments show foust performs favorably compared state art biased estimators much faster unbiased ones best knowledge foust first gradient estimator train deep stochastic neural networks deterministic stochastic layers\n",
            "output sentence:  present low bias estimator boolean stochastic variable models many stochastic layers \n",
            "\n",
            "{'rouge-1': {'r': 0.06976744186046512, 'p': 0.46153846153846156, 'f': 0.12121211893072138}, 'rouge-2': {'r': 0.008771929824561403, 'p': 0.07692307692307693, 'f': 0.01574802965837953}, 'rouge-l': {'r': 0.046511627906976744, 'p': 0.3076923076923077, 'f': 0.08080807852668102}}\n",
            "pair:  reinforcement learning agents typically trained evaluated according performance averaged distribution environment settings distribution environment settings contain important biases lead agents fail certain cases despite high average case performance work consider worst case analysis agents environment settings order detect whether directions agents may failed generalize specifically consider first person task agents must navigate procedurally generated mazes reinforcement learning agents recently achieved human level average case performance optimizing structure mazes find agents suffer catastrophic failures failing find goal even surprisingly simple mazes despite impressive average case performance additionally find failures transfer different agents even significantly different architectures believe findings highlight important role worst case analysis identifying whether directions agents failed generalize hope ability automatically identify failures generalization facilitate development general robust agents end report initial results enriching training settings causing failure\n",
            "output sentence:  find environment settings sota agents trained navigation tasks display extreme failures suggesting failures generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.07865168539325842, 'p': 0.6363636363636364, 'f': 0.139999998042}, 'rouge-2': {'r': 0.034482758620689655, 'p': 0.4, 'f': 0.06349206203073825}, 'rouge-l': {'r': 0.0449438202247191, 'p': 0.36363636363636365, 'f': 0.07999999804200005}}\n",
            "pair:  many real applications show great deal interest learning multiple tasks different data sources modalities unbalanced samples dimensions unfortunately existing cutting edge deep multi task learning mtl approaches cannot directly applied settings due either heterogeneous input dimensions heterogeneity optimal network architectures different tasks thus demanding develop knowledge sharing mechanism handle intrinsic discrepancies among network architectures across tasks end propose flexible knowledge sharing framework jointly learning multiple tasks distinct data sources modalities proposed framework allows task task data specific network design via utilizing compact tensor representation sharing achieved partially shared latent cores providing elaborate sharing control latent cores framework effective transferring task invariant knowledge yet also efficient learning task specific features experiments single multiple data sources modalities settings display promising results proposed method especially favourable insufficient data scenarios\n",
            "output sentence:  distributed latent space based knowledge sharing framework deep multi task learning \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}, 'rouge-2': {'r': 0.03409090909090909, 'p': 0.25, 'f': 0.059999997888000076}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5384615384615384, 'f': 0.16470587976193776}}\n",
            "pair:  goal imitation learning il learn good policy high quality demonstrations however quality demonstrations reality diverse since easier cheaper collect demonstrations mix experts amateurs il situations challenging especially level demonstrators expertise unknown propose new il paradigm called variational imitation learning diverse quality demonstrations vild explicitly model level demonstrators expertise probabilistic graphical model estimate along reward function show naive estimation approach suitable large state action spaces fix issue using variational approach easily implemented using existing reinforcement learning methods experiments continuous control benchmarks demonstrate vild outperforms state art methods work enables scalable data efficient il realistic settings\n",
            "output sentence:  propose imitation learning method learn diverse quality demonstrations collected demonstrators different level expertise \n",
            "\n",
            "{'rouge-1': {'r': 0.1797752808988764, 'p': 0.8421052631578947, 'f': 0.29629629339677643}, 'rouge-2': {'r': 0.09649122807017543, 'p': 0.6111111111111112, 'f': 0.1666666643112948}, 'rouge-l': {'r': 0.16853932584269662, 'p': 0.7894736842105263, 'f': 0.27777777487825794}}\n",
            "pair:  paper propose novel approach improve given surface mapping local refinement approach receives established mapping two surfaces follows four phases inspection mapping creation sparse nset landmarks mismatching regions ii segmentation low distortion region growing process based flattening nsegmented parts iii optimization deformation segmented parts align landmarks planar parameterization domain nand iv aggregation mappings segments update surface mapping addition propose new method deform mesh order meet constraints case landmark alignment phase iii incrementally adjust cotangent weights constraints apply deformation fashion guarantees deformed mesh free flipped faces low conformal distortion new deformation approach iterative least squares conformal mapping ilscm outperforms low distortion deformation methods approach general tested improving mappings different existing surface mapping methods also tested effectiveness editing mappings variety objects\n",
            "output sentence:  propose novel approach improve given cross surface mapping local refinement new iterative method deform mesh order meet user constraints \n",
            "\n",
            "{'rouge-1': {'r': 0.19736842105263158, 'p': 0.9375, 'f': 0.32608695364839324}, 'rouge-2': {'r': 0.12631578947368421, 'p': 0.8, 'f': 0.21818181582644633}, 'rouge-l': {'r': 0.19736842105263158, 'p': 0.9375, 'f': 0.32608695364839324}}\n",
            "pair:  consider problem learning reward policy expert examples unknown dynamics proposed method builds framework generative adversarial networks introduces empowerment regularized maximum entropy inverse reinforcement learning learn near optimal rewards policies empowerment based regularization prevents policy overfitting expert demonstrations advantageously leads generalized behaviors result learning near optimal rewards method simultaneously learns empowerment variational information maximization along reward policy adversarial learning formulation evaluate approach various high dimensional complex control tasks also test learned rewards challenging transfer learning problems training testing environments made different terms dynamics structure results show proposed method learns near optimal rewards policies matching expert behavior also performs significantly better state art inverse reinforcement learning algorithms\n",
            "output sentence:  method introduces empowerment regularized maximum entropy inverse reinforcement learning learn near optimal rewards policies expert demonstrations \n",
            "\n",
            "{'rouge-1': {'r': 0.12857142857142856, 'p': 0.6, 'f': 0.21176470297577857}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.26666666666666666, 'f': 0.07619047374149668}, 'rouge-l': {'r': 0.11428571428571428, 'p': 0.5333333333333333, 'f': 0.1882352912110727}}\n",
            "pair:  gap empirical success deep learning lack strong theoretical guarantees calls studying simpler models observing relu neuron product linear function gate latter determines whether neuron active share jointly trained weight vector propose decouple two introduce galu networks networks neuron product linear unit defined weight vector trained gate defined different weight vector trained generally speaking given base model simpler version two parameters determine quality simpler version whether practical performance close enough base model whether easier analyze theoretically show galu networks perform similarly relu networks standard datasets initiate study theoretical properties demonstrating indeed easier analyze believe research galu networks may fruitful development theory deep learning\n",
            "output sentence:  propose gated linear unit networks model performs similarly relu networks real data much easier analyze theoretically \n",
            "\n",
            "{'rouge-1': {'r': 0.19480519480519481, 'p': 0.8333333333333334, 'f': 0.31578947061274243}, 'rouge-2': {'r': 0.14772727272727273, 'p': 0.7222222222222222, 'f': 0.24528301604841585}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.7777777777777778, 'f': 0.29473683903379505}}\n",
            "pair:  common sense physical reasoning essential ingredient intelligent agent operating real world example used simulate environment infer state parts world currently unobserved order match real world conditions causal knowledge must learned without access supervised data address problem present novel method learns discover objects model physical interactions raw visual images purely unsupervised fashion incorporates prior knowledge compositional nature human perception factor interactions object pairs learn efficiently videos bouncing balls show superior modelling capabilities method compared unsupervised neural approaches incorporate prior knowledge demonstrate ability handle occlusion show extrapolate learned knowledge scenes different numbers objects\n",
            "output sentence:  introduce novel approach common sense physical reasoning learns discover objects model physical interactions raw visual images purely unsupervised fashion \n",
            "\n",
            "{'rouge-1': {'r': 0.1282051282051282, 'p': 0.6666666666666666, 'f': 0.21505376073534513}, 'rouge-2': {'r': 0.08421052631578947, 'p': 0.5333333333333333, 'f': 0.14545454309917355}, 'rouge-l': {'r': 0.10256410256410256, 'p': 0.5333333333333333, 'f': 0.17204300804717312}}\n",
            "pair:  building deep reinforcement learning agents generalize adapt unseen environments remains fundamental challenge ai paper describes progresses challenge context man made environments visually diverse contain intrinsic semantic regularities propose hybrid model based model free approach learning planning semantics leaps consisting multi target sub policy acts visual inputs bayesian model semantic structures placed unseen environment agent plans semantic model make high level decisions proposes next sub target sub policy execute updates semantic model based new observations perform experiments visual navigation tasks using house environment contains diverse human designed indoor scenes real world objects leaps outperforms strong baselines explicitly plan using semantic content\n",
            "output sentence:  propose hybrid model based model free approach using semantic information improve drl generalization man made environments \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.7, 'f': 0.2372881327779374}, 'rouge-2': {'r': 0.05172413793103448, 'p': 0.3, 'f': 0.08823529160899661}, 'rouge-l': {'r': 0.11224489795918367, 'p': 0.55, 'f': 0.18644067515081875}}\n",
            "pair:  symbolic logic allows practitioners build systems perform rule based reasoning interpretable easily augmented prior knowledge however systems traditionally difficult apply problems involving natural language due large linguistic variability language currently work natural language processing focuses neural networks learn distributed representations words composition thereby performing well presence large linguistic variability propose reap benefits approaches applying combination neural networks logic programming natural language question answering propose employ external non differentiable prolog prover utilizes similarity function pretrained sentence encoders fine tune representations via evolution strategies goal multi hop reasoning natural language allows us create system apply rule based reasoning natural language induce domain specific natural language rules training data evaluate proposed system two different question answering tasks showing complements two strong baselines bidaf seo et al fastqa weissenborn et al outperforms used ensemble\n",
            "output sentence:  introduce nlprolog system performs rule based reasoning natural language leveraging pretrained sentence sentence embeddings fine tuning evolution apply two multi hop tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.5294117647058824, 'f': 0.2535211231184289}, 'rouge-2': {'r': 0.05970149253731343, 'p': 0.25, 'f': 0.09638553905646693}, 'rouge-l': {'r': 0.14814814814814814, 'p': 0.47058823529411764, 'f': 0.22535210903392186}}\n",
            "pair:  people ask questions far richer informative creative current ai systems propose neural program generation framework modeling human question asking represents questions formal programs generates programs encoder decoder based deep neural network extensive experiments using information search game show method ask optimal questions synthetic settings predict questions humans likely ask unconstrained settings also propose novel grammar based question generation framework trained reinforcement learning able generate creative questions without supervised data\n",
            "output sentence:  introduce model human question asking combines neural networks symbolic programs learn generate good questions without supervised examples \n",
            "\n",
            "{'rouge-1': {'r': 0.13157894736842105, 'p': 0.5263157894736842, 'f': 0.2105263125894737}, 'rouge-2': {'r': 0.030927835051546393, 'p': 0.14285714285714285, 'f': 0.050847454701235445}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.3684210526315789, 'f': 0.14736841785263163}}\n",
            "pair:  neural networks structured data like graphs studied extensively recent years date bulk research activity focused mainly static graphs however real world networks dynamic since topology tends change time predicting evolution dynamic graphs task high significance area graph mining despite practical importance task explored depth far mainly due challenging nature paper propose model predicts evolution dynamic graphs specifically use graph neural network along recurrent architecture capture temporal evolution patterns dynamic graphs employ generative model predicts topology graph next time step constructs graph instance corresponds topology evaluate proposed model several artificial datasets following common network evolving dynamics well real world datasets results demonstrate effectiveness proposed model\n",
            "output sentence:  combining graph neural networks rnn graph generative model propose novel architecture able learn sequence evolving graphs predict graph topology timesteps learn future \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.875, 'f': 0.21538461322603553}, 'rouge-2': {'r': 0.08955223880597014, 'p': 0.8571428571428571, 'f': 0.162162160449233}, 'rouge-l': {'r': 0.12280701754385964, 'p': 0.875, 'f': 0.21538461322603553}}\n",
            "pair:  consider tackling single agent rl problem distributing learners learners called advisors endeavour solve problem different focus advice taking form action values communicated aggregator control system show local planning method advisors critical none ones found literature flawless textit egocentric planning overestimates values states advisors disagree textit agnostic planning inefficient around danger zones introduce novel approach called textit empathic discuss theoretical aspects empirically examine validate theoretical findings fruit collection task\n",
            "output sentence:  consider tackling single agent rl problem distributing learners \n",
            "\n",
            "{'rouge-1': {'r': 0.10416666666666667, 'p': 0.3333333333333333, 'f': 0.15873015510204092}, 'rouge-2': {'r': 0.017543859649122806, 'p': 0.07142857142857142, 'f': 0.028169010918468917}, 'rouge-l': {'r': 0.0625, 'p': 0.2, 'f': 0.09523809160997747}}\n",
            "pair:  pruning neural networks wiring length efficiency considered three techniques proposed experimentally tested distance based regularization nested rank pruning layer layer bipartite matching first two algorithms used training pruning phases respectively third used arranging neurons phase experiments show distance based regularization weight based pruning tends perform best without layer layer bipartite matching results suggest techniques may useful creating neural networks implementation widely deployed specialized circuits\n",
            "output sentence:  three new algorithms ablation studies prune neural network optimize wiring length opposed number remaining weights \n",
            "\n",
            "{'rouge-1': {'r': 0.06779661016949153, 'p': 0.3076923076923077, 'f': 0.11111110815200625}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.08333333333333333, 'f': 0.025974023342891145}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.23076923076923078, 'f': 0.0833333303742285}}\n",
            "pair:  paper fosters idea deep learning methods sided classical visual odometry pipelines improve accuracy produce uncertainty models estimations show biases inherent visual odom etry process faithfully learnt compensated learning ar chitecture associated probabilistic loss function jointly estimate full covariance matrix residual errors defining heteroscedastic error model experiments autonomous driving image sequences micro aerial vehicles camera acquisitions assess possibility concurrently improve visual odome try estimate error associated outputs\n",
            "output sentence:  paper discusses different methods pairing vo deep learning proposes simultaneous prediction corrections uncertainty \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.75, 'f': 0.1499999982}, 'rouge-2': {'r': 0.046511627906976744, 'p': 0.5714285714285714, 'f': 0.08602150398427566}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.75, 'f': 0.1499999982}}\n",
            "pair:  study benefit sharing representations among tasks enable effective use deep neural networks multi task reinforcement learning leverage assumption learning different tasks sharing common properties helpful generalize knowledge resulting effective feature extraction compared learning single task intuitively resulting set features offers performance benefits used reinforcement learning algorithms prove providing theoretical guarantees highlight conditions convenient share representations among tasks extending well known finite time bounds approximate value iteration multi task setting addition complement analysis proposing multi task extensions three reinforcement learning algorithms empirically evaluate widely used reinforcement learning benchmarks showing significant improvements single task counterparts terms sample efficiency performance\n",
            "output sentence:  study benefit sharing representation multi task reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.234375, 'p': 0.8823529411764706, 'f': 0.37037036705380283}, 'rouge-2': {'r': 0.10227272727272728, 'p': 0.45, 'f': 0.1666666636488341}, 'rouge-l': {'r': 0.234375, 'p': 0.8823529411764706, 'f': 0.37037036705380283}}\n",
            "pair:  due success deep learning solving variety challenging machine learning tasks rising interest understanding loss functions training neural networks theoretical aspect particularly properties critical points landscape around importance determine convergence performance optimization algorithms paper provide necessary sufficient characterization analytical forms critical points well global minimizers square loss functions linear neural networks show analytical forms critical points characterize values corresponding loss functions well necessary sufficient conditions achieve global minimum furthermore exploit analytical forms critical points characterize landscape properties loss functions linear neural networks shallow relu networks one particular conclusion loss function linear networks spurious local minimum loss function one hidden layer nonlinear networks relu activation function local minimum global minimum\n",
            "output sentence:  provide necessary sufficient analytical forms critical points square loss functions various neural networks exploit analytical forms characterize characterize properties properties properties neural networks networks \n",
            "\n",
            "{'rouge-1': {'r': 0.21568627450980393, 'p': 0.9166666666666666, 'f': 0.349206346122449}, 'rouge-2': {'r': 0.14285714285714285, 'p': 0.9090909090909091, 'f': 0.2469135778997104}, 'rouge-l': {'r': 0.21568627450980393, 'p': 0.9166666666666666, 'f': 0.349206346122449}}\n",
            "pair:  propose rapp new methodology novelty detection utilizing hidden space activation values obtained deep autoencoder precisely rapp compares input autoencoder reconstruction input space also hidden spaces show feed reconstructed input autoencoder activated values hidden space equivalent corresponding reconstruction hidden space given original input order aggregate hidden space activation values propose two metrics enhance novelty detection performance extensive experiments using diverse datasets validate rapp improves novelty detection performances autoencoder based approaches besides show rapp outperforms recent novelty detection methods evaluated popular benchmarks\n",
            "output sentence:  new methodology novelty detection utilizing hidden space activation values obtained deep autoencoder \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.46153846153846156, 'f': 0.15189873142765586}, 'rouge-2': {'r': 0.04, 'p': 0.25, 'f': 0.06896551486325811}, 'rouge-l': {'r': 0.07575757575757576, 'p': 0.38461538461538464, 'f': 0.12658227573145336}}\n",
            "pair:  engineered proteins offer potential solve many problems biomedicine energy materials science creating designs succeed difficult practice significant aspect challenge complex coupling protein sequence structure task finding viable design often referred inverse protein folding problem develop generative models protein sequences conditioned graph structured specification design target approach efficiently captures complex dependencies proteins focusing long range sequence local space framework significantly improves upon prior parametric models protein sequences given structure takes step toward rapid targeted biomolecular design aid deep generative models\n",
            "output sentence:  learn conditionally generate protein sequences given structures model captures sparse long range dependencies \n",
            "\n",
            "{'rouge-1': {'r': 0.21428571428571427, 'p': 0.6666666666666666, 'f': 0.324324320642805}, 'rouge-2': {'r': 0.07936507936507936, 'p': 0.23809523809523808, 'f': 0.11904761529761916}, 'rouge-l': {'r': 0.08928571428571429, 'p': 0.2777777777777778, 'f': 0.1351351314536159}}\n",
            "pair:  recent advances illustrated often possible learn solve linear inverse problems imaging using training data outperform traditional regularized least squares solutions along lines present extensions neumann network recently introduced end end learned architecture inspired truncated neumann series expansion solution map regularized least squares problem summarize neumann network approach show form compatible optimal reconstruction function given inverse problem also investigate extension neumann network incorporates sample efficient patch based regularization approach\n",
            "output sentence:  neumann networks end end sample efficient learning approach solving linear inverse problems imaging compatible mse optimal approach admit admit patch learning learning \n",
            "\n",
            "{'rouge-1': {'r': 0.07142857142857142, 'p': 0.42857142857142855, 'f': 0.12244897714285716}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.2857142857142857, 'f': 0.08163265061224496}}\n",
            "pair:  computer simulation provides automatic safe way training robotic control policies achieve complex tasks locomotion however policy trained simulation usually transfer directly real hardware due differences two environments transfer learning using domain randomization promising approach usually assumes target environment close distribution training environments thus relying heavily accurate system identification paper present different approach leverages domain randomization transferring control policies unknown environments key idea instead learning single policy simulation simultaneously learn family policies exhibit different behaviors tested target environment directly search best policy family based task performance without need identify dynamic parameters evaluate method five simulated robotic control problems different discrepancies training testing environment demonstrate method overcome larger modeling errors compared training robust policy adaptive policy\n",
            "output sentence:  propose policy transfer algorithm overcome large challenging discrepancies system dynamics latency actuator modeling error etc \n",
            "\n",
            "{'rouge-1': {'r': 0.09195402298850575, 'p': 0.8, 'f': 0.1649484517589542}, 'rouge-2': {'r': 0.03669724770642202, 'p': 0.4444444444444444, 'f': 0.06779660876041371}, 'rouge-l': {'r': 0.09195402298850575, 'p': 0.8, 'f': 0.1649484517589542}}\n",
            "pair:  twe present new approach novel architecture termed wsnet learning compact efficient deep neural networks existing approaches conventionally learn full model parameters independently compress via emph ad hoc processing model pruning filter factorization alternatively wsnet proposes learning model parameters sampling compact set learnable parameters naturally enforces parameter sharing throughout learning process demonstrate novel weight sampling approach induced wsnet promotes weights computation sharing favorably employing method efficiently learn much smaller networks competitive performance compared baseline networks equal numbers convolution filters specifically consider learning compact efficient convolutional neural networks audio classification extensive experiments multiple audio classification datasets verify effectiveness wsnet combined weight quantization resulted models textbf times smaller theoretically textbf times faster well established baselines without noticeable performance drop\n",
            "output sentence:  present novel network architecture learning compact efficient deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.061224489795918366, 'p': 0.42857142857142855, 'f': 0.10714285495535718}, 'rouge-2': {'r': 0.017241379310344827, 'p': 0.14285714285714285, 'f': 0.0307692288473374}, 'rouge-l': {'r': 0.04081632653061224, 'p': 0.2857142857142857, 'f': 0.0714285692410715}}\n",
            "pair:  introduce masked translation model mtm combines encoding decoding sequences within model component mtm based idea masked language modeling supports autoregressive non autoregressive decoding strategies simply changing order masking experiments wmt romanian english task mtm shows strong constant time translation performance beating related approaches comparable complexity also extensively compare various decoding strategies supported mtm well several length modeling techniques training settings\n",
            "output sentence:  use transformer encoder translation training style masked translation model \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.6, 'f': 0.19354838439125913}, 'rouge-2': {'r': 0.05172413793103448, 'p': 0.3333333333333333, 'f': 0.08955223648028521}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.4, 'f': 0.1290322553590011}}\n",
            "pair:  recent findings show deep generative models judge distribution samples likely drawn distribution training data work focus variational autoencoders vaes address problem misaligned likelihood estimates image data develop novel likelihood function based parameters returned vae also features data learned self supervised fashion way model additionally captures semantic information disregarded usual vae likelihood function demonstrate improvements reliability estimates experiments fashionmnist mnist datasets\n",
            "output sentence:  improved likelihood estimates variational autoencoders using self supervised feature learning \n",
            "\n",
            "{'rouge-1': {'r': 0.07575757575757576, 'p': 0.3333333333333333, 'f': 0.12345678710562423}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.2, 'f': 0.07407407105624156}}\n",
            "pair:  one substitute neuron neural network kernel machine obtain counterpart powered kernel machines new network inherits expressive power architecture original works intuitive way since node enjoys simple interpretation hyperplane reproducing kernel hilbert space using kernel multilayer perceptron example prove classification optimal representation minimizes risk network characterized hidden layer result removes need backpropagation learning model generalized feedforward kernel network moreover unlike backpropagation turns models black boxes optimal hidden representation enjoys intuitive geometric interpretation making dynamics learning deep kernel network simple understand empirical results provided validate theory\n",
            "output sentence:  combine kernel method connectionist models show resulting deep architectures trained layer wise transparent learning dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.14754098360655737, 'p': 0.5, 'f': 0.22784809774715595}, 'rouge-2': {'r': 0.039473684210526314, 'p': 0.16666666666666666, 'f': 0.06382978413761899}, 'rouge-l': {'r': 0.11475409836065574, 'p': 0.3888888888888889, 'f': 0.1772151863547509}}\n",
            "pair:  paper ask main factors determine classifier decision making uncover factors studying latent codes produced auto encoding frameworks deliver explanation classifier behaviour propose method provides series examples highlighting semantic differences classifier decisions generate examples interpolations latent space introduce formalize notion semantic stochastic path suitable stochastic process defined feature space via latent code interpolations introduce concept semantic lagrangians way incorporate desired classifier behaviour find solution associated variational problem allows highlighting differences classifier decision importantly within framework classifier used black box evaluation required\n",
            "output sentence:  generate examples explain classifier desicion via interpolations latent space variational auto encoder cost extended functional classifier generated example data data \n",
            "\n",
            "{'rouge-1': {'r': 0.12987012987012986, 'p': 0.7142857142857143, 'f': 0.2197802171766695}, 'rouge-2': {'r': 0.0380952380952381, 'p': 0.3076923076923077, 'f': 0.0677966082088481}, 'rouge-l': {'r': 0.11688311688311688, 'p': 0.6428571428571429, 'f': 0.19780219519864753}}\n",
            "pair:  ever increasing size modern datasets combined difficulty obtaining label information made semi supervised learning significant practical importance modern machine learning applications comparison supervised learning key difficulty semi supervised learning make full use unlabeled data order utilize manifold information provided unlabeled data propose novel regularization called tangent normal adversarial regularization composed two parts two parts complement jointly enforce smoothness along two different directions crucial semi supervised learning one applied along tangent space data manifold aiming enforce local invariance classifier manifold performed normal space orthogonal tangent space intending impose robustness classifier noise causing observed data deviating underlying data manifold two regularizers achieved strategy virtual adversarial training method achieved state art performance semi supervised learning tasks artificial dataset practical datasets\n",
            "output sentence:  propose novel manifold regularization strategy based adversarial training significantly improve performance semi supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.23529411764705882, 'f': 0.10666666316088902}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.23529411764705882, 'f': 0.10666666316088902}}\n",
            "pair:  growing number available services slightly different parameters preconditions effects automated planning general semantic services become highly relevant however exiting planners consider pddl claim use owl usually translate pddl losing much semantics way paper propose new domain independent heuristic based semantic distance used generic planning algorithms automated planning semantic services described owl heuristic include relevant information calculate heuristic runtime using heuristic able produce better results fewer expanded states less time established techniques\n",
            "output sentence:  describing semantic heuristics builds upon owl service description uses word sentence distance measures evaluate usefulness services given goal \n",
            "\n",
            "{'rouge-1': {'r': 0.05952380952380952, 'p': 0.625, 'f': 0.10869565058601136}, 'rouge-2': {'r': 0.02727272727272727, 'p': 0.375, 'f': 0.05084745636311408}, 'rouge-l': {'r': 0.05952380952380952, 'p': 0.625, 'f': 0.10869565058601136}}\n",
            "pair:  order efficiently learn small amount data new tasks meta learning transfers knowledge learned previous tasks new ones however critical challenge meta learning task heterogeneity cannot well handled traditional globally shared meta learning methods addition current task specific meta learning methods may either suffer hand crafted structure design lack capability capture complex relations tasks paper motivated way knowledge organization knowledge bases propose automated relational meta learning arml framework automatically extracts cross task relations constructs meta knowledge graph new task arrives quickly find relevant structure tailor learned structure knowledge meta learner result proposed framework addresses challenge task heterogeneity learned meta knowledge graph also increases model interpretability conduct extensive experiments toy regression shot image classification results demonstrate superiority arml state art baselines\n",
            "output sentence:  addressing task heterogeneity problem meta learning introducing meta knowledge graph \n",
            "\n",
            "{'rouge-1': {'r': 0.0449438202247191, 'p': 0.5, 'f': 0.08247422529067916}, 'rouge-2': {'r': 0.00980392156862745, 'p': 0.125, 'f': 0.018181816833057952}, 'rouge-l': {'r': 0.033707865168539325, 'p': 0.375, 'f': 0.06185566858964825}}\n",
            "pair:  experimental reproducibility replicability critical topics machine learning authors often raised concerns lack scientific publications improve quality field recently graph representation learning field attracted attention wide research community resulted large stream works several graph neural network models developed effectively tackle graph classification however experimental procedures often lack rigorousness hardly reproducible motivated provide overview common practices avoided fairly compare state art counter troubling trend ran experiments controlled uniform framework evaluate five popular models across nine common benchmarks moreover comparing gnns structure agnostic baselines provide convincing evidence datasets structural information exploited yet believe work contribute development graph learning field providing much needed grounding rigorous evaluations graph classification models\n",
            "output sentence:  provide rigorous comparison different graph neural networks graph classification \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.5714285714285714, 'f': 0.20253164265342097}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.14285714285714285, 'f': 0.038834949107361814}, 'rouge-l': {'r': 0.09230769230769231, 'p': 0.42857142857142855, 'f': 0.1518987312610159}}\n",
            "pair:  determining appropriate batch size mini batch gradient descent always time consuming often relies grid search paper considers resizable mini batch gradient descent rmgd algorithm based multi armed bandit achieves performance equivalent best fixed batch size epoch rmgd samples batch size according certain probability distribution proportional batch successful reducing loss function sampling probability provides mechanism exploring different batch size exploiting batch sizes history success obtaining validation loss epoch sampled batch size probability distribution updated incorporate effectiveness sampled batch size experimental results show rmgd achieves performance better best performing single batch size surprising rmgd achieves better performance grid search furthermore attains performance shorter amount time grid search\n",
            "output sentence:  optimization algorithm explores various batch sizes based probability automatically exploits successful batch size minimizes validation loss \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}}\n",
            "pair:  deep learning nlp represents word single point single mode region semantic space existing multi mode word embeddings cannot represent longer word sequences like phrases sentences introduce phrase representation also applicable sentences phrase distinct set multi mode codebook embeddings capture different semantic facets phrase meaning codebook embeddings viewed cluster centers summarize distribution possibly co occurring words pre trained word embedding space propose end end trainable neural model directly predicts set cluster centers input text sequence phrase sentence test time find per phrase sentence codebook embeddings provide interpretable semantic representation also outperform strong baselines large margin tasks benchmark datasets unsupervised phrase similarity sentence similarity hypernym detection extractive summarization\n",
            "output sentence:  propose unsupervised way learn multiple embeddings sentences phrases \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.9, 'f': 0.2195121929803689}, 'rouge-2': {'r': 0.022727272727272728, 'p': 0.2222222222222222, 'f': 0.04123711171856739}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.7, 'f': 0.1707317051754908}}\n",
            "pair:  many irregular domains social networks financial transactions neuron connections natural language structures represented graphs recent years variety graph neural networks gnns successfully applied representation learning prediction graphs however many applications underlying graph changes time existing gnns inadequate handling dynamic graphs paper propose novel technique learning embeddings dynamic graphs based tensor algebra framework method extends popular graph convolutional network gcn learning representations dynamic graphs using recently proposed tensor product technique theoretical results establish connection proposed tensor approach spectral convolution tensors developed numerical experiments real datasets demonstrate usefulness proposed method edge classification task dynamic graphs\n",
            "output sentence:  propose novel tensor based method graph convolutional networks dynamic graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.6, 'f': 0.15584415358407824}, 'rouge-2': {'r': 0.025, 'p': 0.2222222222222222, 'f': 0.0449438184067669}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.4, 'f': 0.10389610163602636}}\n",
            "pair:  optimization manifold widely used machine learning handle optimization problems constraint previous works focus case single manifold however practice quite common optimization problem involves one constraints constraint corresponding one manifold clear general optimize multiple manifolds effectively provably especially intersection multiple manifolds manifold cannot easily calculated propose unified algorithm framework handle optimization multiple manifolds specifically integrate information multiple manifolds move along ensemble direction viewing information manifold drift adding together prove convergence properties proposed algorithms also apply algorithms training neural network batch normalization layers achieve preferable empirical results\n",
            "output sentence:  paper introduces algorithm handle optimization problem multiple constraints vision manifold \n",
            "\n",
            "{'rouge-1': {'r': 0.14583333333333334, 'p': 0.9333333333333333, 'f': 0.25225224991477974}, 'rouge-2': {'r': 0.0990990990990991, 'p': 0.7857142857142857, 'f': 0.17599999801088}, 'rouge-l': {'r': 0.14583333333333334, 'p': 0.9333333333333333, 'f': 0.25225224991477974}}\n",
            "pair:  paper proposes use spectral element methods citep canuto spectral fast accurate training neural ordinary differential equations ode nets citealp chen neuralod system identification achieved expressing dynamics truncated series legendre polynomials series coefficients well network weights computed minimizing weighted sum loss function violation ode net dynamics problem solved coordinate descent alternately minimizes respect coefficients weights two unconstrained sub problems using standard backpropagation gradient methods resulting optimization scheme fully time parallel results low memory footprint experimental comparison standard methods backpropagation explicit solvers adjoint technique citep chen neuralod training surrogate models small medium scale dynamical systems shows least one order magnitude faster reaching comparable value loss function corresponding testing mse one order magnitude smaller well suggesting generalization capabilities increase\n",
            "output sentence:  paper proposes use spectral element methods fast accurate training neural ordinary differential equations system identification \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.35294117647058826, 'f': 0.12244897672428162}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.037037037037037035, 'p': 0.17647058823529413, 'f': 0.06122448692836332}}\n",
            "pair:  brain performs unsupervised learning perhaps simultaneous supervised learning raises question whether hybrid supervised unsupervised methods produce better learning inspired rich space hebbian learning rules set directly learn unsupervised learning rule local information best augments supervised signal present hebbian augmented training algorithm hat combining gradient based learning unsupervised rule pre synpatic activity post synaptic activities current weights test hat effect simple problem fashion mnist find consistently higher performance supervised learning alone finding provides empirical evidence unsupervised learning synaptic activities provides strong signal used augment gradient based methods find meta learned update rule time varying function thus difficult pinpoint interpretable hebbian update rule aids training find meta learner eventually degenerates non hebbian rule preserves important weights disturb learner convergence\n",
            "output sentence:  metalearning unsupervised update rules neural networks improves performance potentially demonstrates neurons brain learn without access global labels \n",
            "\n",
            "{'rouge-1': {'r': 0.03296703296703297, 'p': 0.21428571428571427, 'f': 0.05714285483174612}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03296703296703297, 'p': 0.21428571428571427, 'f': 0.05714285483174612}}\n",
            "pair:  study problem model extraction natural language processing adversary query access victim model attempts reconstruct local copy model assuming adversary victim model fine tune large pretrained language model bert devlin et al show adversary need real training data successfully mount attack fact attacker need even use grammatical semantically meaningful queries show random sequences words coupled task specific heuristics form effective queries model extraction diverse set nlp tasks including natural language inference question answering work thus highlights exploit made feasible shift towards transfer learning methods within nlp community query budget hundred dollars attacker extract model performs slightly worse victim model finally study two defense strategies model extraction membership classification api watermarking successful adversaries also circumvented clever ones\n",
            "output sentence:  outputs modern nlp apis nonsensical text provide strong signals model internals allowing adversaries steal apis \n",
            "\n",
            "{'rouge-1': {'r': 0.06349206349206349, 'p': 0.3333333333333333, 'f': 0.10666666397866674}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.07692307692307693, 'f': 0.023255811387236626}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.25, 'f': 0.07999999731200008}}\n",
            "pair:  modern deep neural networks achieve high accuracy training distribution test distribution identically distributed assumption frequently violated practice train test distributions mismatched accuracy plummet currently techniques improve robustness unforeseen data shifts encountered deployment work propose technique improve robustness uncertainty estimates image classifiers propose augmix data processing technique simple implement adds limited computational overhead helps models withstand unforeseen corruptions augmix significantly improves robustness uncertainty measures challenging image classification benchmarks closing gap previous methods best possible performance cases half\n",
            "output sentence:  obtain state art robustness data shifts maintain calibration data shift even though even accuracy drops \n",
            "\n",
            "{'rouge-1': {'r': 0.18181818181818182, 'p': 0.9411764705882353, 'f': 0.3047619020480726}, 'rouge-2': {'r': 0.1568627450980392, 'p': 0.9411764705882353, 'f': 0.2689075605762305}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.9411764705882353, 'f': 0.3047619020480726}}\n",
            "pair:  compressed forms deep neural networks essential deploying large scale computational models resource constrained devices contrary analogous domains large scale systems build hierarchical repetition small scale units current practice machine learning largely relies models non repetitive components spirit molecular composition repeating atoms advance state art model compression proposing atomic compression networks acns novel architecture constructed recursive repetition small set neurons words neurons weights stochastically positioned subsequent layers network empirical evidence suggests acns achieve compression rates three orders magnitudes compared fine tuned fully connected neural networks reduction fractional deterioration classification accuracy moreover method yield sub linear model complexities permits learning deep acns less parameters logistic regression decline classification accuracy\n",
            "output sentence:  advance state art model compression proposing atomic compression networks acns novel architecture constructed recursive repetition small set neurons \n",
            "\n",
            "{'rouge-1': {'r': 0.1323529411764706, 'p': 0.6, 'f': 0.21686746691827555}, 'rouge-2': {'r': 0.024390243902439025, 'p': 0.14285714285714285, 'f': 0.041666664175347375}, 'rouge-l': {'r': 0.10294117647058823, 'p': 0.4666666666666667, 'f': 0.16867469583393818}}\n",
            "pair:  propose novel method incorporating conditional information generative adversarial network gan structured prediction tasks method based fusing features generated conditional information feature space allows discriminator better capture higher order statistics data method also increases strength signals passed network real generated data conditional data agree proposed method conceptually simpler joint convolutional neural network conditional markov random field cnn crf models enforces higher order consistency without limited specific class high order potentials experimental results demonstrate method leads improvement variety different structured prediction tasks including image synthesis semantic segmentation depth estimation\n",
            "output sentence:  propose novel way incorporate conditional image information discriminator gans using feature fusion used structured prediction tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.08860759493670886, 'p': 0.7, 'f': 0.15730336879181922}, 'rouge-2': {'r': 0.04081632653061224, 'p': 0.36363636363636365, 'f': 0.07339449359818201}, 'rouge-l': {'r': 0.05063291139240506, 'p': 0.4, 'f': 0.08988763845474061}}\n",
            "pair:  unsupervised image image translation aims learn mapping several visual domains using unpaired training pairs recent studies shown remarkable success image image translation multiple domains suffer two main limitations either built several two domain mappings required learned independently generate low diversity results phenomenon known model collapse overcome limitations propose method named gmm unit based content attribute disentangled representation attribute space fitted gmm gmm component represents domain simple assumption two prominent advantages first dimension attribute space grow linearly number domains case literature second continuous domain encoding allows interpolation domains extrapolation unseen domains additionally show gmm unit constrained different methods literature meaning gmm unit unifying framework unsupervised image image translation\n",
            "output sentence:  gmm unit image image translation model maps image multiple domains stochastic fashion \n",
            "\n",
            "{'rouge-1': {'r': 0.16363636363636364, 'p': 0.6923076923076923, 'f': 0.2647058792603807}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.3333333333333333, 'f': 0.1111111083333334}, 'rouge-l': {'r': 0.12727272727272726, 'p': 0.5384615384615384, 'f': 0.20588234984861592}}\n",
            "pair:  min max formulations attracted great attention ml community due rise deep generative models adversarial methods understanding dynamics stochastic gradient algorithms solving formulations grand challenge first step restrict bilinear zero sum games give systematic analysis popular gradient updates simultaneous alternating versions provide exact conditions convergence find optimal parameter setup convergence rates particular results offer formal evidence alternating updates converge better simultaneous ones\n",
            "output sentence:  systematically analyze convergence behaviour popular gradient algorithms solving bilinear games simultaneous alternating updates \n",
            "\n",
            "{'rouge-1': {'r': 0.1896551724137931, 'p': 0.6875, 'f': 0.29729729390796206}, 'rouge-2': {'r': 0.057971014492753624, 'p': 0.26666666666666666, 'f': 0.09523809230442186}, 'rouge-l': {'r': 0.1206896551724138, 'p': 0.4375, 'f': 0.18918918579985394}}\n",
            "pair:  propose novel score based approach learning directed acyclic graph dag observational data adapt recently proposed continuous constrained optimization formulation allow nonlinear relationships variables using neural networks extension allows model complex interactions global search compared greedy approaches addition comparing method existing continuous optimization methods provide missing empirical comparisons nonlinear greedy search methods synthetic real world data sets new method outperforms current continuous methods tasks competitive existing greedy search methods important metrics causal inference\n",
            "output sentence:  proposing new score based approach structure causal learning leveraging neural networks recent continuous constrained formulation problem \n",
            "\n",
            "{'rouge-1': {'r': 0.175, 'p': 0.875, 'f': 0.2916666638888889}, 'rouge-2': {'r': 0.0990990990990991, 'p': 0.6470588235294118, 'f': 0.17187499769653325}, 'rouge-l': {'r': 0.15, 'p': 0.75, 'f': 0.24999999722222221}}\n",
            "pair:  adversarial training demonstrated one effective methods training robust models defend adversarial examples however adversarially trained models often lack adversarially robust generalization unseen testing data recent works show adversarially trained models biased towards global structure features instead work would like investigate relationship generalization adversarial training robust local features robust local features generalize well unseen shape variation learn robust local features develop random block shuffle rbs transformation break global structure features normal adversarial examples continue propose new approach called robust local features adversarial training rlfat first learns robust local features adversarial training rbs transformed adversarial examples transfers robust local features training normal adversarial examples demonstrate generality argument implement rlfat currently state art adversarial training frameworks extensive experiments stl cifar cifar show rlfat significantly improves adversarially robust generalization standard generalization adversarial training additionally demonstrate models capture local features object images aligning better human perception\n",
            "output sentence:  propose new stream adversarial training approach called robust local features adversarial training rlfat significantly improves adversarially generalization generalization generalization generalization generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.0784313725490196, 'p': 0.5333333333333333, 'f': 0.1367521345167653}, 'rouge-2': {'r': 0.024793388429752067, 'p': 0.2, 'f': 0.04411764509623711}, 'rouge-l': {'r': 0.049019607843137254, 'p': 0.3333333333333333, 'f': 0.08547008323471406}}\n",
            "pair:  present simple nearest neighbor nn approach synthesizes high frequency photorealistic images incomplete signal low resolution image surface normal map edges current state art deep generative models designed conditional image synthesis lack two important things unable generate large set diverse outputs due mode collapse problem interpretable making difficult control synthesized output demonstrate nn approaches potentially address limitations suffer accuracy small datasets design simple pipeline combines best worlds first stage uses convolutional neural network cnn map input overly smoothed image second stage uses pixel wise nearest neighbor method map smoothed output multiple high quality high frequency outputs controllable manner importantly pixel wise matching allows method compose novel high frequency content cutting pasting pixels different training exemplars demonstrate approach various input modalities various domains ranging human faces pets shoes handbags\n",
            "output sentence:  pixel wise nearest neighbors used generating multiple images incomplete priors low res images surface normals edges etc \n",
            "\n",
            "{'rouge-1': {'r': 0.24, 'p': 0.6666666666666666, 'f': 0.3529411725778547}, 'rouge-2': {'r': 0.06557377049180328, 'p': 0.23529411764705882, 'f': 0.1025640991551612}, 'rouge-l': {'r': 0.14, 'p': 0.3888888888888889, 'f': 0.20588234904844294}}\n",
            "pair:  building recent successes distributed training rl agents paper investigate training rnn based rl agents distributed prioritized experience replay study effects parameter lag resulting representational drift recurrent state staleness empirically derive improved training strategy using single network architecture fixed set hyper parameters resulting agent recurrent replay distributed dqn quadruples previous state art atari matches state art dmlab first agent exceed human level performance atari games\n",
            "output sentence:  investigation combining recurrent neural networks experience replay leading state art agent atari dmlab using single set hyper parameters \n",
            "\n",
            "{'rouge-1': {'r': 0.14736842105263157, 'p': 1.0, 'f': 0.25688073170608533}, 'rouge-2': {'r': 0.10852713178294573, 'p': 1.0, 'f': 0.1958041940378503}, 'rouge-l': {'r': 0.14736842105263157, 'p': 1.0, 'f': 0.25688073170608533}}\n",
            "pair:  work presents exploration imitation learning based agent capable state art performance playing text based computer games text based computer games describe world player natural language expect player interact game using text games interest seen testbed language understanding problem solving language generation artificial agents moreover provide learning environment skills acquired interactions environment rather using fixed corpora one aspect makes games particularly challenging learning agents combinatorially large action space existing methods solving text based games limited games either simple action space restricted predetermined set admissible actions work propose use exploration approach go explore ecoffet et al solving text based games specifically initial exploration phase first extract trajectories high rewards train policy solve game imitating trajectories experiments show approach outperforms existing solutions solving text based games sample efficient terms number interactions environment moreover show learned policy generalize better existing solutions unseen games without using restriction action space\n",
            "output sentence:  work presents exploration imitation learning based agent capable state art performance playing text based computer games \n",
            "\n",
            "{'rouge-1': {'r': 0.11320754716981132, 'p': 0.631578947368421, 'f': 0.19199999742208002}, 'rouge-2': {'r': 0.025423728813559324, 'p': 0.16666666666666666, 'f': 0.044117644762110844}, 'rouge-l': {'r': 0.05660377358490566, 'p': 0.3157894736842105, 'f': 0.09599999742208007}}\n",
            "pair:  achieving machine intelligence requires smooth integration perception reasoning yet models developed date tend specialize one sophisticated manipulation symbols acquired rich perceptual spaces far proved elusive consider visual arithmetic task goal carry simple arithmetical algorithms digits presented natural conditions hand written placed randomly propose two tiered architecture tackling kind problem lower tier consists heterogeneous collection information processing modules include pre trained deep neural networks locating extracting characters image well modules performing symbolic transformations representations extracted perception higher tier consists controller trained using reinforcement learning coordinates modules order solve high level task instance controller may learn contexts execute perceptual networks symbolic transformations apply outputs resulting model able solve variety tasks visual arithmetic domain several advantages standard architecturally homogeneous feedforward networks including improved sample efficiency\n",
            "output sentence:  use reinforcement learning train agent solve set visual arithmetic tasks using provided pre trained perceptual modules transformations internal representations \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.375, 'f': 0.1071428546938776}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0625, 'p': 0.375, 'f': 0.1071428546938776}}\n",
            "pair:  key equatorial climate phenomena qbo enso never adequately explained deterministic processes spite recent research showing growing evidence predictable behavior study applies fundamental laplace tidal equations simplifying assumptions along equator coriolis force small angle approximation solutions partial differential equations highly non linear related navier stokes search approaches used fit data\n",
            "output sentence:  analytical formulation equatorial standing wave phenomena application qbo enso \n",
            "\n",
            "{'rouge-1': {'r': 0.16279069767441862, 'p': 0.4117647058823529, 'f': 0.2333333292722223}, 'rouge-2': {'r': 0.020833333333333332, 'p': 0.0625, 'f': 0.031249996250000453}, 'rouge-l': {'r': 0.11627906976744186, 'p': 0.29411764705882354, 'f': 0.16666666260555565}}\n",
            "pair:  show variety large scale deep learning scenarios gradient dynamically converges small subspace short period training subspace spanned top eigenvectors hessian equal number classes dataset mostly preserved long periods training simple argument suggests gradient descent may happen mostly subspace give example effect solvable model classification comment possible implications optimization learning\n",
            "output sentence:  classification problems classes show gradient tends live tiny slowly evolving subspace spanned eigenvectors corresponding largest eigenvalues hessian \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.5555555555555556, 'f': 0.24096385202496737}, 'rouge-2': {'r': 0.05555555555555555, 'p': 0.2777777777777778, 'f': 0.09259258981481489}, 'rouge-l': {'r': 0.12307692307692308, 'p': 0.4444444444444444, 'f': 0.19277108094063003}}\n",
            "pair:  demonstrate possibility call sparse learning accelerated training deep neural networks maintain sparse weights throughout training achieving dense performance levels accomplish developing sparse momentum algorithm uses exponentially smoothed gradients momentum identify layers weights reduce error efficiently sparse momentum redistributes pruned weights across layers according mean momentum magnitude layer within layer sparse momentum grows weights according momentum magnitude zero valued weights demonstrate state art sparse performance mnist cifar imagenet decreasing mean error relative compared sparse algorithms furthermore show sparse momentum reliably reproduces dense performance levels providing faster training analysis ablations show benefits momentum redistribution growth increase depth size network\n",
            "output sentence:  redistributing growing weights according momentum magnitude enables training sparse networks random initializations reach dense performance levels weights accelerating training \n",
            "\n",
            "{'rouge-1': {'r': 0.10112359550561797, 'p': 0.6923076923076923, 'f': 0.17647058601114957}, 'rouge-2': {'r': 0.064, 'p': 0.6666666666666666, 'f': 0.11678831956950293}, 'rouge-l': {'r': 0.10112359550561797, 'p': 0.6923076923076923, 'f': 0.17647058601114957}}\n",
            "pair:  paper addresses unsupervised shot object recognition training images unlabeled share classes labeled support images shot recognition testing use new gan like deep architecture aimed unsupervised learning image representation encode latent object parts thus generalize well unseen classes shot recognition task unsupervised training integrates adversarial self supervision deep metric learning make two contributions first extend vanilla gan reconstruction loss enforce discriminator capture relevant characteristics fake images generated randomly sampled codes second compile training set triplet image examples estimating triplet loss metric learning using image masking procedure suitably designed identify latent object parts hence metric learning ensures deep representation images showing similar object classes share parts closer representations images common parts results show significantly outperform state art well get similar performance common episodic training fully supervised shot learning mini imagenet tiered imagenet datasets\n",
            "output sentence:  address problem unsupervised shot object recognition training images unlabeled share classes test images \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.5, 'f': 0.11428571226122453}, 'rouge-2': {'r': 0.014492753623188406, 'p': 0.14285714285714285, 'f': 0.026315787801246642}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.375, 'f': 0.08571428368979596}}\n",
            "pair:  intuitively image classification profit using spatial information recent work however suggests might overrated standard cnns paper pushing envelope aim investigate reliance necessity spatial information propose analyze three methods namely shuffle conv gap fc conv destroy spatial information training testing phases extensively evaluate methods several object recognition datasets cifar small imagenet imagenet wide range cnn architectures vgg resnet resnet mobilenet squeezenet interestingly consistently observe spatial information completely deleted significant number layers small performance drops\n",
            "output sentence:  spatial information last layers necessary good classification accuracy \n",
            "\n",
            "{'rouge-1': {'r': 0.053763440860215055, 'p': 0.625, 'f': 0.0990098995314185}, 'rouge-2': {'r': 0.008333333333333333, 'p': 0.14285714285714285, 'f': 0.01574803045446098}, 'rouge-l': {'r': 0.03225806451612903, 'p': 0.375, 'f': 0.05940593913537892}}\n",
            "pair:  consider dictionary learning problem aim model given data linear combination columns matrix known dictionary sparse weights forming linear combination known coefficients since dictionary coefficients parameterizing linear model unknown corresponding optimization inherently non convex major challenge recently provable algorithms dictionary learning proposed yet provide guarantees recovery dictionary without explicit recovery guarantees coefficients moreover estimation error dictionary adversely impacts ability successfully localize estimate coefficients potentially limits utility existing provable dictionary learning methods applications coefficient recovery interest end develop noodl simple neurally plausible alternating optimization based online dictionary learning algorithm recovers dictionary coefficients exactly geometric rate initialized appropriately algorithm noodl also scalable amenable large scale distributed implementations neural architectures mean involves simple linear non linear operations finally corroborate theoretical results via experimental evaluation proposed algorithm current state art techniques\n",
            "output sentence:  present provable algorithm exactly recovering factors dictionary learning model \n",
            "\n",
            "{'rouge-1': {'r': 0.038834951456310676, 'p': 0.36363636363636365, 'f': 0.07017543685287785}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.019417475728155338, 'p': 0.18181818181818182, 'f': 0.03508771755463228}}\n",
            "pair:  study continuous action reinforcement learning problems crucial agent interacts environment safe policies policies keep agent desirable situations training convergence formulate problems em constrained markov decision processes cmdps present safe policy optimization algorithms based lyapunov approach solve algorithms use standard policy gradient pg method deep deterministic policy gradient ddpg proximal policy optimization ppo train neural network policy guaranteeing near constraint satisfaction every policy update projecting either policy parameter selected action onto set feasible solutions induced state dependent linearized lyapunov constraints compared existing constrained pg algorithms data efficient able utilize policy policy data moreover action projection algorithm often leads less conservative policy updates allows natural integration end end pg training pipeline evaluate algorithms compare state art baselines several simulated mujoco tasks well real world robot obstacle avoidance problem demonstrating effectiveness terms balancing performance constraint satisfaction\n",
            "output sentence:  general framework incorporating long term safety constraints policy based reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.04477611940298507, 'p': 0.3, 'f': 0.07792207566200039}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04477611940298507, 'p': 0.3, 'f': 0.07792207566200039}}\n",
            "pair:  paper propose data statements design solution professional practice natural language processing technologists research development adoption widespread use data statements field begin address critical scientific ethical issues result use data certain populations development technology populations present form data statements take explore implications adopting part regular practice argue data statements help alleviate issues related exclusion bias language technology lead better precision claims nlp research generalize thus better engineering results protect companies public embarrassment ultimately lead language technology meets users preferred linguistic style furthermore mis represent others appear tacl\n",
            "output sentence:  practical proposal ethical responsive nlp technology operationalizing transparency test training data \n",
            "\n",
            "{'rouge-1': {'r': 0.045454545454545456, 'p': 0.5714285714285714, 'f': 0.08421052495069253}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03409090909090909, 'p': 0.42857142857142855, 'f': 0.06315789337174517}}\n",
            "pair:  deep neural networks dnns attained surprising achievement last decade due advantages automatic feature learning freedom expressiveness however interpretability remains mysterious dnns complex combinations linear nonlinear transformations even though many models proposed explore interpretability dnns several challenges remain unsolved lack interpretability quantity measures dnns lack theory stability dnns difficulty solve nonconvex dnn problems interpretability constraints address challenges simultaneously paper presents novel intrinsic interpretability evaluation framework dnns specifically four independent properties interpretability defined based existing works moreover investigate theory stability dnns important aspect interpretability prove dnns generally stable given different activation functions finally extended version deep learning alternating direction method multipliers dladmm proposed solve dnn problems interpretability constraints efficiently accurately extensive experiments several benchmark datasets validate several dnns proposed interpretability framework\n",
            "output sentence:  propose novel framework evaluate interpretability neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.3333333333333333, 'f': 0.14285713948979603}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.07142857142857142, 'f': 0.025316452780003544}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.3333333333333333, 'f': 0.14285713948979603}}\n",
            "pair:  understanding procedural language requires anticipating causal effects actions even explicitly stated work introduce neural process networks understand procedural text neural simulation action dynamics model complements existing memory architectures dynamic entity tracking explicitly modeling actions state transformers model updates states entities executing learned action operators empirical results demonstrate proposed model reason unstated causal effects actions allowing provide accurate contextual information understanding generating procedural text offering interpretable internal representations existing alternatives\n",
            "output sentence:  propose new recurrent memory architecture track common sense state changes entities simulating causal effects actions \n",
            "\n",
            "{'rouge-1': {'r': 0.08433734939759036, 'p': 0.4375, 'f': 0.1414141387042139}, 'rouge-2': {'r': 0.009174311926605505, 'p': 0.06666666666666667, 'f': 0.016129030131373852}, 'rouge-l': {'r': 0.04819277108433735, 'p': 0.25, 'f': 0.08080807809815335}}\n",
            "pair:  approaches problem inverse reinforcement learning irl focus estimating reward function best explains expert agent policy demonstrated behavior control task often case behavior succinctly represented simple reward combined set hard constraints setting agent attempting maximize cumulative rewards subject given constraints behavior reformulate problem irl markov decision processes mdps given nominal model environment nominal reward function seek estimate state action feature constraints environment motivate agent behavior approach based maximum entropy irl framework allows us reason likelihood expert agent demonstrations given knowledge mdp using method infer constraints added mdp increase likelihood observing demonstrations present algorithm iteratively infers maximum likelihood constraint best explain observed behavior evaluate efficacy using simulated behavior recorded data humans navigating around obstacle\n",
            "output sentence:  method infers constraints task execution leveraging principle maximum entropy quantify demonstrations differ expected un constrained behavior \n",
            "\n",
            "{'rouge-1': {'r': 0.16091954022988506, 'p': 0.9333333333333333, 'f': 0.27450980141291814}, 'rouge-2': {'r': 0.045871559633027525, 'p': 0.35714285714285715, 'f': 0.08130081099081239}, 'rouge-l': {'r': 0.10344827586206896, 'p': 0.6, 'f': 0.17647058572664362}}\n",
            "pair:  study problem generating source code strongly typed java like programming language given label example set api calls types carrying small amount information code desired generated programs expected respect realistic relationship programs labels exemplified corpus labeled programs available training two challenges conditional program generation generated programs must satisfy rich set syntactic semantic constraints source code contains many low level features impede learning address problems training neural generator code program sketches models program syntax abstract names operations generalize across programs generation infer posterior distribution sketches concretize samples distribution type safe programs using combinatorial techniques implement ideas system generating api heavy java code show often predict entire body method given api calls data types appear method\n",
            "output sentence:  give method generating type safe programs java like language given small amount syntactic information desired code \n",
            "\n",
            "{'rouge-1': {'r': 0.09836065573770492, 'p': 0.6, 'f': 0.16901408208688753}, 'rouge-2': {'r': 0.013333333333333334, 'p': 0.1111111111111111, 'f': 0.023809521896258658}, 'rouge-l': {'r': 0.09836065573770492, 'p': 0.6, 'f': 0.16901408208688753}}\n",
            "pair:  estimating frequencies elements data stream fundamental task data analysis machine learning problem typically addressed using streaming algorithms process large data using limited storage today streaming algorithms however cannot exploit patterns input improve performance propose new class algorithms automatically learn relevant patterns input data use improve frequency estimates proposed algorithms combine benefits machine learning formal guarantees available algorithm theory prove learning based algorithms lower estimation errors non learning counterparts also evaluate algorithms two real world datasets demonstrate empirically performance gains\n",
            "output sentence:  data stream algorithms improved using deep learning retaining performance guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.12345679012345678, 'p': 0.5, 'f': 0.19801979880403883}, 'rouge-2': {'r': 0.028037383177570093, 'p': 0.15, 'f': 0.04724409183458382}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.45, 'f': 0.17821781860601904}}\n",
            "pair:  capsule group neurons whose outputs represent different properties entity layer capsule network contains many capsules describe version capsules capsule logistic unit represent presence entity matrix could learn represent relationship entity viewer pose capsule one layer votes pose matrix many different capsules layer multiplying pose matrix trainable viewpoint invariant transformation matrices could learn represent part whole relationships votes weighted assignment coefficient coefficients iteratively updated image using expectation maximization algorithm output capsule routed capsule layer receives cluster similar votes transformation matrices trained discriminatively backpropagating unrolled iterations em pair adjacent capsule layers smallnorb benchmark capsules reduce number test errors compared state art capsules also show far resistance white box adversarial attacks baseline convolutional neural network\n",
            "output sentence:  capsule networks learned pose matrices em routing improves state art classification smallnorb improves generalizability new view points white box adversarial robustness \n",
            "\n",
            "{'rouge-1': {'r': 0.18055555555555555, 'p': 0.8666666666666667, 'f': 0.29885057185889813}, 'rouge-2': {'r': 0.15, 'p': 0.75, 'f': 0.24999999722222221}, 'rouge-l': {'r': 0.18055555555555555, 'p': 0.8666666666666667, 'f': 0.29885057185889813}}\n",
            "pair:  distinct commonality hmms rnns learn hidden representations sequential data addition noted backward computation baum welch algorithm hmms special case back propagation algorithm used neural networks eisner observations suggest despite many apparent differences hmms special case rnns paper investigate series architectural transformations hmms rnns theoretical derivations empirical hybridization answer question particular investigate three key design factors independence assumptions hidden states observation placement softmax use non linearity order pin empirical effects present comprehensive empirical study provide insights interplay expressivity interpretability respect language modeling parts speech induction\n",
            "output sentence:  hmms special case rnns investigate series architectural transformations hmms rnns theoretical derivations empirical hybridization provide new insights \n",
            "\n",
            "{'rouge-1': {'r': 0.13157894736842105, 'p': 0.7142857142857143, 'f': 0.22222221959506175}, 'rouge-2': {'r': 0.06060606060606061, 'p': 0.42857142857142855, 'f': 0.10619468809460417}, 'rouge-l': {'r': 0.10526315789473684, 'p': 0.5714285714285714, 'f': 0.17777777515061732}}\n",
            "pair:  explore idea compositional set embeddings used infer single class set classes associated input data image video audio signal useful example multi object detection images multi speaker diarization one shot learning audio particular devise implement two novel models consisting embedding function trained jointly composite function computes set union opera tions classes encoded two embedding vectors embedding trained jointly query function computes whether classes en coded one embedding subsume classes encoded another embedding contrast prior work models must perceive classes associated input examples also encode relationships different class label sets experiments conducted simulated data omniglot coco datasets proposed composite embedding models outperform baselines based traditional embedding approaches\n",
            "output sentence:  explored novel method compositional set embeddings perceive represent single class entire set classes associated input data \n",
            "\n",
            "{'rouge-1': {'r': 0.07792207792207792, 'p': 0.6, 'f': 0.13793103244814375}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03896103896103896, 'p': 0.3, 'f': 0.06896551520676449}}\n",
            "pair:  two important topics deep learning involve incorporating humans modeling process model priors transfer information humans model regularizing model parameters model attributions transfer information model humans explaining model behavior previous work taken important steps connect topics various forms gradient regularization find however existing methods use attributions align model behavior human intuition ineffective develop efficient theoretically grounded feature attribution method expected gradients novel framework attribution priors enforce prior expectations model behavior training demonstrate attribution priors broadly applicable instantiating three different types data image data gene expression data health care data experiments show models trained attribution priors intuitive achieve better generalization performance equivalent baselines existing methods regularize model behavior\n",
            "output sentence:  method encouraging axiomatic feature attributions deep model match human intuition \n",
            "\n",
            "{'rouge-1': {'r': 0.3333333333333333, 'p': 0.7272727272727273, 'f': 0.45714285283265316}, 'rouge-2': {'r': 0.16, 'p': 0.4, 'f': 0.22857142448979595}, 'rouge-l': {'r': 0.2916666666666667, 'p': 0.6363636363636364, 'f': 0.39999999568979594}}\n",
            "pair:  considering simultaneously finite number tasks multi output learning enables one account similarities tasks via appropriate regularizers propose generalization classical setting continuum tasks using vector valued rkhss\n",
            "output sentence:  propose extension multi output learning continuum tasks using operator valued kernels \n",
            "\n",
            "{'rouge-1': {'r': 0.13, 'p': 0.7222222222222222, 'f': 0.22033898046538355}, 'rouge-2': {'r': 0.048, 'p': 0.3333333333333333, 'f': 0.08391608171548738}, 'rouge-l': {'r': 0.09, 'p': 0.5, 'f': 0.15254237029589202}}\n",
            "pair:  study problem designing provably optimal adversarial noise algorithms induce misclassification settings learner aggregates decisions multiple classifiers given demonstrated vulnerability state art models adversarial examples recent efforts within field robust machine learning focused use ensemble classifiers way boosting robustness individual models paper design provably optimal attacks set classifiers demonstrate problem framed finding strategies equilibrium two player zero sum game learner adversary consequently illustrate need randomization adversarial attacks main technical challenge consider design best response oracles implemented multiplicative weight updates framework find equilibrium strategies zero sum game develop series scalable noise generation algorithms deep neural networks show outperforms state art attacks various image classification tasks although generally guarantees deep learning show well principled approach provably optimal linear classifiers main insight geometric characterization decision space reduces problem designing best response oracles minimizing quadratic function set convex polytopes\n",
            "output sentence:  paper analyzes problem designing adversarial attacks multiple classifiers introducing algorithms optimal linear classifiers provide state art results deep learning \n",
            "\n",
            "{'rouge-1': {'r': 0.1744186046511628, 'p': 0.8823529411764706, 'f': 0.29126213316617966}, 'rouge-2': {'r': 0.11864406779661017, 'p': 0.8235294117647058, 'f': 0.20740740520603568}, 'rouge-l': {'r': 0.1744186046511628, 'p': 0.8823529411764706, 'f': 0.29126213316617966}}\n",
            "pair:  deep learning models vulnerable adversarial examples crafted applying human imperceptible perturbations benign inputs however black box setting existing adversaries often poor transferability attack defense models work perspective regarding adversarial example generation optimization process propose two new methods improve transferability adversarial examples namely nesterov iterative fast gradient sign method ni fgsm scale invariant attack method sim ni fgsm aims adapt nesterov accelerated gradient iterative attacks effectively look ahead improve transferability adversarial examples sim based discovery scale invariant property deep learning models leverage optimize adversarial perturbations scale copies input images avoid overfitting white box model attacked generate transferable adversarial examples ni fgsm sim naturally integrated build robust gradient based attack generate transferable adversarial examples defense models empirical results imagenet dataset demonstrate attack methods exhibit higher transferability achieve higher attack success rates state art gradient based attacks\n",
            "output sentence:  proposed nesterov iterative fast gradient sign method ni fgsm scale invariant attack method sim boost transferability adversarial examples image \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.8181818181818182, 'f': 0.20224718884484283}, 'rouge-2': {'r': 0.04854368932038835, 'p': 0.5, 'f': 0.08849557360795679}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.6363636363636364, 'f': 0.15730336862012376}}\n",
            "pair:  widely recognized adversarial examples easily crafted fool deep networks mainly root locally non linear behavior nearby input examples applying mixup training provides effective mechanism improve generalization performance model robustness adversarial perturbations introduces globally linear behavior training examples however previous work mixup trained models passively defend adversarial attacks inference directly classifying inputs induced global linearity well exploited namely since locality adversarial perturbations would efficient actively break locality via globality model predictions inspired simple geometric intuition develop inference principle named mixup inference mi mixup trained models mi mixups input random clean samples shrink transfer equivalent perturbation input adversarial experiments cifar cifar demonstrate mi improve adversarial robustness models trained mixup variants\n",
            "output sentence:  exploit global linearity mixup trained models inference break locality adversarial perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.16129032258064516, 'p': 0.5, 'f': 0.2439024353361095}, 'rouge-2': {'r': 0.06944444444444445, 'p': 0.25, 'f': 0.10869564877126667}, 'rouge-l': {'r': 0.12903225806451613, 'p': 0.4, 'f': 0.19512194753123147}}\n",
            "pair:  present sequence action parsing approach natural language sql task incrementally fills slots sql query feasible actions pre defined inventory account fact typically multiple correct sql queries similar semantics draw inspiration syntactic parsing techniques propose train sequence action models non deterministic oracles evaluate models wikisql dataset achieve execution accuracy test set absolute improvement models trained traditional static oracles assuming single correct target sql query combined execution guided decoding strategy model sets new state art performance execution accuracy\n",
            "output sentence:  design incremental sequence action parsers text sql task achieve sota results improve using non deterministic oracles allow multiple correct correct correct \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.45454545454545453, 'f': 0.12658227608396094}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.2, 'f': 0.04651162585181188}, 'rouge-l': {'r': 0.04411764705882353, 'p': 0.2727272727272727, 'f': 0.07594936469155592}}\n",
            "pair:  present network embedding algorithms capture information node local distribution node attributes around observed random walks following approach similar skip gram observations neighborhoods different sizes either pooled ae encoded distinctly multi scale approach musae capturing attribute neighborhood relationships multiple scales useful diverse range applications including latent feature identification across disconnected networks similar attributes prove theoretically matrices node feature pointwise mutual information implicitly factorized embeddings experiments show algorithms robust computationally efficient outperform comparable models social web citation network datasets\n",
            "output sentence:  develop efficient multi scale approximate attributed network embedding procedures provable properties \n",
            "\n",
            "{'rouge-1': {'r': 0.14772727272727273, 'p': 0.7647058823529411, 'f': 0.24761904490521544}, 'rouge-2': {'r': 0.05357142857142857, 'p': 0.375, 'f': 0.09374999781250005}, 'rouge-l': {'r': 0.10227272727272728, 'p': 0.5294117647058824, 'f': 0.17142856871473924}}\n",
            "pair:  neural networks could misclassify inputs slightly different training data indicates small margin decision boundaries training dataset work study binary classification linearly separable datasets show linear classifiers could also decision boundaries lie close training dataset cross entropy loss used training particular show features training dataset lie low dimensional affine subspace cross entropy loss minimized using gradient method margin training points decision boundary could much smaller optimal value result contrary conclusions recent related works soudry et al identify reason contradiction order improve margin introduce differential training training paradigm uses loss function defined pairs points class show decision boundary linear classifier trained differential training indeed achieves maximum margin results reveal use cross entropy loss one hidden culprits adversarial examples introduces new direction make neural networks robust\n",
            "output sentence:  show minimizing cross entropy loss using gradient method could lead poor margin features dataset lie low dimensional \n",
            "\n",
            "{'rouge-1': {'r': 0.10810810810810811, 'p': 0.8, 'f': 0.19047618837868482}, 'rouge-2': {'r': 0.043010752688172046, 'p': 0.4444444444444444, 'f': 0.0784313709400231}, 'rouge-l': {'r': 0.06756756756756757, 'p': 0.5, 'f': 0.11904761695011341}}\n",
            "pair:  learning knowledge graph embeddings kges efficient approach knowledge graph completion conventional kges often suffer limited knowledge representation causes less accuracy especially training sparse knowledge graphs remedy present pretrain kges training framework learning better knowledgeable entity relation embeddings leveraging abundant linguistic knowledge pretrained language models specifically propose unified approach first learn entity relation representations via pretrained language models use representations initialize entity relation embeddings training kge models proposed method model agnostic sense applied variant kge models experimental results show method consistently improve results achieve state art performance using different kge models transe quate across four benchmark kg datasets link prediction triplet classification tasks\n",
            "output sentence:  propose learn knowledgeable entity relation representations bert knowledge graph embeddings \n",
            "\n",
            "{'rouge-1': {'r': 0.03296703296703297, 'p': 0.21428571428571427, 'f': 0.05714285483174612}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03296703296703297, 'p': 0.21428571428571427, 'f': 0.05714285483174612}}\n",
            "pair:  study problem model extraction natural language processing adversary query access victim model attempts reconstruct local copy model assuming adversary victim model fine tune large pretrained language model bert devlin et al show adversary need real training data successfully mount attack fact attacker need even use grammatical semantically meaningful queries show random sequences words coupled task specific heuristics form effective queries model extraction diverse set nlp tasks including natural language inference question answering work thus highlights exploit made feasible shift towards transfer learning methods within nlp community query budget hundred dollars attacker extract model performs slightly worse victim model finally study two defense strategies model extraction membership classification api watermarking successful adversaries also circumvented clever ones\n",
            "output sentence:  outputs modern nlp apis nonsensical text provide strong signals model internals allowing adversaries steal apis \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.8571428571428571, 'f': 0.15584415419126327}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.16666666666666666, 'f': 0.022988504462941015}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.7142857142857143, 'f': 0.1298701282172373}}\n",
            "pair:  self attention useful mechanism build generative models language images determines importance context elements comparing element current time step paper show lightweight convolution perform competitively best reported self attention results next introduce dynamic convolutions simpler efficient self attention predict separate convolution kernels based solely current time step order determine importance context elements number operations required approach scales linearly input length whereas self attention quadratic experiments large scale machine translation language modeling abstractive summarization show dynamic convolutions improve strong self attention models wmt english german test set dynamic convolutions achieve new state art bleu\n",
            "output sentence:  dynamic lightweight convolutions competitive self attention language tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.012987012987012988, 'p': 0.1, 'f': 0.022988503712511744}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.012987012987012988, 'p': 0.1, 'f': 0.022988503712511744}}\n",
            "pair:  continuous bag words cbow powerful text embedding method due strong capabilities encode word content cbow embeddings perform well wide range downstream tasks efficient compute however cbow capable capturing word order reason computation cbow word embeddings commutative embeddings xyz zyx order address shortcoming propose learning algorithm continuous matrix space model call continual multiplication words cmow algorithm adaptation word vec trained large quantities unlabeled text empirically show cmow better captures linguistic properties inferior cbow memorizing word content motivated findings propose hybrid model combines strengths cbow cmow results show hybrid cbow cmow model retains cbow strong ability memorize word content time substantially improving ability encode linguistic information result hybrid also performs better supervised downstream tasks average improvement\n",
            "output sentence:  present novel training scheme efficiently obtaining order aware sentence representations \n",
            "\n",
            "{'rouge-1': {'r': 0.0410958904109589, 'p': 0.375, 'f': 0.07407407229385768}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0410958904109589, 'p': 0.375, 'f': 0.07407407229385768}}\n",
            "pair:  federated learning recent advance privacy protection context trusted curator aggregates parameters optimized decentralized fashion multiple clients resulting model distributed back clients ultimately converging joint representative model without explicitly share data however protocol vulnerable differential attacks could originate party contributing federated optimization attack client contribution training information data set revealed analyzing distributed model tackle problem propose algorithm client sided differential privacy preserving federated optimization aim hide clients contributions training balancing trade privacy loss model performance empirical studies suggest given sufficiently large number participating clients proposed procedure maintain client level differential privacy minor cost model performance\n",
            "output sentence:  ensuring models learned federated fashion reveal client participation \n",
            "\n",
            "{'rouge-1': {'r': 0.1232876712328767, 'p': 0.75, 'f': 0.21176470345743945}, 'rouge-2': {'r': 0.05, 'p': 0.45454545454545453, 'f': 0.09009008830452077}, 'rouge-l': {'r': 0.0958904109589041, 'p': 0.5833333333333334, 'f': 0.16470587992802768}}\n",
            "pair:  obtaining policies generalise new environments reinforcement learning challenging work demonstrate language understanding via reading policy learner promising vehicle generalisation new environments propose grounded policy learning problem read fight monsters rtfm agent must jointly reason language goal relevant dynamics described document environment observations procedurally generate environment dynamics corresponding language descriptions dynamics agents must read understand new environment dynamics instead memorising particular information addition propose txt model captures three way interactions goal document observations rtfm txt generalises new environments dynamics seen training via reading furthermore model outperforms baselines film language conditioned cnns rtfm curriculum learning txt produces policies excel complex rtfm tasks requiring several reasoning coreference steps\n",
            "output sentence:  show language understanding via reading promising way learn policies generalise new environments \n",
            "\n",
            "{'rouge-1': {'r': 0.10416666666666667, 'p': 0.7142857142857143, 'f': 0.18181817959669425}, 'rouge-2': {'r': 0.017543859649122806, 'p': 0.16666666666666666, 'f': 0.03174603002267583}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.5714285714285714, 'f': 0.14545454323305787}}\n",
            "pair:  paper propose perform model ensembling multiclass multilabel learning setting using wasserstein barycenters optimal transport metrics wasserstein distance allow incorporating semantic side information word embeddings using barycenters find consensus models allows us balance confidence semantics finding agreement models show applications wasserstein ensembling attribute based classification multilabel learning image captioning generation results show ensembling viable alternative basic geometric arithmetic mean ensembling\n",
            "output sentence:  propose use wasserstein barycenters semantic model ensembling \n",
            "\n",
            "{'rouge-1': {'r': 0.0958904109589041, 'p': 0.5, 'f': 0.16091953752939625}, 'rouge-2': {'r': 0.03529411764705882, 'p': 0.23076923076923078, 'f': 0.06122448749479393}, 'rouge-l': {'r': 0.0821917808219178, 'p': 0.42857142857142855, 'f': 0.13793103178226984}}\n",
            "pair:  algorithms representation learning link prediction relational data designed static data however data applied usually evolves time friend graphs social networks user interactions items recommender systems also case knowledge bases contain facts us president obama valid certain points time problem link prediction temporal constraints answering queries form us president propose solution inspired canonical decomposition tensors order introduce new regularization schemes present extension complex achieves state art performance additionally propose new dataset knowledge base completion constructed wikidata larger previous benchmarks order magnitude new reference evaluating temporal non temporal link prediction methods\n",
            "output sentence:  propose new tensor decompositions associated regularizers obtain state art performances temporal knowledge base completion \n",
            "\n",
            "{'rouge-1': {'r': 0.08, 'p': 0.6666666666666666, 'f': 0.14285714094387758}, 'rouge-2': {'r': 0.0297029702970297, 'p': 0.375, 'f': 0.05504587019947819}, 'rouge-l': {'r': 0.05333333333333334, 'p': 0.4444444444444444, 'f': 0.09523809332482996}}\n",
            "pair:  information bottleneck principle elegant useful approach representation learning paper investigate problem representation learning context reinforcement learning using information bottleneck framework aiming improving sample efficiency learning algorithms analytically derive optimal conditional distribution representation provide variational lower bound maximize lower bound stein variational sv gradient method incorporate framework advantageous actor critic algorithm proximal policy optimization algorithm ppo experimental results show framework improve sample efficiency vanilla ppo significantly finally study information bottleneck ib perspective deep rl algorithm called mutual information neural estimation mine experimentally verify information extraction compression process also exists deep rl framework capable accelerating process also analyze relationship mine method relationship theoretically derive algorithm optimize ib framework without constructing lower bound\n",
            "output sentence:  derive information bottleneck framework reinforcement learning simple relevant theories tools \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.45454545454545453, 'f': 0.13157894489265934}, 'rouge-2': {'r': 0.012987012987012988, 'p': 0.09090909090909091, 'f': 0.02272727053977294}, 'rouge-l': {'r': 0.046153846153846156, 'p': 0.2727272727272727, 'f': 0.07894736594529093}}\n",
            "pair:  aim build complex humanoid agents integrate perception motor control memory work partly factor problem low level motor control proprioception high level coordination low level skills informed vision develop architecture capable surprisingly flexible task directed motor control relatively high dof humanoid body combining pre training low level motor controllers high level task focused controller switches among low level sub policies resulting system able control physically simulated humanoid body solve tasks require coupling visual perception unstabilized egocentric rgb camera locomotion environment supplementary video link https youtu fboir pnxpk\n",
            "output sentence:  solve tasks involving vision guided humanoid locomotion reusing locomotion behavior capture capture \n",
            "\n",
            "{'rouge-1': {'r': 0.07142857142857142, 'p': 0.5, 'f': 0.12499999781250003}, 'rouge-2': {'r': 0.0125, 'p': 0.1111111111111111, 'f': 0.02247190829440742}, 'rouge-l': {'r': 0.04285714285714286, 'p': 0.3, 'f': 0.07499999781250007}}\n",
            "pair:  inverse problems ubiquitous natural sciences refer challenging task inferring complex potentially multi modal posterior distributions hidden parameters given set observations typically model physical process form differential equations available leads intractable inference parameters forward propagation parameters model simulates evolution system inverse problem finding parameters given sequence states unique work propose generalisation bayesian optimisation framework approximate inference resulting method learns approximations posterior distribution applying stein variational gradient descent top estimates gaussian process model preliminary results demonstrate method performance likelihood free inference reinforcement learning environments\n",
            "output sentence:  approach combine variational inference bayesian optimisation solve complicated inverse problems \n",
            "\n",
            "{'rouge-1': {'r': 0.08163265306122448, 'p': 0.7272727272727273, 'f': 0.146788989011026}, 'rouge-2': {'r': 0.03418803418803419, 'p': 0.4, 'f': 0.0629921245334491}, 'rouge-l': {'r': 0.08163265306122448, 'p': 0.7272727272727273, 'f': 0.146788989011026}}\n",
            "pair:  autonomous agents successfully operate real world ability anticipate future scene states key competence real world scenarios future states become increasingly uncertain multi modal particularly long time horizons dropout based bayesian inference provides computationally tractable theoretically well grounded approach learn different hypotheses models deal uncertain futures make predictions correspond well observations well calibrated however turns approaches fall short capture complex real world scenes even falling behind accuracy compared plain deterministic approaches used log likelihood estimate discourages diversity work propose novel bayesian formulation anticipating future scene states leverages synthetic likelihoods encourage learning diverse models accurately capture multi modal nature future scene states show approach achieves accurate state art predictions calibrated probabilities extensive experiments scene anticipation cityscapes dataset moreover show approach generalizes across diverse tasks digit generation precipitation forecasting\n",
            "output sentence:  dropout based bayesian inference extended deal multi modality evaluated scene anticipation tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.16, 'p': 0.8421052631578947, 'f': 0.26890756034178387}, 'rouge-2': {'r': 0.10526315789473684, 'p': 0.631578947368421, 'f': 0.1804511253705693}, 'rouge-l': {'r': 0.12, 'p': 0.631578947368421, 'f': 0.20168066958548125}}\n",
            "pair:  address problem open set authorship verification classification task consists attributing texts unknown authorship given author unknown documents test set excluded training set present end end model building process universally applicable wide variety corpora little modification fine tuning relies transfer learning deep language model uses generative adversarial network number text augmentation techniques improve model generalization ability language model encodes documents known unknown authorship domain invariant space aligning document pairs input classifier keeping separate resulting embeddings used train ensemble recurrent quasi recurrent neural networks entire pipeline bidirectional forward backward pass results averaged perform experiments four traditional authorship verification datasets collection machine learning papers mined web large amazon reviews dataset experimental results surpass baseline current state art techniques validating proposed approach\n",
            "output sentence:  propose end end model building process universally applicable wide variety authorship verification corpora outperforms state art little modification fine tuning \n",
            "\n",
            "{'rouge-1': {'r': 0.09473684210526316, 'p': 0.6923076923076923, 'f': 0.1666666645490398}, 'rouge-2': {'r': 0.015625, 'p': 0.16666666666666666, 'f': 0.02857142700408172}, 'rouge-l': {'r': 0.06315789473684211, 'p': 0.46153846153846156, 'f': 0.11111110899348427}}\n",
            "pair:  machine learning systems often encounter distribution ood errors dealing testing data coming different distribution one used training growing use critical applications becomes important develop systems able accurately quantify predictive uncertainty screen anomalous inputs however unlike standard learning tasks currently well established guiding principle designing architectures accurately quantify uncertainty moreover commonly used ood detection approaches prone errors even sometimes assign higher likelihoods ood samples address problems first seek identify guiding principles designing uncertainty aware architectures proposing neural architecture distribution search nads unlike standard neural architecture search methods seek single best performing architecture nads searches distribution architectures perform well given task allowing us identify building blocks common among uncertainty aware architectures formulation able optimize stochastic outlier detection objective construct ensemble models perform ood detection perform multiple ood detection experiments observe nads performs favorably compared state art ood detection methods\n",
            "output sentence:  propose architecture search method identify distribution architectures use construct bayesian ensemble outlier detection \n",
            "\n",
            "{'rouge-1': {'r': 0.03389830508474576, 'p': 0.2857142857142857, 'f': 0.06060605870982558}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.16666666666666666, 'f': 0.028169012537195087}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.2857142857142857, 'f': 0.06060605870982558}}\n",
            "pair:  framework efficient bayesian inference probabilistic programs introduced embedding sampler inside variational posterior approximation strength lies ease implementation automatically tuning sampler parameters speed mixing time several strategies approximate evidence lower bound elbo computation introduced including rewriting elbo objective experimental evidence shown performing experiments unconditional vae density estimation tasks solving influence diagram high dimensional space conditional variational autoencoder cvae deep bayes classifier state space models time series data\n",
            "output sentence:  embed sg mcmc samplers inside variational approximation \n",
            "\n",
            "{'rouge-1': {'r': 0.1095890410958904, 'p': 0.6666666666666666, 'f': 0.1882352916927336}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.18181818181818182, 'f': 0.0399999980420001}, 'rouge-l': {'r': 0.0821917808219178, 'p': 0.5, 'f': 0.14117646816332183}}\n",
            "pair:  consider problem unconstrained minimization smooth objective function mathbb setting function evaluations possible propose analyze stochastic zeroth order method heavy ball momentum particular propose smtp momentum version stochastic three point method stp bergou et al show new complexity results non convex convex strongly convex functions test method collection learning continuous control tasks several mujoco todorov et al environments varying difficulty compare stp state art derivative free optimization algorithms policy gradient methods smtp significantly outperforms stp methods considered numerical experiments second contribution smtp importance sampling call smtp provide convergence analysis method non convex convex strongly convex objectives\n",
            "output sentence:  develop analyze new derivative free optimization algorithm momentum importance applications continuous control \n",
            "\n",
            "{'rouge-1': {'r': 0.18461538461538463, 'p': 0.631578947368421, 'f': 0.28571428221371886}, 'rouge-2': {'r': 0.023809523809523808, 'p': 0.10526315789473684, 'f': 0.03883494844754477}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.2631578947368421, 'f': 0.11904761554705226}}\n",
            "pair:  generative models natural images progressed towards high fidelity samples strong leveraging scale attempt carry success field video modeling showing large generative adversarial networks trained complex kinetics dataset able produce video samples substantially higher complexity fidelity previous work proposed model dual video discriminator gan dvd gan scales longer higher resolution videos leveraging computationally efficient decomposition discriminator evaluate related tasks video synthesis video prediction achieve new state art fr chet inception distance prediction kinetics well state art inception score synthesis ucf dataset alongside establishing strong baseline synthesis kinetics\n",
            "output sentence:  propose dvd gan large video generative model state art several tasks produces highly complex videos trained large real world datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.017857142857142856, 'p': 0.2857142857142857, 'f': 0.03361344427088486}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.008928571428571428, 'p': 0.14285714285714285, 'f': 0.016806721581809268}}\n",
            "pair:  generative models shown great success generating high dimensional samples conditional low dimensional descriptors learning stroke thickness mnist hair color celeba speaker identity wavenet generation sample poses fundamental problems conditional variational autoencoder cvae simple conditional generative model explicitly relate conditions training hence incentive learning compact joint distribution across conditions overcome limitation matching distributions using maximum mean discrepancy mmd decoder layer follows bottleneck introduces strong regularization reconstructing samples within condition transforming samples across conditions resulting much improved generalization refer architecture transformer vae trvae benchmarking trvae high dimensional image tabular data demonstrate higher robustness higher accuracy existing approaches particular show qualitatively improved predictions cellular perturbation response treatment disease based high dimensional single cell gene expression data tackling previously problematic minority classes multiple conditions generic tasks improve pearson correlations high dimensional estimated means variances ground truths respectively\n",
            "output sentence:  generates never seen data training desired condition \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.4, 'f': 0.18181817830578514}, 'rouge-2': {'r': 0.05063291139240506, 'p': 0.21052631578947367, 'f': 0.08163264993544368}, 'rouge-l': {'r': 0.10294117647058823, 'p': 0.35, 'f': 0.1590909055785125}}\n",
            "pair:  consider simple overarching representation permutation invariant functions sequences set functions approach call janossy pooling expresses permutation invariant function average permutation sensitive function applied reorderings input sequence allows us leverage rich mature literature permutation sensitive functions construct novel flexible permutation invariant functions carried naively janossy pooling computationally prohibitive allow computational tractability consider three kinds approximations canonical orderings sequences functions order interactions stochastic optimization algorithms random permutations framework unifies variety existing work literature suggests possible modeling algorithmic extensions explore experiments demonstrate improved performance current state art methods\n",
            "output sentence:  propose janossy pooling method learning deep permutation invariant functions designed exploit relationships within input sequence tractable inference strategies stochastic optimization \n",
            "\n",
            "{'rouge-1': {'r': 0.1016949152542373, 'p': 0.5454545454545454, 'f': 0.1714285687795919}, 'rouge-2': {'r': 0.013333333333333334, 'p': 0.1, 'f': 0.023529409688581502}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.2727272727272727, 'f': 0.08571428306530622}}\n",
            "pair:  consider task program synthesis presence reward function output programs goal find programs maximal rewards introduce novel iterative optimization scheme train rnn dataset best programs priority queue generated programs far synthesize new programs add priority queue sampling rnn benchmark algorithm called priority queue training pqt genetic algorithm reinforcement learning baselines simple expressive turing complete programming language called bf experimental results show deceptively simple pqt algorithm significantly outperforms baselines adding program length penalty reward function able synthesize short human readable programs\n",
            "output sentence:  use simple search algorithm involving rnn priority queue find solutions coding tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.11363636363636363, 'p': 0.47619047619047616, 'f': 0.18348623542126086}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.18181818181818182, 'f': 0.06249999715332044}, 'rouge-l': {'r': 0.07954545454545454, 'p': 0.3333333333333333, 'f': 0.12844036386162788}}\n",
            "pair:  extended kalman filter ekf classical signal processing algorithm performs efficient approximate bayesian inference non conjugate models linearising local measurement function avoiding need compute intractable integrals calculating posterior cases ekf outperforms methods rely cubature solve integrals especially time critical real world problems drawback ekf local nature whereas state art methods variational inference expectation propagation ep considered global approximations formulate power ep nonlinear kalman filter showing linearisation results globally iterated algorithm exactly matches ekf first pass data iteratively improves linearisation subsequent passes additional benefit ability calculate limit ep power tends zero removes instability ep like algorithm resulting inference scheme solves non conjugate temporal gaussian process models linear time mathcal closed form\n",
            "output sentence:  unify extended kalman filter ekf state space approach power expectation propagation pep solving intractable moment matching integrals pep via leads extension extension extension extension extension extension extension extension extension extension extension extension extension \n",
            "\n",
            "{'rouge-1': {'r': 0.07894736842105263, 'p': 0.6, 'f': 0.13953488166576528}, 'rouge-2': {'r': 0.033707865168539325, 'p': 0.3333333333333333, 'f': 0.061224488127863436}, 'rouge-l': {'r': 0.07894736842105263, 'p': 0.6, 'f': 0.13953488166576528}}\n",
            "pair:  convolutional neural networks cnns achieved state art performance recognizing representing audio images videos volumes domains input characterized regular graph structure however generalizing cnns irregular domains like meshes challenging additionally training data meshes often limited work generalize convolutional autoencoders mesh surfaces perform spectral decomposition meshes apply convolutions directly frequency space addition use max pooling introduce upsampling within network represent meshes low dimensional space construct complex dataset high resolution meshes extreme facial expressions encode using convolutional mesh autoencoder despite limited training data method outperforms state art pca models faces lower error using fewer parameters\n",
            "output sentence:  convolutional autoencoders generalized mesh surfaces encoding reconstructing extreme facial expressions \n",
            "\n",
            "{'rouge-1': {'r': 0.08823529411764706, 'p': 0.6666666666666666, 'f': 0.1558441537797268}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.25, 'f': 0.04123711188861734}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.4444444444444444, 'f': 0.10389610183167484}}\n",
            "pair:  generative adversarial networks gans learn map samples noise distribution chosen data distribution recent work demonstrated gans consequently sensitive limited shape noise distribution example single generator struggles map continuous noise uniform distribution discontinuous output separate gaussians complex output intersecting parabolas address problem learning generate multiple models generator output actually combination several distinct networks contribute novel formulation multi generator models learn prior generators conditioned noise parameterized neural network thus network learns optimal rate sample generator also optimally shapes noise received generator resulting noise prior gan npgan achieves expressivity flexibility surpasses single generator models previous multi generator models\n",
            "output sentence:  multi generator gan framework additional network learn prior input noise \n",
            "\n",
            "{'rouge-1': {'r': 0.04819277108433735, 'p': 0.6666666666666666, 'f': 0.08988763919202121}, 'rouge-2': {'r': 0.01, 'p': 0.2, 'f': 0.01904761814058961}, 'rouge-l': {'r': 0.03614457831325301, 'p': 0.5, 'f': 0.06741572907966167}}\n",
            "pair:  recent studies highlighted adversarial examples ubiquitous threat different neural network models many downstream applications nonetheless unique data properties inspired distinct powerful learning principles paper aims explore potentials towards mitigating adversarial inputs particular results reveal importance using temporal dependency audio data gain discriminate power adversarial examples tested automatic speech recognition asr tasks three recent audio adversarial attacks find input transformation developed image adversarial defense provides limited robustness improvement subtle advanced attacks ii temporal dependency exploited gain discriminative power audio adversarial examples resistant adaptive attacks considered experiments results show promising means improving robustness asr systems also offer novel insights exploiting domain specific data properties mitigate negative effects adversarial examples\n",
            "output sentence:  adversarial audio discrimination using temporal dependency \n",
            "\n",
            "{'rouge-1': {'r': 0.038461538461538464, 'p': 0.6, 'f': 0.07228915549426625}, 'rouge-2': {'r': 0.011494252873563218, 'p': 0.25, 'f': 0.02197802113754381}, 'rouge-l': {'r': 0.02564102564102564, 'p': 0.4, 'f': 0.04819276995209757}}\n",
            "pair:  paper addresses problem representing system belief using multi variate normal distributions mnd underlying model based deep neural network dnn major challenge dnns computational complexity needed obtain model uncertainty using mnds achieve scalable method propose novel approach expresses parameter posterior sparse information form inference algorithm based novel laplace approximation scheme involves diagonal correction kronecker factored eigenbasis makes inversion information matrix intractable operation required full bayesian analysis devise low rank approximation eigenbasis memory efficient sampling scheme provide theoretical analysis empirical evaluation various benchmark data sets showing superiority approach existing methods\n",
            "output sentence:  approximate inference algorithm deep learning \n",
            "\n",
            "{'rouge-1': {'r': 0.03260869565217391, 'p': 0.2727272727272727, 'f': 0.05825242527665196}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03260869565217391, 'p': 0.2727272727272727, 'f': 0.05825242527665196}}\n",
            "pair:  reinforcement learning common let agent interact environment fixed amount time resetting environment repeating process series episodes task agent learn either maximize performance fixed amount time ii indefinite period time limit used training paper investigate theoretically time limits could effectively handled two cases first one argue terminations due time limits fact part environment propose include notion remaining time part agent input second case time limits part environment used facilitate learning argue terminations treated environmental ones propose method specific value based algorithms incorporates insight continuing bootstrap end partial episode illustrate significance proposals perform several experiments range environments simple state transition graphs complex control tasks including novel standard benchmark domains results show proposed methods improve performance stability existing reinforcement learning algorithms\n",
            "output sentence:  consider problem learning optimal policies time limited time unlimited domains using time limited \n",
            "\n",
            "{'rouge-1': {'r': 0.06593406593406594, 'p': 0.5, 'f': 0.11650485231030261}, 'rouge-2': {'r': 0.01818181818181818, 'p': 0.18181818181818182, 'f': 0.03305784958677694}, 'rouge-l': {'r': 0.054945054945054944, 'p': 0.4166666666666667, 'f': 0.09708737658214729}}\n",
            "pair:  recurrent neural networks long dominating choice sequence modeling however severely suffers two issues impotent capturing long term dependencies unable parallelize sequential computation procedure therefore many non recurrent sequence models built convolution attention operations proposed recently notably models multi head attention transformer demonstrated extreme effectiveness capturing long term dependencies variety sequence modeling tasks despite success however models lack necessary components model local structures sequences heavily rely position embeddings limited effects require considerable amount design efforts paper propose transformer enjoys advantages rnns multi head attention mechanism avoids respective drawbacks proposed model effectively capture local structures global long term dependencies sequences without use position embeddings evaluate transformer extensive experiments data wide range domains empirical results show transformer outperforms state art methods large margin tasks\n",
            "output sentence:  paper proposes effective generic sequence model leverages strengths rnns multi head attention \n",
            "\n",
            "{'rouge-1': {'r': 0.08163265306122448, 'p': 0.47058823529411764, 'f': 0.13913043226313804}, 'rouge-2': {'r': 0.02608695652173913, 'p': 0.1875, 'f': 0.04580152457316017}, 'rouge-l': {'r': 0.061224489795918366, 'p': 0.35294117647058826, 'f': 0.10434782356748588}}\n",
            "pair:  work propose sparse deep scattering crois network sdcsn novel architecture based deep scattering network dsn dsn achieved cascading wavelet transform convolutions complex modulus time invariant operator extend work first crossing multiple wavelet family transforms increase feature diversity avoiding learning thus providing informative latent representation benefit development highly specialized wavelet filters last decades beside combining different wavelet representations reduce amount prior information needed regarding signals hand secondly develop optimal thresholding strategy complete filter banks regularizes network controls instabilities inherent non stationary noise signal systematic principled solution sparsifies latent representation network acting local mask distinguishing activity noise thus propose enhance dsn increasing variance scattering coefficients representation well improve robustness respect non stationary noise show new approach robust outperforms dsn bird detection task\n",
            "output sentence:  propose enhance deep scattering network order improve control stability given machine learning pipeline proposing continuous thresholding scheme \n",
            "\n",
            "{'rouge-1': {'r': 0.08791208791208792, 'p': 0.6153846153846154, 'f': 0.1538461516586539}, 'rouge-2': {'r': 0.008547008547008548, 'p': 0.07142857142857142, 'f': 0.015267173663539661}, 'rouge-l': {'r': 0.04395604395604396, 'p': 0.3076923076923077, 'f': 0.076923074735577}}\n",
            "pair:  generative priors become highly effective solving inverse problems including denoising inpainting reconstruction noisy measurements generative model represent image much lower dimensional latent codes context compressive sensing unknown image belongs range pretrained generative network recover image estimating underlying compact latent code available measurements however recent studies revealed even untrained deep neural networks work prior recovering natural images approaches update network weights keeping latent codes fixed reconstruct target image given measurements paper optimize network weights latent codes use untrained generative network prior video compressive sensing problem show optimizing latent code additionally get concise representation frames retain structural similarity video frames also apply low rank constraint latent codes represent video sequences even lower dimensional latent space empirically show proposed methods provide better comparable accuracy low computational complexity compared existing methods\n",
            "output sentence:  recover videos compressive measurements learning low dimensional low rank representation directly measurements training deep generator \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.42857142857142855, 'f': 0.14285714007936512}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.07692307692307693, 'f': 0.019607840913110595}, 'rouge-l': {'r': 0.04285714285714286, 'p': 0.21428571428571427, 'f': 0.07142856865079376}}\n",
            "pair:  current model based reinforcement learning approaches use model simply learned black box simulator augment data policy optimization value function learning paper show make effective use model exploiting differentiability construct policy optimization algorithm uses pathwise derivative learned model policy across future timesteps instabilities learning across many timesteps prevented using terminal value function learning policy actor critic fashion furthermore present derivation monotonic improvement objective terms gradient error model value function show approach consistently sample efficient existing state art model based algorithms ii matches asymptotic performance model free algorithms iii scales long horizons regime typically past model based approaches struggled\n",
            "output sentence:  policy gradient backpropagation time using learned models functions sota results reinforcement learning benchmark environments \n",
            "\n",
            "{'rouge-1': {'r': 0.017391304347826087, 'p': 0.2857142857142857, 'f': 0.03278688416420321}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.017391304347826087, 'p': 0.2857142857142857, 'f': 0.03278688416420321}}\n",
            "pair:  historically pursuit efficient inference one driving forces hind research new deep learning architectures building blocks recent examples include squeeze excitation module hu et al depthwise separable convolutions xception chollet inverted bottleneck mobilenet sandler et al notably cases resulting building blocks enabled higher efficiency also higher accuracy found wide adoption field work expand arsenal efficient building blocks neural network architectures instead combining standard primitives convolution advocate replacement dense primitives sparse counterparts idea using sparsity decrease parameter count new mozer smolensky conventional wisdom reduction theoretical flops translate real world efficiency gains aim correct misconception introducing family efficient sparse kernels several hardware platforms plan open source benefit community equipped efficient implementation sparse primitives show sparse versions mobilenet mobilenet architectures substantially outperform strong dense baselines efficiency accuracy curve snapdragon sparse networks outperform dense equivalents equivalent approximately one entire generation improvement hope findings facilitate wider adoption sparsity tool creating efficient accurate deep learning architectures\n",
            "output sentence:  sparse mobilenets faster dense ones appropriate kernels \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.75, 'f': 0.20338982816432058}, 'rouge-2': {'r': 0.051094890510948905, 'p': 0.4375, 'f': 0.09150326610107228}, 'rouge-l': {'r': 0.11764705882352941, 'p': 0.75, 'f': 0.20338982816432058}}\n",
            "pair:  deep ensembles empirically shown promising approach improving accuracy uncertainty distribution robustness deep learning models deep ensembles theoretically motivated bootstrap non bootstrap ensembles trained random initialization also perform well practice suggests could explanations deep ensembles work well bayesian neural networks learn distributions parameters network theoretically well motivated bayesian principles perform well deep ensembles practice particularly dataset shift one possible explanation gap theory practice popular scalable approximate bayesian methods tend focus single mode whereas deep ensembles tend explore diverse modes function space investigate hypothesis building recent work understanding loss landscape neural networks adding exploration measure similarity functions space predictions results show random initializations explore entirely different modes functions along optimization trajectory sampled subspace thereof cluster within single mode predictions wise often deviating significantly weight space demonstrate low loss connectors modes exist connected space predictions developing concept diversity accuracy plane show decorrelation power random initializations unmatched popular subspace sampling methods\n",
            "output sentence:  study deep ensembles lens loss landscape space predictions demonstrating decorrelation power random initializations unmatched subspace subspace single \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.8333333333333334, 'f': 0.14492753464398234}, 'rouge-2': {'r': 0.02666666666666667, 'p': 0.4, 'f': 0.04999999882812503}, 'rouge-l': {'r': 0.031746031746031744, 'p': 0.3333333333333333, 'f': 0.057971012904851967}}\n",
            "pair:  paper propose framework leverages semi supervised models improve unsupervised clustering performance leverage semi supervised models first need automatically generate labels called pseudo labels find prior approaches generating pseudo labels hurt clustering performance low accuracy instead use ensemble deep networks construct similarity graph extract high accuracy pseudo labels approach finding high quality pseudo labels using ensembles training semi supervised model iterated yielding continued improvement show approach outperforms state art clustering results multiple image text datasets example achieve accuracy cifar news outperforming state art absolute terms\n",
            "output sentence:  using ensembles pseudo labels unsupervised clustering \n",
            "\n",
            "{'rouge-1': {'r': 0.1262135922330097, 'p': 0.6190476190476191, 'f': 0.20967741654136318}, 'rouge-2': {'r': 0.02586206896551724, 'p': 0.15, 'f': 0.044117644550173155}, 'rouge-l': {'r': 0.0970873786407767, 'p': 0.47619047619047616, 'f': 0.16129031976716965}}\n",
            "pair:  much work design convolutional networks last five years revolved around empirical investigation importance depth filter sizes number feature channels recent studies shown branching splitting computation along parallel distinct threads aggregating outputs represents new promising dimension significant improvements performance combat complexity design choices multi branch architectures prior work adopted simple strategies fixed branching factor input fed parallel branches additive combination outputs produced branches aggregation points work remove predefined choices propose algorithm learn connections branches network instead chosen priori human designer multi branch connectivity learned simultaneously weights network optimizing single loss function defined respect end task demonstrate approach problem multi class image classification using four different datasets yields consistently higher accuracy compared state art resnext multi branch network given learning capacity\n",
            "output sentence:  paper introduced algorithm learn connectivity deep multi branch networks approach evaluated image categorization consistently yields accuracy gains state art models use fixed \n",
            "\n",
            "{'rouge-1': {'r': 0.04504504504504504, 'p': 0.5, 'f': 0.0826446265828837}, 'rouge-2': {'r': 0.014184397163120567, 'p': 0.2222222222222222, 'f': 0.026666665538666715}, 'rouge-l': {'r': 0.036036036036036036, 'p': 0.4, 'f': 0.066115700963049}}\n",
            "pair:  predictive coding theories suggest brain learns predicting observations various levels abstraction one basic prediction tasks view prediction would given scene look alternative viewpoint humans excel task ability imagine fill missing visual information tightly coupled perception feel see world dimensions fact information front surface world hits retinas paper explores connection view predictive representation learning role development visual recognition propose inverse graphics networks take input video streams captured moving camera map stable feature maps scene disentangling scene content motion camera model also project feature maps novel viewpoints predict match target views propose contrastive prediction losses handle stochasticity visual input scale view predictive learning photorealistic scenes considered previous works show proposed model learns visual representations useful semi supervised learning object detectors unsupervised learning moving object detectors estimating motion inferred feature maps videos dynamic scenes best knowledge first work empirically shows view prediction useful scalable self supervised task beneficial object detection\n",
            "output sentence:  show right loss architecture view predictive learning improves object detection \n",
            "\n",
            "{'rouge-1': {'r': 0.057971014492753624, 'p': 0.8, 'f': 0.10810810684806428}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.057971014492753624, 'p': 0.8, 'f': 0.10810810684806428}}\n",
            "pair:  reparameterization trick become one useful tools field variational inference however reparameterization trick based standardization transformation restricts scope application method distributions tractable inverse cumulative distribution functions expressible deterministic transformations distributions paper generalized reparameterization trick allowing general transformation discover proposed model special case control variate indicating proposed model combine advantages cv generalized reparameterization based proposed gradient model propose new polynomial based gradient estimator better theoretical performance reparameterization trick certain condition applied larger class variational distributions studies synthetic real data show proposed gradient estimator significantly lower gradient variance state art methods thus enabling faster inference procedure\n",
            "output sentence:  generalized transformation generalized transformation model variational inference \n",
            "\n",
            "{'rouge-1': {'r': 0.07377049180327869, 'p': 0.6923076923076923, 'f': 0.13333333159286695}, 'rouge-2': {'r': 0.02142857142857143, 'p': 0.25, 'f': 0.039473682756232746}, 'rouge-l': {'r': 0.03278688524590164, 'p': 0.3076923076923077, 'f': 0.05925925751879293}}\n",
            "pair:  fault diagnosis modern communication system traditionally supposed difficult even impractical purely data driven machine learning approach humanmade system intensive knowledge labeled raw packet streams extracted fault archive hardly sufficient deduce intricate logic underlying protocols paper supplement limited samples two inexhaustible data sources unlabeled records probed system service labeled data simulated emulation environment transfer inherent knowledge target domain construct directed information flow graph whose nodes neural network components consisting two generators three discriminators one classifier whose every forward path represents pair adversarial optimization goals accord semi supervised transfer learning demands multi headed network trained alternative approach iteration select one target update weights along path upstream refresh residual layer wisely outputs downstream actual results show achieve comparable accuracy classifying transmission control protocol tcp streams without deliberate expert features solution relieved operation engineers massive works understanding maintaining rules provided quick solution independent specific protocols\n",
            "output sentence:  semi supervised transfer learning packet flow classification via system cooperative adversarial neural blocks \n",
            "\n",
            "{'rouge-1': {'r': 0.03333333333333333, 'p': 0.2222222222222222, 'f': 0.05797101222432271}, 'rouge-2': {'r': 0.014705882352941176, 'p': 0.125, 'f': 0.026315787590027836}, 'rouge-l': {'r': 0.03333333333333333, 'p': 0.2222222222222222, 'f': 0.05797101222432271}}\n",
            "pair:  improve previous end end differentiable neural networks nns fast weight memories gate mechanism updates fast weights every time step sequence two separate outer product based matrices generated slow parts net system trained complex sequence sequence variation associative retrieval problem roughly times temporal memory time varying variables similar sized standard recurrent nns rnns terms accuracy number parameters architecture outperforms variety rnns including long short term memory hypernetworks related fast weight architectures\n",
            "output sentence:  improved fast weight network shows better results general toy task \n",
            "\n",
            "{'rouge-1': {'r': 0.12, 'p': 0.6666666666666666, 'f': 0.20338982792301064}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.25, 'f': 0.057142855118367426}, 'rouge-l': {'r': 0.1, 'p': 0.5555555555555556, 'f': 0.1694915228382649}}\n",
            "pair:  well known deeper neural networks harder train shallower ones short paper use full eigenvalue spectrum hessian explore loss landscape changes network gets deeper residual connections added architecture computing series quantitative measures hessian spectrum show hessian eigenvalue distribution deeper networks substantially heavier tails equivalently outlier eigenvalues makes network harder optimize first order methods show adding residual connections mitigates effect substantially suggesting mechanism residual connections improve training\n",
            "output sentence:  network depth increases outlier eigenvalues hessian residual connections mitigate \n",
            "\n",
            "{'rouge-1': {'r': 0.14925373134328357, 'p': 0.9090909090909091, 'f': 0.2564102539875082}, 'rouge-2': {'r': 0.1, 'p': 0.8, 'f': 0.17777777580246917}, 'rouge-l': {'r': 0.13432835820895522, 'p': 0.8181818181818182, 'f': 0.23076922834648256}}\n",
            "pair:  learnability different neural architectures characterized directly computable measures data complexity paper reframe problem architecture selection understanding data determines expressive generalizable architectures suited data beyond inductive bias suggesting algebraic topology measure data complexity show power network express topological complexity dataset decision boundary strictly limiting factor ability generalize provide first empirical characterization topological capacity neural networks empirical analysis shows every level dataset complexity neural networks exhibit topological phase transitions stratification observation allowed us connect existing theory empirically driven conjectures choice architectures single hidden layer neural networks\n",
            "output sentence:  show learnability different neural architectures characterized directly computable measures data complexity \n",
            "\n",
            "{'rouge-1': {'r': 0.10975609756097561, 'p': 0.5625, 'f': 0.18367346665556022}, 'rouge-2': {'r': 0.042105263157894736, 'p': 0.26666666666666666, 'f': 0.0727272703719009}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.3125, 'f': 0.10204081359433577}}\n",
            "pair:  implicit models allow generation samples point wise evaluation probabilities omnipresent real world problems tackled machine learning hot topic current research examples include data simulators widely used engineering scientific research generative adversarial networks gans image synthesis hot press approximate inference techniques relying implicit distributions majority existing approaches learning implicit models rely approximating intractable distribution optimisation objective gradient based optimisation liable produce inaccurate updates thus poor models paper alleviates need approximations proposing emph stein gradient estimator directly estimates score function implicitly defined distribution efficacy proposed estimator empirically demonstrated examples include meta learning approximate inference entropy regularised gans provide improved sample diversity\n",
            "output sentence:  introduced novel gradient estimator using stein method compared methods learning implicit models approximate inference image generation \n",
            "\n",
            "{'rouge-1': {'r': 0.08, 'p': 0.5, 'f': 0.1379310321046374}, 'rouge-2': {'r': 0.03508771929824561, 'p': 0.2857142857142857, 'f': 0.06249999805175788}, 'rouge-l': {'r': 0.08, 'p': 0.5, 'f': 0.1379310321046374}}\n",
            "pair:  success modern machine learning becoming increasingly important understand control learning algorithms interact unfortunately negative results game theory show little hope understanding controlling general player games therefore introduce smooth markets sm games class player games pairwise zero sum interactions sm games codify common design pattern machine learning includes gans adversarial training recent algorithms show sm games amenable analysis optimization using first order methods\n",
            "output sentence:  introduce class player games suited gradient based methods \n",
            "\n",
            "{'rouge-1': {'r': 0.2926829268292683, 'p': 0.9230769230769231, 'f': 0.4444444407887518}, 'rouge-2': {'r': 0.20930232558139536, 'p': 0.75, 'f': 0.327272723861157}, 'rouge-l': {'r': 0.2682926829268293, 'p': 0.8461538461538461, 'f': 0.4074074037517148}}\n",
            "pair:  minecraft videogame offers many interesting challenges ai systems paper focus construction scenarios agent must build complex structure made individual blocks higher level objects formed lower level objects construction naturally modelled hierarchical task network model house construction scenario classical htn planning compare advantages disadvantages kinds models\n",
            "output sentence:  model house construction scenario minecraft classical htn planning compare advantages disadvantages kinds models \n",
            "\n",
            "{'rouge-1': {'r': 0.09278350515463918, 'p': 0.47368421052631576, 'f': 0.15517241105380503}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.1111111111111111, 'f': 0.028571426330612427}, 'rouge-l': {'r': 0.07216494845360824, 'p': 0.3684210526315789, 'f': 0.12068965243311539}}\n",
            "pair:  adversarial feature learning afl one promising ways explicitly constrains neural networks learn desired representations example afl could help learn anonymized representations avoid privacy issues afl learn representations training networks deceive adversary predict sensitive information network therefore success afl heavily relies choice adversary paper proposes novel design adversary em multiple adversaries random subspaces mars instantiate concept em volunerableness proposed method motivated assumption deceiving adversary could fail give meaningful information adversary easily fooled adversary rely single classifier suffer issues contrast proposed method designed less vulnerable utilizing ensemble independent classifiers classifier tries predict sensitive variables different em subset representations empirical validations three user anonymization tasks show proposed method achieves state art performances three datasets without significantly harming utility data significant gives new implications designing adversary important improve performance afl\n",
            "output sentence:  paper improves quality recently proposed adversarial feature leaning afl approach incorporating explicit constrains representations introducing concept em vulnerableness adversary \n",
            "\n",
            "{'rouge-1': {'r': 0.140625, 'p': 0.5294117647058824, 'f': 0.22222221890565466}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.10526315789473684, 'f': 0.038095235131065996}, 'rouge-l': {'r': 0.125, 'p': 0.47058823529411764, 'f': 0.1975308608809633}}\n",
            "pair:  domain adaptation open problem deep reinforcement learning rl often agents asked perform environments data difficult obtain settings agents trained similar environments simulators transferred original environment gap visual observations source target environments often causes agent fail target environment present new rl agent sadala soft attention disentangled representation learning agent sadala first learns compressed state representation jointly learns ignore distracting features solve task presented sadala separation important unimportant visual features leads robust domain transfer sadala outperforms prior disentangled representation based rl domain randomization approaches across rl environments visual cartpole deepmind lab\n",
            "output sentence:  present agent uses beta vae extract visual features attention mechanism ignore irrelevant features visual observations enable robust transfer visual domains \n",
            "\n",
            "{'rouge-1': {'r': 0.15714285714285714, 'p': 0.7333333333333333, 'f': 0.2588235265051903}, 'rouge-2': {'r': 0.043010752688172046, 'p': 0.26666666666666666, 'f': 0.07407407168209884}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.6666666666666666, 'f': 0.23529411474048442}}\n",
            "pair:  introduce learning based algorithm low rank decomposition problem given times matrix parameter compute rank matrix minimizes approximation loss algorithm uses training set input matrices order optimize performance specifically efficient approximate algorithms computing low rank approximations proceed computing projection sa sparse random times sketching matrix performing singular value decomposition sa show replace random matrix learned matrix sparsity reduce error experiments show multiple types data sets learned sketch matrix substantially reduce approximation loss compared random matrix sometimes one order magnitude also study mixed matrices rows trained remaining ones random show matrices still offer improved performance retaining worst case guarantees\n",
            "output sentence:  learning based algorithms improve upon performance classical algorithms low rank approximation problem retaining worst case guarantee \n",
            "\n",
            "{'rouge-1': {'r': 0.046875, 'p': 0.3333333333333333, 'f': 0.08219177866016145}, 'rouge-2': {'r': 0.013513513513513514, 'p': 0.125, 'f': 0.02439024214158252}, 'rouge-l': {'r': 0.046875, 'p': 0.3333333333333333, 'f': 0.08219177866016145}}\n",
            "pair:  learning disentangled representations correspond factors variation real world data critical interpretable human controllable machine learning recently concerns viability learning disentangled representations purely unsupervised manner spurred shift toward incorporation weak supervision however currently formalism identifies weak supervision guarantee disentanglement address issue provide theoretical framework including calculus disentanglement assist analyzing disentanglement guarantees lack thereof conferred weak supervision coupled learning algorithms based distribution matching empirically verify guarantees limitations several weak supervision methods restricted labeling match pairing rank pairing demonstrating predictive power usefulness theoretical framework\n",
            "output sentence:  construct theoretical framework weakly supervised disentanglement conducted lots experiments back \n",
            "\n",
            "{'rouge-1': {'r': 0.16304347826086957, 'p': 0.8823529411764706, 'f': 0.27522935516539015}, 'rouge-2': {'r': 0.08943089430894309, 'p': 0.6470588235294118, 'f': 0.15714285500918368}, 'rouge-l': {'r': 0.15217391304347827, 'p': 0.8235294117647058, 'f': 0.2568807313121791}}\n",
            "pair:  deep learning deep reinforcement learning systems demonstrated impressive results domains image classification game playing robotic control data efficiency remains major challenge particularly algorithms learn individual tasks scratch multi task learning emerged promising approach sharing structure across multiple tasks enable efficient learning however multi task setting presents number optimization challenges making difficult realize large efficiency gains compared learning tasks independently reasons multi task learning challenging compared single task learning fully understood motivated insight gradient interference causes optimization challenges develop simple general approach avoiding interference gradients different tasks altering gradients technique refer gradient surgery propose form gradient surgery projects gradient task onto normal plane gradient task conflicting gradient series challenging multi task supervised multi task reinforcement learning problems find approach leads substantial gains efficiency performance effectively combined previously proposed multi task architectures enhanced performance model agnostic way\n",
            "output sentence:  develop simple general approach avoiding interference gradients different tasks improves performance multi task learning supervised reinforcement learning domains \n",
            "\n",
            "{'rouge-1': {'r': 0.034482758620689655, 'p': 0.8, 'f': 0.06611570168704324}, 'rouge-2': {'r': 0.007042253521126761, 'p': 0.25, 'f': 0.013698629604053315}, 'rouge-l': {'r': 0.02586206896551724, 'p': 0.6, 'f': 0.049586776067208536}}\n",
            "pair:  neural networks brain neuromorphic chips confer systems ability perform multiple cognitive tasks however kinds networks experience wide range physical perturbations ranging damage edges network complete node deletions ultimately could lead network failure critical question understand computational properties neural networks change response node damage whether exist strategies repair networks order compensate performance degradation study damage response characteristics two classes neural networks namely multilayer perceptrons mlps convolutional neural networks cnns trained classify images mnist cifar datasets respectively also propose new framework discover efficient repair strategies rescue damaged neural networks framework involves defining damage repair operators dynamically traversing neural networks loss landscape goal mapping salient geometric features using strategy discover features resemble path connected attractor sets loss landscape also identify dynamic recovery scheme networks constantly damaged repaired produces group networks resilient damage quickly rescued broadly work shows design fault tolerant networks applying line retraining consistently damage real time applications biology machine learning\n",
            "output sentence:  strategy repair damaged neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.22857142857142856, 'p': 0.9411764705882353, 'f': 0.3678160888096182}, 'rouge-2': {'r': 0.18518518518518517, 'p': 0.9375, 'f': 0.3092783477606547}, 'rouge-l': {'r': 0.22857142857142856, 'p': 0.9411764705882353, 'f': 0.3678160888096182}}\n",
            "pair:  computational imaging systems jointly design computation hardware retrieve information traditionally accessible standard imaging systems recently critical aspects experimental design image priors optimized deep neural networks formed unrolled iterations classical physics based reconstructions termed physics based networks however real world large scale systems computing gradients via backpropagation restricts learning due memory limitations graphical processing units work propose memory efficient learning procedure exploits reversibility network layers enable data driven design large scale computational imaging demonstrate methods practicality two large scale systems super resolution optical microscopy multi channel magnetic resonance imaging\n",
            "output sentence:  propose memory efficient learning procedure exploits reversibility network layers enable data driven design large scale computational imaging \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.5714285714285714, 'f': 0.21621621314828346}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.19047619047619047, 'f': 0.06399999720448013}, 'rouge-l': {'r': 0.08888888888888889, 'p': 0.38095238095238093, 'f': 0.14414414107621143}}\n",
            "pair:  pruning large neural networks maintaining performance often desirable due reduced space time complexity existing methods pruning done within iterative optimization procedure either heuristically designed pruning schedules additional hyperparameters undermining utility work present new approach prunes given network initialization prior training achieve introduce saliency criterion based connection sensitivity identifies structurally important connections network given task eliminates need pretraining complex pruning schedule making robust architecture variations pruning sparse network trained standard way method obtains extremely sparse networks virtually accuracy reference network mnist cifar tiny imagenet classification tasks broadly applicable various architectures including convolutional residual recurrent networks unlike existing methods approach enables us demonstrate retained connections indeed relevant given task\n",
            "output sentence:  present new approach snip simple versatile interpretable prunes irrelevant connections given task single shot prior training applicable variety neural network network network \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.5555555555555556, 'f': 0.1666666641166667}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0784313725490196, 'p': 0.4444444444444444, 'f': 0.13333333078333337}}\n",
            "pair:  stability key aspect data analysis many applications natural notion stability geometric illustrated example computer vision scattering transforms construct deep convolutional representations certified stable input deformations stability deformations interpreted stability respect changes metric structure domain work show scattering transforms generalized non euclidean domains using diffusion wavelets preserving notion stability respect metric changes domain measured diffusion maps resulting representation stable metric perturbations domain able capture high frequency information akin euclidean scattering\n",
            "output sentence:  stability scattering transform representations graph data deformations underlying graph support \n",
            "\n",
            "{'rouge-1': {'r': 0.0594059405940594, 'p': 0.6, 'f': 0.10810810646863081}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0297029702970297, 'p': 0.3, 'f': 0.054054052414576795}}\n",
            "pair:  despite impressive performance deep neural networks dnns numerous learning tasks still exhibit uncouth behaviours one puzzling behaviour subtle sensitive reaction dnns various noise attacks nuisance strengthened line research around developing training noise robust networks work propose new training regularizer aims minimize probabilistic expected training loss dnn subject generic gaussian input provide efficient simple approach approximate regularizer arbitrarily deep networks done leveraging analytic expression output mean shallow neural network avoiding need memory computation expensive data augmentation conduct extensive experiments lenet alexnet various datasets including mnist cifar cifar demonstrate effectiveness proposed regularizer particular show networks trained proposed regularizer benefit boost robustness gaussian noise equivalent amount performing folds noisy data augmentation moreover empirically show several architectures datasets improving robustness gaussian noise using new regularizer improve overall robustness types attacks two orders magnitude\n",
            "output sentence:  efficient estimate gaussian first moment dnns regularizer training robust networks \n",
            "\n",
            "{'rouge-1': {'r': 0.08247422680412371, 'p': 0.8888888888888888, 'f': 0.1509433946724813}, 'rouge-2': {'r': 0.03508771929824561, 'p': 0.5, 'f': 0.06557376926632627}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.4444444444444444, 'f': 0.07547169655927378}}\n",
            "pair:  ever increasing demand resultant reduced quality services focus shifted towards easing network congestion enable efficient flow systems like traffic supply chains electrical grids step direction imagine traditional heuristics based training systems approach incapable modelling involved dynamics one apply multi agent reinforcement learning marl problems considering vertex network agent marl based models assume agents independent many real world tasks agents need behave group rather collection individuals paper propose framework induces cooperation coordination amongst agents connected via underlying network using emergent communication marl based setup formulate problem general network setting demonstrate utility communication networks help case study traffic systems furthermore study emergent communication protocol show formation distinct communities grounded vocabulary best knowledge work studies emergent language networked marl setting\n",
            "output sentence:  framework studying emergent communication networked multi agent reinforcement learning setup \n",
            "\n",
            "{'rouge-1': {'r': 0.06542056074766354, 'p': 0.5833333333333334, 'f': 0.11764705701009816}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.18181818181818182, 'f': 0.02758620549441149}, 'rouge-l': {'r': 0.056074766355140186, 'p': 0.5, 'f': 0.10084033432102255}}\n",
            "pair:  although reinforcement learning methods achieve impressive results simulation real world presents two major challenges generating samples exceedingly expensive unexpected perturbations unseen situations cause proficient specialized policies fail test time given impractical train separate policies accommodate situations agent may see real world work proposes learn quickly effectively adapt online new tasks enable sample efficient learning consider learning online adaptation context model based reinforcement learning approach uses meta learning train dynamics model prior combined recent data prior rapidly adapted local context experiments demonstrate online adaptation continuous control tasks simulated real world agents first show simulated agents adapting behavior online novel terrains crippled body parts highly dynamic environments also illustrate importance incorporating online adaptation autonomous agents operate real world applying method real dynamic legged millirobot demonstrate agent learned ability quickly adapt online missing leg adjust novel terrains slopes account miscalibration errors pose estimation compensate pulling payloads\n",
            "output sentence:  model based meta rl algorithm enables real robot adapt online dynamic environments \n",
            "\n",
            "{'rouge-1': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.4444444444444444, 'f': 0.09411764516539796}}\n",
            "pair:  central goal study primate visual cortex hierarchical models object recognition understanding single units trade invariance versus sensitivity image transformations example deep networks visual cortex substantial variation layer layer unit unit degree translation invariance provide theoretical insight variation consequences encoding deep network critical insight comes fact rectification simultaneously decreases response variance correlation across responses transformed stimuli naturally inducing positive relationship invariance dynamic range invariant input units tend drive network sensitive small image transformations discuss consequences relationship ai deep nets naturally weight invariant units sensitive units strengthened training perhaps contributing generalization performance results predict signature relationship invariance dynamic range tested future neurophysiological studies\n",
            "output sentence:  rectification deep neural networks naturally leads favor invariant representation \n",
            "\n",
            "{'rouge-1': {'r': 0.045454545454545456, 'p': 0.8, 'f': 0.08602150435888543}, 'rouge-2': {'r': 0.008620689655172414, 'p': 0.25, 'f': 0.016666666022222245}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.8, 'f': 0.08602150435888543}}\n",
            "pair:  model free reinforcement learning rl methods succeeding growing number tasks aided recent advances deep learning however tend suffer high sample complexity hinders use real world domains alternatively model based reinforcement learning promises reduce sample complexity tends require careful tuning date succeeded mainly restrictive domains simple models sufficient learning paper analyze behavior vanilla model based reinforcement learning methods deep neural networks used learn model policy show learned policy tends exploit regions insufficient data available model learned causing instability training overcome issue propose use ensemble models maintain model uncertainty regularize learning process show use likelihood ratio derivatives yields much stable learning backpropagation time altogether approach model ensemble trust region policy optimization trpo significantly reduces sample complexity compared model free deep rl methods challenging continuous control benchmark tasks\n",
            "output sentence:  deep model based rl works well \n",
            "\n",
            "{'rouge-1': {'r': 0.0684931506849315, 'p': 0.5, 'f': 0.12048192559152272}, 'rouge-2': {'r': 0.011363636363636364, 'p': 0.1, 'f': 0.020408161432736528}, 'rouge-l': {'r': 0.0547945205479452, 'p': 0.4, 'f': 0.09638554004935408}}\n",
            "pair:  deep learning algorithms increasingly used modeling chemical processes however black box predictions without rationales limited used practical applications drug design end learn identify molecular substructures rationales associated target chemical property toxicity rationales learned unsupervised fashion requiring additional information beyond end end task formulate problem reinforcement learning problem molecular graph parametrized two convolution networks corresponding rationale selection prediction based latter induces reward function evaluate approach two benchmark toxicity datasets demonstrate model sustains high performance additional constraint predictions strictly follow rationales additionally validate extracted rationales comparison described chemical literature synthetic experiments\n",
            "output sentence:  use reinforcement learning molecular graphs generate rationales interpretable molecular property prediction \n",
            "\n",
            "{'rouge-1': {'r': 0.1323529411764706, 'p': 0.6428571428571429, 'f': 0.21951219229030342}, 'rouge-2': {'r': 0.04819277108433735, 'p': 0.2857142857142857, 'f': 0.08247422433414824}, 'rouge-l': {'r': 0.11764705882352941, 'p': 0.5714285714285714, 'f': 0.1951219483878644}}\n",
            "pair:  investigate loss surface neural networks prove even one hidden layer networks slightest nonlinearity empirical risks spurious local minima cases results thus indicate general spurious local minim property limited deep linear networks insights obtained linear networks may robust specifically relu like networks constructively prove almost practical datasets exist infinitely many local minima also present counterexample general activations sigmoid tanh arctan relu etc exists bad local minimum results make least restrictive assumptions relative existing results spurious local optima neural networks complete discussion presenting comprehensive characterization global optimality deep linear networks unifies results topic\n",
            "output sentence:  constructively prove even slightest nonlinear activation functions introduce spurious local minima general datasets activation functions \n",
            "\n",
            "{'rouge-1': {'r': 0.057971014492753624, 'p': 0.8, 'f': 0.10810810684806428}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.057971014492753624, 'p': 0.8, 'f': 0.10810810684806428}}\n",
            "pair:  reparameterization trick become one useful tools field variational inference however reparameterization trick based standardization transformation restricts scope application method distributions tractable inverse cumulative distribution functions expressible deterministic transformations distributions paper generalized reparameterization trick allowing general transformation discover proposed model special case control variate indicating proposed model combine advantages cv generalized reparameterization based proposed gradient model propose new polynomial based gradient estimator better theoretical performance reparameterization trick certain condition applied larger class variational distributions studies synthetic real data show proposed gradient estimator significantly lower gradient variance state art methods thus enabling faster inference procedure\n",
            "output sentence:  generalized transformation generalized transformation based variational inference \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.6666666666666666, 'f': 0.12371133852269103}, 'rouge-2': {'r': 0.010101010101010102, 'p': 0.125, 'f': 0.018691587401519886}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.4444444444444444, 'f': 0.08247422512062921}}\n",
            "pair:  recent progress physics based character animation shown impressive breakthroughs human motion synthesis imitating motion capture data via deep reinforcement learning however results mostly demonstrated imitating single distinct motion pattern generalize interactive tasks require flexible motion patterns due varying human object spatial configurations bridge gap focus one class interactive tasks sitting onto chair propose hierarchical reinforcement learning framework relies collection subtask controllers trained imitate simple reusable mocap motions meta controller trained execute subtasks properly complete main task experimentally demonstrate strength approach different single level hierarchical baselines also show approach applied motion prediction given image input video highlight found https youtu xwu wzz ip\n",
            "output sentence:  synthesizing human motions interactive tasks using mocap data hierarchical rl \n",
            "\n",
            "{'rouge-1': {'r': 0.09523809523809523, 'p': 0.6153846153846154, 'f': 0.1649484512870656}, 'rouge-2': {'r': 0.040983606557377046, 'p': 0.38461538461538464, 'f': 0.07407407233360773}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.5384615384615384, 'f': 0.14432989458603465}}\n",
            "pair:  optimistic initialisation effective strategy efficient exploration reinforcement learning rl tabular case provably efficient model free algorithms rely however model free deep rl algorithms use optimistic initialisation despite taking inspiration provably efficient tabular algorithms particular scenarios positive rewards values initialised lowest possible values due commonly used network initialisation schemes pessimistic initialisation merely initialising network output optimistic values enough since cannot ensure remain optimistic novel state action pairs crucial exploration propose simple count based augmentation pessimistically initialised values separates source optimism neural network show scheme provably efficient tabular setting extend deep rl setting algorithm optimistic pessimistically initialised learning opiq augments value estimates dqn based agent count derived bonuses ensure optimism action selection bootstrapping show opiq outperforms non optimistic dqn variants utilise pseudocount based intrinsic motivation hard exploration tasks predicts optimistic estimates novel state action pairs\n",
            "output sentence:  augment value estimates count based bonus ensures optimism action selection bootstrapping even value estimates pessimistic \n",
            "\n",
            "{'rouge-1': {'r': 0.23076923076923078, 'p': 0.9473684210526315, 'f': 0.3711340174683814}, 'rouge-2': {'r': 0.20689655172413793, 'p': 0.9473684210526315, 'f': 0.3396226385671058}, 'rouge-l': {'r': 0.23076923076923078, 'p': 0.9473684210526315, 'f': 0.3711340174683814}}\n",
            "pair:  exists plethora techniques inducing structured sparsity parametric models optimization process final goal resource efficient inference however best knowledge none target specific number floating point operations flops part single end end optimization objective despite reporting flops part results furthermore one size fits approach ignores realistic system constraints differ significantly say gpu mobile phone flops former incur less latency latter thus important practitioners able specify target number flops model compression work extend state art technique directly incorporate flops part optimization objective show given desired flops requirement different neural networks successfully trained image classification\n",
            "output sentence:  extend state art technique directly incorporate flops part optimization objective show given desired flops requirement different neural networks successfully trained \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.7, 'f': 0.16279069561925366}, 'rouge-2': {'r': 0.009523809523809525, 'p': 0.1, 'f': 0.017391302759924534}, 'rouge-l': {'r': 0.07894736842105263, 'p': 0.6, 'f': 0.13953488166576528}}\n",
            "pair:  imitation learning demonstrations usually relies learning policy trajectories optimal states actions however real life expert demonstrations often action information missing state trajectories available present model based imitation learning method learn environment specific optimal actions expert state trajectories proposed method starts model free reinforcement learning algorithm heuristic reward signal sample environment dynamics used train state transition probability subsequently learn optimal actions expert state trajectories supervised learning back propagating error gradients modeled environment dynamics experimental evaluations show proposed method successfully achieves performance similar state action trajectory based traditional imitation learning methods even absence action information much fewer iterations compared conventional model free reinforcement learning methods also demonstrate method learn act video demonstrations expert agent simple games learn achieve desired performance less number iterations\n",
            "output sentence:  learning imitate expert absence optimal actions learning dynamics model exploring environment \n",
            "\n",
            "{'rouge-1': {'r': 0.09574468085106383, 'p': 0.75, 'f': 0.16981131874688504}, 'rouge-2': {'r': 0.03571428571428571, 'p': 0.36363636363636365, 'f': 0.06504064877784392}, 'rouge-l': {'r': 0.06382978723404255, 'p': 0.5, 'f': 0.11320754516197937}}\n",
            "pair:  predictive models generalize well distributional shift often desirable sometimes crucial machine learning applications one example estimation treatment effects observational data subtask predict effect treatment subjects systematically different received treatment data related kind distributional shift appears unsupervised domain adaptation tasked generalizing distribution inputs different one observe labels pose problems prediction shift design popular methods overcoming distributional shift often heuristic rely assumptions rarely true practice well specified model knowing policy gave rise observed data methods hindered need pre specified metric comparing observations poor asymptotic properties work devise bound generalization error design shift based integral probability metrics sample weighting combine idea representation learning generalizing tightening existing results space finally propose algorithmic framework inspired bound verify effectiveness causal effect estimation\n",
            "output sentence:  theory algorithmic framework prediction distributional shift including causal effect estimation domain adaptation \n",
            "\n",
            "{'rouge-1': {'r': 0.043478260869565216, 'p': 0.4, 'f': 0.07843137078046909}, 'rouge-2': {'r': 0.008264462809917356, 'p': 0.1111111111111111, 'f': 0.015384614095858097}, 'rouge-l': {'r': 0.03260869565217391, 'p': 0.3, 'f': 0.058823527643214205}}\n",
            "pair:  program synthesis task automatically generating program consistent specification recent years seen proposal number neural approaches program synthesis many adopt sequence generation paradigm similar neural machine translation sequence sequence models trained maximize likelihood known reference programs achieving impressive results strategy two key limitations first ignores program aliasing fact many different programs may satisfy given specification especially incomplete specifications input output examples maximizing likelihood single reference program penalizes many semantically correct programs adversely affect synthesizer performance second strategy overlooks fact programs strict syntax efficiently checked address first limitation perform reinforcement learning top supervised model objective explicitly maximizes likelihood generating semantically correct programs addressing second limitation introduce training procedure directly maximizes probability generating syntactically correct programs fulfill specification show contributions lead improved accuracy models especially cases training data limited\n",
            "output sentence:  using dsl grammar reinforcement learning improve synthesis programs complex control flow \n",
            "\n",
            "{'rouge-1': {'r': 0.05357142857142857, 'p': 0.6, 'f': 0.09836065423273314}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.25, 'f': 0.028571427493877595}, 'rouge-l': {'r': 0.05357142857142857, 'p': 0.6, 'f': 0.09836065423273314}}\n",
            "pair:  momentum based methods conjunction stochastic gradient descent widely used training machine learning models little theoretical understanding generalization error methods practice momentum parameter often chosen heuristic fashion little theoretical guidance work use framework algorithmic stability provide upper bound generalization error class strongly convex loss functions mild technical assumptions bound decays zero inversely size training set increases momentum parameter increased also develop upper bound expected true risk terms number training steps size training set momentum parameter\n",
            "output sentence:  stochastic gradient method momentum generalizes \n",
            "\n",
            "{'rouge-1': {'r': 0.07317073170731707, 'p': 0.42857142857142855, 'f': 0.1249999975086806}, 'rouge-2': {'r': 0.021052631578947368, 'p': 0.15384615384615385, 'f': 0.037037034919410274}, 'rouge-l': {'r': 0.07317073170731707, 'p': 0.42857142857142855, 'f': 0.1249999975086806}}\n",
            "pair:  many partially observable scenarios reinforcement learning rl agents must rely long term memory order learn optimal policy demonstrate using techniques nlp supervised learning fails rl tasks due stochasticity environment exploration utilizing insights limitations traditional memory methods rl propose amrl class models learn better policies greater sample efficiency resilient noisy inputs specifically models use standard memory module summarize short term context aggregate prior states standard model without respect order show provides advantages terms gradient decay signal noise ratio time evaluating minecraft maze environments test long term memory find model improves average return baseline number parameters stronger baseline far parameters\n",
            "output sentence:  deep rl order invariant functions used conjunction standard memory modules improve gradient decay resilience noise \n",
            "\n",
            "{'rouge-1': {'r': 0.08695652173913043, 'p': 0.5454545454545454, 'f': 0.14999999762812505}, 'rouge-2': {'r': 0.0375, 'p': 0.3, 'f': 0.06666666469135808}, 'rouge-l': {'r': 0.07246376811594203, 'p': 0.45454545454545453, 'f': 0.12499999762812504}}\n",
            "pair:  much recent research devoted video prediction generation mostly short scale time horizons hierarchical video prediction method villegas et al example state art method long term video prediction however method limited applicability practical settings requires ground truth pose poses joints human training time paper presents long term hierarchical video prediction model restriction show network learns higher level structure pose equivalent hidden variables works better cases ground truth pose fully capture information needed predict next frame method gives sharper results video prediction methods require ground truth pose efficiency shown humans robot pushing datasets\n",
            "output sentence:  show ways train hierarchical video prediction model without needing pose labels \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.5, 'f': 0.1408450680023805}, 'rouge-2': {'r': 0.039473684210526314, 'p': 0.3, 'f': 0.06976743980530022}, 'rouge-l': {'r': 0.08196721311475409, 'p': 0.5, 'f': 0.1408450680023805}}\n",
            "pair:  propose novel unsupervised generative model elastic infogan learns disentangle object identity low level aspects class imbalanced datasets first investigate issues surrounding assumptions uniformity made infogan demonstrate ineffectiveness properly disentangle object identity imbalanced data key idea make discovery discrete latent factor variation invariant identity preserving transformations real images use signal learn latent distribution parameters experiments artificial mnist real world youtube faces datasets demonstrate effectiveness approach imbalanced data better disentanglement object identity latent factor variation ii better approximation class imbalance data reflected learned parameters latent distribution\n",
            "output sentence:  elastic infogan modification infogan learns without supervision disentangled representations class imbalanced data \n",
            "\n",
            "{'rouge-1': {'r': 0.26865671641791045, 'p': 1.0, 'f': 0.42352940842629766}, 'rouge-2': {'r': 0.1839080459770115, 'p': 0.8421052631578947, 'f': 0.30188678951050196}, 'rouge-l': {'r': 0.26865671641791045, 'p': 1.0, 'f': 0.42352940842629766}}\n",
            "pair:  statistical inference methods fundamentally important machine learning state art inference algorithms variants markov chain monte carlo mcmc variational inference vi however methods struggle limitations practice mcmc methods computationally demanding vi methods may large bias work aim improve upon mcmc vi novel hybrid method based idea reducing simulation bias finite length mcmc chains using gradient based optimisation proposed method generate low biased samples increasing length mcmc simulation optimising mcmc hyper parameters offers attractive balance approximation bias computational efficiency show method produces promising results popular benchmarks compared recent hybrid methods mcmc vi\n",
            "output sentence:  work aim improve upon mcmc vi novel hybrid method based idea reducing simulation bias finite chains using using gradient gradient using \n",
            "\n",
            "{'rouge-1': {'r': 0.04040404040404041, 'p': 0.4444444444444444, 'f': 0.07407407254629635}, 'rouge-2': {'r': 0.008620689655172414, 'p': 0.1111111111111111, 'f': 0.015999998663680112}, 'rouge-l': {'r': 0.030303030303030304, 'p': 0.3333333333333333, 'f': 0.055555554027777815}}\n",
            "pair:  adaptive optimization methods adagrad rmsprop adam proposed achieve rapid training process element wise scaling term learning rates though prevailing observed generalize poorly compared sgd even fail converge due unstable extreme learning rates recent work put forward algorithms amsgrad tackle issue failed achieve considerable improvement existing methods paper demonstrate extreme learning rates lead poor performance provide new variants adam amsgrad called adabound amsbound respectively employ dynamic bounds learning rates achieve gradual smooth transition adaptive methods sgd give theoretical proof convergence conduct experiments various popular tasks models often insufficient previous work experimental results show new variants eliminate generalization gap adaptive methods sgd maintain higher learning speed early training time moreover bring significant improvement prototypes especially complex deep networks implementation algorithm found https github com luolc adabound\n",
            "output sentence:  novel variants optimization methods combine benefits adaptive non adaptive methods \n",
            "\n",
            "{'rouge-1': {'r': 0.13580246913580246, 'p': 0.6470588235294118, 'f': 0.22448979305081215}, 'rouge-2': {'r': 0.05102040816326531, 'p': 0.3125, 'f': 0.08771929583256394}, 'rouge-l': {'r': 0.08641975308641975, 'p': 0.4117647058823529, 'f': 0.14285713998958774}}\n",
            "pair:  multivariate time series missing values common areas healthcare finance grown number complexity years raises question whether deep learning methodologies outperform classical data imputation methods domain however naive applications deep learning fall short giving reliable confidence estimates lack interpretability propose new deep sequential latent variable model dimensionality reduction data imputation modeling assumption simple interpretable high dimensional time series lower dimensional representation evolves smoothly time according gaussian process non linear dimensionality reduction presence missing data achieved using vae approach novel structured variational approximation demonstrate approach outperforms several classical deep learning based data imputation methods high dimensional data domains computer vision healthcare additionally improving smoothness imputations providing interpretable uncertainty estimates\n",
            "output sentence:  perform amortized variational inference latent gaussian process model achieve superior imputation performance multivariate time series missing data data \n",
            "\n",
            "{'rouge-1': {'r': 0.08888888888888889, 'p': 0.6666666666666666, 'f': 0.15686274302191466}, 'rouge-2': {'r': 0.020833333333333332, 'p': 0.2, 'f': 0.037735847347810686}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.5, 'f': 0.11764705674740487}}\n",
            "pair:  propose extend existing deep reinforcement learning deep rl algorithms allowing additionally choose sequences actions part policy modification forces network anticipate reward action sequences show improves exploration leading better convergence proposal simple flexible easily incorporated deep rl framework show power scheme consistently outperforming state art ga algorithm several popular atari games\n",
            "output sentence:  anticipation improves convergence deep reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.05714285714285714, 'p': 0.4444444444444444, 'f': 0.10126582076590292}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}, 'rouge-l': {'r': 0.05714285714285714, 'p': 0.4444444444444444, 'f': 0.10126582076590292}}\n",
            "pair:  understanding flow information deep neural networks dnns challenging problem gain increasing attention last years several methods proposed explain network predictions attempts compare theoretical perspective exhaustive empirical comparison performed past work analyze four gradient based attribution methods formally prove conditions equivalence approximation reformulating two methods construct unified framework enables direct comparison well easier implementation finally propose novel evaluation metric called sensitivity test gradient based attribution methods alongside simple perturbation based attribution method several datasets domains image text classification using various network architectures\n",
            "output sentence:  four existing backpropagation based attribution methods fundamentally similar assess \n",
            "\n",
            "{'rouge-1': {'r': 0.017391304347826087, 'p': 0.2857142857142857, 'f': 0.03278688416420321}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.017391304347826087, 'p': 0.2857142857142857, 'f': 0.03278688416420321}}\n",
            "pair:  historically pursuit efficient inference one driving forces hind research new deep learning architectures building blocks recent examples include squeeze excitation module hu et al depthwise separable convolutions xception chollet inverted bottleneck mobilenet sandler et al notably cases resulting building blocks enabled higher efficiency also higher accuracy found wide adoption field work expand arsenal efficient building blocks neural network architectures instead combining standard primitives convolution advocate replacement dense primitives sparse counterparts idea using sparsity decrease parameter count new mozer smolensky conventional wisdom reduction theoretical flops translate real world efficiency gains aim correct misconception introducing family efficient sparse kernels several hardware platforms plan open source benefit community equipped efficient implementation sparse primitives show sparse versions mobilenet mobilenet architectures substantially outperform strong dense baselines efficiency accuracy curve snapdragon sparse networks outperform dense equivalents equivalent approximately one entire generation improvement hope findings facilitate wider adoption sparsity tool creating efficient accurate deep learning architectures\n",
            "output sentence:  sparse mobilenets faster dense ones appropriate kernels \n",
            "\n",
            "{'rouge-1': {'r': 0.061224489795918366, 'p': 0.42857142857142855, 'f': 0.10714285495535718}, 'rouge-2': {'r': 0.016666666666666666, 'p': 0.16666666666666666, 'f': 0.03030302865013783}, 'rouge-l': {'r': 0.04081632653061224, 'p': 0.2857142857142857, 'f': 0.0714285692410715}}\n",
            "pair:  field deep learning craving optimization method shows outstanding property optimization generalization propose method mathematical optimization based flows along geodesics shortest paths two points respect riemannian metric induced non linear function method flows refer exponentially decaying flows edf designed converge local solutions exponentially paper conduct experiments show high performance optimization benchmarks convergence properties well potential producing good machine learning benchmarks generalization properties\n",
            "output sentence:  introduction new optimization method application deep learning \n",
            "\n",
            "{'rouge-1': {'r': 0.07526881720430108, 'p': 0.6363636363636364, 'f': 0.1346153827237426}, 'rouge-2': {'r': 0.016260162601626018, 'p': 0.18181818181818182, 'f': 0.029850744761639642}, 'rouge-l': {'r': 0.053763440860215055, 'p': 0.45454545454545453, 'f': 0.09615384426220416}}\n",
            "pair:  object recognition real world requires handling long tailed even open ended data ideal visual system needs reliably recognize populated visual concepts meanwhile efficiently learn emerging new categories training instances class balanced many shot learning shot learning tackle one side problem via either learning strong classifiers populated categories learning learn shot classifiers tail classes paper investigate problem generalized shot learning gfsl model deployment required learn tail categories shots simultaneously classify head tail categories propose classifier synthesis learning castle learning framework learns synthesize calibrated shot classifiers addition multi class classifiers head classes leveraging shared neural dictionary castle sheds light upon inductive gfsl optimizing one clean effective gfsl learning objective demonstrates superior performances existing gfsl algorithms strong baselines miniimagenet tieredimagenet data sets interestingly outperforms previous state art methods evaluated standard shot learning\n",
            "output sentence:  propose learn synthesizing shot classifiers many shot classifiers using one single objective function gfsl \n",
            "\n",
            "{'rouge-1': {'r': 0.1016949152542373, 'p': 0.5454545454545454, 'f': 0.1714285687795919}, 'rouge-2': {'r': 0.013333333333333334, 'p': 0.1, 'f': 0.023529409688581502}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.2727272727272727, 'f': 0.08571428306530622}}\n",
            "pair:  consider task program synthesis presence reward function output programs goal find programs maximal rewards introduce novel iterative optimization scheme train rnn dataset best programs priority queue generated programs far synthesize new programs add priority queue sampling rnn benchmark algorithm called priority queue training pqt genetic algorithm reinforcement learning baselines simple expressive turing complete programming language called bf experimental results show deceptively simple pqt algorithm significantly outperforms baselines adding program length penalty reward function able synthesize short human readable programs\n",
            "output sentence:  use simple search algorithm involving rnn priority queue find solutions coding tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.0759493670886076, 'p': 0.75, 'f': 0.137931032812789}, 'rouge-2': {'r': 0.00980392156862745, 'p': 0.14285714285714285, 'f': 0.018348622651292057}, 'rouge-l': {'r': 0.0759493670886076, 'p': 0.75, 'f': 0.137931032812789}}\n",
            "pair:  fundamental trait intelligence ability achieve goals face novel circumstances work address one setting requires solving task novel set actions empowering machines ability requires generalization way agent perceives available actions along way uses actions solve tasks hence propose framework enable generalization aspects understanding action functionality using actions solve tasks reinforcement learning specifically agent interprets action behavior using unsupervised representation learning collection data samples reflecting diverse properties action employ reinforcement learning architecture works action representations propose regularization metrics essential enabling generalization policy illustrate generalizability representation learning method policy enable zero shot generalization previously unseen actions challenging sequential decision making environments results videos found sites google com view action generalization\n",
            "output sentence:  address problem generalization reinforcement learning unseen action spaces \n",
            "\n",
            "{'rouge-1': {'r': 0.18421052631578946, 'p': 0.875, 'f': 0.30434782321361065}, 'rouge-2': {'r': 0.10465116279069768, 'p': 0.6, 'f': 0.17821781925301447}, 'rouge-l': {'r': 0.17105263157894737, 'p': 0.8125, 'f': 0.282608692778828}}\n",
            "pair:  recent work focused combining kernel methods deep learning mind introduce deepstr networks new architecture neural networks use replace top dense layers standard convolutional architectures approximation kernel function relying nystr approximation approach easy highly flexible compatible kernel function allows exploiting multiple kernels show deepstr networks reach state art performance standard datasets like svhn cifar one benefit method lies limited number learnable parameters make particularly suited small training set sizes samples per class finally illustrate two ways using multiple kernels including multiple deepstr setting exploits kernel feature map output convolutional part model\n",
            "output sentence:  new neural architecture top dense layers standard convolutional architectures replaced approximation kernel function relying nystr approximation \n",
            "\n",
            "{'rouge-1': {'r': 0.11702127659574468, 'p': 0.6875, 'f': 0.1999999975140496}, 'rouge-2': {'r': 0.027522935779816515, 'p': 0.1875, 'f': 0.04799999776768011}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.5, 'f': 0.14545454296859506}}\n",
            "pair:  exploration key component successful reinforcement learning optimal approaches computationally intractable researchers focused hand designing mechanisms based exploration bonuses intrinsic reward inspired curious behavior natural systems work propose strategy encoding curiosity algorithms programs domain specific language searching meta learning phase algorithms enable rl agents perform well new domains rich language programs combine neural networks building blocks including nearest neighbor modules choose loss functions enables expression highly generalizable programs perform well domains disparate grid navigation image input acrobot lunar lander ant hopper make approach feasible develop several pruning techniques including learning predict program success based syntactic properties demonstrate effectiveness approach empirically finding curiosity strategies similar published literature well novel strategies competitive generalize well\n",
            "output sentence:  meta learning curiosity algorithms searching rich space programs yields novel mechanisms generalize across different reinforcement learning domains \n",
            "\n",
            "{'rouge-1': {'r': 0.05128205128205128, 'p': 0.5714285714285714, 'f': 0.09411764554740484}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.038461538461538464, 'p': 0.42857142857142855, 'f': 0.07058823378269899}}\n",
            "pair:  automatic classification objects one important tasks engineering data mining applications although using complex advanced classifiers help improve accuracy classification systems done analyzing data sets features particular problem feature combination one improve quality features paper structure similar feed forward neural network ffnn used generate optimized linear non linear combination features classification genetic algorithm ga applied update weights biases since nature data sets features impact effectiveness combination classification system linear non linear activation functions transfer function used achieve reliable system experiments several uci data sets using minimum distance classifier simple classifier indicate proposed linear non linear intelligent ffnn based feature combination present reliable promising results using feature combination method need use powerful complex classifier anymore\n",
            "output sentence:  method enriching combining features improve classification accuracy \n",
            "\n",
            "{'rouge-1': {'r': 0.21052631578947367, 'p': 0.9411764705882353, 'f': 0.3440860185177477}, 'rouge-2': {'r': 0.16279069767441862, 'p': 0.875, 'f': 0.27450980127643215}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.9411764705882353, 'f': 0.3440860185177477}}\n",
            "pair:  deep neural networks shown incredible performance inference tasks variety domains unfortunately current deep networks enormous cloud based structures require significant storage space limits scaling deep learning service dlaas use device augmented intelligence paper finds algorithms directly use lossless compressed representations deep feedforward networks synaptic weights drawn discrete sets perform inference without full decompression basic insight allows less rate naive approaches recognition bipartite graph layers feedforward networks kind permutation invariance labeling nodes terms inferential operation inference operation depends locally edges directly connected also provide experimental results approach mnist dataset\n",
            "output sentence:  paper finds algorithms directly use lossless compressed representations deep feedforward networks perform inference without full decompression decompression \n",
            "\n",
            "{'rouge-1': {'r': 0.22666666666666666, 'p': 0.9444444444444444, 'f': 0.36559139472771424}, 'rouge-2': {'r': 0.1590909090909091, 'p': 0.7368421052631579, 'f': 0.2616822400698751}, 'rouge-l': {'r': 0.22666666666666666, 'p': 0.9444444444444444, 'f': 0.36559139472771424}}\n",
            "pair:  many tasks natural language processing related domains require high precision output obeys dataset specific constraints level fine grained control difficult obtain large scale neural network models work propose structured latent variable approach adds discrete control states within standard autoregressive neural paradigm formulation include range rich posterior constraints enforce task specific knowledge effectively trained neural model approach allows us provide arbitrary grounding internal model decisions without sacrificing representational power neural models experiments consider applications approach text generation part speech induction natural language generation find method improves standard benchmarks also providing fine grained control\n",
            "output sentence:  structured latent variable approach adds discrete control states within standard autoregressive neural paradigm provide arbitrary internal model internal internal internal \n",
            "\n",
            "{'rouge-1': {'r': 0.30158730158730157, 'p': 0.9047619047619048, 'f': 0.45238094863095246}, 'rouge-2': {'r': 0.20987654320987653, 'p': 0.7391304347826086, 'f': 0.3269230734781805}, 'rouge-l': {'r': 0.30158730158730157, 'p': 0.9047619047619048, 'f': 0.45238094863095246}}\n",
            "pair:  paper proposes new actor critic style algorithm called dual actor critic dual ac derived principled way lagrangian dual form bellman optimality equation viewed two player game actor critic like function named dual critic compared actor critic relatives dual ac desired property actor dual critic updated cooperatively optimize objective function providing transparent way learning critic directly related objective function actor provide concrete algorithm effectively solve minimax optimization problem using techniques multi step bootstrapping path regularization stochastic dual ascent algorithm demonstrate proposed algorithm achieves state art performances across several benchmarks\n",
            "output sentence:  propose dual actor critic algorithm derived principled way lagrangian dual form bellman optimality equation algorithm achieves state art performances across several benchmarks several benchmarks \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.6923076923076923, 'f': 0.2195121924538965}, 'rouge-2': {'r': 0.06172839506172839, 'p': 0.3333333333333333, 'f': 0.10416666402994797}, 'rouge-l': {'r': 0.07246376811594203, 'p': 0.38461538461538464, 'f': 0.12195121684414047}}\n",
            "pair:  multi domain learning mdl aims obtaining model minimal average risk across multiple domains empirical motivation automated microscopy data cultured cells imaged exposed known unknown chemical perturbations dataset displays significant experimental bias paper presents multi domain adversarial learning approach mulann leverage multiple datasets overlapping distinct class sets semi supervised setting contributions include bound average worst domain risk mdl obtained using divergence ii new loss accommodate semi supervised multi domain learning domain adaptation iii experimental validation approach improving state art two standard image benchmarks novel bioimage dataset cell\n",
            "output sentence:  adversarial domain adaptation multi domain learning new loss handle multi single domain classes semi supervised setting \n",
            "\n",
            "{'rouge-1': {'r': 0.08888888888888889, 'p': 0.5, 'f': 0.15094339366322537}, 'rouge-2': {'r': 0.04672897196261682, 'p': 0.3125, 'f': 0.08130081074492702}, 'rouge-l': {'r': 0.07777777777777778, 'p': 0.4375, 'f': 0.13207546913492352}}\n",
            "pair:  capturing long range feature relations central issue convolutional neural networks cnns tackle attempts integrate end end trainable attention module cnns widespread main goal works adjust feature maps considering spatial channel correlation inside convolution layer paper focus modeling relationships among layers propose novel structure recurrent layer attention network stores hierarchy features recurrent neural networks rnns concurrently propagating cnn adaptively scales feature volumes layers introduce several structural derivatives demonstrating compatibility recent attention modules expandability proposed network semantic understanding learned features also visualize intermediate layers plot curve layer scaling coefficients layer attention recurrent layer attention network achieves significant performance enhancement requiring slight increase parameters image classification task cifar imagenet dataset object detection task microsoft coco dataset\n",
            "output sentence:  propose new type end end trainable attention module applies global weight among layers utilizing co propagating cnn cnn \n",
            "\n",
            "{'rouge-1': {'r': 0.10714285714285714, 'p': 0.5, 'f': 0.17647058532871976}, 'rouge-2': {'r': 0.01834862385321101, 'p': 0.11764705882352941, 'f': 0.031746029411690774}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.3333333333333333, 'f': 0.11764705591695508}}\n",
            "pair:  unsupervised image image translation recently proposed task translating image different style domain given unpaired image examples training time paper formulate new task unsupervised video video translation poses unique challenges translating video implies learning appearance objects scenes also realistic motion transitions consecutive frames investigate performance per frame video video translation using existing image image translation networks propose spatio temporal translator alternative solution problem evaluate method multiple synthetic datasets moving colorized digits well realistic segmentation video gta dataset new ct mri volumetric images translation dataset results show frame wise translation produces realistic results single frame level underperforms significantly scale whole video compared three dimensional translation approach better able learn complex structure video motion continuity object appearance\n",
            "output sentence:  proposed new task datasets baselines conv cyclegan preserves object properties across frames batch structure frame level methods matters \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.7142857142857143, 'f': 0.2409638526172159}, 'rouge-2': {'r': 0.04, 'p': 0.21428571428571427, 'f': 0.06741572768589836}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.42857142857142855, 'f': 0.14457831044854116}}\n",
            "pair:  limited angle ct reconstruction determined linear inverse problem requires appropriate regularization techniques solved work study pre trained generative adversarial networks gans used clean noisy highly artifact laden reconstructions conventional techniques effectively projecting onto inferred image manifold particular use robust version popularly used gan prior inverse problems based recent technique called corruption mimicking significantly improves reconstruction quality proposed approach operates image space directly result need trained require access measurement model scanner agnostic work wide range sensing scenarios\n",
            "output sentence:  show robust gan priors work better gan priors limited angle ct reconstruction highly determined inverse problem \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.75, 'f': 0.13953488203353168}, 'rouge-2': {'r': 0.02040816326530612, 'p': 0.2857142857142857, 'f': 0.03809523685079368}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.625, 'f': 0.11627906808004328}}\n",
            "pair:  cloze test widely adopted language exams evaluate students language proficiency paper propose first large scale human designed cloze test dataset cloth questions used middle school high school language exams missing blanks carefully created teachers candidate choices purposely designed confusing cloth requires deeper language understanding wider attention span previous automatically generated cloze datasets show humans outperform dedicated designed baseline models significant margin even model trained sufficiently large external data investigate source performance gap trace model deficiencies distinct properties cloth identify limited ability comprehending long term context key bottleneck addition find human designed data leads larger gap model performance human performance compared automatically generated data\n",
            "output sentence:  cloze test dataset designed teachers assess language proficiency \n",
            "\n",
            "{'rouge-1': {'r': 0.14516129032258066, 'p': 0.75, 'f': 0.24324324052593138}, 'rouge-2': {'r': 0.0379746835443038, 'p': 0.2727272727272727, 'f': 0.06666666452098773}, 'rouge-l': {'r': 0.11290322580645161, 'p': 0.5833333333333334, 'f': 0.1891891864718773}}\n",
            "pair:  conversational question answering cqa novel qa task requires understanding dialogue context different traditional single turn machine reading comprehension mrc cqa comprehensive task comprised passage reading coreference resolution contextual understanding paper propose innovative contextualized attention based deep neural network sdnet fuse context traditional mrc models model leverages inter attention self attention comprehend conversation passage furthermore demonstrate novel method integrate bert contextual model sub module network empirical results show effectiveness sdnet coqa leaderboard outperforms previous best model score ensemble model improves score\n",
            "output sentence:  neural method conversational question answering attention mechanism novel usage bert contextual embedder \n",
            "\n",
            "{'rouge-1': {'r': 0.07575757575757576, 'p': 0.38461538461538464, 'f': 0.12658227573145336}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.08333333333333333, 'f': 0.02352940933979264}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.3076923076923077, 'f': 0.10126582003525084}}\n",
            "pair:  present data driven approach construct library feedback motion primitives non holonomic vehicles guarantees bounded error following arbitrarily long trajectories ensures motion planning avoided long disturbances vehicle remain within certain bound also potentially obstacles displaced within certain bound library constructed along local abstractions dynamics enables addition new motion primitives abstraction refinement provide sufficient conditions construction robust motion primitives large class nonlinear dynamics including commonly used models standard reeds shepp model algorithm applied motion planning control rover slipping without prior modelling\n",
            "output sentence:  show assumptions vehicle dynamics environment uncertainty possible automatically synthesize motion primitives accumulate error time \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 0.875, 'f': 0.20895522177767878}, 'rouge-2': {'r': 0.05714285714285714, 'p': 0.5714285714285714, 'f': 0.10389610224321134}, 'rouge-l': {'r': 0.0847457627118644, 'p': 0.625, 'f': 0.14925372924036534}}\n",
            "pair:  introduce cgnn framework learn functional causal models generative neural networks networks trained using backpropagation minimize maximum mean discrepancy observed data unlike previous approaches cgnn leverages conditional independences distributional asymmetries seamlessly discover bivariate multivariate causal structures without hidden variables cgnn estimate causal structure full differentiable generative model data throughout extensive variety experiments illustrate competitive esults cgnn state art alternatives observational causal discovery simulated real data tasks cause effect inference structure identification multivariate causal discovery\n",
            "output sentence:  discover structure functional causal models generative neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.09433962264150944, 'p': 0.4166666666666667, 'f': 0.15384615083550301}, 'rouge-2': {'r': 0.04918032786885246, 'p': 0.2727272727272727, 'f': 0.08333333074459885}, 'rouge-l': {'r': 0.09433962264150944, 'p': 0.4166666666666667, 'f': 0.15384615083550301}}\n",
            "pair:  learn identify decision states namely parsimonious set states decisions meaningfully affect future states agent reach environment utilize vic framework maximizes agent empowerment ie ability reliably reach diverse set states formulate sandwich bound empowerment objective allows identification decision states unlike previous work decision states discovered without extrinsic rewards simply interacting world results show decision states often interpretable lead better exploration downstream goal driven tasks partially observable environments\n",
            "output sentence:  identify decision states agent take actions matter without reward supervision use transfer \n",
            "\n",
            "{'rouge-1': {'r': 0.11956521739130435, 'p': 0.6470588235294118, 'f': 0.20183485975254614}, 'rouge-2': {'r': 0.00909090909090909, 'p': 0.058823529411764705, 'f': 0.015748029177258697}, 'rouge-l': {'r': 0.05434782608695652, 'p': 0.29411764705882354, 'f': 0.09174311663328011}}\n",
            "pair:  adversarial examples defined inputs model induce mistake model output different oracle perhaps surprising malicious ways original models adversarial attacks primarily studied context classification computer vision tasks several attacks proposed natural language processing nlp settings often vary defining parameters attack successful attack would look like goal work propose unifying model adversarial examples suitable nlp tasks generative classification settings define notion adversarial gain based control theory measure change output system relative perturbation input caused called adversary presented learner definition show used different feature spaces distance conditions determine attack defense effectiveness across different intuitive manifolds notion adversarial gain provides useful way evaluating adversaries defenses act building block future work robustness adversaries due rooted nature stability manifold theory\n",
            "output sentence:  propose alternative measure determining effectiveness adversarial attacks nlp models according distance measure based method like incremental gain control \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.6, 'f': 0.1499999978125}, 'rouge-2': {'r': 0.01282051282051282, 'p': 0.1111111111111111, 'f': 0.022988503892191984}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.5, 'f': 0.12499999781250003}}\n",
            "pair:  open question deep learning community neural networks trained gradient descent generalize well real datasets even though capable fitting random data propose approach answering question based hypothesis dynamics gradient descent call coherent gradients gradients similar examples similar overall gradient stronger certain directions reinforce thus changes network parameters training biased towards locally simultaneously benefit many examples similarity exists support hypothesis heuristic arguments perturbative experiments outline explain several common empirical observations deep learning furthermore analysis descriptive prescriptive suggests natural modification gradient descent greatly reduce overfitting\n",
            "output sentence:  propose hypothesis gradient descent generalizes based per example gradients interact \n",
            "\n",
            "{'rouge-1': {'r': 0.14473684210526316, 'p': 0.6111111111111112, 'f': 0.2340425500950657}, 'rouge-2': {'r': 0.030927835051546393, 'p': 0.15, 'f': 0.051282048447658865}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.3888888888888889, 'f': 0.14893616711634228}}\n",
            "pair:  neural networks structured data like graphs studied extensively recent years date bulk research activity focused mainly static graphs however real world networks dynamic since topology tends change time predicting evolution dynamic graphs task high significance area graph mining despite practical importance task explored depth far mainly due challenging nature paper propose model predicts evolution dynamic graphs specifically use graph neural network along recurrent architecture capture temporal evolution patterns dynamic graphs employ generative model predicts topology graph next time step constructs graph instance corresponds topology evaluate proposed model several artificial datasets following common network evolving dynamics well real world datasets results demonstrate effectiveness proposed model\n",
            "output sentence:  combining graph neural networks rnn graph generative model propose novel architecture able learn sequence evolving graphs predict graph topology topology timesteps \n",
            "\n",
            "{'rouge-1': {'r': 0.09302325581395349, 'p': 1.0, 'f': 0.1702127644001811}, 'rouge-2': {'r': 0.06481481481481481, 'p': 0.875, 'f': 0.1206896538882283}, 'rouge-l': {'r': 0.09302325581395349, 'p': 1.0, 'f': 0.1702127644001811}}\n",
            "pair:  message passing neural networks mpnns successfully applied wide variety applications real world however two fundamental weaknesses mpnns aggregators limit ability represent graph structured data losing structural information nodes neighborhoods lacking ability capture long range dependencies disassortative graphs studies noticed weaknesses different perspectives observations classical neural network network geometry propose novel geometric aggregation scheme graph neural networks overcome two weaknesses behind basic idea aggregation graph benefit continuous space underlying graph proposed aggregation scheme permutation invariant consists three modules node embedding structural neighborhood bi level aggregation also present implementation scheme graph convolutional networks termed geom gcn perform transductive learning graphs experimental results show proposed geom gcn achieved state art performance wide range open datasets graphs\n",
            "output sentence:  graph neural networks aggregation graph benefit continuous space underlying graph \n",
            "\n",
            "{'rouge-1': {'r': 0.0273972602739726, 'p': 0.25, 'f': 0.04938271426916635}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0273972602739726, 'p': 0.25, 'f': 0.04938271426916635}}\n",
            "pair:  major drawback backpropagation time bptt difficulty learning long term dependencies coming propagate credit information backwards every single step forward computation makes bptt computationally impractical biologically implausible reason full backpropagation time rarely used long sequences truncated backpropagation time used heuristic however usually leads biased estimates gradient longer term dependencies ignored addressing issue propose alternative algorithm sparse attentive backtracking might also related principles used brains learn long term dependencies sparse attentive backtracking learns attention mechanism hidden states past selectively backpropagates paths high attention weights allows model learn long term dependencies backtracking small number time steps recent past also attended relevant past states\n",
            "output sentence:  towards efficient credit assignment recurrent networks without backpropagation time \n",
            "\n",
            "{'rouge-1': {'r': 0.2054794520547945, 'p': 0.8333333333333334, 'f': 0.3296703264967999}, 'rouge-2': {'r': 0.1797752808988764, 'p': 0.8421052631578947, 'f': 0.29629629339677643}, 'rouge-l': {'r': 0.2054794520547945, 'p': 0.8333333333333334, 'f': 0.3296703264967999}}\n",
            "pair:  capability reliably detecting distribution samples one key factors deploying good classifier test distribution always match training distribution real world applications work propose deep generative classifier effective detect distribution samples well classify distribution samples integrating concept gaussian discriminant analysis deep neural networks unlike discriminative softmax classifier focuses decision boundary partitioning latent space multiple regions generative classifier aims explicitly model class conditional distributions separable gaussian distributions thereby define confidence score distance test sample center distribution empirical evaluation multi class images tabular data demonstrate generative classifier achieves best performances distinguishing distribution samples also generalized well various types deep neural networks\n",
            "output sentence:  paper proposes deep generative classifier effective detect distribution samples well classify distribution samples integrating concept gaussian discriminant analysis deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.14864864864864866, 'p': 0.6111111111111112, 'f': 0.2391304316351607}, 'rouge-2': {'r': 0.047058823529411764, 'p': 0.2222222222222222, 'f': 0.07766990002827799}, 'rouge-l': {'r': 0.10810810810810811, 'p': 0.4444444444444444, 'f': 0.1739130403308129}}\n",
            "pair:  recent advances generative adversarial networks facilitated improvements framework successful application various problems resulted extensions multiple domains irgan attempts leverage framework information retrieval ir task described modeling correct conditional probability distribution documents given query work proposes irgan claims optimizing minimax loss function result generator learn distribution setup baseline term steer model away exact adversarial formulation work attempts point certain inaccuracies formulation analyzing loss curves gives insight possible mistakes loss functions better performance obtained using co training like setup propose two models trained co operative rather adversarial fashion\n",
            "output sentence:  points problems loss function used irgan recently proposed gan framework information retrieval model motivated co training proposed performance performance \n",
            "\n",
            "{'rouge-1': {'r': 0.10416666666666667, 'p': 0.7142857142857143, 'f': 0.18181817959669425}, 'rouge-2': {'r': 0.017543859649122806, 'p': 0.16666666666666666, 'f': 0.03174603002267583}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.5714285714285714, 'f': 0.14545454323305787}}\n",
            "pair:  paper propose perform model ensembling multiclass multilabel learning setting using wasserstein barycenters optimal transport metrics wasserstein distance allow incorporating semantic side information word embeddings using barycenters find consensus models allows us balance confidence semantics finding agreement models show applications wasserstein ensembling attribute based classification multilabel learning image captioning generation results show ensembling viable alternative basic geometric arithmetic mean ensembling\n",
            "output sentence:  propose use wasserstein barycenters semantic model ensembling \n",
            "\n",
            "{'rouge-1': {'r': 0.06172839506172839, 'p': 0.8333333333333334, 'f': 0.1149425274514467}, 'rouge-2': {'r': 0.030927835051546393, 'p': 0.6, 'f': 0.058823528479431}, 'rouge-l': {'r': 0.06172839506172839, 'p': 0.8333333333333334, 'f': 0.1149425274514467}}\n",
            "pair:  effectively inferring discriminative coherent latent topics short texts critical task many real world applications nevertheless task proven great challenge traditional topic models due data sparsity problem induced characteristics short texts moreover complex inference algorithm also become bottleneck traditional models rapidly explore variations paper propose novel model called neural variational sparse topic model nvstm based sparsity enhanced topic model named sparse topical coding stc model auxiliary word embeddings utilized improve generation representations variational autoencoder vae approach applied inference model efficiently makes model easy explore extensions black box inference process experimental results onweb snippets newsgroups bbc biomedical datasets show effectiveness efficiency model\n",
            "output sentence:  neural sparsity enhanced topic model based vae \n",
            "\n",
            "{'rouge-1': {'r': 0.15254237288135594, 'p': 0.6428571428571429, 'f': 0.24657533936573467}, 'rouge-2': {'r': 0.075, 'p': 0.4, 'f': 0.12631578681440447}, 'rouge-l': {'r': 0.11864406779661017, 'p': 0.5, 'f': 0.1917808188177895}}\n",
            "pair:  multi step greedy policies extensively used model based reinforcement learning rl case model environment available game go work explore benefits multi step greedy policies model free rl employed framework multi step dynamic programming dp multi step policy value iteration algorithms iteratively solve short horizon decision problems converge optimal solution original one using model free algorithms solvers short horizon problems derive fully model free algorithms instances multi step dp framework model free algorithms prone instabilities decision problem horizon simple approach help mitigating instabilities results improved model free algorithms test approach show results discrete continuous control problems\n",
            "output sentence:  use model free algorithms like dqn trpo solve short horizon problems model free policy value value iteration \n",
            "\n",
            "{'rouge-1': {'r': 0.06172839506172839, 'p': 0.5, 'f': 0.10989010793382442}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.2222222222222222, 'f': 0.03883494986143847}, 'rouge-l': {'r': 0.04938271604938271, 'p': 0.4, 'f': 0.08791208595580248}}\n",
            "pair:  determinantal point processes dpps provide elegant versatile way sample sets items balance point wise quality set wise diversity selected items reason gained prominence many machine learning applications rely subset selection however sampling dpp ground set size costly operation requiring general preprocessing cost nk sampling cost subsets size approach problem introducing dppnets generative deep models produce dpp like samples arbitrary ground sets develop inhibitive attention mechanism based transformer networks captures notion dissimilarity feature vectors show theoretically approximation sensible maintains guarantees inhibition dissimilarity makes dpp powerful unique empirically demonstrate samples model receive high likelihood expensive dpp alternative\n",
            "output sentence:  approximate determinantal point processes neural nets justify model theoretically empirically \n",
            "\n",
            "{'rouge-1': {'r': 0.06666666666666667, 'p': 0.21052631578947367, 'f': 0.10126581913154957}, 'rouge-2': {'r': 0.014084507042253521, 'p': 0.045454545454545456, 'f': 0.021505372732108365}, 'rouge-l': {'r': 0.05, 'p': 0.15789473684210525, 'f': 0.07594936343534708}}\n",
            "pair:  current classical planners successful finding non optimal plans even large planning instances planners rely preprocessing stage computes grounded representation task whenever grounded task big generated whenever preprocess fails instance cannot even tackled actual planner address issue introduce partial grounding approach grounds projection task complete grounding feasible propose guiding mechanism given domain identifies parts task relevant find plan using shelf machine learning methods empirical evaluation attests approach capable solving planning instances big fully grounded\n",
            "output sentence:  paper introduces partial grounding tackle problem arises full grounding process translation input task task ground representation like strips like due memory due memory \n",
            "\n",
            "{'rouge-1': {'r': 0.034482758620689655, 'p': 0.4, 'f': 0.06349206203073825}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.034482758620689655, 'p': 0.4, 'f': 0.06349206203073825}}\n",
            "pair:  structural planning important producing long sentences missing part current language generation models work add planning phase neural machine translation control coarse structure output sentences model first generates planner codes predicts real output words conditioned codes learned capture coarse structure target sentence order learn codes design end end neural network discretization bottleneck predicts simplified part speech tags target sentences experiments show translation performance generally improved planning ahead also find translations different structures obtained manipulating planner codes\n",
            "output sentence:  plan syntactic structural translation using codes \n",
            "\n",
            "{'rouge-1': {'r': 0.136986301369863, 'p': 0.7142857142857143, 'f': 0.22988505477077553}, 'rouge-2': {'r': 0.04, 'p': 0.2857142857142857, 'f': 0.07017543644198222}, 'rouge-l': {'r': 0.1232876712328767, 'p': 0.6428571428571429, 'f': 0.2068965490236491}}\n",
            "pair:  orthogonal recurrent neural networks address vanishing gradient problem parameterizing recurrent connections using orthogonal matrix class models particularly effective solve tasks require memorization long sequences propose alternative solution based explicit memorization using linear autoencoders sequences show recently proposed recurrent architecture linear memory network composed nonlinear feedforward layer separate linear recurrence used solve hard memorization tasks propose initialization schema sets weights recurrent architecture approximate linear autoencoder input sequences found closed form solution initialization schema easily adapted recurrent architecture argue approach superior random orthogonal initialization due autoencoder allows memorization long sequences even training empirical analysis show approach achieves competitive results alternative orthogonal models lstm sequential mnist permuted mnist timit\n",
            "output sentence:  show initialize recurrent architectures closed form solution linear autoencoder sequences show advantages approach compared orthogonal rnns \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.5333333333333333, 'f': 0.20253164249319022}, 'rouge-2': {'r': 0.04054054054054054, 'p': 0.2, 'f': 0.06741572753440234}, 'rouge-l': {'r': 0.125, 'p': 0.5333333333333333, 'f': 0.20253164249319022}}\n",
            "pair:  conversational machine comprehension requires deep understanding conversation history enable traditional single turn models encode history comprehensively introduce flow mechanism incorporate intermediate representations generated process answering previous questions alternating parallel processing structure compared shallow approaches concatenate previous questions answers input flow integrates latent semantics conversation history deeply model flowqa shows superior performance two recently proposed conversational challenges coqa quac effectiveness flow also shows tasks reducing sequential instruction understanding conversational machine comprehension flowqa outperforms best models three domains scone improvement accuracy\n",
            "output sentence:  propose flow mechanism end end architecture flowqa achieves sota two conversational qa datasets sequential instruction understanding task \n",
            "\n",
            "{'rouge-1': {'r': 0.15151515151515152, 'p': 0.5, 'f': 0.23255813596538674}, 'rouge-2': {'r': 0.02531645569620253, 'p': 0.10526315789473684, 'f': 0.04081632340483156}, 'rouge-l': {'r': 0.12121212121212122, 'p': 0.4, 'f': 0.18604650805841003}}\n",
            "pair:  present deep graph infomax dgi general approach learning node representations within graph structured data unsupervised manner dgi relies maximizing mutual information patch representations corresponding high level summaries graphs derived using established graph convolutional network architectures learnt patch representations summarize subgraphs centered around nodes interest thus reused downstream node wise learning tasks contrast prior approaches unsupervised learning gcns dgi rely random walk objectives readily applicable transductive inductive learning setups demonstrate competitive performance variety node classification benchmarks times even exceeds performance supervised learning\n",
            "output sentence:  new method unsupervised representation learning graphs relying maximizing mutual information local global representations graph state art results competitive supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.5, 'f': 0.24999999625000005}, 'rouge-2': {'r': 0.10638297872340426, 'p': 0.35714285714285715, 'f': 0.16393442269282457}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.42857142857142855, 'f': 0.21428571053571432}}\n",
            "pair:  learn functional organization cortical microcircuits large scale recordings neural activity obtain explicit interpretable model time dependent functional connections neurons establish dynamics cortical information flow develop dynamic neural relational inference dnri study synthetic real world neural spiking data demonstrate developed method able uncover dynamic relations neurons reliably existing baselines\n",
            "output sentence:  develop dynamic neural relational inference variational autoencoder model explicitly interpretably represent hidden dynamic relations neurons \n",
            "\n",
            "{'rouge-1': {'r': 0.17475728155339806, 'p': 0.8571428571428571, 'f': 0.29032257783168575}, 'rouge-2': {'r': 0.06557377049180328, 'p': 0.4, 'f': 0.1126760539178735}, 'rouge-l': {'r': 0.1262135922330097, 'p': 0.6190476190476191, 'f': 0.20967741654136318}}\n",
            "pair:  state art computer vision models shown vulnerable small adversarial perturbations input words images data distribution correctly classified model close visually similar misclassified image despite substantial research interest cause phenomenon still poorly understood remains unsolved hypothesize counter intuitive behavior naturally occurring result high dimensional geometry data manifold first step towards exploring hypothesis study simple synthetic dataset classifying two concentric high dimensional spheres dataset show fundamental tradeoff amount test error average distance nearest error particular prove model misclassifies small constant fraction sphere vulnerable adversarial perturbations size sqrt surprisingly train several different architectures dataset error sets naturally approach theoretical bound result theory vulnerability neural networks small adversarial perturbations logical consequence amount test error observed hope theoretical analysis simple case point way forward explore geometry complex real world data sets leads adversarial examples\n",
            "output sentence:  hypothesize vulnerability image models small adversarial perturbation naturally occurring result high dimensional geometry data manifold explore theoretically prove hypothesis simple synthetic \n",
            "\n",
            "{'rouge-1': {'r': 0.07575757575757576, 'p': 0.3333333333333333, 'f': 0.12345678710562423}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.2, 'f': 0.07407407105624156}}\n",
            "pair:  one substitute neuron neural network kernel machine obtain counterpart powered kernel machines new network inherits expressive power architecture original works intuitive way since node enjoys simple interpretation hyperplane reproducing kernel hilbert space using kernel multilayer perceptron example prove classification optimal representation minimizes risk network characterized hidden layer result removes need backpropagation learning model generalized feedforward kernel network moreover unlike backpropagation turns models black boxes optimal hidden representation enjoys intuitive geometric interpretation making dynamics learning deep kernel network simple understand empirical results provided validate theory\n",
            "output sentence:  combine kernel method connectionist models show resulting deep architectures trained layer wise transparent learning dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.6153846153846154, 'f': 0.18823529152664362}, 'rouge-2': {'r': 0.02531645569620253, 'p': 0.16666666666666666, 'f': 0.04395604166646552}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5384615384615384, 'f': 0.16470587976193776}}\n",
            "pair:  develop stochastic whole brain body simulator nematode roundworm caenorhabditis elegans elegans show sufficiently regularizing allow imputation latent membrane potentials partial calcium fluorescence imaging observations first attempt know complete circle anatomically grounded whole connectome simulator used impute time varying brain state single cell fidelity covariates measurable practice using state art bayesian machine learning methods condition readily obtainable data method paves way neuroscientists recover interpretable connectome wide state representations automatically estimate physiologically relevant parameter values data perform simulations investigating intelligent lifeforms silico\n",
            "output sentence:  develop whole connectome body simulator elegans demonstrate joint state space parameter inference simulator \n",
            "\n",
            "{'rouge-1': {'r': 0.0641025641025641, 'p': 0.7142857142857143, 'f': 0.11764705731211073}, 'rouge-2': {'r': 0.028846153846153848, 'p': 0.5, 'f': 0.0545454535140496}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.7142857142857143, 'f': 0.11764705731211073}}\n",
            "pair:  neural embeddings used great success natural language processing nlp provide compact representations encapsulate word similarity attain state art performance range linguistic tasks success neural embeddings prompted significant amounts research applications domains language one domain graph structured data embeddings vertices learned encapsulate vertex similarity improve performance tasks including edge prediction vertex labelling nlp graph based tasks embeddings high dimensional euclidean spaces learned however recent work shown appropriate isometric space embedding complex networks flat euclidean space negatively curved hyperbolic space present new concept exploits recent insights propose learning neural embeddings graphs hyperbolic space provide experimental evidence hyperbolic embeddings significantly outperform euclidean embeddings vertex classification tasks several real world public datasets\n",
            "output sentence:  learn neural embeddings graphs hyperbolic instead euclidean space \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.7692307692307693, 'f': 0.24390243635633554}, 'rouge-2': {'r': 0.0449438202247191, 'p': 0.3333333333333333, 'f': 0.0792079186981669}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.6153846153846154, 'f': 0.19512194855145748}}\n",
            "pair:  graphs fundamental data structures required model many important real world data knowledge graphs physical social interactions molecules proteins paper study problem learning generative models graphs dataset graphs interest learning models used generate samples similar properties ones dataset models useful lot applications drug discovery knowledge graph construction task learning generative models graphs however unique challenges particular handle symmetries graphs ordering elements generation process important issues propose generic graph neural net based model capable generating arbitrary graph study performance graph generation tasks compared baselines exploit domain knowledge discuss potential issues open problems generative models going forward\n",
            "output sentence:  study graph generation problem propose powerful deep generative model capable generating arbitrary graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.8, 'f': 0.19753086203322667}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.1, 'f': 0.02197802002173668}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.6, 'f': 0.14814814598384393}}\n",
            "pair:  autoencoders provide powerful framework learning compressed representations encoding information needed reconstruct data point latent code cases autoencoders interpolate decoding convex combination latent codes two datapoints autoencoder produce output semantically mixes characteristics datapoints paper propose regularization procedure encourages interpolated outputs appear realistic fooling critic network trained recover mixing coefficient interpolated data develop simple benchmark task quantitatively measure extent various autoencoders interpolate show regularizer dramatically improves interpolation setting also demonstrate empirically regularizer produces latent codes effective downstream tasks suggesting possible link interpolation abilities learning useful representations\n",
            "output sentence:  propose regularizer improves interpolation autoencoders show also improves learned representation downstream tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.12698412698412698, 'p': 0.5714285714285714, 'f': 0.2077922048170012}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.15384615384615385, 'f': 0.044943817730084726}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.42857142857142855, 'f': 0.1558441528689493}}\n",
            "pair:  visual attention mechanisms widely used image captioning models paper better link image structure generated text replace traditional softmax attention mechanism two alternative sparsity promoting transformations sparsemax total variation sparse attention tvmax sparsemax obtain sparse attention weights selecting relevant features order promote sparsity encourage fusing related adjacent spatial locations propose tvmax selecting relevant groups features tvmax transformation improves interpretability present results microsoft coco flickr datasets obtaining gains comparison softmax tvmax outperforms compared attention mechanisms terms human rated caption quality attention relevance\n",
            "output sentence:  propose new sparse structured attention mechanism tvmax promotes sparsity encourages weight related adjacent locations \n",
            "\n",
            "{'rouge-1': {'r': 0.14754098360655737, 'p': 0.75, 'f': 0.24657533971852133}, 'rouge-2': {'r': 0.05970149253731343, 'p': 0.36363636363636365, 'f': 0.10256410014135443}, 'rouge-l': {'r': 0.14754098360655737, 'p': 0.75, 'f': 0.24657533971852133}}\n",
            "pair:  paper propose end end deep learning model called efold rna secondary structure prediction effectively take account inherent constraints problem key idea efold directly predict rna base pairing matrix use unrolled constrained programming algorithm building block architecture enforce constraints comprehensive experiments benchmark datasets demonstrate superior performance efold predicts significantly better structures compared previous sota improvement cases scores even larger improvement pseudoknotted structures runs efficient fastest algorithms terms inference time\n",
            "output sentence:  dl model rna secondary structure prediction uses unrolled algorithm architecture enforce constraints \n",
            "\n",
            "{'rouge-1': {'r': 0.0379746835443038, 'p': 0.42857142857142855, 'f': 0.06976744036506224}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02531645569620253, 'p': 0.2857142857142857, 'f': 0.04651162641157387}}\n",
            "pair:  variational auto encoders vae capable generating realistic images sounds video sequences practitioners point view usually interested solving problems tasks learned sequentially way avoids revisiting previous data stage address problem introducing conceptually simple scalable end end approach incorporating past knowledge learning prior directly data consider scalable boosting like approximation intractable theoretical optimal prior provide empirical studies two commonly used benchmarks namely mnist fashion mnist disjoint sequential image generation tasks dataset proposed method delivers best results among comparable approaches avoiding catastrophic forgetting fully automatic way fixed model architecture\n",
            "output sentence:  novel algorithm incremental learning vae fixed architecture \n",
            "\n",
            "{'rouge-1': {'r': 0.03260869565217391, 'p': 0.42857142857142855, 'f': 0.060606059291909015}, 'rouge-2': {'r': 0.008333333333333333, 'p': 0.16666666666666666, 'f': 0.015873014965986445}, 'rouge-l': {'r': 0.03260869565217391, 'p': 0.42857142857142855, 'f': 0.060606059291909015}}\n",
            "pair:  investigate extent individual attention heads pretrained transformer language models bert roberta implicitly capture syntactic dependency relations employ two methods taking maximum attention weight computing maximum spanning tree extract implicit dependency relations attention weights layer head compare ground truth universal dependency ud trees show ud relation types exist heads recover dependency type significantly better baselines parsed english text suggesting self attention heads act proxy syntactic structure also analyze bert fine tuned two datasets syntax oriented cola semantics oriented mnli investigate whether fine tuning affects patterns self attention observe substantial differences overall dependency relations extracted using methods results suggest models specialist attention heads track individual dependency types generalist head performs holistic parsing significantly better trivial baseline analyzing attention weights directly may reveal much syntactic knowledge bert style models known learn\n",
            "output sentence:  attention weights fully expose bert knows syntax \n",
            "\n",
            "{'rouge-1': {'r': 0.15625, 'p': 0.7142857142857143, 'f': 0.2564102534648258}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.16666666666666666, 'f': 0.04081632438150782}, 'rouge-l': {'r': 0.15625, 'p': 0.7142857142857143, 'f': 0.2564102534648258}}\n",
            "pair:  design simple quantifiable testing global translation invariance deep learning models trained mnist dataset experiments convolutional capsules neural networks show models poor performance dealing global translation invariance however performance improved using data augmentation although capsule network better mnist testing dataset convolutional neural network generally better performance translation invariance\n",
            "output sentence:  testing global translational invariance convolutional capsule networks \n",
            "\n",
            "{'rouge-1': {'r': 0.19607843137254902, 'p': 0.7142857142857143, 'f': 0.30769230431242606}, 'rouge-2': {'r': 0.08928571428571429, 'p': 0.38461538461538464, 'f': 0.14492753317370308}, 'rouge-l': {'r': 0.19607843137254902, 'p': 0.7142857142857143, 'f': 0.30769230431242606}}\n",
            "pair:  investigate learned dynamical landscape recurrent neural network solving simple task requiring interaction two memory mechanisms long short term results show long term memory implemented asymptotic attractors sequential recall additionally implemented oscillatory dynamics transverse subspace basins attraction stable steady states based observations propose different types memory mechanisms coexist work together single neural network discuss possible applications fields artificial intelligence neuroscience\n",
            "output sentence:  investigate recurrent neural network successfully learns task combining long term memory sequential recall recall \n",
            "\n",
            "{'rouge-1': {'r': 0.16901408450704225, 'p': 0.9230769230769231, 'f': 0.28571428309807256}, 'rouge-2': {'r': 0.13636363636363635, 'p': 0.9230769230769231, 'f': 0.23762376013332023}, 'rouge-l': {'r': 0.16901408450704225, 'p': 0.9230769230769231, 'f': 0.28571428309807256}}\n",
            "pair:  state art face super resolution methods employ deep convolutional neural networks learn mapping low high resolution facial patterns exploring local appearance knowledge however methods well exploit facial structures identity information struggle deal facial images exhibit large pose variation misalignment paper propose novel face super resolution method explicitly incorporates facial priors grasp sharp facial structures firstly face rendering branch set obtain priors salient facial structures identity knowledge secondly spatial attention mechanism used better exploit hierarchical information intensity similarity facial structure identity content super resolution problem extensive experiments demonstrate proposed algorithm achieves superior face super resolution results outperforms state art\n",
            "output sentence:  propose novel face super resolution method explicitly incorporates facial priors grasp sharp facial structures \n",
            "\n",
            "{'rouge-1': {'r': 0.02857142857142857, 'p': 0.2857142857142857, 'f': 0.051948050295159434}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02857142857142857, 'p': 0.2857142857142857, 'f': 0.051948050295159434}}\n",
            "pair:  paper present method adversarial decomposition text representation method used decompose representation input sentence several independent vectors vector responsible specific aspect input sentence evaluate proposed method two case studies conversion different social registers diachronic language change show proposed method capable fine grained con trolled change aspects input sentence example model capable learning continuous rather categorical representation style sentence line reality language use model uses adversarial motivational training includes special motivational loss acts opposite discriminator encourages better decomposition finally evaluate obtained meaning embeddings downstream task para phrase detection show significantly better embeddings regular autoencoder\n",
            "output sentence:  method learns separate representations meaning form sentence \n",
            "\n",
            "{'rouge-1': {'r': 0.08181818181818182, 'p': 0.6923076923076923, 'f': 0.14634146152422503}, 'rouge-2': {'r': 0.028985507246376812, 'p': 0.3333333333333333, 'f': 0.05333333186133337}, 'rouge-l': {'r': 0.06363636363636363, 'p': 0.5384615384615384, 'f': 0.11382113632097297}}\n",
            "pair:  unsupervised domain adaptive object detection aims learn robust detector domain shift circumstance training source domain label rich bounding box annotations testing target domain label agnostic feature distributions training testing domains dissimilar even totally different paper propose gradient detach based stacked complementary losses scl method uses detection objective cross entropy smooth regression primary objective cuts several auxiliary losses different network stages utilize information complement data target images effective adapting model parameters source target domains gradient detach operation applied detection context sub networks training force networks learn discriminative representations argue conventional training primary objective mainly leverages information source domain maximizing likelihood ignores complement data shallow layers networks leads insufficient integration within different domains thus proposed method syncretic adaptation learning process conduct comprehensive experiments seven datasets results demonstrate method performs favorably better state art methods large margin instance cityscapes foggycityscapes achieve map outperforming previous art strong weak\n",
            "output sentence:  introduce new gradient detach based complementary objective training strategy domain adaptive object detection \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.16666666666666666, 'f': 0.03960395830212735}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.46153846153846156, 'f': 0.14117646799723185}}\n",
            "pair:  present meta learning approach adaptive text speech tts data training learn multi speaker model using shared conditional wavenet core independent learned embeddings speaker aim training produce neural network fixed weights deployed tts system instead aim produce network requires data deployment time rapidly adapt new speakers introduce benchmark three strategies learning speaker embedding keeping wavenet core fixed ii fine tuning entire architecture stochastic gradient descent iii predicting speaker embedding trained neural network encoder experiments show approaches successful adapting multi speaker neural network new speakers obtaining state art results sample naturalness voice similarity merely minutes audio data new speakers\n",
            "output sentence:  sample efficient algorithms adapt text speech model new voice style state art performance \n",
            "\n",
            "{'rouge-1': {'r': 0.10606060606060606, 'p': 0.4375, 'f': 0.1707317041760857}, 'rouge-2': {'r': 0.0547945205479452, 'p': 0.26666666666666666, 'f': 0.09090908808109513}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.375, 'f': 0.1463414602736467}}\n",
            "pair:  detection photo manipulation relies subtle statistical traces notoriously removed aggressive lossy compression employed online demonstrate end end modeling complex photo dissemination channels allows codec optimization explicit provenance objectives design lightweight trainable lossy image codec delivers competitive rate distortion performance par best hand engineered alternatives lower computational footprint modern gpu enabled platforms results show significant improvements manipulation detection accuracy possible fractional costs bandwidth storage codec improved accuracy even low bit rates well practicality jpeg qf\n",
            "output sentence:  learn efficient lossy image codec optimized facilitate reliable photo manipulation detection fractional cost payload quality even low \n",
            "\n",
            "{'rouge-1': {'r': 0.10869565217391304, 'p': 0.9090909090909091, 'f': 0.19417475537373932}, 'rouge-2': {'r': 0.0743801652892562, 'p': 0.8181818181818182, 'f': 0.13636363483585862}, 'rouge-l': {'r': 0.10869565217391304, 'p': 0.9090909090909091, 'f': 0.19417475537373932}}\n",
            "pair:  goal standard compressive sensing estimate unknown vector linear measurements assumption sparsity basis recently shown significantly fewer measurements may required sparsity assumption replaced assumption unknown vector lies near range suitably chosen generative model particular bora em et al shown roughly log random gaussian measurements suffice accurate recovery input generative model bounded lipschitz kd log measurements suffice input relu networks depth width paper establish corresponding algorithm independent lower bounds sample complexity using tools minimax statistical analysis accordance upper bounds results summarized follows construct lipschitz generative model capable generating group sparse signals show resulting necessary number measurements omega log ii using similar ideas construct two layer relu networks high width requiring omega log measurements well lower width deep relu networks requiring omega measurements result establish scaling laws derived bora em et al optimal near optimal absence assumptions\n",
            "output sentence:  establish scaling laws derived bora et al optimal near optimal absence assumptions \n",
            "\n",
            "{'rouge-1': {'r': 0.09183673469387756, 'p': 0.5, 'f': 0.15517241117122477}, 'rouge-2': {'r': 0.02608695652173913, 'p': 0.17647058823529413, 'f': 0.04545454321051435}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.3888888888888889, 'f': 0.12068965255053513}}\n",
            "pair:  work propose sparse deep scattering crois network sdcsn novel architecture based deep scattering network dsn dsn achieved cascading wavelet transform convolutions complex modulus time invariant operator extend work first crossing multiple wavelet family transforms increase feature diversity avoiding learning thus providing informative latent representation benefit development highly specialized wavelet filters last decades beside combining different wavelet representations reduce amount prior information needed regarding signals hand secondly develop optimal thresholding strategy complete filter banks regularizes network controls instabilities inherent non stationary noise signal systematic principled solution sparsifies latent representation network acting local mask distinguishing activity noise thus propose enhance dsn increasing variance scattering coefficients representation well improve robustness respect non stationary noise show new approach robust outperforms dsn bird detection task\n",
            "output sentence:  propose enhance deep scattering network order improve control stability given machine learning pipeline proposing continuous wavelet thresholding scheme \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.8571428571428571, 'f': 0.2891566237015532}, 'rouge-2': {'r': 0.046511627906976744, 'p': 0.3076923076923077, 'f': 0.08080807852668102}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.5714285714285714, 'f': 0.19277108153287853}}\n",
            "pair:  exploration fundamental aspect reinforcement learning typically implemented using stochastic action selection exploration however efficient directed toward gaining new world knowledge visit counters proven useful practice theory directed exploration however major limitation counters locality model based solutions shortcoming model free approach still missing propose values generalization counters used evaluate propagating exploratory value state action trajectories compare approach commonly used rl techniques show using values improves learning performance traditional counters also show method implemented function approximation efficiently learn continuous mdps demonstrate showing approach surpasses state art performance freeway atari game\n",
            "output sentence:  propose generalization visit counters evaluate propagating exploratory value trajectories enabling efficient exploration model free rl \n",
            "\n",
            "{'rouge-1': {'r': 0.038461538461538464, 'p': 0.21428571428571427, 'f': 0.06521738872400767}, 'rouge-2': {'r': 0.01098901098901099, 'p': 0.07692307692307693, 'f': 0.019230767043269485}, 'rouge-l': {'r': 0.038461538461538464, 'p': 0.21428571428571427, 'f': 0.06521738872400767}}\n",
            "pair:  current trade depth computational cost makes difficult adopt deep neural networks many industrial applications especially computing power limited inspired idea deeper embeddings needed discriminate difficult samples large number samples well discriminated via much shallower embeddings study introduce concept decision gates gate modules trained decide whether sample needs projected deeper embedding early prediction made gate thus enabling computation dynamic representations different depths proposed gate modules integrated deep neural network reduces average computational cost deep neural networks maintaining modeling accuracy experimental results show leveraging proposed gate modules led speed flops reduction resnet speed sim flops reduction densenet trained cifar dataset drop accuracy\n",
            "output sentence:  paper introduces new dynamic feature representation approach provide efficient way inference deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.14545454545454545, 'p': 0.6666666666666666, 'f': 0.23880596720873248}, 'rouge-2': {'r': 0.0821917808219178, 'p': 0.46153846153846156, 'f': 0.13953488115467824}, 'rouge-l': {'r': 0.10909090909090909, 'p': 0.5, 'f': 0.17910447467141904}}\n",
            "pair:  verification planning domain models crucial ensure safety integrity correctness planning based automated systems task usually performed using model checking techniques however directly applying model checkers verify planning domain models result false positives counterexamples unreachable sound planner using domain verification planning task paper discuss downside unconstrained planning domain model verification propose fail safe practice designing planning domain models inherently guarantee safety produced plans case undetected errors domain models addition demonstrate model checkers well state trajectory constraints planning techniques used verify planning domain models unreachable counterexamples returned\n",
            "output sentence:  constrain planning domain model verification planning goals avoid unreachable counterexamples false positives verification outcomes \n",
            "\n",
            "{'rouge-1': {'r': 0.11428571428571428, 'p': 0.8, 'f': 0.1999999978125}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.4444444444444444, 'f': 0.08247422512062921}, 'rouge-l': {'r': 0.08571428571428572, 'p': 0.6, 'f': 0.1499999978125}}\n",
            "pair:  aligning knowledge graphs different sources languages aims align entity relation critical variety applications knowledge graph construction question answering existing methods knowledge graph alignment usually rely large number aligned knowledge triplets train effective models however aligned triplets may available expensive obtain many domains therefore paper study design fully unsupervised methods weakly supervised methods align knowledge graphs without aligned triplets propose unsupervised framework based adversarial training able map entities relations source knowledge graph target knowledge graph framework seamlessly integrated existing supervised methods limited number aligned triplets utilized guidance experiments real world datasets prove effectiveness proposed approach weakly supervised unsupervised settings\n",
            "output sentence:  paper studies weakly supervised knowledge graph alignment adversarial training frameworks \n",
            "\n",
            "{'rouge-1': {'r': 0.15584415584415584, 'p': 0.9230769230769231, 'f': 0.26666666419506174}, 'rouge-2': {'r': 0.10101010101010101, 'p': 0.8333333333333334, 'f': 0.1801801782517653}, 'rouge-l': {'r': 0.15584415584415584, 'p': 0.9230769230769231, 'f': 0.26666666419506174}}\n",
            "pair:  paper introduces network architecture solve structure motion sfm problem via feature metric bundle adjustment ba explicitly enforces multi view geometry constraints form feature metric error whole pipeline differentiable network learn suitable features make ba problem tractable furthermore work introduces novel depth parameterization recover dense per pixel depth network first generates several basis depth maps according input image optimizes final depth linear combination basis depth maps via feature metric ba basis depth maps generator also learned via end end training whole system nicely combines domain knowledge hard coded multi view geometry constraints deep learning feature learning basis depth maps learning address challenging dense sfm problem experiments large scale real data prove success proposed method\n",
            "output sentence:  paper introduces network architecture solve structure motion sfm problem via feature bundle adjustment ba \n",
            "\n",
            "{'rouge-1': {'r': 0.1896551724137931, 'p': 0.6875, 'f': 0.29729729390796206}, 'rouge-2': {'r': 0.057971014492753624, 'p': 0.26666666666666666, 'f': 0.09523809230442186}, 'rouge-l': {'r': 0.1206896551724138, 'p': 0.4375, 'f': 0.18918918579985394}}\n",
            "pair:  propose novel score based approach learning directed acyclic graph dag observational data adapt recently proposed continuous constrained optimization formulation allow nonlinear relationships variables using neural networks extension allows model complex interactions global search compared greedy approaches addition comparing method existing continuous optimization methods provide missing empirical comparisons nonlinear greedy search methods synthetic real world data sets new method outperforms current continuous methods tasks competitive existing greedy search methods important metrics causal inference\n",
            "output sentence:  proposing new score based approach structure causal learning leveraging neural networks recent continuous constrained formulation problem \n",
            "\n",
            "{'rouge-1': {'r': 0.3548387096774194, 'p': 0.9166666666666666, 'f': 0.5116279029529476}, 'rouge-2': {'r': 0.19736842105263158, 'p': 0.625, 'f': 0.29999999635200003}, 'rouge-l': {'r': 0.25806451612903225, 'p': 0.6666666666666666, 'f': 0.3720930192320173}}\n",
            "pair:  interactive fiction games text based simulations agent interacts world purely natural language ideal environments studying extend reinforcement learning agents meet challenges natural language understanding partial observability action generation combinatorially large text based action spaces present kg agent builds dynamic knowledge graph exploring generates actions using template based action space contend dual uses knowledge graph reason game state constrain natural language generation keys scalable exploration combinatorially large natural language actions results across wide variety games show kg outperforms current agents despite exponential increase action space size\n",
            "output sentence:  present kg reinforcement learning agent builds dynamic knowledge graph exploring generates natural language using template based action space outperforming current agents wide set text based games \n",
            "\n",
            "{'rouge-1': {'r': 0.15151515151515152, 'p': 0.5, 'f': 0.23255813596538674}, 'rouge-2': {'r': 0.02531645569620253, 'p': 0.10526315789473684, 'f': 0.04081632340483156}, 'rouge-l': {'r': 0.12121212121212122, 'p': 0.4, 'f': 0.18604650805841003}}\n",
            "pair:  present deep graph infomax dgi general approach learning node representations within graph structured data unsupervised manner dgi relies maximizing mutual information patch representations corresponding high level summaries graphs derived using established graph convolutional network architectures learnt patch representations summarize subgraphs centered around nodes interest thus reused downstream node wise learning tasks contrast prior approaches unsupervised learning gcns dgi rely random walk objectives readily applicable transductive inductive learning setups demonstrate competitive performance variety node classification benchmarks times even exceeds performance supervised learning\n",
            "output sentence:  new method unsupervised representation learning graphs relying maximizing mutual information local global representations graph state art results competitive supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.11428571428571428, 'p': 0.6153846153846154, 'f': 0.19277108169545654}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.3333333333333333, 'f': 0.07999999788800007}, 'rouge-l': {'r': 0.08571428571428572, 'p': 0.46153846153846156, 'f': 0.14457831061111923}}\n",
            "pair:  markov logic networks mlns elegantly combine logic rules probabilistic graphical models used address many knowledge graph problems however inference mln computationally intensive making industrial scale application mln difficult recent years graph neural networks gnns emerged efficient effective tools large scale graph problems nevertheless gnns explicitly incorporate prior logic rules models may require many labeled examples target task paper explore combination mlns gnns use graph neural networks variational inference mln propose gnn variant named expressgnn strikes nice balance representation power simplicity model extensive experiments several benchmark datasets demonstrate expressgnn leads effective efficient probabilistic logic reasoning\n",
            "output sentence:  employ graph neural networks variational em framework efficient inference learning markov logic networks \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.8571428571428571, 'f': 0.12765957308963333}, 'rouge-2': {'r': 0.0297029702970297, 'p': 0.5, 'f': 0.05607476529653246}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.8571428571428571, 'f': 0.12765957308963333}}\n",
            "pair:  deep neural networks achieved outstanding performance many real world applications expense huge computational resources densenet one recently proposed neural network architecture achieved state art performance many visual tasks however great redundancy due dense connections internal structure leads high computational costs training dense networks address issue design reinforcement learning framework search efficient densenet architectures layer wise pruning lwp different tasks retaining original advantages densenet feature reuse short paths etc framework agent evaluates importance connection two block layers prunes redundant connections addition novel reward shaping trick introduced make densenet reach better trade accuracy float point operations flops experiments show densenet lwp compact efficient existing alternatives\n",
            "output sentence:  learning search efficient densenet layer wise pruning \n",
            "\n",
            "{'rouge-1': {'r': 0.21428571428571427, 'p': 0.6666666666666666, 'f': 0.324324320642805}, 'rouge-2': {'r': 0.07936507936507936, 'p': 0.23809523809523808, 'f': 0.11904761529761916}, 'rouge-l': {'r': 0.10714285714285714, 'p': 0.3333333333333333, 'f': 0.1621621584806429}}\n",
            "pair:  recent advances illustrated often possible learn solve linear inverse problems imaging using training data outperform traditional regularized least squares solutions along lines present extensions neumann network recently introduced end end learned architecture inspired truncated neumann series expansion solution map regularized least squares problem summarize neumann network approach show form compatible optimal reconstruction function given inverse problem also investigate extension neumann network incorporates sample efficient patch based regularization approach\n",
            "output sentence:  neumann networks end end sample efficient learning approach solving linear inverse problems imaging mse mse optimal approach admit admit extension patch learning \n",
            "\n",
            "{'rouge-1': {'r': 0.021505376344086023, 'p': 0.3333333333333333, 'f': 0.04040403926538112}, 'rouge-2': {'r': 0.00909090909090909, 'p': 0.2, 'f': 0.01739130351606809}, 'rouge-l': {'r': 0.021505376344086023, 'p': 0.3333333333333333, 'f': 0.04040403926538112}}\n",
            "pair:  passage time unmanned autonomous vehicles uavs especially autonomous flying drones grabbed lot attention artificial intelligence since electronic technology getting smaller cheaper efficient huge advancement study uavs observed recently monitoring floods discerning spread algae water bodies detecting forest trail application far wide work mainly focused autonomous flying drones establish case study towards efficiency robustness accuracy uavs showed results well supported experiments provide details software hardware architecture used study discuss implementation algorithms present experiments provide comparison three different state art algorithms namely trailnet inceptionresnet mobilenet terms accuracy robustness power consumption inference time study shown mobilenet produced better results less computational requirement power consumption also reported challenges faced work well brief discussion future work improve safety features performance\n",
            "output sentence:  case study optimal deep learning model uavs \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.45, 'f': 0.16822429602585381}, 'rouge-2': {'r': 0.010101010101010102, 'p': 0.05263157894736842, 'f': 0.01694914984056349}, 'rouge-l': {'r': 0.05747126436781609, 'p': 0.25, 'f': 0.09345794088566697}}\n",
            "pair:  beyond understanding discussed human communication requires awareness someone feeling one challenge dialogue agents recognizing feelings conversation partner replying accordingly key communicative skill trivial humans research area made difficult paucity suitable publicly available datasets emotion dialogues work proposes new task empathetic dialogue generation empatheticdialogues dataset conversations grounded emotional situations facilitate training evaluating dialogue systems experiments indicate dialogue models use dataset perceived empathetic human evaluators improving metrics well perceived relevance responses bleu scores compared models merely trained large scale internet conversation data also present empirical comparisons several ways improve performance given model leveraging existing models datasets without requiring lengthy training full model\n",
            "output sentence:  improve existing dialogue systems responding people sharing personal stories incorporating emotion prediction representations also release new benchmark dataset empathetic dialogues \n",
            "\n",
            "{'rouge-1': {'r': 0.2037037037037037, 'p': 0.8461538461538461, 'f': 0.3283582058275786}, 'rouge-2': {'r': 0.12121212121212122, 'p': 0.6153846153846154, 'f': 0.2025316428200609}, 'rouge-l': {'r': 0.14814814814814814, 'p': 0.6153846153846154, 'f': 0.23880596702160842}}\n",
            "pair:  wide range defenses proposed harden neural networks adversarial attacks however pattern emerged majority adversarial defenses quickly broken new attacks given lack success generating robust defenses led ask fundamental question adversarial attacks inevitable paper analyzes adversarial examples theoretical perspective identifies fundamental bounds susceptibility classifier adversarial attacks show certain classes problems adversarial examples inescapable using experiments explore implications theoretical guarantees real world problems discuss factors dimensionality image complexity limit classifier robustness adversarial examples\n",
            "output sentence:  paper identifies classes problems adversarial examples inescapable derives fundamental bounds susceptibility classifier adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.14084507042253522, 'p': 1.0, 'f': 0.2469135780826094}, 'rouge-2': {'r': 0.10112359550561797, 'p': 1.0, 'f': 0.18367346771970014}, 'rouge-l': {'r': 0.14084507042253522, 'p': 1.0, 'f': 0.2469135780826094}}\n",
            "pair:  deep neural networks use deeper broader structures achieve better performance consequently use increasingly gpu memory well however limited gpu memory restricts many potential designs neural networks paper propose reinforcement learning based variable swapping recomputation algorithm reduce memory cost without sacrificing accuracy models variable swapping transfer variables cpu gpu memory reduce variables stored gpu memory recomputation trade time space removing feature maps forward propagation forward functions executed get feature maps reuse however automatically decide variables swapped recomputed remains challenging problem address issue propose use deep network dqn make plans combining variable swapping recomputation results outperform several well known benchmarks\n",
            "output sentence:  propose reinforcement learning based variable swapping recomputation algorithm reduce memory cost \n",
            "\n",
            "{'rouge-1': {'r': 0.05063291139240506, 'p': 0.5, 'f': 0.09195402131853615}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05063291139240506, 'p': 0.5, 'f': 0.09195402131853615}}\n",
            "pair:  paper study adversarial attack defence problem deep learning perspective fourier analysis first explicitly compute fourier transform deep relu neural networks show exist decaying non zero high frequency components fourier spectrum neural networks demonstrate vulnerability neural networks towards adversarial samples attributed insignificant non zero high frequency components based analysis propose use simple post averaging technique smooth high frequency components improve robustness neural networks adversarial attacks experimental results imagenet cifar datasets shown proposed method universally effective defend many existing adversarial attacking methods proposed literature including fgsm pgd deepfool attacks post averaging method simple since require training meanwhile successfully defend adversarial samples generated methods without introducing significant performance degradation less original clean images\n",
            "output sentence:  insight reason adversarial vulnerability effective defense method adversarial attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.0759493670886076, 'p': 0.5, 'f': 0.13186812957855334}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.18181818181818182, 'f': 0.03809523621950123}, 'rouge-l': {'r': 0.06329113924050633, 'p': 0.4166666666666667, 'f': 0.1098901076005314}}\n",
            "pair:  deep learning natural language processing models often use vector word embeddings word vec glove represent words discrete sequence words much easily integrated downstream neural layers represented sequence continuous vectors also semantic relationships words learned text corpus encoded relative configurations embedding vectors however storing accessing embedding vectors words dictionary requires large amount space may stain systems limited gpu memory used approaches inspired quantum computing propose two related methods word ket word ketxs storing word embedding matrix training inference highly efficient way approach achieves hundred fold reduction space required store embeddings almost relative drop accuracy practical natural language processing tasks\n",
            "output sentence:  use ideas quantum computing proposed word embeddings utilize much fewer trainable parameters \n",
            "\n",
            "{'rouge-1': {'r': 0.12727272727272726, 'p': 0.5833333333333334, 'f': 0.2089552209400757}, 'rouge-2': {'r': 0.04411764705882353, 'p': 0.2727272727272727, 'f': 0.07594936469155592}, 'rouge-l': {'r': 0.12727272727272726, 'p': 0.5833333333333334, 'f': 0.2089552209400757}}\n",
            "pair:  holistically exploring perceptual neural representations underlying animal communication traditionally difficult complexity underlying signal present novel set techniques project entire communicative repertoires low dimensional spaces systematically sampled exploring relationship perceptual representations neural representations latent representational spaces learned machine learning algorithms showcase method one ongoing experiment studying sequential temporal maintenance context songbird neural perceptual representations syllables discuss studying neural mechanisms underlying maintenance long range information content present birdsong inform informed machine sequence modeling\n",
            "output sentence:  compare perceptual neural modeled representations animal communication using machine learning behavior physiology \n",
            "\n",
            "{'rouge-1': {'r': 0.19148936170212766, 'p': 0.6923076923076923, 'f': 0.2999999966055556}, 'rouge-2': {'r': 0.07547169811320754, 'p': 0.3333333333333333, 'f': 0.12307692006627227}, 'rouge-l': {'r': 0.14893617021276595, 'p': 0.5384615384615384, 'f': 0.23333332993888892}}\n",
            "pair:  transfer learning uses trained weights source model initial weightsfor training target dataset well chosen source large numberof labeled data leads significant improvement accuracy demonstrate atechnique automatically labels large unlabeled datasets trainsource models transfer learning experimentally evaluate method usinga baseline dataset human annotated imagenet labels five variationsof technique show performance automatically trainedmodels come within baseline average\n",
            "output sentence:  technique automatically labeling large unlabeled datasets train source models transfer learning experimental evaluation \n",
            "\n",
            "{'rouge-1': {'r': 0.05555555555555555, 'p': 0.09090909090909091, 'f': 0.06896551253269949}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.09090909090909091, 'f': 0.06896551253269949}}\n",
            "pair:  main goal network pruning imposing sparsity neural network increasing number parameters zero value order reduce architecture size computational speedup\n",
            "output sentence:  proposing novel method based guided attention enforce sparisty deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.2857142857142857, 'p': 0.8333333333333334, 'f': 0.4255319110909914}, 'rouge-2': {'r': 0.20454545454545456, 'p': 0.75, 'f': 0.3214285680612245}, 'rouge-l': {'r': 0.2857142857142857, 'p': 0.8333333333333334, 'f': 0.4255319110909914}}\n",
            "pair:  show information whether neural network output correct incorrect present outputs network intermediate layers demonstrate effect train new meta network predict either final output underlying base network output one base network intermediate layers whether base network correct incorrect particular input find wide range tasks base networks meta network achieve accuracies ranging making determination\n",
            "output sentence:  information whether neural network output correct incorrect somewhat present outputs network intermediate layers \n",
            "\n",
            "{'rouge-1': {'r': 0.11428571428571428, 'p': 0.6153846153846154, 'f': 0.19277108169545654}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.3333333333333333, 'f': 0.07999999788800007}, 'rouge-l': {'r': 0.08571428571428572, 'p': 0.46153846153846156, 'f': 0.14457831061111923}}\n",
            "pair:  markov logic networks mlns elegantly combine logic rules probabilistic graphical models used address many knowledge graph problems however inference mln computationally intensive making industrial scale application mln difficult recent years graph neural networks gnns emerged efficient effective tools large scale graph problems nevertheless gnns explicitly incorporate prior logic rules models may require many labeled examples target task paper explore combination mlns gnns use graph neural networks variational inference mln propose gnn variant named expressgnn strikes nice balance representation power simplicity model extensive experiments several benchmark datasets demonstrate expressgnn leads effective efficient probabilistic logic reasoning\n",
            "output sentence:  employ graph neural networks variational em framework efficient inference learning markov logic networks \n",
            "\n",
            "{'rouge-1': {'r': 0.16853932584269662, 'p': 1.0, 'f': 0.2884615359929734}, 'rouge-2': {'r': 0.10810810810810811, 'p': 0.7058823529411765, 'f': 0.18749999769653322}, 'rouge-l': {'r': 0.16853932584269662, 'p': 1.0, 'f': 0.2884615359929734}}\n",
            "pair:  integration knowledge base kb neural dialogue agent one key challenges conversational ai memory networks proven effective encode kb information external memory thus generate fluent informed responses unfortunately memory becomes full latent representations training common strategy overwrite old memory entries randomly paper question approach provide experimental evidence showing conventional memory networks generate many redundant latent vectors resulting overfitting need larger memories introduce memory dropout automatic technique encourages diversity latent space aging redundant memories increase probability overwritten training sampling new memories summarize knowledge acquired redundant memories technique allows us incorporate knowledge bases achieve state art dialogue generation stanford multi turn dialogue dataset considering architecture use provides improvement bleu points automatic generation responses increase recognition named entities\n",
            "output sentence:  conventional memory networks generate many redundant latent vectors resulting overfitting need larger memories memory memory automatic encourages latent space \n",
            "\n",
            "{'rouge-1': {'r': 0.05102040816326531, 'p': 0.45454545454545453, 'f': 0.091743117451393}, 'rouge-2': {'r': 0.008928571428571428, 'p': 0.1, 'f': 0.016393441117979177}, 'rouge-l': {'r': 0.030612244897959183, 'p': 0.2727272727272727, 'f': 0.05504586974497103}}\n",
            "pair:  propose fully convolutional conditional generative model latent transformation neural network ltnn capable view synthesis using light weight neural network suited real time applications contrast existing conditional generative models incorporate conditioning information via concatenation introduce dedicated network component conditional transformation unit ctu designed learn latent space transformations corresponding specified target views addition consistency loss term defined guide network toward learning desired latent space mappings task divided decoder constructed refine quality generated views adaptive discriminator introduced improve adversarial training process generality proposed methodology demonstrated collection three diverse tasks multi view reconstruction real hand depth images view synthesis real synthetic faces rotation rigid objects proposed model shown exceed state art results category simultaneously achieving reduction computational demand required inference average\n",
            "output sentence:  introduce effective general framework incorporating conditioning information inference based generative models \n",
            "\n",
            "{'rouge-1': {'r': 0.22448979591836735, 'p': 0.7333333333333333, 'f': 0.34374999641113285}, 'rouge-2': {'r': 0.13793103448275862, 'p': 0.5, 'f': 0.21621621282688094}, 'rouge-l': {'r': 0.22448979591836735, 'p': 0.7333333333333333, 'f': 0.34374999641113285}}\n",
            "pair:  giving provable guarantees learning neural networks core challenge machine learning theory prior work gives parameter recovery guarantees one hidden layer networks however networks used practice multiple non linear layers work show strengthen results deeper networks address problem uncovering lowest layer deep neural network assumption lowest layer uses high threshold applying activation upper network modeled well behaved polynomial input distribution gaussian\n",
            "output sentence:  provably recover lowest layer deep neural network assuming lowest layer uses high threshold activation network well behaved polynomial \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 0.85, 'f': 0.32380952072562363}, 'rouge-2': {'r': 0.09565217391304348, 'p': 0.5238095238095238, 'f': 0.16176470327097756}, 'rouge-l': {'r': 0.17647058823529413, 'p': 0.75, 'f': 0.2857142826303855}}\n",
            "pair:  goal imitation learning il enable learner imitate expert behavior given expert demonstrations recently generative adversarial imitation learning gail shown significant progress il complex continuous tasks however gail extensions require large number environment interactions training real world environments il method requires learner interact environment better imitation training time requires damage causes environments learner believe il algorithms could applicable real world problems number interactions could reduced paper propose model free il algorithm continuous control algorithm made mainly three changes existing adversarial imitation learning ail methods adopting policy actor critic pac algorithm optimize learner policy estimating state action value using policy samples without learning reward functions representing stochastic policy function outputs bounded experimental results show algorithm achieves competitive results gail significantly reducing environment interactions\n",
            "output sentence:  paper proposed model free policy il algorithm continuous control experimental results showed algorithm achieves competitive results gail significantly reducing interactions environment interactions \n",
            "\n",
            "{'rouge-1': {'r': 0.19387755102040816, 'p': 0.95, 'f': 0.32203389548980177}, 'rouge-2': {'r': 0.12295081967213115, 'p': 0.7894736842105263, 'f': 0.21276595511493385}, 'rouge-l': {'r': 0.19387755102040816, 'p': 0.95, 'f': 0.32203389548980177}}\n",
            "pair:  presence bias confounding effects inarguably one critical challenges machine learning applications alluded pivotal debates recent years challenges range spurious associations confounding variables medical studies bias race gender face recognition systems one solution enhance datasets organize reflect biases cumbersome intensive task alternative make use available data build models considering biases traditional statistical methods apply straightforward techniques residualization stratification precomputed features account confounding variables however techniques general applicable end end deep learning methods paper propose method based adversarial training strategy learn discriminative features unbiased invariant confounder enabled incorporating new adversarial loss function encourages vanished correlation bias learned features apply method synthetic medical diagnosis gender classification gender shades dataset results show learned features method result superior prediction performance also uncorrelated bias confounder variables code available http blinded review\n",
            "output sentence:  propose method based adversarial training strategy learn discriminative features unbiased invariant confounder incorporating loss function encourages vanished correlation learned features \n",
            "\n",
            "{'rouge-1': {'r': 0.09411764705882353, 'p': 0.8, 'f': 0.16842105074792246}, 'rouge-2': {'r': 0.04032258064516129, 'p': 0.5555555555555556, 'f': 0.07518796866301093}, 'rouge-l': {'r': 0.08235294117647059, 'p': 0.7, 'f': 0.14736841916897506}}\n",
            "pair:  existing defenses adversarial attacks consider robustness bounded distortions reality specific attack rarely known advance adversaries free modify images ways lie outside fixed distortion model example adversarial rotations lie outside set bounded distortions work advocate measuring robustness much broader range unforeseen attacks attacks whose precise form unknown defense design propose several new attacks methodology evaluating defense diverse range unforeseen distortions first construct novel adversarial jpeg fog gabor snow distortions simulate diverse adversaries introduce uar summary metric measures robustness defense given distortion using uar assess robustness existing novel attacks perform extensive study adversarial robustness find evaluation existing attacks yields redundant information generalize attacks instead recommend evaluating significantly diverse set attacks find adversarial training either one multiple distortions fails confer robustness attacks distortion types results underscore need evaluate study robustness unforeseen distortions\n",
            "output sentence:  propose several new attacks methodology measure robustness unforeseen adversarial attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.35714285714285715, 'f': 0.09174311702718631}, 'rouge-2': {'r': 0.014814814814814815, 'p': 0.15384615384615385, 'f': 0.027027025424580086}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.35714285714285715, 'f': 0.09174311702718631}}\n",
            "pair:  knowledge distillation kd common method transferring knowledge learned one machine learning model teacher another model student typically teacher greater capacity parameters higher bit widths knowledge existing methods overlook fact although student absorbs extra knowledge teacher models share input data data medium teacher knowledge demonstrated due difference model capacities student may benefit fully data points teacher trained hand human teacher may demonstrate piece knowledge individualized examples adapted particular student instance terms cultural background interests inspired behavior design data augmentation agents distinct roles facilitate knowledge distillation data augmentation agents generate distinct training data teacher student respectively focus specifically kd teacher network greater precision bit width student network find empirically specially tailored data points enable teacher knowledge demonstrated effectively student compare approach existing kd methods training popular neural architectures demonstrate role wise data augmentation improves effectiveness kd strong prior approaches code reproducing results made publicly available\n",
            "output sentence:  study whether adaptive data augmentation knowledge distillation leveraged simultaneously synergistic manner better student networks \n",
            "\n",
            "{'rouge-1': {'r': 0.15254237288135594, 'p': 0.8181818181818182, 'f': 0.2571428544938776}, 'rouge-2': {'r': 0.0684931506849315, 'p': 0.5, 'f': 0.12048192559152272}, 'rouge-l': {'r': 0.1016949152542373, 'p': 0.5454545454545454, 'f': 0.1714285687795919}}\n",
            "pair:  present novel architecture gan disentangled representation learning new model architecture inspired information bottleneck ib theory thereby named ib gan ib gan objective similar infogan crucial difference capacity regularization mutual information adopted thanks generator ib gan harness latent representation disentangled interpretable manner facilitate optimization ib gan practice new variational upper bound derived experiments celeba dchairs dsprites datasets demonstrate visual quality samples generated ib gan often better vaes moreover ib gan achieves much higher disentanglement metrics score vaes infogan dsprites dataset\n",
            "output sentence:  inspired information bottleneck theory propose new architecture gan disentangled representation learning \n",
            "\n",
            "{'rouge-1': {'r': 0.08247422680412371, 'p': 0.6666666666666666, 'f': 0.14678898886625708}, 'rouge-2': {'r': 0.04838709677419355, 'p': 0.5, 'f': 0.08823529250865055}, 'rouge-l': {'r': 0.07216494845360824, 'p': 0.5833333333333334, 'f': 0.12844036501304604}}\n",
            "pair:  convolutional neural networks cnns generally acknowledged one driving forces advancement computer vision despite promising performances many tasks cnns still face major obstacles road achieving ideal machine intelligence one cnns complex hard interpret another standard cnns require large amounts annotated data sometimes hard obtain desirable able learn examples work address limitations cnns developing novel simple interpretable models shot learn ing models based idea encoding objects terms visual concepts interpretable visual cues represented feature vectors within cnns first adapt learning visual concepts shot setting uncover two key properties feature encoding using visual concepts call category sensitivity spatial pattern motivated properties present two intuitive models problem shot learning experiments show models achieve competitive performances much flexible interpretable alternative state art shot learning methods conclude using visual concepts helps expose natural capability cnns shot learning\n",
            "output sentence:  enable ordinary cnns shot learning exploiting visual concepts interpretable visual cues learnt within cnns \n",
            "\n",
            "{'rouge-1': {'r': 0.09574468085106383, 'p': 0.6428571428571429, 'f': 0.1666666644101509}, 'rouge-2': {'r': 0.04, 'p': 0.3125, 'f': 0.07092198380363167}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.5714285714285714, 'f': 0.1481481458916324}}\n",
            "pair:  neural style transfer become popular technique generating images distinct artistic styles using convolutional neural networks recent success image style transfer raised question whether similar methods leveraged alter style musical audio work attempt long time scale high quality audio transfer texture synthesis time domain captures harmonic rhythmic timbral elements related musical style using examples may different lengths musical keys demonstrate ability use randomly initialized convolutional neural networks transfer aspects musical style one piece onto another using different representations audio log magnitude short time fourier transform stft mel spectrogram constant transform spectrogram propose using representations way generating modifying perceptually significant characteristics musical audio content demonstrate representation shortcomings advantages others carefully designing neural network structures complement nature musical audio finally show compelling style transfer examples make use ensemble representations help capture varying desired characteristics audio signals\n",
            "output sentence:  present long time scale musical audio style transfer algorithm synthesizes audio time domain uses time frequency representations audio \n",
            "\n",
            "{'rouge-1': {'r': 0.08, 'p': 0.5, 'f': 0.1379310321046374}, 'rouge-2': {'r': 0.021052631578947368, 'p': 0.15384615384615385, 'f': 0.037037034919410274}, 'rouge-l': {'r': 0.08, 'p': 0.5, 'f': 0.1379310321046374}}\n",
            "pair:  paper present approach learn recomposable motor primitives across large scale diverse manipulation demonstrations current approaches decomposing demonstrations primitives often assume manually defined primitives bypass difficulty discovering primitives hand approaches primitive discovery put restrictive assumptions complexity primitive limit applicability narrow tasks approach attempts circumvent challenges jointly learning underlying motor primitives recomposing primitives form original demonstration constraints parsimony primitive decomposition simplicity given primitive able learn diverse set motor primitives well coherent latent representation primitives demonstrate qualitatively quantitatively learned primitives capture semantically meaningful aspects demonstration allows us compose primitives hierarchical reinforcement learning setup efficiently solve robotic manipulation tasks like reaching pushing\n",
            "output sentence:  learn space motor primitives unannotated robot demonstrations show primitives semantically meaningful composed new robot tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.1038961038961039, 'p': 0.6153846153846154, 'f': 0.1777777753061729}, 'rouge-2': {'r': 0.061224489795918366, 'p': 0.46153846153846156, 'f': 0.10810810604009419}, 'rouge-l': {'r': 0.1038961038961039, 'p': 0.6153846153846154, 'f': 0.1777777753061729}}\n",
            "pair:  propose novel framework generate clean video frames single motion blurred image broad range literature focuses recovering single image blurred image work tackle challenging task video restoration blurred image formulate video restoration single blurred image inverse problem setting clean image sequence respective motion latent factors blurred image observation framework based encoder decoder structure spatial transformer network modules restore video sequence underlying motion end end manner design loss function regularizers complementary properties stabilize training analyze variant models proposed network effectiveness transferability network highlighted large set experiments two different types datasets camera rotation blurs generated panorama scenes dynamic motion blurs high speed videos code models publicly available\n",
            "output sentence:  present novel unified architecture restores video frames single motion blurred image end end manner \n",
            "\n",
            "{'rouge-1': {'r': 0.20270270270270271, 'p': 1.0, 'f': 0.3370786488827169}, 'rouge-2': {'r': 0.1320754716981132, 'p': 1.0, 'f': 0.23333333127222225}, 'rouge-l': {'r': 0.20270270270270271, 'p': 1.0, 'f': 0.3370786488827169}}\n",
            "pair:  mixup data augmentation scheme pairs training samples corresponding labels mixed using linear coefficients without label mixing mixup becomes conventional scheme input samples moved original labels retained samples preferentially moved direction classes iffalse typically clustered input space fi refer method directional adversarial training dat show two mild conditions mixup asymptotically convergences subset dat define untied mixup umixup superset mixup wherein training labels mixed different linear coefficients corresponding samples show mild conditions untied mixup converges entire class dat schemes motivated understanding umixup generalization mixup form adversarial training experiment different datasets loss functions show umixup provides improved performance mixup short present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup\n",
            "output sentence:  present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.5, 'f': 0.25806451229968785}, 'rouge-2': {'r': 0.05434782608695652, 'p': 0.21739130434782608, 'f': 0.08695651853913056}, 'rouge-l': {'r': 0.14492753623188406, 'p': 0.4166666666666667, 'f': 0.21505375961151582}}\n",
            "pair:  meta reinforcement learning approaches aim develop learning procedures adapt quickly distribution tasks help examples developing efficient exploration strategies capable finding useful samples becomes critical settings existing approaches finding efficient exploration strategies add auxiliary objectives promote exploration pre update policy however makes adaptation using gradient steps difficult pre update exploration post update exploitation policies quite different instead propose explicitly model separate exploration policy task distribution two different policies gives flexibility training exploration policy also makes adaptation specific task easier show using self supervised supervised learning objectives adaptation stabilizes training process also demonstrate superior performance model compared prior works domain\n",
            "output sentence:  propose use separate exploration policy collect pre adaptation trajectories maml also show using self supervised objective inner loop leads stable training much better performance \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.5555555555555556, 'f': 0.09174311775103107}, 'rouge-2': {'r': 0.007936507936507936, 'p': 0.125, 'f': 0.014925372011583955}, 'rouge-l': {'r': 0.03, 'p': 0.3333333333333333, 'f': 0.055045870044609084}}\n",
            "pair:  effectiveness convolutional neural networks stems large part ability exploit translation invariance inherent many learning problems recently shown cnns exploit invariances rotation invariance using group convolutions instead planar convolutions however reasons performance ease implementation necessary limit group convolution transformations applied filters without interpolation thus images square pixels integer translations rotations multiples degrees reflections admissible whereas square tiling provides fold rotational symmetry hexagonal tiling plane fold rotational symmetry paper show one efficiently implement planar convolution group convolution hexagonal lattices using existing highly optimized convolution routines find due reduced anisotropy hexagonal filters planar hexaconv provides better accuracy planar convolution square filters given fixed parameter budget furthermore find increased degree symmetry hexagonal grid increases effectiveness group convolutions allowing parameter sharing show method significantly outperforms conventional cnns aid aerial scene classification dataset even outperforming imagenet pre trained models\n",
            "output sentence:  introduce hexaconv group equivariant convolutional neural network hexagonal lattices \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.75, 'f': 0.1999999976888889}, 'rouge-2': {'r': 0.029411764705882353, 'p': 0.2727272727272727, 'f': 0.05309734337536226}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.5833333333333334, 'f': 0.15555555324444445}}\n",
            "pair:  shot image classification aims learning classifier limited labeled data generating classification weights applied many meta learning approaches shot image classification due simplicity effectiveness however argue difficult generate exact universal classification weights diverse query samples training samples work introduce attentive weights generation shot learning via information maximization awgim addresses current issues two novel contributions awgim generates different classification weights different query samples letting query samples attends whole support set ii guarantee generated weights adaptive different query sample formulate problem maximize lower bound mutual information generated weights query well support data far see first attempt unify information maximization shot learning two contributions proved effective extensive experiments show awgim able achieve state art performance benchmark datasets\n",
            "output sentence:  novel shot learning method generate query specific classification weights via information maximization \n",
            "\n",
            "{'rouge-1': {'r': 0.036036036036036036, 'p': 0.36363636363636365, 'f': 0.06557376885111534}, 'rouge-2': {'r': 0.0072992700729927005, 'p': 0.09090909090909091, 'f': 0.01351351213750927}, 'rouge-l': {'r': 0.036036036036036036, 'p': 0.36363636363636365, 'f': 0.06557376885111534}}\n",
            "pair:  recent advances neural variational inference allowed renaissance latent variable models variety domains involving high dimensional data paper introduce two generic variational inference frameworks generative models knowledge graphs latent fact model latent information model traditional variational methods derive analytical approximation intractable distribution latent variables construct inference network conditioned symbolic representation entities relation types knowledge graph provide variational distributions new framework create models able discover underlying probabilistic semantics symbolic representation utilising parameterisable distributions permit training back propagation context neural variational inference resulting highly scalable method bernoulli sampling framework provide alternative justification commonly used techniques large scale stochastic variational inference drastically reduces training time cost additional approximation variational lower bound generative frameworks flexible enough allow training prior distribution permits parametrisation trick well scoring function permits maximum likelihood estimation parameters experiment results display potential efficiency framework improving upon multiple benchmarks gaussian prior representations code publicly available github\n",
            "output sentence:  working toward generative knowledge graph models better estimate predictive uncertainty knowledge inference \n",
            "\n",
            "{'rouge-1': {'r': 0.16417910447761194, 'p': 0.55, 'f': 0.2528735596776325}, 'rouge-2': {'r': 0.0759493670886076, 'p': 0.3, 'f': 0.1212121179879605}, 'rouge-l': {'r': 0.08955223880597014, 'p': 0.3, 'f': 0.13793103094200035}}\n",
            "pair:  study emergence cooperative behaviors reinforcement learning agents introducing challenging competitive multi agent soccer environment continuous simulated physics demonstrate decentralized population based training co play lead progression agents behaviors random simple ball chasing finally showing evidence cooperation study highlights several challenges encountered large scale multi agent training continuous control particular demonstrate automatic optimization simple shaping rewards conducive co operative behavior lead long horizon team behavior apply evaluation scheme grounded game theoretic principals assess agent performance absence pre defined evaluation tasks human baselines\n",
            "output sentence:  introduce new mujoco soccer environment continuous multi agent reinforcement learning research show population based training independent reinforcement learners learn cooperative behaviors \n",
            "\n",
            "{'rouge-1': {'r': 0.1875, 'p': 0.8333333333333334, 'f': 0.3061224459808413}, 'rouge-2': {'r': 0.0761904761904762, 'p': 0.42105263157894735, 'f': 0.129032255469563}, 'rouge-l': {'r': 0.1125, 'p': 0.5, 'f': 0.1836734663890046}}\n",
            "pair:  non autoregressive machine translation nat systems predict sequence output tokens parallel achieving substantial improvements generation speed compared autoregressive models existing nat models usually rely technique knowledge distillation creates training data pretrained autoregressive model better performance knowledge distillation empirically useful leading large gains accuracy nat models reason success yet unclear paper first design systematic experiments investigate knowledge distillation crucial nat training find knowledge distillation reduce complexity data sets help nat model variations output data furthermore strong correlation observed capacity nat model optimal complexity distilled data best translation quality based findings propose several approaches alter complexity data sets improve performance nat models achieve state art performance nat based models close gap autoregressive baseline wmt en de benchmark\n",
            "output sentence:  systematically examine knowledge distillation crucial training non autoregressive translation nat models propose methods improve distilled data best best capacity nat model \n",
            "\n",
            "{'rouge-1': {'r': 0.11363636363636363, 'p': 0.625, 'f': 0.192307689704142}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.5, 'f': 0.1538461512426036}}\n",
            "pair:  work propose self supervised method learn sentence representations injection linguistic knowledge multiple linguistic frameworks propose diverse sentence structures semantic meaning might expressed compositional words operations aim take advantage linguist diversity learn represent sentences contrasting diverse views formally multiple views sentence mapped close representations contrary views sentences mapped contrasting different linguistic views aim building embeddings better capture semantic less sensitive sentence outward form\n",
            "output sentence:  aim exploit diversity linguistic structures build sentence representations \n",
            "\n",
            "{'rouge-1': {'r': 0.1232876712328767, 'p': 0.6428571428571429, 'f': 0.2068965490236491}, 'rouge-2': {'r': 0.043478260869565216, 'p': 0.26666666666666666, 'f': 0.0747663527294961}, 'rouge-l': {'r': 0.1095890410958904, 'p': 0.5714285714285714, 'f': 0.1839080432765227}}\n",
            "pair:  rate medical questions asked online significantly exceeds capacity qualified people answer leaving many questions unanswered inadequately answered many questions unique reliable identification similar questions would enable efficient effective question answering schema many research efforts focused problem general question similarity approaches generalize well medical domain medical expertise often required determine semantic similarity paper show semi supervised approach pre training neural network medical question answer pairs particularly useful intermediate task ultimate goal determining medical question similarity pre training tasks yield accuracy task model achieves accuracy number training examples accuracy much smaller training set accuracy full corpus medical question answer data used\n",
            "output sentence:  show question answer matching particularly good pre training task question similarity release dataset medical question similarity \n",
            "\n",
            "{'rouge-1': {'r': 0.047619047619047616, 'p': 0.75, 'f': 0.08955223768322566}, 'rouge-2': {'r': 0.028169014084507043, 'p': 0.6666666666666666, 'f': 0.05405405327611396}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.75, 'f': 0.08955223768322566}}\n",
            "pair:  real world relation extraction tasks challenging deal either due limited training data class imbalance issues work present data augmented relation extraction dare simple method augment training data properly finetuning gpt generate examples specific relation types generated training data used combination gold dataset train bert based classifier series experiments show advantages method leads improvements score points compared strong baseline also dare achieves new state art three widely used biomedical datasets surpassing previous best results points average\n",
            "output sentence:  data augmented relation extraction gpt \n",
            "\n",
            "{'rouge-1': {'r': 0.1282051282051282, 'p': 0.6666666666666666, 'f': 0.21505376073534513}, 'rouge-2': {'r': 0.08421052631578947, 'p': 0.5333333333333333, 'f': 0.14545454309917355}, 'rouge-l': {'r': 0.10256410256410256, 'p': 0.5333333333333333, 'f': 0.17204300804717312}}\n",
            "pair:  building deep reinforcement learning agents generalize adapt unseen environments remains fundamental challenge ai paper describes progresses challenge context man made environments visually diverse contain intrinsic semantic regularities propose hybrid model based model free approach learning planning semantics leaps consisting multi target sub policy acts visual inputs bayesian model semantic structures placed unseen environment agent plans semantic model make high level decisions proposes next sub target sub policy execute updates semantic model based new observations perform experiments visual navigation tasks using house environment contains diverse human designed indoor scenes real world objects leaps outperforms strong baselines explicitly plan using semantic content\n",
            "output sentence:  propose hybrid model based model free approach using semantic information improve drl generalization man made environments \n",
            "\n",
            "{'rouge-1': {'r': 0.042105263157894736, 'p': 0.6666666666666666, 'f': 0.07920791967454172}, 'rouge-2': {'r': 0.008620689655172414, 'p': 0.2, 'f': 0.016528924827539142}, 'rouge-l': {'r': 0.042105263157894736, 'p': 0.6666666666666666, 'f': 0.07920791967454172}}\n",
            "pair:  energy based models outputs unmormalized log probability values given datasamples estimation essential variety application problems suchas sample generation denoising sample restoration outlier detection bayesianreasoning many however standard maximum likelihood training iscomputationally expensive due requirement sampling model distribution score matching potentially alleviates problem denoising score matching vincent particular convenient version however previous attemptsfailed produce models capable high quality sample synthesis believethat performed denoising score matching singlenoise scale overcome limitation instead learn energy functionusing noise scales sampled using annealed langevin dynamics andsingle step denoising jump model produced high quality samples comparableto state art techniques gans addition assigning likelihood totest data comparable previous likelihood models model set new sam ple quality baseline likelihood based models demonstrate model learns sample distribution generalize well image inpainting tasks\n",
            "output sentence:  learned energy based model score matching \n",
            "\n",
            "{'rouge-1': {'r': 0.12048192771084337, 'p': 0.5555555555555556, 'f': 0.19801979905107345}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.2222222222222222, 'f': 0.06557376797635056}, 'rouge-l': {'r': 0.08433734939759036, 'p': 0.3888888888888889, 'f': 0.13861385845701407}}\n",
            "pair:  intrinsically motivated goal exploration algorithms enable machines discover repertoires policies produce diversity effects complex environments exploration algorithms shown allow real world robots acquire skills tool use high dimensional continuous state action spaces however far assumed self generated goals sampled specifically engineered feature space limiting autonomy work propose approach using deep representation learning algorithms learn adequate goal space developmental stage approach first perceptual learning stage deep learning algorithms use passive raw sensor observations world changes learn corresponding latent space goal exploration happens second stage sampling goals latent space present experiments simulated robot arm interacting object show exploration algorithms using learned representations closely match even sometimes improve performance obtained using engineered representations\n",
            "output sentence:  propose novel intrinsically motivated goal exploration architecture unsupervised learning goal space representations evaluate various implementations enable discovery diversity policies \n",
            "\n",
            "{'rouge-1': {'r': 0.09722222222222222, 'p': 0.5, 'f': 0.16279069494862092}, 'rouge-2': {'r': 0.011627906976744186, 'p': 0.07692307692307693, 'f': 0.020202017920620603}, 'rouge-l': {'r': 0.06944444444444445, 'p': 0.35714285714285715, 'f': 0.1162790670416442}}\n",
            "pair:  simulation useful tool situations training data machine learning models costly annotate even hard acquire work propose reinforcement learning based method automatically adjusting parameters non differentiable simulator thereby controlling distribution synthesized data order maximize accuracy model trained data contrast prior art hand crafts simulation parameters adjusts parts available parameters approach fully controls simulator actual underlying goal maximizing accuracy rather mimicking real data distribution randomly generating large volume data find approach quickly converges optimal simulation parameters controlled experiments ii indeed discover good sets parameters image rendering simulator actual computer vision applications\n",
            "output sentence:  propose algorithm automatically adjusts parameters simulation engine generate training data neural network validation accuracy \n",
            "\n",
            "{'rouge-1': {'r': 0.14666666666666667, 'p': 0.8461538461538461, 'f': 0.24999999748192153}, 'rouge-2': {'r': 0.06542056074766354, 'p': 0.5384615384615384, 'f': 0.11666666473472223}, 'rouge-l': {'r': 0.14666666666666667, 'p': 0.8461538461538461, 'f': 0.24999999748192153}}\n",
            "pair:  injecting adversarial examples training known adversarial training improve robustness one step attacks unknown iterative attacks address challenge first show iteratively generated adversarial images easily transfer networks trained strategy inspired observation propose cascade adversarial training transfers knowledge end results adversarial training train network scratch injecting iteratively generated adversarial images crafted already defended networks addition one step adversarial images network trained also propose utilize embedding space classification low level pixel level similarity learning ignore unknown pixel level perturbation training inject adversarial images without replacing corresponding clean images penalize distance two embeddings clean adversarial experimental results show cascade adversarial training together proposed low level similarity learning efficiently enhances robustness iterative attacks expense decreased robustness one step attacks show combining two techniques also improve robustness worst case black box attack scenario\n",
            "output sentence:  cascade adversarial training low level similarity learning improve robustness white box black box attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.18181818181818182, 'p': 0.7692307692307693, 'f': 0.294117643966263}, 'rouge-2': {'r': 0.08620689655172414, 'p': 0.4166666666666667, 'f': 0.14285714001632657}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.7692307692307693, 'f': 0.294117643966263}}\n",
            "pair:  reinforcement learning rl methods achieved major advances multiple tasks surpassing human performance however rl strategies show certain degree weakness may become computationally intractable dealing high dimensional non stationary environments paper build meta reinforcement learning mrl method embedding adaptive neural network nn controller efficient policy iteration changing task conditions main goal extend rl application challenging task urban autonomous driving carla simulator\n",
            "output sentence:  meta reinforcement learning approach embedding neural network controller applied autonomous driving carla simulator \n",
            "\n",
            "{'rouge-1': {'r': 0.1320754716981132, 'p': 0.5384615384615384, 'f': 0.21212120895775943}, 'rouge-2': {'r': 0.046153846153846156, 'p': 0.25, 'f': 0.07792207529094292}, 'rouge-l': {'r': 0.11320754716981132, 'p': 0.46153846153846156, 'f': 0.18181817865472916}}\n",
            "pair:  show output residual cnn appropriate prior weights biases gp limit infinitely many convolutional filters extending similar results dense networks cnn equivalent kernel computed exactly unlike deep kernels parameters hyperparameters original cnn show kernel two properties allow computed efficiently cost evaluating kernel pair images similar single forward pass original cnn one filter per layer kernel equivalent layer resnet obtains classification error mnist new record gp comparable number parameters\n",
            "output sentence:  show cnns resnets appropriate priors parameters gaussian processes limit infinitely many convolutional filters \n",
            "\n",
            "{'rouge-1': {'r': 0.14035087719298245, 'p': 0.6666666666666666, 'f': 0.23188405509766857}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.16666666666666666, 'f': 0.054054051336742286}, 'rouge-l': {'r': 0.08771929824561403, 'p': 0.4166666666666667, 'f': 0.14492753335853814}}\n",
            "pair:  batch normalization batch norm often used attempt stabilize accelerate training deep neural networks many cases indeed decreases number parameter updates required achieve low training error however also reduces robustness small adversarial input perturbations noise double digit percentages show five standard datasets furthermore substituting weight decay batch norm sufficient nullify relationship adversarial vulnerability input dimension work consistent mean field analysis found batch norm causes exploding gradients\n",
            "output sentence:  batch normalization reduces adversarial robustness well general robustness many cases particularly noise corruptions \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.75, 'f': 0.2142857118367347}, 'rouge-2': {'r': 0.03488372093023256, 'p': 0.25, 'f': 0.0612244876468139}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5833333333333334, 'f': 0.1666666642176871}}\n",
            "pair:  monitoring patients icu challenging high cost task hence predicting condition patients icu stay help provide better acute care plan hospital resources continuous progress machine learning research icu management work focused using time series signals recorded icu instruments work show adding clinical notes another modality improves performance model three benchmark tasks hospital mortality prediction modeling decompensation length stay forecasting play important role icu management time series data measured regular intervals doctor notes charted irregular times making challenging model together propose method model jointly achieving considerable improvement across benchmark tasks baseline time series model\n",
            "output sentence:  demostarte using clinical notes conjuntion icu instruments data improves perfomance icu management benchmark tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.1323529411764706, 'p': 0.8181818181818182, 'f': 0.22784809886877105}, 'rouge-2': {'r': 0.08139534883720931, 'p': 0.7, 'f': 0.1458333314670139}, 'rouge-l': {'r': 0.1323529411764706, 'p': 0.8181818181818182, 'f': 0.22784809886877105}}\n",
            "pair:  system identification process building mathematical model unknown system measurements inputs outputs key step model based control estimator design output prediction work presents algorithm non linear offline system identification partial observations situations system full state directly observable algorithm presented called sisl iteratively infers system full state non linear optimization updates model parameters test algorithm simulated system coupled lorenz attractors showing algorithm ability identify high dimensional systems prove intractable particle based approaches also use sisl identify dynamics aerobatic helicopter augmenting state unobserved fluid states learn model predicts acceleration helicopter better state art approaches\n",
            "output sentence:  work presents scalable algorithm non linear offline system identification partial observations \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.46153846153846156, 'f': 0.16901408151557235}, 'rouge-2': {'r': 0.04477611940298507, 'p': 0.25, 'f': 0.0759493645120975}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.3076923076923077, 'f': 0.11267605334655831}}\n",
            "pair:  word embedding powerful tool natural language processing paper consider problem word embedding composition given vector representations two words compute vector entire phrase give generative model capture specific syntactic relations words model prove correlations three words measured pmi form tensor approximate low rank tucker decomposition result tucker decomposition gives word embeddings well core tensor used produce better compositions word embeddings also complement theoretical results experiments verify assumptions demonstrate effectiveness new composition method\n",
            "output sentence:  present generative model compositional word embeddings captures syntactic relations provide empirical verification evaluation \n",
            "\n",
            "{'rouge-1': {'r': 0.011764705882352941, 'p': 0.3333333333333333, 'f': 0.022727272068698366}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.011764705882352941, 'p': 0.3333333333333333, 'f': 0.022727272068698366}}\n",
            "pair:  disentangled encoding important step towards better representation learning however despite numerous efforts still clear winner captures independent features data unsupervised fashion work empirically evaluate performance six unsupervised disentanglement approaches mpi toy dataset curated released neurips disentanglement challenge methods investigated work beta vae factor vae dip vae dip ii vae info vae beta tcvae capacities models progressively increased throughout training hyper parameters kept intact across experiments methods evaluated based five disentanglement metrics namely dci factor vae irs mig sap score within limitations study beta tcvae approach found outperform alternatives respect normalized sum metrics however qualitative study encoded latents reveal consistent correlation reported metrics disentanglement potential model\n",
            "output sentence:  inadequacy disentanglement metrics \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.6666666666666666, 'f': 0.1578947347541551}, 'rouge-2': {'r': 0.02702702702702703, 'p': 0.25, 'f': 0.048780486044021486}, 'rouge-l': {'r': 0.07462686567164178, 'p': 0.5555555555555556, 'f': 0.13157894528047093}}\n",
            "pair:  present simple effective algorithm designed address covariate shift problem imitation learning operates training ensemble policies expert demonstration data using variance predictions cost minimized rl together supervised behavioral cloning cost unlike adversarial imitation methods uses fixed reward function easy optimize prove regret bound algorithm tabular setting linear time horizon multiplied coefficient show low certain problems behavioral cloning fails evaluate algorithm empirically across multiple pixel based atari environments continuous control tasks show matches significantly outperforms behavioral cloning generative adversarial imitation learning\n",
            "output sentence:  method addressing covariate shift imitation learning using ensemble uncertainty \n",
            "\n",
            "{'rouge-1': {'r': 0.10909090909090909, 'p': 0.5, 'f': 0.17910447467141904}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.18181818181818182, 'f': 0.056338025550486136}, 'rouge-l': {'r': 0.05454545454545454, 'p': 0.25, 'f': 0.08955223586544897}}\n",
            "pair:  paper proposes self supervised learning approach video features results significantly improved performance downstream tasks video classification captioning segmentation compared existing methods method extends bert model text sequences case sequences real valued feature vectors replacing softmax loss noise contrastive estimation nce also show learn representations sequences visual features sequences words derived asr automatic speech recognition show cross modal training possible helps even\n",
            "output sentence:  generalized bert continuous cross modal inputs state art self supervised video representations \n",
            "\n",
            "{'rouge-1': {'r': 0.07446808510638298, 'p': 0.6363636363636364, 'f': 0.13333333145759638}, 'rouge-2': {'r': 0.024, 'p': 0.3, 'f': 0.04444444307270237}, 'rouge-l': {'r': 0.07446808510638298, 'p': 0.6363636363636364, 'f': 0.13333333145759638}}\n",
            "pair:  high quality node embeddings learned graph neural networks gnns applied wide range node based applications achieved state art sota performance however applying node embeddings learned gnns generate graph embeddings scalar node representation may suffice preserve node graph properties efficiently resulting sub optimal graph embeddings inspired capsule neural network capsnet propose capsule graph neural network capsgnn adopts concept capsules address weakness existing gnn based graph embeddings algorithms extracting node features form capsules routing mechanism utilized capture important information graph level result model generates multiple embeddings graph capture graph properties different aspects attention module incorporated capsgnn used tackle graphs various sizes also enables model focus critical parts graphs extensive evaluations graph structured datasets demonstrate capsgnn powerful mechanism operates capture macroscopic properties whole graph data driven outperforms sota techniques several graph classification tasks virtue new instrument\n",
            "output sentence:  inspired capsnet propose novel architecture graph embeddings basis node features extracted gnn \n",
            "\n",
            "{'rouge-1': {'r': 0.1896551724137931, 'p': 0.6875, 'f': 0.29729729390796206}, 'rouge-2': {'r': 0.06578947368421052, 'p': 0.3125, 'f': 0.10869564930056717}, 'rouge-l': {'r': 0.1724137931034483, 'p': 0.625, 'f': 0.27027026688093503}}\n",
            "pair:  last years deep learning tremendously successful many applications however theoretical understanding deep learning thus ability providing principled improvements seems lag behind theoretical puzzle concerns ability deep networks predict well despite intriguing apparent lack generalization classification accuracy training set proxy performance test set possible training performance independent testing performance indeed deep networks require drastically new theory generalization measurements based training data predictive network performance future data show performance measured appropriately training performance fact predictive expected performance consistently classical machine learning theory\n",
            "output sentence:  contrary previous beliefs training performance deep networks measured appropriately predictive test performance consistent classical machine learning theory \n",
            "\n",
            "{'rouge-1': {'r': 0.24193548387096775, 'p': 0.9375, 'f': 0.3846153813543722}, 'rouge-2': {'r': 0.17721518987341772, 'p': 0.9333333333333333, 'f': 0.29787233774332283}, 'rouge-l': {'r': 0.24193548387096775, 'p': 0.9375, 'f': 0.3846153813543722}}\n",
            "pair:  intrinsic rewards reinforcement learning provide powerful algorithmic capability agents learn interact environment task generic way however increased incentives motivation come cost increased fragility stochasticity introduce method computing intrinsic reward curiosity using metrics derived sampling latent variable model used estimate dynamics ultimately estimate conditional probability observed states used intrinsic reward curiosity experiments video game agent uses model autonomously learn play atari games using curiosity reward combination extrinsic rewards game achieve improved performance games sparse extrinsic rewards stochasticity introduced environment method still demonstrates improved performance baseline\n",
            "output sentence:  introduce method computing intrinsic reward curiosity using metrics derived sampling latent variable model used estimate dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.5714285714285714, 'f': 0.11594202716236088}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.42857142857142855, 'f': 0.08695651991598406}}\n",
            "pair:  identifying analogies across domains without supervision key task artificial intelligence recent advances cross domain image mapping concentrated translating images across domains although progress made impressive visual fidelity many times suffice identifying matching sample domain paper tackle task finding exact analogies datasets every image domain find analogous image domain present matching synthesis approach gan show outperforms current techniques show cross domain mapping task broken two parts domain alignment learning mapping function tasks iteratively solved alignment improved unsupervised translation function reaches quality comparable full supervision\n",
            "output sentence:  finding correspondences domains performing matching mapping iterations \n",
            "\n",
            "{'rouge-1': {'r': 0.140625, 'p': 0.5294117647058824, 'f': 0.22222221890565466}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.10526315789473684, 'f': 0.038095235131065996}, 'rouge-l': {'r': 0.125, 'p': 0.47058823529411764, 'f': 0.1975308608809633}}\n",
            "pair:  domain adaptation open problem deep reinforcement learning rl often agents asked perform environments data difficult obtain settings agents trained similar environments simulators transferred original environment gap visual observations source target environments often causes agent fail target environment present new rl agent sadala soft attention disentangled representation learning agent sadala first learns compressed state representation jointly learns ignore distracting features solve task presented sadala separation important unimportant visual features leads robust domain transfer sadala outperforms prior disentangled representation based rl domain randomization approaches across rl environments visual cartpole deepmind lab\n",
            "output sentence:  present agent uses beta vae extract visual features attention mechanism ignore irrelevant features visual observations enable robust transfer visual domains \n",
            "\n",
            "{'rouge-1': {'r': 0.07446808510638298, 'p': 0.6363636363636364, 'f': 0.13333333145759638}, 'rouge-2': {'r': 0.016666666666666666, 'p': 0.2, 'f': 0.030769229349112494}, 'rouge-l': {'r': 0.07446808510638298, 'p': 0.6363636363636364, 'f': 0.13333333145759638}}\n",
            "pair:  learning tasks source code formal languages considered recently work tried transfer natural language methods capitalize unique opportunities offered code known syntax example long range dependencies induced using variable function distant locations often considered propose use graphs represent syntactic semantic structure code use graph based deep learning methods learn reason program structures work present construct graphs source code scale gated graph neural networks training large graphs evaluate method two tasks varnaming network attempts predict name variable given usage varmisuse network learns reason selecting correct variable used given program location comparison methods use less structured program representations shows advantages modeling known structure suggests models learn infer meaningful names solve varmisuse task many cases additionally testing showed varmisuse identifies number bugs mature open source projects\n",
            "output sentence:  programs structure represented graphs graph neural networks learn find bugs graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.8461538461538461, 'f': 0.27848100990866853}, 'rouge-2': {'r': 0.09375, 'p': 0.6923076923076923, 'f': 0.1651376125780658}, 'rouge-l': {'r': 0.13636363636363635, 'p': 0.6923076923076923, 'f': 0.22784809851626345}}\n",
            "pair:  weakly supervised learning based clustering framework proposed paper core framework introduce novel multiple instance learning task based bag level label called unique class count ucc number unique classes among instances inside bag task annotations individual instances inside bag needed training models mathematically prove perfect ucc classifier perfect clustering individual instances inside bags possible even annotations individual instances given training constructed neural network based ucc classifier experimentally shown clustering performance framework weakly supervised ucc classifier comparable fully supervised learning models labels instances known furthermore tested applicability framework real world task semantic segmentation breast cancer metastases histological lymph node sections shown performance weakly supervised framework comparable performance fully supervised unet model\n",
            "output sentence:  weakly supervised learning based clustering framework performs comparable fully supervised learning models exploiting unique class count \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.5, 'f': 0.14285714040816327}, 'rouge-2': {'r': 0.01818181818181818, 'p': 0.14285714285714285, 'f': 0.032258062513007404}, 'rouge-l': {'r': 0.0625, 'p': 0.375, 'f': 0.1071428546938776}}\n",
            "pair:  present methodology using dynamic evaluation improve neural sequence models models adapted recent history via gradient descent based mechanism causing assign higher probabilities occurring sequential patterns dynamic evaluation outperforms existing adaptation approaches comparisons dynamic evaluation improves state art word level perplexities penn treebank wikitext datasets respectively state art character level cross entropies text hutter prize datasets bits char bits char respectively\n",
            "output sentence:  paper presents dynamic evaluation methodology adaptive sequence modelling \n",
            "\n",
            "{'rouge-1': {'r': 0.039603960396039604, 'p': 0.8, 'f': 0.07547169721431114}, 'rouge-2': {'r': 0.007936507936507936, 'p': 0.25, 'f': 0.015384614788165704}, 'rouge-l': {'r': 0.0297029702970297, 'p': 0.6, 'f': 0.05660377268600926}}\n",
            "pair:  state art performances language comprehension tasks achieved huge language models pre trained massive unlabeled text corpora light subsequent fine tuning task specific supervised manner seems pre training procedure learns good common initialization training various natural language understanding tasks steps need taken parameter space learn task work using bidirectional encoder representations transformers bert example verify hypothesis showing task specific fine tuned language models highly close parameter space pre trained one taking advantage observations show fine tuned versions huge models order floating point parameters made computationally efficient first fine tuning fraction critical layers suffices second fine tuning adequately performed learning binary multiplicative mask pre trained weights textit parameter sparsification result single effort achieve three desired outcomes learning perform specific tasks saving memory storing binary masks certain layers task saving compute appropriate hardware performing sparse operations model parameters\n",
            "output sentence:  sparsification fine tuning language models \n",
            "\n",
            "{'rouge-1': {'r': 0.08620689655172414, 'p': 0.2777777777777778, 'f': 0.1315789437534627}, 'rouge-2': {'r': 0.014492753623188406, 'p': 0.058823529411764705, 'f': 0.02325581078150395}, 'rouge-l': {'r': 0.05172413793103448, 'p': 0.16666666666666666, 'f': 0.07894736480609435}}\n",
            "pair:  image classifier makes prediction parts image relevant rephrase question ask parts image seen classifier would change decision producing answer requires marginalizing images could seen sample plausible image fills conditioning generative model rest image optimize find image regions change classifier decision fill approach contrasts ad hoc filling approaches blurring injecting noise generate inputs far data distribution ignore informative relationships different parts image method produces compact relevant saliency maps fewer artifacts compared previous methods\n",
            "output sentence:  compute saliency using strong generative model efficiently marginalize plausible alternative inputs revealing concentrated pixel areas preserve label information \n",
            "\n",
            "{'rouge-1': {'r': 0.053763440860215055, 'p': 0.625, 'f': 0.0990098995314185}, 'rouge-2': {'r': 0.008333333333333333, 'p': 0.14285714285714285, 'f': 0.01574803045446098}, 'rouge-l': {'r': 0.03225806451612903, 'p': 0.375, 'f': 0.05940593913537892}}\n",
            "pair:  consider dictionary learning problem aim model given data linear combination columns matrix known dictionary sparse weights forming linear combination known coefficients since dictionary coefficients parameterizing linear model unknown corresponding optimization inherently non convex major challenge recently provable algorithms dictionary learning proposed yet provide guarantees recovery dictionary without explicit recovery guarantees coefficients moreover estimation error dictionary adversely impacts ability successfully localize estimate coefficients potentially limits utility existing provable dictionary learning methods applications coefficient recovery interest end develop noodl simple neurally plausible alternating optimization based online dictionary learning algorithm recovers dictionary coefficients exactly geometric rate initialized appropriately algorithm noodl also scalable amenable large scale distributed implementations neural architectures mean involves simple linear non linear operations finally corroborate theoretical results via experimental evaluation proposed algorithm current state art techniques\n",
            "output sentence:  present provable algorithm exactly recovering factors dictionary learning model \n",
            "\n",
            "{'rouge-1': {'r': 0.189873417721519, 'p': 0.8333333333333334, 'f': 0.30927834749282607}, 'rouge-2': {'r': 0.056074766355140186, 'p': 0.35294117647058826, 'f': 0.09677419118236218}, 'rouge-l': {'r': 0.11392405063291139, 'p': 0.5, 'f': 0.1855670072866405}}\n",
            "pair:  work aim solve data driven optimization problems goal find input maximizes unknown score function given access dataset input score pairs inputs may lie extremely thin manifolds high dimensional spaces making optimization prone falling manifold evaluating unknown function may expensive algorithm able exploit static offline data propose model inversion networks mins approach solve problems unlike prior work mins scale extremely high dimensional input spaces efficiently leverage offline logged datasets optimization contextual non contextual settings show mins also extended active setting commonly studied prior work via simple novel effective scheme active data collection experiments show mins act powerful optimizers range contextual non contextual static active problems including optimization images protein designs learning logged bandit feedback\n",
            "output sentence:  propose novel approach solve data driven model based optimization problems passive active settings scale high dimensional input spaces \n",
            "\n",
            "{'rouge-1': {'r': 0.030612244897959183, 'p': 0.3, 'f': 0.05555555387517152}, 'rouge-2': {'r': 0.008695652173913044, 'p': 0.1, 'f': 0.015999998528000135}, 'rouge-l': {'r': 0.02040816326530612, 'p': 0.2, 'f': 0.03703703535665302}}\n",
            "pair:  recent advances recurrent neural nets rnns shown much promise many applications natural language processing tasks sentiment analysis customer reviews recurrent neural net model parses entire review forming decision argue reading entire input always necessary practice since lot reviews often easy classify decision formed reading crucial sentences words provided text paper present approach fast reading text classification inspired several well known human reading techniques approach implements intelligent recurrent agent evaluates importance current snippet order decide whether make prediction skip texts read part sentence agent uses rnn module encode information past current tokens applies policy module form decisions end end training algorithm based policy gradient train test agent several text classification datasets achieve higher efficiency better accuracy compared previous approaches\n",
            "output sentence:  develop end end trainable approach skimming rereading early stopping applicable classification tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.10526315789473684, 'p': 0.7692307692307693, 'f': 0.1851851830675583}, 'rouge-2': {'r': 0.05084745762711865, 'p': 0.46153846153846156, 'f': 0.09160305164733994}, 'rouge-l': {'r': 0.08421052631578947, 'p': 0.6153846153846154, 'f': 0.1481481460305213}}\n",
            "pair:  high intra class diversity inter class similarity characteristic remote sensing scene image data sets currently posing significant difficulty deep learning algorithms classification tasks improve accuracy post classification methods proposed smoothing results model predictions however approaches require additional neural network perform smoothing operation adds overhead task propose approach involves learning deep features directly neighboring scene images without requiring use cleanup model approach utilizes siamese network improve discriminative power convolutional neural networks pair neighboring scene images exploits semantic coherence pair enrich feature vector image want predict label empirical results show approach provides viable alternative existing methods example model improved prediction accuracy percentage point dropped mean squared error value baseline disease density estimation task performance gains comparable results existing post classification methods moreover without implementation overheads\n",
            "output sentence:  approach improving prediction accuracy learning deep features neighboring scene images satellite scene image analysis \n",
            "\n",
            "{'rouge-1': {'r': 0.09523809523809523, 'p': 0.3076923076923077, 'f': 0.1454545418446282}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.08333333333333333, 'f': 0.03389830184429793}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.15384615384615385, 'f': 0.07272726911735555}}\n",
            "pair:  validation key challenge search safe autonomy simulations often either simple provide robust validation complex tractably compute therefore approximate validation methods needed tractably find failures without unsafe simplifications paper presents theory behind one black box approach adaptive stress testing ast also provide three examples validation problems formulated work ast\n",
            "output sentence:  formulation black box reinforcement learning method find likely failure system acting complex scenarios \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}, 'rouge-2': {'r': 0.03409090909090909, 'p': 0.25, 'f': 0.059999997888000076}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5384615384615384, 'f': 0.16470587976193776}}\n",
            "pair:  goal imitation learning il learn good policy high quality demonstrations however quality demonstrations reality diverse since easier cheaper collect demonstrations mix experts amateurs il situations challenging especially level demonstrators expertise unknown propose new il paradigm called variational imitation learning diverse quality demonstrations vild explicitly model level demonstrators expertise probabilistic graphical model estimate along reward function show naive estimation approach suitable large state action spaces fix issue using variational approach easily implemented using existing reinforcement learning methods experiments continuous control benchmarks demonstrate vild outperforms state art methods work enables scalable data efficient il realistic settings\n",
            "output sentence:  propose imitation learning method learn diverse quality demonstrations collected demonstrators different level expertise \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5, 'f': 0.19047618739229027}, 'rouge-2': {'r': 0.04918032786885246, 'p': 0.2727272727272727, 'f': 0.08333333074459885}, 'rouge-l': {'r': 0.11764705882352941, 'p': 0.5, 'f': 0.19047618739229027}}\n",
            "pair:  make following striking observation fully convolutional vae models trained imagenet generalize well also far larger photographs changes model use property applying fully convolutional models lossless compression demonstrating method scale vae based bits back ans algorithm lossless compression large color photographs achieving state art compression full size imagenet images release craystack open source library convenient prototyping lossless compression using probabilistic models along full implementations compression results\n",
            "output sentence:  scale lossless compression latent variables beating existing approaches full size imagenet images \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.75, 'f': 0.13953488203353168}, 'rouge-2': {'r': 0.02040816326530612, 'p': 0.2857142857142857, 'f': 0.03809523685079368}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.625, 'f': 0.11627906808004328}}\n",
            "pair:  cloze test widely adopted language exams evaluate students language proficiency paper propose first large scale human designed cloze test dataset cloth questions used middle school high school language exams missing blanks carefully created teachers candidate choices purposely designed confusing cloth requires deeper language understanding wider attention span previous automatically generated cloze datasets show humans outperform dedicated designed baseline models significant margin even model trained sufficiently large external data investigate source performance gap trace model deficiencies distinct properties cloth identify limited ability comprehending long term context key bottleneck addition find human designed data leads larger gap model performance human performance compared automatically generated data\n",
            "output sentence:  cloze test dataset designed teachers assess language proficiency \n",
            "\n",
            "{'rouge-1': {'r': 0.11956521739130435, 'p': 0.6470588235294118, 'f': 0.20183485975254614}, 'rouge-2': {'r': 0.00909090909090909, 'p': 0.058823529411764705, 'f': 0.015748029177258697}, 'rouge-l': {'r': 0.05434782608695652, 'p': 0.29411764705882354, 'f': 0.09174311663328011}}\n",
            "pair:  adversarial examples defined inputs model induce mistake model output different oracle perhaps surprising malicious ways original models adversarial attacks primarily studied context classification computer vision tasks several attacks proposed natural language processing nlp settings often vary defining parameters attack successful attack would look like goal work propose unifying model adversarial examples suitable nlp tasks generative classification settings define notion adversarial gain based control theory measure change output system relative perturbation input caused called adversary presented learner definition show used different feature spaces distance conditions determine attack defense effectiveness across different intuitive manifolds notion adversarial gain provides useful way evaluating adversaries defenses act building block future work robustness adversaries due rooted nature stability manifold theory\n",
            "output sentence:  propose alternative measure determining effectiveness adversarial attacks nlp models according distance measure based method like incremental gain control \n",
            "\n",
            "{'rouge-1': {'r': 0.03614457831325301, 'p': 0.42857142857142855, 'f': 0.0666666652320988}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.16666666666666666, 'f': 0.019999998872000067}, 'rouge-l': {'r': 0.024096385542168676, 'p': 0.2857142857142857, 'f': 0.04444444300987659}}\n",
            "pair:  prepositions among frequent words good prepositional representation great syntactic semantic interest computational linguistics existing methods preposition representation either treat prepositions content words word vec glove depend heavily external linguistic resources including syntactic parsing training task dataset specific representations paper use word triple counts one words preposition capture preposition interaction head children prepositional embeddings derived via tensor decompositions large unlabeled corpus reveal new geometry involving hadamard products empirically demonstrate utility paraphrasing phrasal verbs furthermore prepositional embeddings used simple features two challenging downstream tasks preposition selection prepositional attachment disambiguation achieve comparable better results state art multiple standardized datasets\n",
            "output sentence:  work tensor based method preposition representation training \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.45454545454545453, 'f': 0.12658227608396094}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.2, 'f': 0.04651162585181188}, 'rouge-l': {'r': 0.04411764705882353, 'p': 0.2727272727272727, 'f': 0.07594936469155592}}\n",
            "pair:  present network embedding algorithms capture information node local distribution node attributes around observed random walks following approach similar skip gram observations neighborhoods different sizes either pooled ae encoded distinctly multi scale approach musae capturing attribute neighborhood relationships multiple scales useful diverse range applications including latent feature identification across disconnected networks similar attributes prove theoretically matrices node feature pointwise mutual information implicitly factorized embeddings experiments show algorithms robust computationally efficient outperform comparable models social web citation network datasets\n",
            "output sentence:  develop efficient multi scale approximate attributed network embedding procedures provable properties \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.4166666666666667, 'f': 0.15873015564625853}, 'rouge-2': {'r': 0.01818181818181818, 'p': 0.09090909090909091, 'f': 0.03030302752525278}, 'rouge-l': {'r': 0.09803921568627451, 'p': 0.4166666666666667, 'f': 0.15873015564625853}}\n",
            "pair:  deep neural networks proven powerful tool many recognition classification tasks stability properties still well understood past image classifiers shown vulnerable called adversarial attacks created additively perturbing correctly classified image paper propose adef algorithm construct different kind adversarial attack created iteratively applying small deformations image found gradient descent step demonstrate results mnist convolutional neural networks imagenet inception resnet\n",
            "output sentence:  propose new efficient algorithm construct adversarial examples means deformations rather additive perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.6666666666666666, 'f': 0.19047618802721092}, 'rouge-2': {'r': 0.02, 'p': 0.2, 'f': 0.03636363471074388}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.6666666666666666, 'f': 0.19047618802721092}}\n",
            "pair:  protein classification responsible biological sequence came idea whichdeals classification proteomics using deep learning algorithm algorithm focusesmainly classify sequences protein vector used representation proteomics selection type protein representation challenging based output terms ofaccuracy depended protein representation used gram gram kerasembedding used biological sequences like protein paper working proteinclassification show strength representation biological sequence proteins\n",
            "output sentence:  protein family classification using deep learning \n",
            "\n",
            "{'rouge-1': {'r': 0.3333333333333333, 'p': 0.7272727272727273, 'f': 0.45714285283265316}, 'rouge-2': {'r': 0.16, 'p': 0.4, 'f': 0.22857142448979595}, 'rouge-l': {'r': 0.2916666666666667, 'p': 0.6363636363636364, 'f': 0.39999999568979594}}\n",
            "pair:  considering simultaneously finite number tasks multi output learning enables one account similarities tasks via appropriate regularizers propose generalization classical setting continuum tasks using vector valued rkhss\n",
            "output sentence:  propose extension multi output learning continuum tasks using operator valued kernels \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.8571428571428571, 'f': 0.12631578810858723}, 'rouge-2': {'r': 0.043478260869565216, 'p': 0.8333333333333334, 'f': 0.082644627156615}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.8571428571428571, 'f': 0.12631578810858723}}\n",
            "pair:  predicting future real world settings particularly raw sensory observations images exceptionally challenging real world events stochastic unpredictable high dimensionality complexity natural images requires predictive model build intricate understanding natural world many existing methods tackle problem making simplifying assumptions environment one common assumption outcome deterministic one plausible future lead low quality predictions real world settings stochastic dynamics paper develop stochastic variational video prediction sv method predicts different possible future sample latent variables best knowledge model first provide effective stochastic multi frame prediction real world video demonstrate capability proposed method predicting detailed future frames videos multiple real world datasets action free action conditioned find proposed method produces substantially improved video predictions compared model without stochasticity stochastic video prediction methods sv implementation open sourced upon publication\n",
            "output sentence:  stochastic variational video prediction real world settings \n",
            "\n",
            "{'rouge-1': {'r': 0.011764705882352941, 'p': 0.3333333333333333, 'f': 0.022727272068698366}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.011764705882352941, 'p': 0.3333333333333333, 'f': 0.022727272068698366}}\n",
            "pair:  disentangled encoding important step towards better representation learning however despite numerous efforts still clear winner captures independent features data unsupervised fashion work empirically evaluate performance six unsupervised disentanglement approaches mpi toy dataset curated released neurips disentanglement challenge methods investigated work beta vae factor vae dip vae dip ii vae info vae beta tcvae capacities models progressively increased throughout training hyper parameters kept intact across experiments methods evaluated based five disentanglement metrics namely dci factor vae irs mig sap score within limitations study beta tcvae approach found outperform alternatives respect normalized sum metrics however qualitative study encoded latents reveal consistent correlation reported metrics disentanglement potential model\n",
            "output sentence:  inadequacy disentanglement metrics \n",
            "\n",
            "{'rouge-1': {'r': 0.10576923076923077, 'p': 0.9166666666666666, 'f': 0.18965517055885853}, 'rouge-2': {'r': 0.06451612903225806, 'p': 0.7272727272727273, 'f': 0.11851851702167354}, 'rouge-l': {'r': 0.08653846153846154, 'p': 0.75, 'f': 0.15517241193816883}}\n",
            "pair:  reduce memory footprint run time latency techniques neural net work pruning binarization explored separately however un clear combine best two worlds get extremely small efficient models paper first time define filter level pruning problem binary neural networks cannot solved simply migrating existing structural pruning methods full precision models novel learning based approach proposed prune filters main subsidiary network frame work main network responsible learning representative features optimize prediction performance subsidiary component works filter selector main network avoid gradient mismatch training subsidiary component propose layer wise bottom scheme also provide theoretical experimental comparison learning based greedy rule based methods finally empirically demonstrate effectiveness approach applied several binary models including binarizednin vgg resnet various image classification datasets bi nary resnet imagenet use filters achieve slightly better test error original model\n",
            "output sentence:  define filter level pruning problem binary neural networks first time propose method solve \n",
            "\n",
            "{'rouge-1': {'r': 0.05063291139240506, 'p': 0.5, 'f': 0.09195402131853615}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05063291139240506, 'p': 0.5, 'f': 0.09195402131853615}}\n",
            "pair:  paper study adversarial attack defence problem deep learning perspective fourier analysis first explicitly compute fourier transform deep relu neural networks show exist decaying non zero high frequency components fourier spectrum neural networks demonstrate vulnerability neural networks towards adversarial samples attributed insignificant non zero high frequency components based analysis propose use simple post averaging technique smooth high frequency components improve robustness neural networks adversarial attacks experimental results imagenet cifar datasets shown proposed method universally effective defend many existing adversarial attacking methods proposed literature including fgsm pgd deepfool attacks post averaging method simple since require training meanwhile successfully defend adversarial samples generated methods without introducing significant performance degradation less original clean images\n",
            "output sentence:  insight reason adversarial vulnerability effective defense method adversarial attacks \n",
            "\n",
            "{'rouge-1': {'r': 0.06578947368421052, 'p': 0.4166666666666667, 'f': 0.11363636128099178}, 'rouge-2': {'r': 0.019417475728155338, 'p': 0.18181818181818182, 'f': 0.03508771755463228}, 'rouge-l': {'r': 0.039473684210526314, 'p': 0.25, 'f': 0.06818181582644636}}\n",
            "pair:  understanding behavior stochastic gradient descent sgd context deep neural networks raised lots concerns recently along line theoretically study general form gradient based optimization dynamics unbiased noise unifies sgd standard langevin dynamics investigating general optimization dynamics analyze behavior sgd escaping minima regularization effects novel indicator derived characterize efficiency escaping minima measuring alignment noise covariance curvature loss function based indicator two conditions established show type noise structure superior isotropic noise term escaping efficiency show anisotropic noise sgd satisfies two conditions thus helps escape sharp poor minima effectively towards stable flat minima typically generalize well verify understanding comparing anisotropic diffusion full gradient descent plus isotropic diffusion langevin dynamics types position dependent noise\n",
            "output sentence:  provide theoretical empirical analysis role anisotropic noise introduced stochastic gradient escaping minima \n",
            "\n",
            "{'rouge-1': {'r': 0.0641025641025641, 'p': 0.7142857142857143, 'f': 0.11764705731211073}, 'rouge-2': {'r': 0.028846153846153848, 'p': 0.5, 'f': 0.0545454535140496}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.7142857142857143, 'f': 0.11764705731211073}}\n",
            "pair:  neural embeddings used great success natural language processing nlp provide compact representations encapsulate word similarity attain state art performance range linguistic tasks success neural embeddings prompted significant amounts research applications domains language one domain graph structured data embeddings vertices learned encapsulate vertex similarity improve performance tasks including edge prediction vertex labelling nlp graph based tasks embeddings high dimensional euclidean spaces learned however recent work shown appropriate isometric space embedding complex networks flat euclidean space negatively curved hyperbolic space present new concept exploits recent insights propose learning neural embeddings graphs hyperbolic space provide experimental evidence hyperbolic embeddings significantly outperform euclidean embeddings vertex classification tasks several real world public datasets\n",
            "output sentence:  learn neural embeddings graphs hyperbolic instead euclidean space \n",
            "\n",
            "{'rouge-1': {'r': 0.1388888888888889, 'p': 0.7692307692307693, 'f': 0.23529411505605538}, 'rouge-2': {'r': 0.046511627906976744, 'p': 0.3333333333333333, 'f': 0.08163265091212}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5384615384615384, 'f': 0.16470587976193776}}\n",
            "pair:  propose evaluate new techniques compressing speeding dense matrix multiplications found fully connected recurrent layers neural networks embedded large vocabulary continuous speech recognition lvcsr compression introduce study trace norm regularization technique training low rank factored versions matrix multiplications compared standard low rank training show method leads good accuracy versus number parameter trade offs used speed training large models speedup enable faster inference arm processors new open sourced kernels optimized small batch sizes resulting speed ups widely used gemmlowp library beyond lvcsr expect techniques kernels generally applicable embedded neural networks large fully connected recurrent layers\n",
            "output sentence:  compress speed speech recognition models embedded devices trace norm regularization technique optimized kernels \n",
            "\n",
            "{'rouge-1': {'r': 0.12371134020618557, 'p': 0.9230769230769231, 'f': 0.21818181609752071}, 'rouge-2': {'r': 0.08108108108108109, 'p': 0.75, 'f': 0.1463414616537775}, 'rouge-l': {'r': 0.12371134020618557, 'p': 0.9230769230769231, 'f': 0.21818181609752071}}\n",
            "pair:  continuous normalizing flows cnfs emerged promising deep generative models wide range tasks thanks invertibility exact likelihood estimation however conditioning cnfs signals interest conditional image generation downstream predictive tasks inefficient due high dimensional latent code generated model needs size input data paper propose infocnf efficient conditional cnf partitions latent space class specific supervised code unsupervised code shared among classes efficient use labeled information since partitioning strategy slightly increases number function evaluations nfes infocnf also employs gating networks learn error tolerances ordinary differential equation ode solvers better speed performance show empirically infocnf improves test accuracy baseline yielding comparable likelihood scores reducing nfes cifar furthermore applying partitioning strategy infocnf time series data helps improve extrapolation performance\n",
            "output sentence:  propose infocnf efficient conditional cnf employs gating networks learn error tolerances ode solvers \n",
            "\n",
            "{'rouge-1': {'r': 0.18421052631578946, 'p': 0.8235294117647058, 'f': 0.3010752658295757}, 'rouge-2': {'r': 0.08653846153846154, 'p': 0.5625, 'f': 0.14999999768888891}, 'rouge-l': {'r': 0.17105263157894737, 'p': 0.7647058823529411, 'f': 0.27956988948548966}}\n",
            "pair:  employing deep neural networks natural image priors solve inverse problems either requires large amounts data sufficiently train expressive generative models succeed data via untrained neural networks however works considered interpolate high data regimes particular one use availability small amount data even examples one advantage solving inverse problems system performance increase amount data increases well work consider solving linear inverse problems given small number examples images drawn distribution image interest comparing untrained neural networks use data show one pre train neural network given examples improve reconstruction results compressed sensing semantic image recovery problems colorization approach leads improved reconstruction amount available data increases par fully trained generative models requiring less data needed train generative model\n",
            "output sentence:  show pre training untrained neural network examples improve reconstruction results compressed sensing semantic recovery problems like colorization \n",
            "\n",
            "{'rouge-1': {'r': 0.1320754716981132, 'p': 0.5833333333333334, 'f': 0.21538461237396456}, 'rouge-2': {'r': 0.06060606060606061, 'p': 0.3333333333333333, 'f': 0.10256409996055234}, 'rouge-l': {'r': 0.11320754716981132, 'p': 0.5, 'f': 0.18461538160473379}}\n",
            "pair:  present eda easy data augmentation techniques boosting performance text classification tasks eda consists four simple powerful operations synonym replacement random insertion random swap random deletion five text classification tasks show eda improves performance convolutional recurrent neural networks eda demonstrates particularly strong results smaller datasets average across five datasets training eda using available training set achieved accuracy normal training available data also performed extensive ablation studies suggest parameters practical use\n",
            "output sentence:  simple text augmentation techniques significantly boost performance text classification tasks especially small datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.9285714285714286, 'f': 0.22033898095949442}, 'rouge-2': {'r': 0.10218978102189781, 'p': 0.875, 'f': 0.18300653407492845}, 'rouge-l': {'r': 0.11538461538461539, 'p': 0.8571428571428571, 'f': 0.20338982841712153}}\n",
            "pair:  training neural network desired task one may prefer adapt pretrained network rather start randomly initialized one due lacking enough training data performing lifelong learning system learn new task previously trained tasks wishing encode priors network via preset weights commonly employed approaches network adaptation fine tuning using pre trained network fixed feature extractor among others paper propose straightforward alternative side tuning side tuning adapts pretrained network training lightweight side network fused unchanged pre rained network using simple additive process simple method works well better existing solutions resolves basic issues fine tuning fixed features several common baselines particular side tuning less prone overfitting little training data available yields better results using fixed feature extractor suffer catastrophic forgetting lifelong learning demonstrate performance side tuning diverse set scenarios including lifelong learning icifar taskonomy reinforcement learning imitation learning visual navigation habitat nlp question answering squad single task transfer learning taskonomy consistently promising results\n",
            "output sentence:  side tuning adapts pre trained network training lightweight side network fused unchanged pre trained network using simple additive process \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.9090909090909091, 'f': 0.2631578922610804}, 'rouge-2': {'r': 0.0975609756097561, 'p': 0.8, 'f': 0.17391304154064277}, 'rouge-l': {'r': 0.15384615384615385, 'p': 0.9090909090909091, 'f': 0.2631578922610804}}\n",
            "pair:  paper introduces probabilistic framework shot image classification goal generalise initial large scale classification task separate task comprising new classes small numbers examples new approach leverages feature based representation learned neural network initial task representational transfer also information classes concept transfer concept information encapsulated probabilistic model final layer weights neural network acts prior probabilistic shot learning show even simple probabilistic model achieves state art standard shot learning dataset large margin moreover able accurately model uncertainty leading well calibrated classifiers easily extensible flexible unlike many recent approaches shot learning\n",
            "output sentence:  paper introduces probabilistic framework shot image classification achieves state art results \n",
            "\n",
            "{'rouge-1': {'r': 0.21212121212121213, 'p': 1.0, 'f': 0.3499999971125}, 'rouge-2': {'r': 0.1728395061728395, 'p': 1.0, 'f': 0.29473683959224384}, 'rouge-l': {'r': 0.21212121212121213, 'p': 1.0, 'f': 0.3499999971125}}\n",
            "pair:  propose novel projection based way incorporate conditional information discriminator gans respects role conditional information underlining probabilistic model approach contrast frameworks conditional gans used application today use conditional information concatenating embedded conditional vector feature vectors modification able significantly improve quality class conditional image generation ilsvrc imagenet dataset current state art result achieved single pair discriminator generator also able extend application super resolution succeeded producing highly discriminative super resolution images new structure also enabled high quality category transformation based parametric functional transformation conditional batch normalization layers generator\n",
            "output sentence:  propose novel projection based way incorporate conditional information discriminator gans respects role conditional information underlining probabilistic model \n",
            "\n",
            "{'rouge-1': {'r': 0.06779661016949153, 'p': 0.5714285714285714, 'f': 0.12121211931588613}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.3333333333333333, 'f': 0.04878048644854257}, 'rouge-l': {'r': 0.06779661016949153, 'p': 0.5714285714285714, 'f': 0.12121211931588613}}\n",
            "pair:  generative model suitable continual learning paper aims evaluating comparing generative models disjoint sequential image generation tasks investigate several models learn forget considering various strategies rehearsal regularization generative replay fine tuning used two quantitative metrics estimate generation quality memory ability experiment sequential tasks three commonly used benchmarks continual learning mnist fashion mnist cifar found among models original gan performs best among continual learning strategies generative replay outperforms methods even found satisfactory combinations mnist fashion mnist training generative models sequentially cifar particularly instable remains challenge\n",
            "output sentence:  comparative study generative models continual learning scenarios \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.42857142857142855, 'f': 0.13953488099513256}, 'rouge-2': {'r': 0.03125, 'p': 0.21428571428571427, 'f': 0.054545452323967035}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.2857142857142857, 'f': 0.09302325308815583}}\n",
            "pair:  need large amounts training image data clearly defined features major obstacle applying generative adversarial networks gan image generation training data limited diverse since insufficient latent feature representation already scarce data often leads instability mode collapse gan training overcome hurdle limited data applying gan limited datasets propose paper strategy textit parallel recurrent data augmentation gan model progressively enriches training set sample images constructed gans trained parallel consecutive training epochs experiments variety small yet diverse datasets demonstrate method little model specific considerations produces images better quality compared images generated without strategy source code generated images paper made public review\n",
            "output sentence:  introduced novel simple efficient data augmentation method boosts performances existing gans training data limited diverse \n",
            "\n",
            "{'rouge-1': {'r': 0.045454545454545456, 'p': 0.5714285714285714, 'f': 0.08421052495069253}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03409090909090909, 'p': 0.42857142857142855, 'f': 0.06315789337174517}}\n",
            "pair:  deep neural networks dnns attained surprising achievement last decade due advantages automatic feature learning freedom expressiveness however interpretability remains mysterious dnns complex combinations linear nonlinear transformations even though many models proposed explore interpretability dnns several challenges remain unsolved lack interpretability quantity measures dnns lack theory stability dnns difficulty solve nonconvex dnn problems interpretability constraints address challenges simultaneously paper presents novel intrinsic interpretability evaluation framework dnns specifically four independent properties interpretability defined based existing works moreover investigate theory stability dnns important aspect interpretability prove dnns generally stable given different activation functions finally extended version deep learning alternating direction method multipliers dladmm proposed solve dnn problems interpretability constraints efficiently accurately extensive experiments several benchmark datasets validate several dnns proposed interpretability framework\n",
            "output sentence:  propose novel framework evaluate interpretability neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.875, 'f': 0.14141413992857876}, 'rouge-2': {'r': 0.059322033898305086, 'p': 0.875, 'f': 0.1111111099218947}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.875, 'f': 0.14141413992857876}}\n",
            "pair:  present end end design methodology efficient deep learning deployment unlike previous methods separately optimize neural network architecture pruning policy quantization policy jointly optimize end end manner deal larger design space brings train quantization aware accuracy predictor fed evolutionary search select best fit first generate large dataset nn architecture imagenet accuracy pairs without training architecture sampling unified supernet use data train accuracy predictor without quantization using predictor transfer technique get quantization aware predictor reduces amount post quantization fine tuning time extensive experiments imagenet show benefits end end methodology maintains accuracy resnet float model saving bitops comparing bit model obtain level accuracy mobilenetv haq achieving latency energy saving end end optimization outperforms separate optimizations using proxylessnas amc haq accuracy reducing orders magnitude gpu hours co emission\n",
            "output sentence:  present end end design methodology efficient deep learning deployment \n",
            "\n",
            "{'rouge-1': {'r': 0.25, 'p': 0.8333333333333334, 'f': 0.3846153810650888}, 'rouge-2': {'r': 0.1282051282051282, 'p': 0.5263157894736842, 'f': 0.20618556386013392}, 'rouge-l': {'r': 0.23333333333333334, 'p': 0.7777777777777778, 'f': 0.3589743554240632}}\n",
            "pair:  sequence sequence seq seq neural models actively investigated abstractive summarization nevertheless existing neural abstractive systems frequently generate factually incorrect summaries vulnerable adversarial information suggesting crucial lack semantic understanding paper propose novel semantic aware neural abstractive summarization model learns generate high quality summaries semantic interpretation salient content novel evaluation scheme adversarial samples introduced measure well model identifies topic information model yields significantly better performance popular pointer generator summarizer human evaluation also confirms system summaries uniformly informative faithful well less redundant seq seq model\n",
            "output sentence:  propose semantic aware neural abstractive summarization model novel automatic summarization evaluation scheme measures well model identifies topic information adversarial samples \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5, 'f': 0.18181817884297521}, 'rouge-2': {'r': 0.0684931506849315, 'p': 0.35714285714285715, 'f': 0.1149425260351434}, 'rouge-l': {'r': 0.07936507936507936, 'p': 0.35714285714285715, 'f': 0.1298701268949233}}\n",
            "pair:  convolution neural network cnn gained tremendous success computer vision tasks outstanding ability capture local latent features recently increasing interest extending cnns general spatial domain although various types graph convolution geometric convolution methods proposed connections traditional convolution well understood paper show depthwise separable convolution path unify two kinds convolution methods one mathematical view based derive novel depthwise separable graph convolution subsumes existing graph convolution methods special cases formulation experiments show proposed approach consistently outperforms graph convolution geometric convolution baselines benchmark datasets multiple domains\n",
            "output sentence:  devise novel depthwise separable graph convolution dsgc generic spatial domain data highly compatible depthwise separable convolution \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.7058823529411765, 'f': 0.2790697642698756}, 'rouge-2': {'r': 0.0375, 'p': 0.17647058823529413, 'f': 0.061855667212243726}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.35294117647058826, 'f': 0.13953488054894544}}\n",
            "pair:  deep learning computer vision depends mainly source supervision photo realistic simulators generate large scale automatically labeled synthetic data introduce domain gap negatively impacting performance propose new unsupervised domain adaptation algorithm called spigan relying simulator privileged information pi generative adversarial networks gan use internal data simulator pi training target task network experimentally evaluate approach semantic segmentation train networks real world cityscapes vistas datasets using unlabeled real world images synthetic labeled data buffer depth pi synthia dataset method improves adaptation state art unsupervised domain adaptation techniques\n",
            "output sentence:  unsupervised sim real domain adaptation method semantic segmentation using privileged information simulator gan based image translation translation translation \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5454545454545454, 'f': 0.19354838417793965}, 'rouge-2': {'r': 0.017543859649122806, 'p': 0.1, 'f': 0.029850743729115833}, 'rouge-l': {'r': 0.09803921568627451, 'p': 0.45454545454545453, 'f': 0.16129031966181065}}\n",
            "pair:  traditional set prediction models struggle simple datasets due issue call responsibility problem introduce pooling method sets feature vectors based sorting features across elements set used construct permutation equivariant auto encoder avoids responsibility problem toy dataset polygons set version mnist show auto encoder produces considerably better reconstructions representations replacing pooling function existing set encoders fspool improves accuracy convergence speed variety datasets\n",
            "output sentence:  sort encoder undo sorting decoder avoid responsibility problem set auto encoders \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.5454545454545454, 'f': 0.12765957240153916}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.4, 'f': 0.07547169640441441}, 'rouge-l': {'r': 0.060240963855421686, 'p': 0.45454545454545453, 'f': 0.10638297665685832}}\n",
            "pair:  autonomous vehicles becoming common city transportation companies begin find need teach vehicles smart city fleet coordination currently simulation based modeling along hand coded rules dictate decision making autonomous vehicles believe complex intelligent behavior learned agents reinforcement learning paper discuss work solving system adapting deep learning dqn model multi agent setting approach applies deep reinforcement learning combining convolutional neural networks dqn teach agents fulfill customer demand environment partially observ able also demonstrate utilize transfer learning teach agents balance multiple objectives navigating charging station en ergy level low two evaluations presented show solution shown hat successfully able teach agents cooperation policies balancing multiple objectives\n",
            "output sentence:  utilized deep reinforcement learning teach agents ride sharing fleet style coordination \n",
            "\n",
            "{'rouge-1': {'r': 0.09574468085106383, 'p': 0.9, 'f': 0.1730769213387574}, 'rouge-2': {'r': 0.056074766355140186, 'p': 0.6, 'f': 0.10256410100080358}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.8, 'f': 0.15384615210798816}}\n",
            "pair:  generative adversarial networks gans become gold standard comes learning generative models high dimensional distributions since advent numerous variations gans introduced literature primarily focusing utilization novel loss functions optimization regularization strategies network architectures paper turn attention generator investigate use high order polynomials alternative class universal function approximators concretely propose polygan model data generator means high order polynomial whose unknown parameters naturally represented high order tensors introduce two tensor decompositions significantly reduce number parameters show efficiently implemented hierarchical neural networks employ linear convolutional blocks exhibit first time using approach gan generator approximate data distribution without using activation functions thorough experimental evaluation synthetic real data images point clouds demonstrates merits polygan state art\n",
            "output sentence:  model data generator gan means high order polynomial represented high order tensors \n",
            "\n",
            "{'rouge-1': {'r': 0.21621621621621623, 'p': 1.0, 'f': 0.3555555526320988}, 'rouge-2': {'r': 0.1744186046511628, 'p': 0.9375, 'f': 0.2941176444136871}, 'rouge-l': {'r': 0.21621621621621623, 'p': 1.0, 'f': 0.3555555526320988}}\n",
            "pair:  introduce novel end end approach learning cluster absence labeled examples clustering objective based optimizing normalized cuts criterion measures intra cluster similarity well inter cluster dissimilarity define differentiable loss function equivalent expected normalized cuts unlike much work unsupervised deep learning trained model directly outputs final cluster assignments rather embeddings need processing usable approach generalizes unseen datasets across wide variety domains including text image specifically achieve state art results popular unsupervised clustering benchmarks mnist reuters cifar cifar outperforming strongest baselines generalization results superior recent top performing clustering approach ability generalize\n",
            "output sentence:  introduce novel end end approach learning cluster absence labeled examples define differentiable loss function equivalent expected normalized cuts \n",
            "\n",
            "{'rouge-1': {'r': 0.05454545454545454, 'p': 0.3333333333333333, 'f': 0.09374999758300787}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03636363636363636, 'p': 0.2222222222222222, 'f': 0.06249999758300791}}\n",
            "pair:  present soseleto source selection target optimization new method exploiting source dataset solve classification problem target dataset soseleto based following simple intuition source examples informative others target problem capture intuition source samples given weights weights solved jointly source target classification problems via bilevel optimization scheme target therefore gets choose source samples informative classification task furthermore bilevel nature optimization acts kind regularization target mitigating overfitting soseleto may applied classic transfer learning well problem training datasets noisy labels show state art results problems\n",
            "output sentence:  learning limited training data exploiting helpful instances rich data source \n",
            "\n",
            "{'rouge-1': {'r': 0.16883116883116883, 'p': 0.9285714285714286, 'f': 0.2857142831107355}, 'rouge-2': {'r': 0.11904761904761904, 'p': 0.7692307692307693, 'f': 0.2061855646891274}, 'rouge-l': {'r': 0.15584415584415584, 'p': 0.8571428571428571, 'f': 0.2637362611327135}}\n",
            "pair:  various gradient compression schemes proposed mitigate communication cost distributed training large scale machine learning models sign based methods signsgd bernstein et al recently gaining popularity simple compression rule connection adaptive gradient methods like adam paper perform general analysis sign based methods non convex optimization analysis built intuitive bounds success probabilities rely special noise distributions boundedness variance stochastic gradients extending theory distributed setting within parameter server framework assure exponentially fast variance reduction respect number nodes maintaining bit compression directions using small mini batch sizes validate theoretical findings experimentally\n",
            "output sentence:  general analysis sign based methods signsgd non convex optimization built intuitive bounds success probabilities \n",
            "\n",
            "{'rouge-1': {'r': 0.10526315789473684, 'p': 0.6153846153846154, 'f': 0.1797752784042419}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.3076923076923077, 'f': 0.07920791854916191}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.5384615384615384, 'f': 0.15730336829188238}}\n",
            "pair:  reinforcement learning provides powerful general framework decision making control application practice often hindered need extensive feature reward engineering deep reinforcement learning methods remove need explicit engineering policy value features still require manually specified reward function inverse reinforcement learning holds promise automatic reward acquisition proven exceptionally difficult apply large high dimensional problems unknown dynamics work propose airl practical scalable inverse reinforcement learning algorithm based adversarial reward learning formulation competitive direct imitation learning algorithms additionally show airl able recover portable reward functions robust changes dynamics enabling us learn policies even significant variation environment seen training\n",
            "output sentence:  propose adversarial inverse reinforcement learning algorithm capable learning reward functions transfer new unseen environments \n",
            "\n",
            "{'rouge-1': {'r': 0.03508771929824561, 'p': 0.2, 'f': 0.05970148999777244}, 'rouge-2': {'r': 0.014705882352941176, 'p': 0.1111111111111111, 'f': 0.02597402390959706}, 'rouge-l': {'r': 0.03508771929824561, 'p': 0.2, 'f': 0.05970148999777244}}\n",
            "pair:  nowadays deep learning one main topics almost every field helped get amazing results great number tasks main problem kind learning consequently neural networks defined deep resource intensive need specialized hardware perform computation reasonable time unfortunately sufficient make deep learning usable real life many tasks mandatory much possible real time needed optimize many components code algorithms numeric accuracy hardware make efficient usable optimizations help us produce incredibly accurate fast learning models\n",
            "output sentence:  embedded architecture deep learning optimized devices face detection emotion recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.08620689655172414, 'p': 0.8333333333333334, 'f': 0.1562499983007813}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.6666666666666666, 'f': 0.12499999830078126}}\n",
            "pair:  autoregressive recurrent neural decoders generate sequences tokens one one left right workhorse modern machine translation work propose new decoder architecture generate natural language sequences arbitrary order along generating tokens given vocabulary model additionally learns select optimal position produced token proposed decoder architecture fully compatible seq seq framework used drop replacement classical decoder demonstrate performance new decoder iwslt machine translation task well inspect interpret learned decoding patterns analyzing model selects new positions subsequent token\n",
            "output sentence:  new order decoder neural machine translation \n",
            "\n",
            "{'rouge-1': {'r': 0.10909090909090909, 'p': 0.5, 'f': 0.17910447467141904}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.09090909090909091, 'f': 0.02564102321827768}, 'rouge-l': {'r': 0.07272727272727272, 'p': 0.3333333333333333, 'f': 0.11940298213410566}}\n",
            "pair:  open domain dialogue generation gained increasing attention natural language processing comparing methods requires holistic means dialogue evaluation human ratings deemed gold standard human evaluation inefficient costly automated substitute desirable paper propose holistic evaluation metrics capture quality diversity dialogues metrics consists gpt based context coherence sentences dialogue gpt based fluency phrasing gram based diversity responses augmented queries empirical validity metrics demonstrated strong correlation human judgments provide associated code datasets human ratings\n",
            "output sentence:  propose automatic metrics holistically evaluate open dialogue generation strongly correlate human evaluation \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.5555555555555556, 'f': 0.1298701278057008}, 'rouge-2': {'r': 0.02666666666666667, 'p': 0.25, 'f': 0.048192769342430025}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.4444444444444444, 'f': 0.10389610183167484}}\n",
            "pair:  human annotation syntactic parsing expensive large resources available fraction languages question ask whether one leverage abundant unlabeled texts improve syntactic parsers beyond using texts obtain generalisable lexical features beyond word embeddings end propose novel latent variable generative model semi supervised syntactic dependency parsing exact inference intractable introduce differentiable relaxation obtain approximate samples compute gradients respect parser parameters method differentiable perturb parse relies differentiable dynamic programming stochastically perturbed edge scores demonstrate effectiveness approach experiments english french swedish\n",
            "output sentence:  differentiable dynamic programming perturbed input weights application semi supervised vae \n",
            "\n",
            "{'rouge-1': {'r': 0.04918032786885246, 'p': 0.375, 'f': 0.08695651968914099}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04918032786885246, 'p': 0.375, 'f': 0.08695651968914099}}\n",
            "pair:  collection scientific papers often accompanied tags keywords topics concepts etc associated paper sometimes tags human generated sometimes machine generated propose simple measure consistency tagging scientific papers whether tags predictive citation graph links since authors tend cite papers topics close publications consistent tagging system could predict citations present algorithm calculate consistency experiments human machine generated tags show augmentation combination manual tags machine generated ones enhance consistency tags introduce cross consistency ability predict citation links papers tagged different taggers manually machine cross consistency used evaluate tagging quality amount labeled data limited\n",
            "output sentence:  good tagger gives similar tags given paper papers cites \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.5714285714285714, 'f': 0.11594202716236088}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.42857142857142855, 'f': 0.08695651991598406}}\n",
            "pair:  recommendation prevalent application machine learning affects many users therefore crucial recommender models accurate interpretable work propose method interpret augment predictions black box recommender systems particular propose extract feature interaction interpretations source recommender model explicitly encode interactions target recommender model source target models black boxes assuming structure recommender system approach used general settings experiments focus prominent use machine learning recommendation ad click prediction found interaction interpretations informative predictive significantly outperforming existing recommender models approach interpreting interactions provide new insights domains even beyond recommendation\n",
            "output sentence:  proposed method extract leverage interpretations feature interactions \n",
            "\n",
            "{'rouge-1': {'r': 0.039473684210526314, 'p': 0.42857142857142855, 'f': 0.07228915508201483}, 'rouge-2': {'r': 0.0297029702970297, 'p': 0.42857142857142855, 'f': 0.05555555434327849}, 'rouge-l': {'r': 0.039473684210526314, 'p': 0.42857142857142855, 'f': 0.07228915508201483}}\n",
            "pair:  paper propose combine imitation reinforcement learning via idea reward shaping using oracle study effectiveness near optimal cost go oracle planning horizon demonstrate cost go oracle shortens learner planning horizon function accuracy globally optimal oracle shorten planning horizon one leading one step greedy markov decision process much easier optimize oracle far away optimality requires planning longer horizon achieve near optimal performance hence new insight bridges gap interpolates imitation learning reinforcement learning motivated mentioned insights propose truncated horizon policy search thor method focuses searching policies maximize total reshaped reward finite planning horizon oracle sub optimal experimentally demonstrate gradient based implementation thor achieve superior performance compared rl baselines il baselines even oracle sub optimal\n",
            "output sentence:  combining imitation learning reinforcement learning learn outperform expert \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 1.0, 'f': 0.2121212102249771}, 'rouge-2': {'r': 0.07746478873239436, 'p': 0.8461538461538461, 'f': 0.14193548233423517}, 'rouge-l': {'r': 0.0847457627118644, 'p': 0.7142857142857143, 'f': 0.15151514961891646}}\n",
            "pair:  inspired neurophysiological discoveries navigation cells mammalian brain introduce first deep neural network architecture modeling egocentric spatial memory esm learns estimate pose agent progressively construct top global maps egocentric views spatially extended environment exploration proposed esm network model updates belief global map based local observations using recurrent neural network also augments local mapping novel external memory encode store latent representations visited places based corresponding locations egocentric coordinate enables agents perform loop closure mapping correction work contributes following aspects first proposed esm network provides accurate mapping ability vitally important embodied agents navigate goal locations experiments demonstrate functionalities esm network random walks complicated mazes comparing several competitive baselines state art simultaneous localization mapping slam algorithms secondly faithfully hypothesize functionality working mechanism navigation cells brain comprehensive analysis model suggests essential role individual modules proposed architecture demonstrates efficiency communications among modules hope work would advance research collaboration communications fields computer science computational neuroscience\n",
            "output sentence:  first deep neural network modeling egocentric spatial memory inspired neurophysiological discoveries navigation cells mammalian brain \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.6666666666666666, 'f': 0.21621621349890432}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.09090909090909091, 'f': 0.02380952153344693}, 'rouge-l': {'r': 0.08064516129032258, 'p': 0.4166666666666667, 'f': 0.13513513241782327}}\n",
            "pair:  training generative adversarial networks gans notoriously challenging propose study architectural modification self modulation improves gan performance across different data sets architectures losses regularizers hyperparameter settings intuitively self modulation allows intermediate feature maps generator change function input noise vector reminiscent conditioning techniques requires labeled data large scale empirical study observe relative decrease fid furthermore else equal adding modification generator leads improved performance studied settings self modulation simple architectural change requires additional parameter tuning suggests applied readily gan\n",
            "output sentence:  simple gan modification improves performance across many losses architectures regularization schemes datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.36363636363636365, 'f': 0.09195402077949535}, 'rouge-2': {'r': 0.01904761904761905, 'p': 0.2, 'f': 0.03478260710775055}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.36363636363636365, 'f': 0.09195402077949535}}\n",
            "pair:  interpreting generative adversarial network gan training approximate divergence minimization theoretically insightful spurred discussion lead theoretically practically interesting extensions gans wasserstein gans classic gans gans original variant training non saturating variant uses alternative form generator update original variant theoretically easier study alternative variant frequently performs better recommended use practice alternative generator update often regarded simple modification deal optimization issues appears common misconception two variants minimize divergence short note derive divergences approximately minimized original alternative variants gan gan training highlights important differences two variants example show alternative variant kl gan training actually minimizes reverse kl divergence alternative variant conventional gan training minimizes softened version reverse kl hope results may help clarify theoretical discussion surrounding divergence minimization view gan training\n",
            "output sentence:  typical gan training optimize jensen shannon something like reverse kl divergence \n",
            "\n",
            "{'rouge-1': {'r': 0.1375, 'p': 0.6875, 'f': 0.22916666388888896}, 'rouge-2': {'r': 0.06451612903225806, 'p': 0.4, 'f': 0.11111110871913585}, 'rouge-l': {'r': 0.1, 'p': 0.5, 'f': 0.16666666388888893}}\n",
            "pair:  direct policy gradient methods reinforcement learning continuous control problems popular napproach variety reasons easy implement without explicit knowledge underlying model end end approach directly optimizing performance metric interest inherently allow richly parameterized policies notable drawback even basic continuous control problem linear quadratic regulators methods must solve non convex optimization problem little understood efficiency computational statistical perspectives contrast system identification model based planning optimal control theory much solid theoretical footing much known regards computational statistical properties work bridges gap showing model free policy gradient methods globally converge optimal solution efficient polynomially relevant problem dependent quantities regards sample computational complexities\n",
            "output sentence:  paper shows model free policy gradient methods converge global optimal solution non convex linearized control problems \n",
            "\n",
            "{'rouge-1': {'r': 0.1686746987951807, 'p': 0.875, 'f': 0.28282828011835537}, 'rouge-2': {'r': 0.0392156862745098, 'p': 0.25, 'f': 0.06779660782533764}, 'rouge-l': {'r': 0.10843373493975904, 'p': 0.5625, 'f': 0.1818181791082543}}\n",
            "pair:  data augmentation techniques flipping cropping systematically enlarge training dataset explicitly generating training samples effective improving generalization performance deep neural networks supervised setting common practice data augmentation assign label augmented samples source however augmentation results large distributional discrepancy among rotations forcing label invariance may difficult solve often hurts performance tackle challenge suggest simple yet effective idea learning joint distribution original self supervised labels augmented samples joint learning framework easier train enables aggregated inference combining predictions different augmented samples improving performance speed aggregation process also propose knowledge transfer technique self distillation transfers knowledge augmentation model demonstrate effectiveness data augmentation framework various fully supervised settings including shot imbalanced classification scenarios\n",
            "output sentence:  propose simple self supervised data augmentation technique improves performance fully supervised scenarios including shot learning imbalanced classification \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 0.8181818181818182, 'f': 0.32142856827168376}, 'rouge-2': {'r': 0.057692307692307696, 'p': 0.3, 'f': 0.09677419084287209}, 'rouge-l': {'r': 0.15555555555555556, 'p': 0.6363636363636364, 'f': 0.24999999684311228}}\n",
            "pair:  work propose novel formulation planning views probabilistic inference problem future optimal trajectories enables us use sampling methods thus tackle planning continuous domains using fixed computational budget design new algorithm sequential monte carlo planning leveraging classical methods sequential monte carlo bayesian smoothing context control inference furthermore show sequential monte carlo planning capture multimodal policies quickly learn continuous control tasks\n",
            "output sentence:  leveraging control inference sequential monte carlo methods proposed probabilistic planning algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5, 'f': 0.19047618739229027}, 'rouge-2': {'r': 0.0234375, 'p': 0.15, 'f': 0.04054053820306806}, 'rouge-l': {'r': 0.09411764705882353, 'p': 0.4, 'f': 0.15238094929705223}}\n",
            "pair:  adversarial attacks machine learning classifiers small perturbations added input correctly classified perturbations yield adversarial examples virtually indistinguishable unperturbed input yet misclassified standard neural networks used deep learning attackers craft adversarial examples input cause misclassification choice introduce new type network units called rbfi units whose non linear structure makes inherently resistant adversarial attacks permutation invariant mnist absence adversarial attacks networks using rbfi units match performance networks using sigmoid units slightly accuracy networks relu units subjected adversarial attacks based projected gradient descent fast gradient sign methods networks rbfi units retain accuracies relu sigmoid see accuracies reduced rbfi networks trained regular input either exceed closely match accuracy sigmoid relu network trained help adversarial examples non linear structure rbfi units makes difficult train using standard gradient descent show rbfi networks rbfi units efficiently trained high accuracies using pseudogradients computed using functions especially crafted facilitate learning instead true derivatives\n",
            "output sentence:  introduce type neural network structurally resistant adversarial attacks even trained unaugmented training sets resistance due stability network units wrt input perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.031914893617021274, 'p': 0.3333333333333333, 'f': 0.05825242558959378}, 'rouge-2': {'r': 0.007518796992481203, 'p': 0.1111111111111111, 'f': 0.01408450585498919}, 'rouge-l': {'r': 0.02127659574468085, 'p': 0.2222222222222222, 'f': 0.03883494986143847}}\n",
            "pair:  long known single layer fully connected neural network prior parameters equivalent gaussian process gp limit infinite network width correspondence enables exact bayesian inference infinite width neural networks regression tasks means evaluating corresponding gp recently kernel functions mimic multi layer random neural networks developed outside bayesian framework previous work identified kernels used covariance functions gps allow fully bayesian prediction deep neural network work derive exact equivalence infinitely wide deep networks gps particular covariance function develop computationally efficient pipeline compute covariance function use resulting gp perform bayesian inference deep neural networks mnist cifar observe trained neural network accuracy approaches corresponding gp increasing layer width gp uncertainty strongly correlated trained network prediction error find test performance increases finite width trained networks made wider similar gp gp based predictions typically outperform finite width networks finally connect prior distribution weights variances gp formulation recent development signal propagation random neural networks\n",
            "output sentence:  show make predictions using deep networks without training deep networks \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.4, 'f': 0.11764705631487893}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.1111111111111111, 'f': 0.027027024890431142}, 'rouge-l': {'r': 0.05172413793103448, 'p': 0.3, 'f': 0.08823529160899661}}\n",
            "pair:  challenges natural sciences often phrased optimization problems machine learning techniques recently applied solve problems one example chemistry design tailor made organic materials molecules requires efficient methods explore chemical space present genetic algorithm ga enhanced neural network dnn based discriminator model improve diversity generated molecules time steer ga show algorithm outperforms generative models optimization tasks furthermore present way increase interpretability genetic algorithms helped us derive design principles\n",
            "output sentence:  tackling inverse design via genetic algorithms augmented deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0, 'p': 0.0, 'f': 0.0}}\n",
            "pair:  despite recent progress generative image modeling successfully generating high resolution diverse samples complex datasets imagenet remains elusive goal end train generative adversarial networks largest scale yet attempted study instabilities specific scale find applying orthogonal regularization generator renders amenable simple truncation trick allowing fine control trade sample fidelity variety reducing variance generator input modifications lead models set new state art class conditional image synthesis trained imagenet resolution models biggans achieve inception score frechet inception distance fid improving previous best fid\n",
            "output sentence:  gans benefit scaling \n",
            "\n",
            "{'rouge-1': {'r': 0.16494845360824742, 'p': 0.7619047619047619, 'f': 0.2711864377520828}, 'rouge-2': {'r': 0.046296296296296294, 'p': 0.25, 'f': 0.07812499736328134}, 'rouge-l': {'r': 0.1134020618556701, 'p': 0.5238095238095238, 'f': 0.18644067504021836}}\n",
            "pair:  sequence prediction models learned example sequences variety training algorithms maximum likelihood learning simple efficient yet suffer compounding error test time reinforcement learning policy gradient addresses issue prohibitively poor exploration efficiency rich set algorithms data noising raml softmax policy gradient also developed different perspectives paper present formalism entropy regularized policy optimization show apparently distinct algorithms including mle reformulated special instances formulation difference characterized reward function two weight hyperparameters unifying interpretation enables us systematically compare algorithms side side gain new insights trade offs algorithm design new perspective also leads improved approach dynamically interpolates among family algorithms learns model scheduled way experiments machine translation text summarization game imitation learning demonstrate superiority proposed approach\n",
            "output sentence:  entropy regularized policy optimization formalism subsumes set sequence prediction learning algorithms new interpolation algorithm improved results text generation game imitation learning \n",
            "\n",
            "{'rouge-1': {'r': 0.10227272727272728, 'p': 0.5, 'f': 0.1698113179352083}, 'rouge-2': {'r': 0.02727272727272727, 'p': 0.17647058823529413, 'f': 0.04724409216938445}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.3333333333333333, 'f': 0.11320754435030267}}\n",
            "pair:  recent research intensively revealed vulnerability deep neural networks especially convolutional neural networks cnns task image recognition creating adversarial samples slightly differ legitimate samples vulnerability indicates powerful models sensitive specific perturbations cannot filter adversarial perturbations work propose quantization based method enables cnn filter adversarial perturbations effectively notably different prior work input quantization apply quantization intermediate layers cnn approach naturally aligned clustering coarse grained semantic information learned cnn furthermore compensate loss information inevitably caused quantization propose multi head quantization project data points different sub spaces perform quantization within sub space enclose design quantization layer named layer results obtained mnist fashion mnsit datasets demonstrate adding one layer cnn could significantly improve robustness white box black box attacks\n",
            "output sentence:  propose quantization based method regularizes cnn learned representations automatically aligned trainable concept matrix hence effectively filtering adversarial perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.22950819672131148, 'p': 1.0, 'f': 0.3733333302968889}, 'rouge-2': {'r': 0.15492957746478872, 'p': 0.8461538461538461, 'f': 0.26190475928854884}, 'rouge-l': {'r': 0.22950819672131148, 'p': 1.0, 'f': 0.3733333302968889}}\n",
            "pair:  propose new method train neural networks based novel combination adversarial training provable defenses key idea model training procedure includes verifier adversary every iteration verifier aims certify network using convex relaxation adversary tries find inputs inside convex relaxation cause verification fail experimentally show training method promising achieves best worlds produces model state art accuracy certified robustness challenging cifar dataset infinity perturbation significant improvement currently known best results accuracy certified robustness achieved using times larger network work\n",
            "output sentence:  propose novel combination adversarial training provable defenses produces model state art accuracy certified robustness cifar \n",
            "\n",
            "{'rouge-1': {'r': 0.3090909090909091, 'p': 0.7391304347826086, 'f': 0.4358974317389876}, 'rouge-2': {'r': 0.11475409836065574, 'p': 0.2916666666666667, 'f': 0.1647058783003461}, 'rouge-l': {'r': 0.2545454545454545, 'p': 0.6086956521739131, 'f': 0.3589743548159106}}\n",
            "pair:  study control symmetric linear dynamical systems unknown dynamics hidden state using recent spectral filtering technique concisely representing systems linear basis formulate optimal control setting convex program approach eliminates need solve non convex problem explicit identification system latent state allows provable optimality guarantees control signal give first efficient algorithm finding optimal control signal arbitrary time horizon sample complexity number training rollouts polynomial log relevant parameters\n",
            "output sentence:  using novel representation symmetric linear dynamical systems latent state formulate optimal control convex program giving first polynomial time algorithm solves time sample polylogarithmic polylogarithmic complexity \n",
            "\n",
            "{'rouge-1': {'r': 0.16279069767441862, 'p': 0.8235294117647058, 'f': 0.27184465743802433}, 'rouge-2': {'r': 0.11016949152542373, 'p': 0.7647058823529411, 'f': 0.19259259039122084}, 'rouge-l': {'r': 0.16279069767441862, 'p': 0.8235294117647058, 'f': 0.27184465743802433}}\n",
            "pair:  deep learning models vulnerable adversarial examples crafted applying human imperceptible perturbations benign inputs however black box setting existing adversaries often poor transferability attack defense models work perspective regarding adversarial example generation optimization process propose two new methods improve transferability adversarial examples namely nesterov iterative fast gradient sign method ni fgsm scale invariant attack method sim ni fgsm aims adapt nesterov accelerated gradient iterative attacks effectively look ahead improve transferability adversarial examples sim based discovery scale invariant property deep learning models leverage optimize adversarial perturbations scale copies input images avoid overfitting white box model attacked generate transferable adversarial examples ni fgsm sim naturally integrated build robust gradient based attack generate transferable adversarial examples defense models empirical results imagenet dataset demonstrate attack methods exhibit higher transferability achieve higher attack success rates state art gradient based attacks\n",
            "output sentence:  proposed nesterov iterative fast gradient sign method ni fgsm scale invariant attack method sim boost transferability adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.13793103448275862, 'p': 1.0, 'f': 0.24242424029384757}, 'rouge-2': {'r': 0.10144927536231885, 'p': 1.0, 'f': 0.18421052464335183}, 'rouge-l': {'r': 0.13793103448275862, 'p': 1.0, 'f': 0.24242424029384757}}\n",
            "pair:  cost annotating training data traditionally bottleneck supervised learning approaches problem exacerbated supervised learning applied number correlated tasks simultaneously since amount labels required scales number tasks mitigate concern propose active multitask learning algorithm achieves knowledge transfer tasks approach forms called committee task jointly makes decisions directly shares data across similar tasks approach reduces number queries needed training maintaining high accuracy test data empirical results benchmark datasets show significant improvements accuracy number query requests\n",
            "output sentence:  propose active multitask learning algorithm achieves knowledge transfer tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.04477611940298507, 'p': 0.5, 'f': 0.08219177931319198}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04477611940298507, 'p': 0.5, 'f': 0.08219177931319198}}\n",
            "pair:  robustness verification aims formally certify prediction behavior neural networks become important tool understanding behavior given model obtaining safety guarantees however previous methods usually limited relatively simple neural networks paper consider robustness verification problem transformers transformers complex self attention layers pose many challenges verification including cross nonlinearity cross position dependency discussed previous work resolve challenges develop first verification algorithm transformers certified robustness bounds computed method significantly tighter naive interval bound propagation bounds also shed light interpreting transformers consistently reflect importance words sentiment analysis\n",
            "output sentence:  propose first algorithm verifying robustness transformers \n",
            "\n",
            "{'rouge-1': {'r': 0.09278350515463918, 'p': 0.6428571428571429, 'f': 0.16216215995779565}, 'rouge-2': {'r': 0.025210084033613446, 'p': 0.23076923076923078, 'f': 0.04545454367883386}, 'rouge-l': {'r': 0.05154639175257732, 'p': 0.35714285714285715, 'f': 0.09009008788572362}}\n",
            "pair:  deep learning based models speech enhancement mainly focused estimating magnitude spectrogram reusing phase noisy speech reconstruction due difficulty estimating phase clean speech improve speech enhancement performance tackle phase estimation problem three ways first propose deep complex net advanced net structured model incorporating well defined complex valued building blocks deal complex valued spectrograms second propose polar coordinate wise complex valued masking method reflect distribution complex ideal ratio masks third define novel loss function weighted source distortion ratio wsdr loss designed directly correlate quantitative evaluation measure model evaluated mixture voice bank corpus demand database widely used many deep learning models speech enhancement ablation experiments conducted mixed dataset showing three proposed approaches empirically valid experimental results show proposed method achieves state art performance metrics outperforming previous approaches large margin\n",
            "output sentence:  paper proposes novel complex masking method speech enhancement along loss function efficient phase estimation \n",
            "\n",
            "{'rouge-1': {'r': 0.0975609756097561, 'p': 0.42105263157894735, 'f': 0.15841583852955599}, 'rouge-2': {'r': 0.009615384615384616, 'p': 0.05263157894736842, 'f': 0.016260159989424705}, 'rouge-l': {'r': 0.08536585365853659, 'p': 0.3684210526315789, 'f': 0.1386138583315362}}\n",
            "pair:  solving tasks sparse rewards one important challenges reinforcement learning single agent setting challenge addressed introducing intrinsic rewards motivate agents explore unseen regions state spaces applying techniques naively multi agent setting results agents exploring independently without coordination among argue learning cooperative multi agent settings accelerated improved agents coordinate respect explored paper propose approach learning dynamically select different types intrinsic rewards consider individual agent explored agents agents coordinate exploration maximize extrinsic returns concretely formulate approach hierarchical policy high level controller selects among sets policies trained different types intrinsic rewards low level controllers learn action policies agents specific rewards demonstrate effectiveness proposed approach multi agent gridworld domain sparse rewards show method scales complex settings evaluating vizdoom platform\n",
            "output sentence:  propose several intrinsic reward functions encouraging coordinated exploration multi agent problems introduce approach dynamically selecting best exploration method given task task \n",
            "\n",
            "{'rouge-1': {'r': 0.12857142857142856, 'p': 0.6, 'f': 0.21176470297577857}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.26666666666666666, 'f': 0.07619047374149668}, 'rouge-l': {'r': 0.11428571428571428, 'p': 0.5333333333333333, 'f': 0.1882352912110727}}\n",
            "pair:  gap empirical success deep learning lack strong theoretical guarantees calls studying simpler models observing relu neuron product linear function gate latter determines whether neuron active share jointly trained weight vector propose decouple two introduce galu networks networks neuron product linear unit defined weight vector trained gate defined different weight vector trained generally speaking given base model simpler version two parameters determine quality simpler version whether practical performance close enough base model whether easier analyze theoretically show galu networks perform similarly relu networks standard datasets initiate study theoretical properties demonstrating indeed easier analyze believe research galu networks may fruitful development theory deep learning\n",
            "output sentence:  propose gated linear unit networks model performs similarly relu networks real data much easier analyze theoretically \n",
            "\n",
            "{'rouge-1': {'r': 0.08620689655172414, 'p': 0.35714285714285715, 'f': 0.13888888575617292}, 'rouge-2': {'r': 0.015873015873015872, 'p': 0.07142857142857142, 'f': 0.025974022998819703}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.2857142857142857, 'f': 0.11111110797839514}}\n",
            "pair:  recent neural network language models begun rely softmax distributions extremely large number categories context calculating softmax normalizing constant prohibitively expensive spurred growing literature efficiently computable biased estimates softmax paper present first two unbiased algorithms maximizing softmax likelihood whose work per iteration independent number classes datapoints require extra work end epoch compare unbiased methods empirical performance state art seven real world datasets comprehensively outperform competitors\n",
            "output sentence:  propose first methods exactly optimizing softmax distribution using stochastic gradient runtime independent number number datapoints \n",
            "\n",
            "{'rouge-1': {'r': 0.07272727272727272, 'p': 0.4444444444444444, 'f': 0.12499999758300785}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.125, 'f': 0.031249997812500156}, 'rouge-l': {'r': 0.05454545454545454, 'p': 0.3333333333333333, 'f': 0.09374999758300787}}\n",
            "pair:  modern neural networks parametrized particular rectified linear hidden unit modified multiplicative factor adjusting input put weights without changing rest network inspired sinkhorn knopp algorithm introduce fast iterative method minimizing norm weights equivalently weight decay regularizer provably converges unique solution interleaving algorithm sgd training improves test accuracy small batches approach offers alternative batch group normalization cifar imagenet resnet\n",
            "output sentence:  fast iterative algorithm balance energy network staying functional equivalence class \n",
            "\n",
            "{'rouge-1': {'r': 0.0847457627118644, 'p': 0.3333333333333333, 'f': 0.13513513190284887}, 'rouge-2': {'r': 0.013888888888888888, 'p': 0.07142857142857142, 'f': 0.02325581122769096}, 'rouge-l': {'r': 0.06779661016949153, 'p': 0.26666666666666666, 'f': 0.10810810487582187}}\n",
            "pair:  quantum machine learning methods potential facilitate learning using extremely large datasets availability data training machine learning models steadily increasing oftentimes much easier collect feature vectors obtain corresponding labels one approaches addressing issue use semi supervised learning leverages labeled samples also unlabeled feature vectors present quantum machine learning algorithm training semi supervised kernel support vector machines algorithm uses recent advances quantum sample based hamiltonian simulation extend existing quantum ls svm algorithm handle semi supervised term loss maintaining quantum speedup quantum ls svm\n",
            "output sentence:  extend quantum svms semi supervised setting deal likely problem many missing class labels huge datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.22448979591836735, 'p': 0.7333333333333333, 'f': 0.34374999641113285}, 'rouge-2': {'r': 0.13793103448275862, 'p': 0.5, 'f': 0.21621621282688094}, 'rouge-l': {'r': 0.22448979591836735, 'p': 0.7333333333333333, 'f': 0.34374999641113285}}\n",
            "pair:  giving provable guarantees learning neural networks core challenge machine learning theory prior work gives parameter recovery guarantees one hidden layer networks however networks used practice multiple non linear layers work show strengthen results deeper networks address problem uncovering lowest layer deep neural network assumption lowest layer uses high threshold applying activation upper network modeled well behaved polynomial input distribution gaussian\n",
            "output sentence:  provably recover lowest layer deep neural network assuming lowest layer uses high threshold activation network well behaved polynomial \n",
            "\n",
            "{'rouge-1': {'r': 0.024390243902439025, 'p': 0.5, 'f': 0.046511627020010826}, 'rouge-2': {'r': 0.009615384615384616, 'p': 0.3333333333333333, 'f': 0.018691588240020977}, 'rouge-l': {'r': 0.024390243902439025, 'p': 0.5, 'f': 0.046511627020010826}}\n",
            "pair:  reinforcement learning typically requires carefully designed reward functions order learn desired behavior present novel reward estimation method based finite sample optimal state trajectories expert demon strations used guiding agent mimic expert behavior optimal state trajectories used learn generative predictive model good states distribution reward signal computed function difference actual next state acquired agent predicted next state given learned generative predictive model inferred reward function perform standard reinforcement learning inner loop guide agent learn given task experimental evaluations across range tasks demonstrate proposed method produces superior performance compared standard reinforcement learning complete sparse hand engineered rewards furthermore show method successfully enables agent learn good actions directly expert player video games super mario bros flappy bird\n",
            "output sentence:  reward estimation game videos \n",
            "\n",
            "{'rouge-1': {'r': 0.06666666666666667, 'p': 0.4444444444444444, 'f': 0.11594202671707628}, 'rouge-2': {'r': 0.014492753623188406, 'p': 0.125, 'f': 0.02597402411199204}, 'rouge-l': {'r': 0.05, 'p': 0.3333333333333333, 'f': 0.0869565194706995}}\n",
            "pair:  significant recent evidence supervised learning parametrized setting wider networks achieve better test error words bias variance tradeoff directly observable increasing network width arbitrarily investigate whether corresponding phenomenon present reinforcement learning experiment four openai gym environments increasing width value policy networks beyond prescribed values empirical results lend support hypothesis however tuning hyperparameters network width separately remains important future work environments algorithms optimal hyperparameters vary noticably across widths confounding results hyperparameters used widths\n",
            "output sentence:  parametrization width seems help deep reinforcement learning supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.09433962264150944, 'p': 0.4166666666666667, 'f': 0.15384615083550301}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.09090909090909091, 'f': 0.025974023525046613}, 'rouge-l': {'r': 0.07547169811320754, 'p': 0.3333333333333333, 'f': 0.12307692006627227}}\n",
            "pair:  introduce new procedural dynamic system generate variety shapes often appear curves technically figures plots many points name spiroplots show new system relates procedures processes generate figures spiroplots extremely simple process surprising visual variety prove fundamental properties analyze instances see geometry topology input determines generated figures show spiroplots finite cycle return initial situation whereas others produce new points infinitely often paper accompanied javascript app allows anyone generate spiroplots\n",
            "output sentence:  new simple dynamic system introduced generates pretty patterns properties proved possibilities explored \n",
            "\n",
            "{'rouge-1': {'r': 0.06349206349206349, 'p': 0.3333333333333333, 'f': 0.10666666397866674}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.07692307692307693, 'f': 0.023255811387236626}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.25, 'f': 0.07999999731200008}}\n",
            "pair:  modern deep neural networks achieve high accuracy training distribution test distribution identically distributed assumption frequently violated practice train test distributions mismatched accuracy plummet currently techniques improve robustness unforeseen data shifts encountered deployment work propose technique improve robustness uncertainty estimates image classifiers propose augmix data processing technique simple implement adds limited computational overhead helps models withstand unforeseen corruptions augmix significantly improves robustness uncertainty measures challenging image classification benchmarks closing gap previous methods best possible performance cases half\n",
            "output sentence:  obtain state art robustness data shifts maintain calibration data shift even though even accuracy drops \n",
            "\n",
            "{'rouge-1': {'r': 0.09722222222222222, 'p': 0.5833333333333334, 'f': 0.1666666642176871}, 'rouge-2': {'r': 0.024096385542168676, 'p': 0.16666666666666666, 'f': 0.042105260950692634}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.3333333333333333, 'f': 0.09523809278911571}}\n",
            "pair:  generative models singing voice mostly concerned task singing voice synthesis produce singing voice waveforms given musical scores text lyrics work explore novel yet challenging alternative singing voice generation without pre assigned scores lyrics training inference time particular experiment three different schemes free singer model generates singing voices without taking conditions accompanied singer model generates singing voices waveform instrumental music solo singer model improvises chord sequence first uses generate voices outline associated challenges propose pipeline tackle new tasks involves development source separation transcription models data preparation adversarial networks audio generation customized metrics evaluation\n",
            "output sentence:  models generate singing voices without lyrics scores take accompaniment input output singing voices \n",
            "\n",
            "{'rouge-1': {'r': 0.10204081632653061, 'p': 0.38461538461538464, 'f': 0.16129031926638923}, 'rouge-2': {'r': 0.03389830508474576, 'p': 0.15384615384615385, 'f': 0.05555555259645077}, 'rouge-l': {'r': 0.061224489795918366, 'p': 0.23076923076923078, 'f': 0.09677419023413122}}\n",
            "pair:  propose novel subgraph image representation classification network fragments target parent networks graph image representation based image embeddings adjacency matrices use image representation two modes first input machine learning algorithm second input pure transfer learner conclusions multiple datasets deep learning using structured image features performs best compared graph kernel classical features based methods pure transfer learning works effectively minimum interference user robust small data\n",
            "output sentence:  convert subgraphs structured images classify using deep learning transfer learning caffe achieve stunning results \n",
            "\n",
            "{'rouge-1': {'r': 0.09876543209876543, 'p': 0.4444444444444444, 'f': 0.16161615864095505}, 'rouge-2': {'r': 0.010526315789473684, 'p': 0.05263157894736842, 'f': 0.017543856871345468}, 'rouge-l': {'r': 0.06172839506172839, 'p': 0.2777777777777778, 'f': 0.10101009803489448}}\n",
            "pair:  generative adversarial networks gans achieved remarkable results task generating realistic natural images applications gan models share two aspects common one hand gans training involves solving challenging saddle point optimization problem interpreted adversarial game generator discriminator functions hand generator discriminator parametrized terms deep convolutional neural networks goal paper disentangle contribution two factors success gans particular introduce generative latent optimization glo framework train deep convolutional generators without using discriminators thus avoiding instability adversarial optimization problems throughout variety experiments show glo enjoys many desirable properties gans learning large data synthesizing visually appealing samples interpolating meaningfully samples performing linear arithmetic noise vectors\n",
            "output sentence:  gans successful adversarial training use convnets show convnet generator trained simple reconstruction loss learnable vectors vectors many desirable many properties gan \n",
            "\n",
            "{'rouge-1': {'r': 0.08771929824561403, 'p': 0.4166666666666667, 'f': 0.14492753335853814}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.25, 'f': 0.0869565188657846}}\n",
            "pair:  introduce notion property signatures representation programs program specifications meant consumption machine learning algorithms given function input type output type property function type bool informally describes simple property function consideration instance lists type one property might ask input list length output list list properties evaluate function get list outputs call property signature crucially guess property signature function given set input output pairs meant specify function discuss several potential applications property signatures show experimentally used improve baseline synthesizer emits twice many programs less one tenth time\n",
            "output sentence:  represent computer program using set simpler programs use representation improve program synthesis techniques \n",
            "\n",
            "{'rouge-1': {'r': 0.09523809523809523, 'p': 0.3076923076923077, 'f': 0.1454545418446282}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.08333333333333333, 'f': 0.03389830184429793}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.15384615384615385, 'f': 0.07272726911735555}}\n",
            "pair:  validation key challenge search safe autonomy simulations often either simple provide robust validation complex tractably compute therefore approximate validation methods needed tractably find failures without unsafe simplifications paper presents theory behind one black box approach adaptive stress testing ast also provide three examples validation problems formulated work ast\n",
            "output sentence:  formulation black box reinforcement learning method find likely failure system acting complex scenarios \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.8, 'f': 0.19753086203322667}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.1, 'f': 0.02197802002173668}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.6, 'f': 0.14814814598384393}}\n",
            "pair:  autoencoders provide powerful framework learning compressed representations encoding information needed reconstruct data point latent code cases autoencoders interpolate decoding convex combination latent codes two datapoints autoencoder produce output semantically mixes characteristics datapoints paper propose regularization procedure encourages interpolated outputs appear realistic fooling critic network trained recover mixing coefficient interpolated data develop simple benchmark task quantitatively measure extent various autoencoders interpolate show regularizer dramatically improves interpolation setting also demonstrate empirically regularizer produces latent codes effective downstream tasks suggesting possible link interpolation abilities learning useful representations\n",
            "output sentence:  propose regularizer improves interpolation autoencoders show also improves learned representation downstream tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.35714285714285715, 'f': 0.13333333029688896}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03278688524590164, 'p': 0.14285714285714285, 'f': 0.05333333029688906}}\n",
            "pair:  basis pursuit compressed sensing optimization norm minimized subject model error constraints use deep neural network prior instead regularization using known noise statistics jointly learn prior reconstruct images without access ground truth data training use alternating minimization across unrolled iterative network jointly solve neural network weights training set image reconstructions inference fix weights pass measurements network compare reconstruction performance unsupervised supervised ground truth methods hypothesize technique could used learn reconstruction ground truth data unavailable high resolution dynamic mri\n",
            "output sentence:  present unsupervised deep learning reconstruction imaging inverse problems combines neural networks model based constraints \n",
            "\n",
            "{'rouge-1': {'r': 0.04040404040404041, 'p': 0.8, 'f': 0.07692307600776628}, 'rouge-2': {'r': 0.007692307692307693, 'p': 0.2, 'f': 0.014814814101508953}, 'rouge-l': {'r': 0.04040404040404041, 'p': 0.8, 'f': 0.07692307600776628}}\n",
            "pair:  recent studies show convolutional neural networks cnns vulnerable various settings including adversarial examples backdoor attacks distribution shifting motivated findings human visual system pays attention global structure shape recognition cnns biased towards local texture features images propose unified framework edgeganrob based robust edge features improve robustness cnns general first explicitly extracts shape structure features given image reconstructs new image refilling texture information trained generative adversarial network gan addition reduce sensitivity edge detection algorithm adversarial perturbation propose robust edge detection approach robust canny based vanilla canny algorithm gain insights also compare edgeganrob simplified backbone procedure edgenetrob performs learning tasks directly extracted robust edge features find edgenetrob help boost model robustness significantly cost clean model accuracy edgeganrob hand able improve clean model accuracy compared edgenetrob without losing robustness benefits introduced edgenetrob extensive experiments show edgeganrob resilient different learning tasks diverse settings\n",
            "output sentence:  unified model improve model robustness multiple tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.02857142857142857, 'p': 0.2727272727272727, 'f': 0.05172413621432823}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02857142857142857, 'p': 0.2727272727272727, 'f': 0.05172413621432823}}\n",
            "pair:  music relies heavily repetition build structure meaning self reference occurs multiple timescales motifs phrases reusing entire sections music pieces aba structure transformer vaswani et al sequence model based self attention achieved compelling results many generation tasks require maintaining long range coherence suggests self attention might also well suited modeling music musical composition performance however relative timing critically important existing approaches representing relative positional information transformer modulate attention based pairwise distance shaw et al impractical long sequences musical compositions since memory complexity quadratic sequence length propose algorithm reduces intermediate memory requirements linear sequence length enables us demonstrate transformer modified relative attention mechanism generate minute long thousands steps compositions compelling structure generate continuations coherently elaborate given motif seq seq setup generate accompaniments conditioned melodies evaluate transformer relative attention mechanism two datasets jsb chorales piano competition obtain state art results latter\n",
            "output sentence:  show first successful use transformer generating music exhibits long term structure \n",
            "\n",
            "{'rouge-1': {'r': 0.140625, 'p': 0.5294117647058824, 'f': 0.22222221890565466}, 'rouge-2': {'r': 0.0547945205479452, 'p': 0.2222222222222222, 'f': 0.08791208473855826}, 'rouge-l': {'r': 0.140625, 'p': 0.5294117647058824, 'f': 0.22222221890565466}}\n",
            "pair:  focus problem black box adversarial attacks aim generate adversarial examples using information limited loss function evaluations input output pairs use bayesian optimization bo specifically cater scenarios involving low query budgets develop query efficient adversarial attacks alleviate issues surrounding bo regards optimizing high dimensional deep learning models effective dimension upsampling techniques proposed approach achieves performance comparable state art black box adversarial attacks albeit much lower average query count particular low query budget regimes proposed method reduces query count respect state art methods\n",
            "output sentence:  show relatively simple black box adversarial attack scheme using bayesian optimization dimension upsampling preferable methods methods number number available \n",
            "\n",
            "{'rouge-1': {'r': 0.04, 'p': 0.5, 'f': 0.07407407270233198}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.2, 'f': 0.02061855572324375}, 'rouge-l': {'r': 0.04, 'p': 0.5, 'f': 0.07407407270233198}}\n",
            "pair:  single cell rna sequencing scrnaseq technology enables quantifying gene expression profiles individual cells within cancer dimension reduction methods commonly used cell clustering analysis visualization data current dimension reduction methods tend overly eliminate expression variations correspond less dominating characteristics fail find homogenious properties cancer development paper proposed new clustering analysis method scrnaseq data namely bbsc via implementing binarization gene expression profile frequency changes boolean matrix factorization low rank representation expression matrix recovered bbsc increase resolution identifying distinct cell types functions application bbsc two cancer scrnaseq data successfully discovered homogeneous heterogeneous cancer cell clusters finding showed potential preventing cancer progression\n",
            "output sentence:  finding shed lights preventing cancer progression \n",
            "\n",
            "{'rouge-1': {'r': 0.11494252873563218, 'p': 0.5263157894736842, 'f': 0.1886792423406907}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.1111111111111111, 'f': 0.03076922838343214}, 'rouge-l': {'r': 0.09195402298850575, 'p': 0.42105263157894735, 'f': 0.1509433932840869}}\n",
            "pair:  paper study implicit regularization gradient descent algorithm homogeneous neural networks including fully connected convolutional neural networks relu leakyrelu activations particular study gradient descent gradient flow gradient descent infinitesimal step size optimizing logistic loss cross entropy loss homogeneous model possibly non smooth show training loss decreases certain threshold define smoothed version normalized margin increases time also formulate natural constrained optimization problem related margin maximization prove normalized margin smoothed version converge objective value kkt point optimization problem results generalize previous results logistic regression one layer multi layer linear networks provide quantitative convergence results weaker assumptions previous results homogeneous smooth neural networks conduct several experiments justify theoretical finding mnist cifar datasets finally margin closely related robustness discuss potential benefits training longer improving robustness model\n",
            "output sentence:  study implicit bias gradient descent prove minimal set assumptions parameter direction homogeneous models converges kkt points natural margin maximization \n",
            "\n",
            "{'rouge-1': {'r': 0.14736842105263157, 'p': 1.0, 'f': 0.25688073170608533}, 'rouge-2': {'r': 0.10852713178294573, 'p': 1.0, 'f': 0.1958041940378503}, 'rouge-l': {'r': 0.14736842105263157, 'p': 1.0, 'f': 0.25688073170608533}}\n",
            "pair:  work presents exploration imitation learning based agent capable state art performance playing text based computer games text based computer games describe world player natural language expect player interact game using text games interest seen testbed language understanding problem solving language generation artificial agents moreover provide learning environment skills acquired interactions environment rather using fixed corpora one aspect makes games particularly challenging learning agents combinatorially large action space existing methods solving text based games limited games either simple action space restricted predetermined set admissible actions work propose use exploration approach go explore ecoffet et al solving text based games specifically initial exploration phase first extract trajectories high rewards train policy solve game imitating trajectories experiments show approach outperforms existing solutions solving text based games sample efficient terms number interactions environment moreover show learned policy generalize better existing solutions unseen games without using restriction action space\n",
            "output sentence:  work presents exploration imitation learning based agent capable state art performance playing text based computer games \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5333333333333333, 'f': 0.183908043123266}, 'rouge-2': {'r': 0.0673076923076923, 'p': 0.4117647058823529, 'f': 0.11570247692370746}, 'rouge-l': {'r': 0.06944444444444445, 'p': 0.3333333333333333, 'f': 0.1149425258818867}}\n",
            "pair:  high dimensional data often lie close low dimensional subspaces sparse subspace clustering methods sparsity induced norm sparse subspace clustering ssc demonstrated effective counterpart sparse subspace clustering ssc however norm based subspace clustering methods restricted clean data lie exactly subspaces real data often suffer noise may lie close subspaces propose noisy ssc handle noisy data improve robustness show optimal solution optimization problem noisy ssc achieves subspace detection property sdp key element data different subspaces separated deterministic randomized models results provide theoretical guarantee correctness noisy ssc terms sdp noisy data propose noisy dr ssc provably recovers subspaces dimensionality reduced data noisy dr ssc first projects data onto lower dimensional space linear transformation performs noisy ssc dimensionality reduced data improve efficiency experimental results demonstrate effectiveness noisy ssc noisy dr ssc\n",
            "output sentence:  propose noisy dr ssc noisy dimension reduction sparse subspace clustering efficiently partition noisy data accordance underlying subspace structure \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.5714285714285714, 'f': 0.13559321824763004}, 'rouge-2': {'r': 0.017543859649122806, 'p': 0.16666666666666666, 'f': 0.03174603002267583}, 'rouge-l': {'r': 0.057692307692307696, 'p': 0.42857142857142855, 'f': 0.10169491316288427}}\n",
            "pair:  abstract stochastic gradient descent sgd adam commonly used optimize deep neural networks choosing one usually means making tradeoffs speed accuracy stability present intuition tradeoffs exist well method unifying two continuous way makes possible control way models trained much greater detail show default parameters new algorithm equals outperforms sgd adam across range models image classification tasks outperforms sgd language modeling tasks\n",
            "output sentence:  algorithm unifying sgd adam empirical study performance \n",
            "\n",
            "{'rouge-1': {'r': 0.15789473684210525, 'p': 0.7058823529411765, 'f': 0.25806451314140366}, 'rouge-2': {'r': 0.08653846153846154, 'p': 0.47368421052631576, 'f': 0.14634146080243246}, 'rouge-l': {'r': 0.15789473684210525, 'p': 0.7058823529411765, 'f': 0.25806451314140366}}\n",
            "pair:  cooperative multi agent reinforcement learning marl design suitable reward signal accelerate learning stabilize convergence critical problem global reward signal assigns global reward agents without distinguishing contributions local reward signal provides different local rewards agent based solely individual behavior two reward assignment approaches shortcomings former might encourage lazy agents latter might produce selfish agents paper study reward design problem cooperative marl based packet routing environments firstly show two reward signals prone produce suboptimal policies inspired observations considerations design mixed reward signals shelf learn better policies finally turn mixed reward signals adaptive counterparts achieve best results experiments reward signals also discussed paper reward design fundamental problem rl especially marl hope marl researchers rethink rewards used systems\n",
            "output sentence:  study reward design problem cooperative marl based packet routing environments experimental results remind us design design rewards really important important guide \n",
            "\n",
            "{'rouge-1': {'r': 0.1320754716981132, 'p': 0.5384615384615384, 'f': 0.21212120895775943}, 'rouge-2': {'r': 0.046153846153846156, 'p': 0.25, 'f': 0.07792207529094292}, 'rouge-l': {'r': 0.11320754716981132, 'p': 0.46153846153846156, 'f': 0.18181817865472916}}\n",
            "pair:  show output residual cnn appropriate prior weights biases gp limit infinitely many convolutional filters extending similar results dense networks cnn equivalent kernel computed exactly unlike deep kernels parameters hyperparameters original cnn show kernel two properties allow computed efficiently cost evaluating kernel pair images similar single forward pass original cnn one filter per layer kernel equivalent layer resnet obtains classification error mnist new record gp comparable number parameters\n",
            "output sentence:  show cnns resnets appropriate priors parameters gaussian processes limit infinitely many convolutional filters \n",
            "\n",
            "{'rouge-1': {'r': 0.16901408450704225, 'p': 0.9230769230769231, 'f': 0.28571428309807256}, 'rouge-2': {'r': 0.13636363636363635, 'p': 0.9230769230769231, 'f': 0.23762376013332023}, 'rouge-l': {'r': 0.16901408450704225, 'p': 0.9230769230769231, 'f': 0.28571428309807256}}\n",
            "pair:  state art face super resolution methods employ deep convolutional neural networks learn mapping low high resolution facial patterns exploring local appearance knowledge however methods well exploit facial structures identity information struggle deal facial images exhibit large pose variation misalignment paper propose novel face super resolution method explicitly incorporates facial priors grasp sharp facial structures firstly face rendering branch set obtain priors salient facial structures identity knowledge secondly spatial attention mechanism used better exploit hierarchical information intensity similarity facial structure identity content super resolution problem extensive experiments demonstrate proposed algorithm achieves superior face super resolution results outperforms state art\n",
            "output sentence:  propose novel face super resolution method explicitly incorporates facial priors grasp sharp facial structures \n",
            "\n",
            "{'rouge-1': {'r': 0.05714285714285714, 'p': 0.4, 'f': 0.09999999781250005}, 'rouge-2': {'r': 0.022900763358778626, 'p': 0.2, 'f': 0.04109588856727349}, 'rouge-l': {'r': 0.05714285714285714, 'p': 0.4, 'f': 0.09999999781250005}}\n",
            "pair:  adversarial training procedure proposed madry et al one effective methods defend adversarial examples deep neural net works dnns paper shed lights practicality hardness adversarial training showing effectiveness robustness test set adversarial training strong correlation distance test point manifold training data embedded network test examples relatively far away manifold likely vulnerable adversarial attacks consequentially adversarial training based defense susceptible new class attacks blind spot attack input images reside blind spots low density regions empirical distri bution training data still ground truth data manifold mnist found blind spots easily found simply scaling shifting image pixel values importantly large datasets high dimensional complex data manifold cifar imagenet etc existence blind spots adversarial training makes defending valid test examples difficult due curse dimensionality scarcity training data additionally find blind spots also exist provable defenses including kolter wong sinha et al trainable robustness certificates practically optimized limited set training data\n",
            "output sentence:  show even strongest adversarial training methods cannot defend adversarial examples crafted slightly scaled shifted test images \n",
            "\n",
            "{'rouge-1': {'r': 0.07792207792207792, 'p': 0.5, 'f': 0.13483145834111857}, 'rouge-2': {'r': 0.021052631578947368, 'p': 0.18181818181818182, 'f': 0.0377358471965113}, 'rouge-l': {'r': 0.05194805194805195, 'p': 0.3333333333333333, 'f': 0.08988763811639951}}\n",
            "pair:  generative adversarial networks gans powerful framework generative modeling however often hard train learning gans often becomes unstable wasserstein gan wgan promising framework deal instability problem good convergence property one drawback wgan evaluates wasserstein distance dual domain requires approximation may fail optimize true wasserstein distance paper propose evaluating exact empirical optimal transport cost efficiently primal domain performing gradient descent respect derivative train generator network experiments mnist dataset show method significantly stable converge achieves lowest wasserstein distance among wgan variants cost sharpness generated images experiments gaussian toy dataset show better gradients generator obtained method addition proposed method enables flexible generative modeling wgan\n",
            "output sentence:  proposed flexible generative model learns stably directly minimizing exact empirical wasserstein distance \n",
            "\n",
            "{'rouge-1': {'r': 0.14705882352941177, 'p': 0.7142857142857143, 'f': 0.24390243619274243}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.3076923076923077, 'f': 0.08988763795480376}, 'rouge-l': {'r': 0.1323529411764706, 'p': 0.6428571428571429, 'f': 0.21951219229030342}}\n",
            "pair:  training generative adversarial networks requires balancing delicate adversarial dynamics even careful tuning training may diverge end bad equilibrium dropped modes work introduce new form latent optimisation inspired cs gan show improves adversarial dynamics enhancing interactions discriminator generator develop supporting theoretical analysis perspectives differentiable games stochastic approximation experiments demonstrate latent optimisation significantly improve gan training obtaining state art performance imagenet dataset model achieves inception score frechet inception distance fid improvement fid respectively compared baseline biggan deep model architecture number parameters\n",
            "output sentence:  latent optimisation improves adversarial training dynamics present theoretical analysis state art image generation imagenet \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 0.5833333333333334, 'f': 0.19718309578258283}, 'rouge-2': {'r': 0.029411764705882353, 'p': 0.18181818181818182, 'f': 0.050632908995353415}, 'rouge-l': {'r': 0.1016949152542373, 'p': 0.5, 'f': 0.16901408169807583}}\n",
            "pair:  propose new method training neural networks online bandit setting similar prior work model uncertainty last layer network treating rest network feature extractor allows us successfully balance exploration exploitation due efficient closed form uncertainty estimates available linear models train rest network take advantage posterior last layer optimizing values last layer distribution weighted probability derive closed form differential approximation objective show empirically various models datasets training rest network fashion leads better online offline performance compared methods\n",
            "output sentence:  paper proposes new method neural network learning online bandit settings marginalizing last layer \n",
            "\n",
            "{'rouge-1': {'r': 0.02666666666666667, 'p': 0.5, 'f': 0.050632910431020695}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02666666666666667, 'p': 0.5, 'f': 0.050632910431020695}}\n",
            "pair:  neural tangents library working infinite width neural networks provides high level api specifying complex hierarchical neural network architectures networks trained evaluated either finite width usual infinite width limit infinite width networks neural tangents performs exact inference either via bayes rule gradient descent generates corresponding neural network gaussian process neural tangent kernels additionally neural tangents provides tools study gradient descent training dynamics wide finite networks entire library runs box cpu gpu tpu computations automatically distributed multiple accelerators near linear scaling number devices addition repository provide accompanying interactive colab notebook https colab sandbox google com github google neural tangents blob master notebooks neural tangents cookbook ipynb\n",
            "output sentence:  keras infinite neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.06097560975609756, 'p': 0.38461538461538464, 'f': 0.10526315553241003}, 'rouge-2': {'r': 0.009708737864077669, 'p': 0.08333333333333333, 'f': 0.017391302478639143}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.38461538461538464, 'f': 0.10526315553241003}}\n",
            "pair:  training larger number parameters keeping fast iterations increasingly adopted strategy trend developing better performing deep neural network dnn models necessitates increased memory footprint computational requirements training introduce novel methodology training deep neural networks using bit floating point fp numbers reduced bit precision allows larger effective memory increased computational speed name method shifted squeezed fp fp show unlike previous bit precision training methods proposed method works box representative models resnet transformer ncf method maintain model accuracy without requiring fine tuning loss scaling parameters keeping certain layers single precision introduce two learnable statistics dnn tensors shifted squeezed factors used optimally adjust range tensors bits thus minimizing loss information due quantization\n",
            "output sentence:  propose novel bit format eliminates need loss scaling stochastic rounding low precision techniques \n",
            "\n",
            "{'rouge-1': {'r': 0.22857142857142856, 'p': 0.9411764705882353, 'f': 0.3678160888096182}, 'rouge-2': {'r': 0.21333333333333335, 'p': 0.9411764705882353, 'f': 0.3478260839437619}, 'rouge-l': {'r': 0.22857142857142856, 'p': 0.9411764705882353, 'f': 0.3678160888096182}}\n",
            "pair:  recent developments autonomous vehicle av technology highlight substantial progress lack tools rigorous scalable testing real world testing de facto evaluation environment places public danger due rare nature accidents require billions miles order statistically validate performance claims implement simulation framework test entire modern autonomous driving system including particular systems employ deep learning perception control algorithms using adaptive sampling methods accelerate rare event probability evaluation estimate probability accident base distribution governing standard traffic behavior demonstrate framework highway scenario\n",
            "output sentence:  using adaptive sampling methods accelerate rare event probability evaluation estimate probability accident base distribution governing standard traffic behavior \n",
            "\n",
            "{'rouge-1': {'r': 0.22666666666666666, 'p': 0.9444444444444444, 'f': 0.36559139472771424}, 'rouge-2': {'r': 0.1590909090909091, 'p': 0.7777777777777778, 'f': 0.26415094057671773}, 'rouge-l': {'r': 0.22666666666666666, 'p': 0.9444444444444444, 'f': 0.36559139472771424}}\n",
            "pair:  many tasks natural language processing related domains require high precision output obeys dataset specific constraints level fine grained control difficult obtain large scale neural network models work propose structured latent variable approach adds discrete control states within standard autoregressive neural paradigm formulation include range rich posterior constraints enforce task specific knowledge effectively trained neural model approach allows us provide arbitrary grounding internal model decisions without sacrificing representational power neural models experiments consider applications approach text generation part speech induction natural language generation find method improves standard benchmarks also providing fine grained control\n",
            "output sentence:  structured latent variable approach adds discrete control states within standard autoregressive neural paradigm provide arbitrary internal model internal model internal \n",
            "\n",
            "{'rouge-1': {'r': 0.06060606060606061, 'p': 0.5454545454545454, 'f': 0.10909090729090913}, 'rouge-2': {'r': 0.03389830508474576, 'p': 0.4, 'f': 0.06249999855957035}, 'rouge-l': {'r': 0.04040404040404041, 'p': 0.36363636363636365, 'f': 0.07272727092727278}}\n",
            "pair:  compressed representations generalize better shamir et al may crucial learning limited noisy labeled data information bottleneck ib method tishby et al provides insightful principled approach balancing compression prediction representation learning ib objective employs lagrange multiplier tune trade however little theoretical guidance select also lack theoretical understanding relationship dataset model capacity learnability work show improperly chosen learning cannot happen trivial representation becomes global minimum ib objective show avoided identifying sharp phase transition unlearnable learnable arises varies phase transition defines concept ib learnability prove several sufficient conditions ib learnability providing theoretical guidance selecting show ib learnability determined largest confident typical imbalanced subset training examples give practical algorithm estimate minimum given dataset test theoretical results synthetic datasets mnist cifar noisy labels make surprising observation accuracy may non monotonic\n",
            "output sentence:  theory predicts phase transition unlearnable learnable values beta information bottleneck objective \n",
            "\n",
            "{'rouge-1': {'r': 0.17543859649122806, 'p': 0.7142857142857143, 'f': 0.28169013767903195}, 'rouge-2': {'r': 0.078125, 'p': 0.35714285714285715, 'f': 0.12820512525969763}, 'rouge-l': {'r': 0.15789473684210525, 'p': 0.6428571428571429, 'f': 0.25352112359452494}}\n",
            "pair:  improve robustness deep neural nets adversarial attacks using interpolating function output activation data dependent activation function remarkably improves classification accuracy stability adversarial perturbations together total variation minimization adversarial images augmented training strongest attack achieve accuracy improvement fast gradient sign method iterative fast gradient sign method carlini wagnerl attacks respectively defense strategy additive many existing methods give intuitive explanation defense strategy via analyzing geometry feature space reproducibility code available github\n",
            "output sentence:  proposal strategies adversarial defense based data dependent activation function total variation minimization training data augmentation \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.75, 'f': 0.1999999976888889}, 'rouge-2': {'r': 0.029411764705882353, 'p': 0.2727272727272727, 'f': 0.05309734337536226}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.5833333333333334, 'f': 0.15555555324444445}}\n",
            "pair:  shot image classification aims learning classifier limited labeled data generating classification weights applied many meta learning approaches shot image classification due simplicity effectiveness however argue difficult generate exact universal classification weights diverse query samples training samples work introduce attentive weights generation shot learning via information maximization awgim addresses current issues two novel contributions awgim generates different classification weights different query samples letting query samples attends whole support set ii guarantee generated weights adaptive different query sample formulate problem maximize lower bound mutual information generated weights query well support data far see first attempt unify information maximization shot learning two contributions proved effective extensive experiments show awgim able achieve state art performance benchmark datasets\n",
            "output sentence:  novel shot learning method generate query specific classification weights via information maximization \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.5, 'f': 0.14285714040816327}, 'rouge-2': {'r': 0.01818181818181818, 'p': 0.14285714285714285, 'f': 0.032258062513007404}, 'rouge-l': {'r': 0.0625, 'p': 0.375, 'f': 0.1071428546938776}}\n",
            "pair:  present methodology using dynamic evaluation improve neural sequence models models adapted recent history via gradient descent based mechanism causing assign higher probabilities occurring sequential patterns dynamic evaluation outperforms existing adaptation approaches comparisons dynamic evaluation improves state art word level perplexities penn treebank wikitext datasets respectively state art character level cross entropies text hutter prize datasets bits char bits char respectively\n",
            "output sentence:  paper presents dynamic evaluation methodology adaptive sequence modelling \n",
            "\n",
            "{'rouge-1': {'r': 0.19607843137254902, 'p': 0.8333333333333334, 'f': 0.31746031437641725}, 'rouge-2': {'r': 0.06349206349206349, 'p': 0.3333333333333333, 'f': 0.10666666397866674}, 'rouge-l': {'r': 0.11764705882352941, 'p': 0.5, 'f': 0.19047618739229027}}\n",
            "pair:  adversarial training improve robust accuracy adversary sometimes hurts standard accuracy adversary previous work studied tradeoff standard robust accuracy setting predictor performs well objectives infinite data limit paper show even optimal predictor infinite data performs well objectives tradeoff still manifest finite data furthermore since construction based convex learning problem rule optimization concerns thus laying bare fundamental tension robustness generalization finally show robust self training mostly eliminates tradeoff leveraging unlabeled data\n",
            "output sentence:  even tradeoff infinite data limit adversarial training worse standard accuracy even convex problem \n",
            "\n",
            "{'rouge-1': {'r': 0.075, 'p': 0.6, 'f': 0.13333333135802472}, 'rouge-2': {'r': 0.03125, 'p': 0.3333333333333333, 'f': 0.05714285557551025}, 'rouge-l': {'r': 0.05, 'p': 0.4, 'f': 0.08888888691358031}}\n",
            "pair:  paper focus two challenges offset promise sparse signal representation sensing recovery first real world signals seldom described perfectly sparse vectors known basis traditionally used random measurement schemes seldom optimal sensing second existing signal recovery algorithms usually fast enough make applicable real time problems paper address two challenges presenting novel framework based deep learning first challenge cast problem finding informative measurements using maximum likelihood ml formulation show build data driven dimensionality reduction protocol sensing signals using convolutional architectures second challenge discuss analyze novel parallelization scheme show significantly speeds signal recovery process demonstrate significant improvement method obtains competing methods series experiments\n",
            "output sentence:  use deep learning techniques solve sparse signal representation recovery problem \n",
            "\n",
            "{'rouge-1': {'r': 0.12, 'p': 0.6, 'f': 0.19999999722222223}, 'rouge-2': {'r': 0.0625, 'p': 0.4444444444444444, 'f': 0.10958903893413402}, 'rouge-l': {'r': 0.12, 'p': 0.6, 'f': 0.19999999722222223}}\n",
            "pair:  study bert language representation model sequence generation model bert encoder multi label text classification task experiment models explore special qualities setting also introduce examine experimentally mixed model ensemble multi label bert sequence generating bert models experiments demonstrated bert based models mixed model particular outperform current baselines several metrics achieving state art results three well studied multi label classification datasets english texts two private yandex taxi datasets russian texts\n",
            "output sentence:  using bert encoder sequential prediction labels multi label text classification task \n",
            "\n",
            "{'rouge-1': {'r': 0.15, 'p': 0.8, 'f': 0.25263157628808863}, 'rouge-2': {'r': 0.0673076923076923, 'p': 0.4666666666666667, 'f': 0.1176470566202952}, 'rouge-l': {'r': 0.1125, 'p': 0.6, 'f': 0.18947368155124658}}\n",
            "pair:  conventional generative adversarial networks gans text generation tend issues reward sparsity mode collapse affect quality diversity generated samples address issues propose novel self adversarial learning sal paradigm improving gans performance text generation contrast standard gans use binary classifier discriminator predict whether sample real generated sal employs comparative discriminator pairwise classifier comparing text quality pair samples training sal rewards generator currently generated sentence found better previously generated samples self improvement reward mechanism allows model receive credits easily avoid collapsing towards limited number real samples helps alleviate reward sparsity issue also reduces risk mode collapse experiments text generation benchmark datasets show proposed approach substantially improves quality diversity yields stable performance compared previous gans text generation\n",
            "output sentence:  propose self adversarial learning sal paradigm improves generator self play fashion improving gans performance text generation \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.625, 'f': 0.1369862994182774}, 'rouge-2': {'r': 0.04411764705882353, 'p': 0.42857142857142855, 'f': 0.0799999983075556}, 'rouge-l': {'r': 0.046153846153846156, 'p': 0.375, 'f': 0.0821917788703322}}\n",
            "pair:  unsupervised embedding learning aims extract good representations data without use human annotated labels techniques apparently limelight challenges collecting massive scale labels required supervised learning paper proposes comprehensive approach called super based anchor neighbourhood discovery model multiple losses defined super make similar samples gather even within low density space keep features invariant augmentation result model outperforms existing approaches various benchmark datasets achieves accuracy cifar resnet backbone network gain state art\n",
            "output sentence:  proposed comprehensive approach unsupervised embedding learning basis algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.08235294117647059, 'p': 0.6363636363636364, 'f': 0.1458333313042535}, 'rouge-2': {'r': 0.028846153846153848, 'p': 0.3, 'f': 0.05263157734687601}, 'rouge-l': {'r': 0.047058823529411764, 'p': 0.36363636363636365, 'f': 0.08333333130425352}}\n",
            "pair:  inverse reinforcement learning irl used infer reward function actions expert running markov decision process mdp novel approach using variational inference learning reward function proposed research using technique intractable posterior distribution continuous latent variable reward function case analytically approximated appear close prior belief trying reconstruct future state conditioned current state action reward function derived using well known deep generative model known conditional variational auto encoder cvae wasserstein loss function thus referred conditional wasserstein auto encoder irl cwae irl analyzed combination backward forward inference form efficient alternative previous approaches irl knowledge system dynamics agent experimental results standard benchmarks objectworld pendulum show proposed algorithm effectively learn latent reward function complex high dimensional environments\n",
            "output sentence:  using supervised latent variable modeling framework determine reward inverse reinforcement learning task \n",
            "\n",
            "{'rouge-1': {'r': 0.23076923076923078, 'p': 0.9473684210526315, 'f': 0.3711340174683814}, 'rouge-2': {'r': 0.20689655172413793, 'p': 0.9473684210526315, 'f': 0.3396226385671058}, 'rouge-l': {'r': 0.23076923076923078, 'p': 0.9473684210526315, 'f': 0.3711340174683814}}\n",
            "pair:  exists plethora techniques inducing structured sparsity parametric models optimization process final goal resource efficient inference however best knowledge none target specific number floating point operations flops part single end end optimization objective despite reporting flops part results furthermore one size fits approach ignores realistic system constraints differ significantly say gpu mobile phone flops former incur less latency latter thus important practitioners able specify target number flops model compression work extend state art technique directly incorporate flops part optimization objective show given desired flops requirement different neural networks successfully trained image classification\n",
            "output sentence:  extend state art technique directly incorporate flops part optimization objective show given desired flops requirement different neural networks successfully trained \n",
            "\n",
            "{'rouge-1': {'r': 0.047619047619047616, 'p': 0.75, 'f': 0.08955223768322566}, 'rouge-2': {'r': 0.028169014084507043, 'p': 0.6666666666666666, 'f': 0.05405405327611396}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.75, 'f': 0.08955223768322566}}\n",
            "pair:  real world relation extraction tasks challenging deal either due limited training data class imbalance issues work present data augmented relation extraction dare simple method augment training data properly finetuning gpt generate examples specific relation types generated training data used combination gold dataset train bert based classifier series experiments show advantages method leads improvements score points compared strong baseline also dare achieves new state art three widely used biomedical datasets surpassing previous best results points average\n",
            "output sentence:  data augmented relation extraction gpt \n",
            "\n",
            "{'rouge-1': {'r': 0.07058823529411765, 'p': 0.5, 'f': 0.12371133803804871}, 'rouge-2': {'r': 0.018691588785046728, 'p': 0.16666666666666666, 'f': 0.0336134435647201}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.4166666666666667, 'f': 0.10309278133701778}}\n",
            "pair:  demand abstractive dialog summary growing real world applications example customer service center hospitals would like summarize customer service interaction doctor patient interaction however researchers explored abstractive summarization dialogs due lack suitable datasets propose abstractive dialog summarization dataset based multiwoz directly apply previous state art document summarization methods dialogs two significant drawbacks informative entities restaurant names difficult preserve contents different dialog domains sometimes mismatched address two drawbacks propose scaffold pointer network spnet utilize existing annotation speaker role semantic slot dialog domain spnet incorporates semantic scaffolds dialog summarization since rouge cannot capture two drawbacks mentioned also propose new evaluation metric considers critical informative entities text multiwoz proposed spnet outperforms state art abstractive summarization methods automatic human evaluation metrics\n",
            "output sentence:  propose novel end end model spnet incorporate semantic scaffolds improving abstractive dialog summarization \n",
            "\n",
            "{'rouge-1': {'r': 0.10810810810810811, 'p': 0.9230769230769231, 'f': 0.19354838521982312}, 'rouge-2': {'r': 0.061068702290076333, 'p': 0.5714285714285714, 'f': 0.11034482584161713}, 'rouge-l': {'r': 0.08108108108108109, 'p': 0.6923076923076923, 'f': 0.14516128844562956}}\n",
            "pair:  offset regression standard method spatial localization many vision tasks including human pose estimation object detection instance segmentation however nif high localization accuracy crucial task convolutional neural networks offset regression nusually struggle deliver attributed locality convolution operation exacerbated variance scale clutter viewpoint even fundamental issue multi modality real world images consequence cannot approximated adequately using single mode model instead propose use mixture density networks mdn offset regression allowing model manage various modes efficiently learning predict full conditional density outputs given input human pose estimation wild requires accurate localisation body keypoints show yields significant improvement localization accuracy particular experiments reveal viewpoint variation dominant multi modal factor carefully initializing mdn parameters face instabilities training known big obstacle widespread deployment mdn method readily applied task spatial regression component findings highlight multi modal nature real world vision significance explicitly accounting viewpoint variation least spatial localization concerned\n",
            "output sentence:  use mixture density networks full conditional density estimation spatial offset regression apply human pose estimation task \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5454545454545454, 'f': 0.19354838417793965}, 'rouge-2': {'r': 0.017543859649122806, 'p': 0.1, 'f': 0.029850743729115833}, 'rouge-l': {'r': 0.09803921568627451, 'p': 0.45454545454545453, 'f': 0.16129031966181065}}\n",
            "pair:  traditional set prediction models struggle simple datasets due issue call responsibility problem introduce pooling method sets feature vectors based sorting features across elements set used construct permutation equivariant auto encoder avoids responsibility problem toy dataset polygons set version mnist show auto encoder produces considerably better reconstructions representations replacing pooling function existing set encoders fspool improves accuracy convergence speed variety datasets\n",
            "output sentence:  sort encoder undo sorting decoder avoid responsibility problem set auto encoders \n",
            "\n",
            "{'rouge-1': {'r': 0.09523809523809523, 'p': 0.8571428571428571, 'f': 0.17142856962857145}, 'rouge-2': {'r': 0.06329113924050633, 'p': 0.8333333333333334, 'f': 0.11764705751141871}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.8571428571428571, 'f': 0.17142856962857145}}\n",
            "pair:  work presents modular hierarchical approach learn policies exploring environments approach leverages strengths classical learning based methods using analytical path planners learned mappers global local policies use learning provides flexibility respect input modalities mapper leverages structural regularities world global policies provides robustness errors state estimation local policies use learning within module retains benefits time hierarchical decomposition modular training allow us sidestep high sample complexities associated training end end policies experiments visually physically realistic simulated environments demonstrate effectiveness proposed approach past learning geometry based approaches\n",
            "output sentence:  modular hierarchical approach learn policies exploring environments \n",
            "\n",
            "{'rouge-1': {'r': 0.08433734939759036, 'p': 0.5833333333333334, 'f': 0.14736841884542937}, 'rouge-2': {'r': 0.010309278350515464, 'p': 0.09090909090909091, 'f': 0.018518516688957657}, 'rouge-l': {'r': 0.060240963855421686, 'p': 0.4166666666666667, 'f': 0.10526315568753467}}\n",
            "pair:  capsule networks shown encouraging results textit defacto benchmark computer vision datasets mnist cifar smallnorb although yet tested tasks entities detected inherently complex internal representations instances per class learn point wise classification suitable hence paper carries experiments face verification controlled uncontrolled settings together address points introduce textit siamese capsule networks new variant used pairwise learning tasks find model improves baselines shot learning setting suggesting capsule networks efficient learning discriminative representations given samples find textit siamese capsule networks perform well strong baselines pairwise learning datasets trained using contrastive loss ell normalized capsule encoded pose features yielding best results shot learning setting image pairs test set contain unseen subjects\n",
            "output sentence:  pairwise learned capsule network performs well face verification tasks given limited labeled data \n",
            "\n",
            "{'rouge-1': {'r': 0.06349206349206349, 'p': 0.5, 'f': 0.11267605433842494}, 'rouge-2': {'r': 0.025974025974025976, 'p': 0.2857142857142857, 'f': 0.04761904609126989}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.375, 'f': 0.08450704025391792}}\n",
            "pair:  show beneficial express metropolis accept reject decisions terms comparison uniform value update uniform value non reversibly part markov chain state rather sampling independently iteration provides small improvement random walk metropolis langevin updates high dimensions produces larger improvement using langevin updates persistent momentum giving performance comparable hamiltonian monte carlo hmc long trajectories significance variables updated methods since hmc used updates done trajectories whereas done often langevin updates seen bayesian neural network model connection weights updated persistent langevin hmc hyperparameters updated gibbs sampling\n",
            "output sentence:  non reversible way making accept reject decisions beneficial \n",
            "\n",
            "{'rouge-1': {'r': 0.25, 'p': 0.7857142857142857, 'f': 0.3793103411652795}, 'rouge-2': {'r': 0.16071428571428573, 'p': 0.6923076923076923, 'f': 0.2608695621592103}, 'rouge-l': {'r': 0.25, 'p': 0.7857142857142857, 'f': 0.3793103411652795}}\n",
            "pair:  propose non adversarial feature matching based approach train generative models approach generative feature matching networks gfmn leverages pretrained neural networks autoencoders convnet classifiers perform feature extraction perform extensive number experiments different challenging datasets including imagenet experimental results demonstrate due expressiveness features pretrained imagenet classifiers even matching first order statistics approach achieve state art results challenging benchmarks cifar stl\n",
            "output sentence:  new non adversarial feature matching based approach train generative models achieves state art results \n",
            "\n",
            "{'rouge-1': {'r': 0.07291666666666667, 'p': 0.875, 'f': 0.1346153831952663}, 'rouge-2': {'r': 0.040983606557377046, 'p': 0.7142857142857143, 'f': 0.0775193788185806}, 'rouge-l': {'r': 0.0625, 'p': 0.75, 'f': 0.11538461396449705}}\n",
            "pair:  exploration learning representations one main challenges deep reinforcement learning drl faces today learned representation dependant observed data exploration strategy crucial role popular dqn algorithm improved significantly capabilities reinforcement learning rl algorithms learn state representations raw data yet uses naive exploration strategy statistically inefficient randomized least squares value iteration rlsvi algorithm osband et al hand explores generalizes efficiently via linearly parameterized value functions however based hand designed state representation requires prior engineering work every environment paper propose deep learning adaptation rlsvi rather using hand design state representation use state representation learned directly data dqn agent representation optimized learning process key component suggested method likelihood matching mechanism adapts changing representations demonstrate importance various properties algorithm toy problem show method outperforms dqn five atari benchmarks reaching competitive results rainbow algorithm\n",
            "output sentence:  deep learning adaptation randomized least squares value iteration \n",
            "\n",
            "{'rouge-1': {'r': 0.1282051282051282, 'p': 0.7692307692307693, 'f': 0.21978021733124017}, 'rouge-2': {'r': 0.022222222222222223, 'p': 0.16666666666666666, 'f': 0.03921568419838535}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.46153846153846156, 'f': 0.1318681294191523}}\n",
            "pair:  propose method quantifying uncertainty neural network regression models targets real values dimensional simplex probabilities show target modeled sample dirichlet distribution parameters dirichlet provided output neural network combined model trained using gradient data likelihood approach provides interpretable predictions form multidimensional distributions rather point estimates one obtain confidence intervals quantify risk decision making furthermore show approach used model targets form empirical counts samples dirichlet multinomial compound distribution experiments verify approach provides benefits without harming performance point estimate predictions two diverse applications distilling deep convolutional networks trained cifar predicting location particle collisions xenon dark matter detector\n",
            "output sentence:  neural network regression use dirichlet output distribution targets probabilities order quantify uncertainty predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.18421052631578946, 'p': 0.9333333333333333, 'f': 0.30769230493901706}, 'rouge-2': {'r': 0.15217391304347827, 'p': 0.7777777777777778, 'f': 0.2545454518082645}, 'rouge-l': {'r': 0.18421052631578946, 'p': 0.9333333333333333, 'f': 0.30769230493901706}}\n",
            "pair:  plain recurrent networks greatly suffer vanishing gradient problem gated neural networks gnns long short term memory lstm gated recurrent unit gru deliver promising results many sequence learning tasks sophisticated network designs paper shows address problem plain recurrent network analyzing gating mechanisms gnns propose novel network called recurrent identity network rin allows plain recurrent network overcome vanishing gradient problem training deep models without use gates compare model irnns lstms multiple sequence modeling benchmarks rins demonstrate competitive performance converge faster tasks notably small rin models produce higher accuracy sequential permuted mnist datasets reach state art performance babi question answering dataset\n",
            "output sentence:  propose novel network called recurrent identity network rin allows plain recurrent network overcome vanishing gradient training training training models models \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.8333333333333334, 'f': 0.13513513364499638}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.8, 'f': 0.08421052531855956}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.8333333333333334, 'f': 0.13513513364499638}}\n",
            "pair:  scarcity labeled training data often prohibits internationalization nlp models multiple languages cross lingual understanding made progress area using language universal representations however current approaches focus problem one aligning language address natural domain drift across languages cultures paper address domain gap setting semi supervised cross lingual document classification labeled data available source language unlabeled data available target language combine state art unsupervised learning method masked language modeling pre training recent method semi supervised learning unsupervised data augmentation uda simultaneously close language domain gap show addressing domain gap cross lingual tasks crucial improve strong baselines achieve new state art cross lingual document classification\n",
            "output sentence:  semi supervised cross lingual document classification \n",
            "\n",
            "{'rouge-1': {'r': 0.08064516129032258, 'p': 0.8333333333333334, 'f': 0.14705882192041522}, 'rouge-2': {'r': 0.04054054054054054, 'p': 0.6, 'f': 0.07594936590290019}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.6666666666666666, 'f': 0.11764705721453288}}\n",
            "pair:  recent work exhibited surprising cross lingual abilities multilingual bert bert surprising since trained without cross lingual objective aligned data work provide comprehensive study contribution different components bert cross lingual ability study impact linguistic properties languages architecture model learning objectives experimental study done context three typologically different languages spanish hindi russian using two conceptually different nlp tasks textual entailment named entity recognition among key conclusions fact lexical overlap languages plays negligible role cross lingual success depth network important part\n",
            "output sentence:  cross lingual ability multilingual bert empirical study \n",
            "\n",
            "{'rouge-1': {'r': 0.17307692307692307, 'p': 0.6, 'f': 0.26865671294274895}, 'rouge-2': {'r': 0.09836065573770492, 'p': 0.42857142857142855, 'f': 0.15999999696355563}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.26666666666666666, 'f': 0.11940298159946547}}\n",
            "pair:  analyze joint probability distribution lengths vectors hidden variables different layers fully connected deep network weights biases chosen randomly according gaussian distributions input binary valued show activation function satisfies minimal set assumptions satisfied activation functions know used practice width network gets large length process converges probability length map determined simple function variances random weights biases activation function also show convergence may fail activation functions violate assumptions\n",
            "output sentence:  prove activation functions satisfying conditions deep network gets wide lengths vectors hidden variables converge length map \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.75, 'f': 0.2142857118367347}, 'rouge-2': {'r': 0.03488372093023256, 'p': 0.25, 'f': 0.0612244876468139}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5833333333333334, 'f': 0.1666666642176871}}\n",
            "pair:  monitoring patients icu challenging high cost task hence predicting condition patients icu stay help provide better acute care plan hospital resources continuous progress machine learning research icu management work focused using time series signals recorded icu instruments work show adding clinical notes another modality improves performance model three benchmark tasks hospital mortality prediction modeling decompensation length stay forecasting play important role icu management time series data measured regular intervals doctor notes charted irregular times making challenging model together propose method model jointly achieving considerable improvement across benchmark tasks baseline time series model\n",
            "output sentence:  demostarte using clinical notes conjuntion icu instruments data improves perfomance icu management benchmark tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.2647058823529412, 'p': 0.8181818181818182, 'f': 0.3999999963061729}, 'rouge-2': {'r': 0.2, 'p': 0.7, 'f': 0.311111107654321}, 'rouge-l': {'r': 0.2647058823529412, 'p': 0.8181818181818182, 'f': 0.3999999963061729}}\n",
            "pair:  paper presents system immersive visualization non euclidean spaces using real time ray tracing exploits capabilities new generation gpu based nvidia turing architecture order develop new methods intuitive exploration landscapes featuring non trivial geometry topology virtual reality\n",
            "output sentence:  immersive visualization classical non euclidean spaces using real time ray tracing \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.8181818181818182, 'f': 0.224999997628125}, 'rouge-2': {'r': 0.0898876404494382, 'p': 0.7272727272727273, 'f': 0.15999999804200002}, 'rouge-l': {'r': 0.13043478260869565, 'p': 0.8181818181818182, 'f': 0.224999997628125}}\n",
            "pair:  describe two end end autoencoding models semi supervised graph based dependency parsing first model local autoencoding parser lap encoding input using continuous latent variables sequential manner second model global autoencoding parser gap encoding input dependency trees latent variables exact inference models consist two parts encoder enhanced deep neural networks dnn utilize contextual information encode input latent variables decoder generative model able reconstruct input lap gap admit unified structure different loss functions labeled unlabeled data shared parameters conducted experiments wsj ud dependency parsing data sets showing models exploit unlabeled data boost performance given limited amount labeled data\n",
            "output sentence:  describe two end end autoencoding parsers semi supervised graph based dependency parsing \n",
            "\n",
            "{'rouge-1': {'r': 0.02247191011235955, 'p': 0.3333333333333333, 'f': 0.042105261974515265}, 'rouge-2': {'r': 0.00909090909090909, 'p': 0.2, 'f': 0.01739130351606809}, 'rouge-l': {'r': 0.02247191011235955, 'p': 0.3333333333333333, 'f': 0.042105261974515265}}\n",
            "pair:  adversarial examples somewhat disrupted enormous success machine learning ml causing concern regards trustworthiness small perturbation input results arbitrary failure otherwise seemingly well trained ml system studies conducted discover intrinsic properties adversarial examples transferability universality insufficient theoretic analysis help understand phenomenon way influence design process ml experiments paper deduce information theoretic model explains adversarial attacks universally abuse feature redundancies ml algorithms prove feature redundancy necessary condition existence adversarial examples model helps explain major questions raised many anecdotal studies adversarial examples theory backed empirical measurements information content benign adversarial examples image text datasets measurements show typical adversarial examples introduce enough redundancy overflow decision making machine learner trained corresponding benign examples conclude actionable recommendations improve robustness machine learners adversarial examples\n",
            "output sentence:  new theoretical explanation existence adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.9, 'f': 0.17999999820000004}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.3, 'f': 0.048387095291363195}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  recent years deep reinforcement learning shown adept solving sequential decision processes high dimensional state spaces atari games many reinforcement learning problems however involve high dimensional discrete action spaces well high dimensional state spaces paper develop novel policy gradient methodology case large multidimensional discrete action spaces propose two approaches creating parameterized policies lstm parameterization modified mdp mmdp giving rise feed forward network ffn parameterization approaches provide expressive models backpropagation applied training consider entropy bonus typically added reward function enhance exploration case high dimensional action spaces calculating entropy gradient entropy requires enumerating actions action space running forward backpropagation action may computationally infeasible develop several novel unbiased estimators entropy bonus gradient finally test algorithms two environments multi hunter multi rabbit grid game multi agent multi arm bandit problem\n",
            "output sentence:  policy parameterizations unbiased policy entropy estimators mdp large multidimensional discrete action space \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.6, 'f': 0.15584415358407824}, 'rouge-2': {'r': 0.0379746835443038, 'p': 0.3333333333333333, 'f': 0.06818181634555791}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.4, 'f': 0.10389610163602636}}\n",
            "pair:  backpropagation driving today artificial neural networks however despite extensive research remains unclear brain implements algorithm among neuroscientists reinforcement learning rl algorithms often seen realistic alternative however convergence rate learning scales poorly number involved neurons propose hybrid learning approach neuron uses rl type strategy learn approximate gradients backpropagation would provide show approach learns approximate gradient match performance gradient based learning fully connected convolutional networks learning feedback weights provides biologically plausible mechanism achieving good performance without need precise pre specified learning rules\n",
            "output sentence:  perturbations used learn feedback weights large fully connected convolutional networks \n",
            "\n",
            "{'rouge-1': {'r': 0.02857142857142857, 'p': 0.13333333333333333, 'f': 0.04705882062283755}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02857142857142857, 'p': 0.13333333333333333, 'f': 0.04705882062283755}}\n",
            "pair:  paper proposes method efficient training function continuous state markov decision processes mdp traces resulting policies satisfy linear temporal logic ltl property ltl modal logic express wide range time dependent logical properties including safety liveness convert ltl property limit deterministic buchi automaton synchronized product mdp constructed control policy synthesised reinforcement learning algorithm assuming prior knowledge available mdp proposed method evaluated numerical study test quality generated control policy compared conventional methods policy synthesis mdp abstraction voronoi quantizer approximate dynamic programming fitted value iteration\n",
            "output sentence:  safety becoming critical notion machine learning believe work act foundation number research directions safety aware learning algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.034482758620689655, 'p': 0.4, 'f': 0.06349206203073825}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.034482758620689655, 'p': 0.4, 'f': 0.06349206203073825}}\n",
            "pair:  structural planning important producing long sentences missing part current language generation models work add planning phase neural machine translation control coarse structure output sentences model first generates planner codes predicts real output words conditioned codes learned capture coarse structure target sentence order learn codes design end end neural network discretization bottleneck predicts simplified part speech tags target sentences experiments show translation performance generally improved planning ahead also find translations different structures obtained manipulating planner codes\n",
            "output sentence:  plan syntactic structural translation using codes \n",
            "\n",
            "{'rouge-1': {'r': 0.16981132075471697, 'p': 0.6, 'f': 0.26470587891435987}, 'rouge-2': {'r': 0.06153846153846154, 'p': 0.2857142857142857, 'f': 0.10126581986861088}, 'rouge-l': {'r': 0.1509433962264151, 'p': 0.5333333333333333, 'f': 0.23529411420847754}}\n",
            "pair:  paper presents variation network varnet generative model providing means manipulate high level attributes given input originality approach varnet capable handling pre defined attributes also learn relevant attributes dataset two settings easily combined makes varnet applicable wide variety tasks varnet sound probabilistic interpretation grants us novel way navigate latent spaces well means control attributes learned demonstrate experimentally model capable performing interesting input manipulation learned attributes relevant interpretable\n",
            "output sentence:  variation network generative model able learn high level attributes without supervision used controlled input manipulation \n",
            "\n",
            "{'rouge-1': {'r': 0.056338028169014086, 'p': 0.4, 'f': 0.09876542993446126}, 'rouge-2': {'r': 0.011764705882352941, 'p': 0.1, 'f': 0.02105262969529103}, 'rouge-l': {'r': 0.056338028169014086, 'p': 0.4, 'f': 0.09876542993446126}}\n",
            "pair:  deep latent variable models seen recent success many data domains lossless compression application models despite potential highly useful yet implemented practical manner present bits back ans bb ans scheme perform lossless compression latent variable models near optimal rate demonstrate scheme using compress mnist dataset variational auto encoder model vae achieving compression rates superior standard methods simple vae given scheme highly amenable parallelization conclude sufficiently high quality generative model scheme could used achieve substantial improvements compression rate acceptable running time make implementation available open source https github com bits back bits back\n",
            "output sentence:  lossless compression large image datasets using vae beat existing compression algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.14893617021276595, 'p': 0.9333333333333333, 'f': 0.2568807315714166}, 'rouge-2': {'r': 0.125, 'p': 0.8125, 'f': 0.21666666435555557}, 'rouge-l': {'r': 0.14893617021276595, 'p': 0.9333333333333333, 'f': 0.2568807315714166}}\n",
            "pair:  established diverse behaviors spanning controllable subspace markov decision process trained rewarding policy distinguishable policies however one limitation formulation difficulty generalize beyond finite set behaviors explicitly learned may needed subsequent tasks successor features provide appealing solution generalization problem require defining reward function linear grounded feature space paper show two techniques combined method solves primary limitation introduce variational intrinsic successor features visr novel algorithm learns controllable features leveraged provide enhanced generalization fast task inference successor features framework empirically validate visr full atari suite novel setup wherein rewards exposed briefly long unsupervised phase achieving human level performance games beating baselines believe visr represents step towards agents rapidly learn limited feedback\n",
            "output sentence:  introduce variational intrinsic successor features visr novel algorithm learns controllable features leveraged provide fast task successor features framework \n",
            "\n",
            "{'rouge-1': {'r': 0.0392156862745098, 'p': 0.25, 'f': 0.06779660782533764}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0196078431372549, 'p': 0.125, 'f': 0.03389830274059195}}\n",
            "pair:  propose end end trainable attention module convolutional neural network cnn architectures built image classification module takes input feature vector maps form intermediate representations input image different stages cnn pipeline outputs matrix scores map standard cnn architectures modified incorporation module trained constraint convex combination intermediate feature vectors parametrised score matrices must alone used classification incentivised amplify relevant suppress irrelevant misleading scores thus assume role attention values experimental observations provide clear evidence effect learned attention maps neatly highlight regions interest suppressing background clutter consequently proposed function able bootstrap standard cnn architectures task image classification demonstrating superior generalisation unseen benchmark datasets binarised attention maps outperform cnn based attention maps traditional saliency maps top object proposals weakly supervised segmentation demonstrated object discovery dataset also demonstrate improved robustness fast gradient sign method adversarial attack\n",
            "output sentence:  paper proposes method forcing cnns leverage spatial attention learning object centric representations perform better various respects \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.5454545454545454, 'f': 0.12765957240153916}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.4, 'f': 0.07547169640441441}, 'rouge-l': {'r': 0.060240963855421686, 'p': 0.45454545454545453, 'f': 0.10638297665685832}}\n",
            "pair:  autonomous vehicles becoming common city transportation companies begin find need teach vehicles smart city fleet coordination currently simulation based modeling along hand coded rules dictate decision making autonomous vehicles believe complex intelligent behavior learned agents reinforcement learning paper discuss work solving system adapting deep learning dqn model multi agent setting approach applies deep reinforcement learning combining convolutional neural networks dqn teach agents fulfill customer demand environment partially observ able also demonstrate utilize transfer learning teach agents balance multiple objectives navigating charging station en ergy level low two evaluations presented show solution shown hat successfully able teach agents cooperation policies balancing multiple objectives\n",
            "output sentence:  utilized deep reinforcement learning teach agents ride sharing fleet style coordination \n",
            "\n",
            "{'rouge-1': {'r': 0.2413793103448276, 'p': 0.875, 'f': 0.37837837498904314}, 'rouge-2': {'r': 0.11688311688311688, 'p': 0.5625, 'f': 0.19354838424788995}, 'rouge-l': {'r': 0.1896551724137931, 'p': 0.6875, 'f': 0.29729729390796206}}\n",
            "pair:  bilingual student learns solve word problems math expect student able solve problem languages student fluent even math lessons taught one language however current representations machine learning language dependent work present method decouple language problem learning language agnostic representations therefore allowing training model one language applying different one zero shot fashion learn representations taking inspiration linguistics specifically universal grammar hypothesis learn universal latent representations language agnostic chomsky montague demonstrate capabilities representations showing models trained single language using language agnostic representations achieve similar accuracies languages\n",
            "output sentence:  taking inspiration linguistics specifically universal grammar hypothesis learn language agnostic universal representations utilize zero shot learning across \n",
            "\n",
            "{'rouge-1': {'r': 0.08737864077669903, 'p': 0.5625, 'f': 0.15126050187416146}, 'rouge-2': {'r': 0.032, 'p': 0.25, 'f': 0.05673758664051111}, 'rouge-l': {'r': 0.06796116504854369, 'p': 0.4375, 'f': 0.11764705649601022}}\n",
            "pair:  hamiltonian formalism plays central role classical quantum physics hamiltonians main tool modelling continuous time evolution systems conserved quantities come equipped many useful properties like time reversibility smooth interpolation time properties important many machine learning problems sequence prediction reinforcement learning density modelling typically provided box standard tools recurrent neural networks paper introduce hamiltonian generative network hgn first approach capable consistently learning hamiltonian dynamics high dimensional observations images without restrictive domain assumptions trained use hgn sample new trajectories perform rollouts forward backward time even speed slow learned dynamics demonstrate simple modification network architecture turns hgn powerful normalising flow model called neural hamiltonian flow nhf uses hamiltonian dynamics model expressive densities hence hope work serves first practical demonstration value hamiltonian formalism bring machine learning results video evaluations available http tiny cc hgn\n",
            "output sentence:  introduce class generative models reliably learn hamiltonian dynamics high dimensional observations learnt hamiltonian applied sequence modeling normalising flow \n",
            "\n",
            "{'rouge-1': {'r': 0.06172839506172839, 'p': 0.5, 'f': 0.10989010793382442}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.2222222222222222, 'f': 0.03883494986143847}, 'rouge-l': {'r': 0.04938271604938271, 'p': 0.4, 'f': 0.08791208595580248}}\n",
            "pair:  determinantal point processes dpps provide elegant versatile way sample sets items balance point wise quality set wise diversity selected items reason gained prominence many machine learning applications rely subset selection however sampling dpp ground set size costly operation requiring general preprocessing cost nk sampling cost subsets size approach problem introducing dppnets generative deep models produce dpp like samples arbitrary ground sets develop inhibitive attention mechanism based transformer networks captures notion dissimilarity feature vectors show theoretically approximation sensible maintains guarantees inhibition dissimilarity makes dpp powerful unique empirically demonstrate samples model receive high likelihood expensive dpp alternative\n",
            "output sentence:  approximate determinantal point processes neural nets justify model theoretically empirically \n",
            "\n",
            "{'rouge-1': {'r': 0.16470588235294117, 'p': 0.9333333333333333, 'f': 0.27999999745}, 'rouge-2': {'r': 0.10434782608695652, 'p': 0.8571428571428571, 'f': 0.18604650969292713}, 'rouge-l': {'r': 0.16470588235294117, 'p': 0.9333333333333333, 'f': 0.27999999745}}\n",
            "pair:  generating formal language represented relational tuples lisp programs mathematical expressions natural language input extremely challenging task requires explicitly capture discrete symbolic structural information input generate output state art neural sequence models explicitly capture structure information thus perform well tasks paper propose new encoder decoder model based tensor product representations tprs natural formal language generation called tp encoder tp employs tpr binding encode natural language symbolic structure vector space decoder uses tpr unbinding generate sequence relational tuples consisting relation operation number arguments symbolic space tp considerably outperforms lstm based seq seq models creating new state art results two benchmarks mathqa dataset math problem solving algolist dataset program synthesis ablation studies show improvements mainly attributed use tprs encoder decoder explicitly capture relational structure information symbolic reasoning\n",
            "output sentence:  paper propose new encoder decoder model based tensor product representations natural formal language generation called tp \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.16666666666666666, 'f': 0.03960395830212735}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.46153846153846156, 'f': 0.14117646799723185}}\n",
            "pair:  present meta learning approach adaptive text speech tts data training learn multi speaker model using shared conditional wavenet core independent learned embeddings speaker aim training produce neural network fixed weights deployed tts system instead aim produce network requires data deployment time rapidly adapt new speakers introduce benchmark three strategies learning speaker embedding keeping wavenet core fixed ii fine tuning entire architecture stochastic gradient descent iii predicting speaker embedding trained neural network encoder experiments show approaches successful adapting multi speaker neural network new speakers obtaining state art results sample naturalness voice similarity merely minutes audio data new speakers\n",
            "output sentence:  sample efficient algorithms adapt text speech model new voice style state art performance \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.3333333333333333, 'f': 0.0869565194706995}, 'rouge-2': {'r': 0.014285714285714285, 'p': 0.125, 'f': 0.025641023800131623}, 'rouge-l': {'r': 0.05, 'p': 0.3333333333333333, 'f': 0.0869565194706995}}\n",
            "pair:  deep neural networks recently demonstrated vulnerable backdoor attacks specifically altering small set training examples adversary able install backdoor used inference fully control model behavior attack powerful crucially relies adversary able introduce arbitrary often clearly mislabeled inputs training set thus detected even fairly rudimentary data filtering paper introduce new approach executing backdoor attacks utilizing adversarial examples gan generated data key feature resulting poisoned inputs appear consistent label thus seem benign even upon human inspection\n",
            "output sentence:  show successfully perform backdoor attacks without changing training labels \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.9230769230769231, 'f': 0.26373626128728417}, 'rouge-2': {'r': 0.061224489795918366, 'p': 0.46153846153846156, 'f': 0.10810810604009419}, 'rouge-l': {'r': 0.14102564102564102, 'p': 0.8461538461538461, 'f': 0.24175823930926216}}\n",
            "pair:  representation learning one foundations deep learning allowed important improvements several machine learning tasks neural machine translation question answering speech recognition recent works proposed new methods learning representations nodes edges graphs several methods based skipgram algorithm usually process large number multi hop neighbors order produce context node representations learned paper propose effective also efficient method generating node embeddings graphs employs restricted number permutations immediate neighborhood node context generate representation thus ego centric representations present thorough evaluation showing method outperforms state art methods six different datasets related problems link prediction node classification one three orders magnitude faster baselines generating node embeddings large graphs\n",
            "output sentence:  faster method generating node embeddings employs number permutations node immediate neighborhood context generate representation \n",
            "\n",
            "{'rouge-1': {'r': 0.0392156862745098, 'p': 0.2857142857142857, 'f': 0.06896551511890613}, 'rouge-2': {'r': 0.009009009009009009, 'p': 0.07692307692307693, 'f': 0.01612903038111364}, 'rouge-l': {'r': 0.029411764705882353, 'p': 0.21428571428571427, 'f': 0.05172413580856132}}\n",
            "pair:  brain computer interfaces bci may help patients faltering communication abilities due neurodegenerative diseases produce text speech direct neural processing however practical realization proven difficult due limitations speed accuracy generalizability existing interfaces end aim create bci decodes text directly neural signals implement framework initially isolates frequency bands input signal encapsulating differential information regarding production various phonemic classes bands form feature set feeds lstm discerns time point probability distributions across phonemes uttered subject finally particle filtering algorithm temporally smooths probabilities incorporating prior knowledge english language output text corresponding decoded word producing output abstain constraining reconstructed word given bag words unlike previous studies empirical success proposed approach offers promise employment interface patients unfettered naturalistic environments\n",
            "output sentence:  present open loop brain machine interface whose performance unconstrained traditionally used bag words approach \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.7, 'f': 0.2692307661242604}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.3, 'f': 0.11538461227810658}}\n",
            "pair:  peripheral nervous system represents input output system brain cuff electrodes implanted peripheral nervous system allow observation control system however data produced electrodes low signal noise ratio complex signal content paper consider analysis neural data recorded vagus nerve animal models develop unsupervised learner based convolutional neural networks able simultaneously de noise cluster regions data signal content\n",
            "output sentence:  unsupervised analysis data recorded peripheral nervous system denoises categorises signals \n",
            "\n",
            "{'rouge-1': {'r': 0.12962962962962962, 'p': 0.875, 'f': 0.22580644936524458}, 'rouge-2': {'r': 0.10071942446043165, 'p': 0.7777777777777778, 'f': 0.17834394701448333}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.75, 'f': 0.1935483848491155}}\n",
            "pair:  knowledge distillation effective model compression technique smaller model trained mimic larger pretrained model however order make compact models suitable real world deployment need reduce performance gap also need make robust commonly occurring adversarial perturbations noise permeates every level nervous system perception sensory signals generation motor responses therefore believe noise could crucial element improving neural networks training addressing apparently contradictory goals improving generalization robustness model inspired trial trial variability brain result multiple noise sources introduce variability noise either input level supervision signals results show noise improve generalization robustness model fickle teacher uses dropout teacher model source response variation leads significant generalization improvement soft randomization matches output distribution student model image gaussian noise output teacher original image improves adversarial robustness manifolds compared student model trained gaussian noise show surprising effect random label corruption model adversarial robustness study highlights benefits adding constructive noise knowledge distillation framework hopes inspire work area\n",
            "output sentence:  inspired trial trial variability brain result multiple noise sources introduce variability noise knowledge distillation framework studied effect generalization robustness \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.625, 'f': 0.2197802168820191}, 'rouge-2': {'r': 0.052083333333333336, 'p': 0.3125, 'f': 0.08928571183673477}, 'rouge-l': {'r': 0.10666666666666667, 'p': 0.5, 'f': 0.17582417292597516}}\n",
            "pair:  many approaches causal discovery limited inability discriminate markov equivalent graphs given observational data formulate causal discovery marginal likelihood based bayesian model selection problem adopt parameterization based notion independence causal mechanisms renders markov equivalent graphs distinguishable complement empirical bayesian approach setting priors actual underlying causal graph assigned higher marginal likelihood alternatives adopting bayesian approach also allows straightforward modeling unobserved confounding variables provide variational algorithm approximate marginal likelihood since desirable feat renders computation marginal likelihood intractable believe bayesian approach causal discovery allows rich methodology bayesian inference used various difficult aspects problem provides unifying framework causal discovery research demonstrate promising results experiments conducted real data supporting modeling approach inference methodology\n",
            "output sentence:  cast causal structure discovery bayesian model selection way allows us discriminate markov equivalent graphs identify unique causal graph \n",
            "\n",
            "{'rouge-1': {'r': 0.057971014492753624, 'p': 0.5714285714285714, 'f': 0.10526315622229918}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.42857142857142855, 'f': 0.07894736674861498}}\n",
            "pair:  existing neural networks learning graphs deal issue permutation invariance conceiving network message passing scheme node sums feature vectors coming neighbors argue imposes limitation representation power instead propose new general architecture representing objects consisting hierarchy parts call covariant compositional networks ccns covariance means activation neuron must transform specific way permutations similarly steerability cnns achieve covariance making activation transform according tensor representation permutation group derive corresponding tensor aggregation rules neuron must implement experiments show ccns outperform competing methods standard graph learning benchmarks\n",
            "output sentence:  general framework creating covariant graph neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.10227272727272728, 'p': 0.6, 'f': 0.17475727906494487}, 'rouge-2': {'r': 0.02, 'p': 0.14285714285714285, 'f': 0.03508771714373667}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.26666666666666666, 'f': 0.07766990042416824}}\n",
            "pair:  semmelhack et al achieved high classification accuracy distinguishing swim bouts zebrafish using support vector machine svm convolutional neural networks cnns reached superior performance various image recognition tasks svms powerful networks remain black box reaching better transparency helps build trust classifications makes learned features interpretable experts using recently developed technique called deep taylor decomposition generated heatmaps highlight input regions high relevance predictions find cnn makes predictions analyzing steadiness tail trunk markedly differs manually extracted features used semmelhack et al uncovered network paid attention experimental artifacts removing artifacts ensured validity predictions correction best cnn beats svm achieving classification accuracy work thus demonstrates utility ai explainability cnns\n",
            "output sentence:  demonstrate utility recent ai explainability technique visualizing learned features cnn trained binary classification zebrafish movements \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.625, 'f': 0.11363636198347109}, 'rouge-2': {'r': 0.010752688172043012, 'p': 0.14285714285714285, 'f': 0.019999998698000086}, 'rouge-l': {'r': 0.05, 'p': 0.5, 'f': 0.09090908925619837}}\n",
            "pair:  click rate ctr prediction critical task industrial applications especially online social commerce applications challenging find proper way automatically discover effective cross features ctr tasks propose novel model ctr tasks called deep neural networks encoder enhanced factorization machine deepenfm instead learning cross features directly deepenfm adopts transformer encoder backbone align feature embeddings clues fields embeddings generated encoder beneficial feature interactions particularly deepenfm utilizes bilinear approach generate different similarity functions respect different field pairs furthermore max pooling method makes deepenfm feasible capture supplementary suppressing information among different attention heads model validated criteo avazu datasets achieves state art performance\n",
            "output sentence:  dnn encoder enhanced fm bilinear attention max pooling ctr \n",
            "\n",
            "{'rouge-1': {'r': 0.056179775280898875, 'p': 0.3125, 'f': 0.09523809265487536}, 'rouge-2': {'r': 0.008403361344537815, 'p': 0.06666666666666667, 'f': 0.014925371146135262}, 'rouge-l': {'r': 0.0449438202247191, 'p': 0.25, 'f': 0.07619047360725632}}\n",
            "pair:  present cross view training cvt simple effective method deep semi supervised learning labeled examples model trained standard cross entropy loss unlabeled example model first performs inference acting teacher produce soft targets model learns soft targets acting student deviate prior work adding multiple auxiliary student prediction layers model input student layer sub network full model restricted view input seeing one region image students learn teacher full model teacher sees example concurrently students improve quality representations used teacher learn make predictions limited data combined virtual adversarial training cvt improves upon current state art semi supervised cifar semi supervised svhn also apply cvt train models five natural language processing tasks using hundreds millions sentences unlabeled data tasks cvt substantially outperforms supervised learning alone resulting models improve upon competitive current state art\n",
            "output sentence:  self training different views input gives excellent results semi supervised image recognition sequence tagging dependency parsing \n",
            "\n",
            "{'rouge-1': {'r': 0.29411764705882354, 'p': 0.9375, 'f': 0.44776119039429724}, 'rouge-2': {'r': 0.20967741935483872, 'p': 0.7222222222222222, 'f': 0.32499999651250006}, 'rouge-l': {'r': 0.29411764705882354, 'p': 0.9375, 'f': 0.44776119039429724}}\n",
            "pair:  introduce doc dial end end framework generating conversational data grounded business documents via crowdsourcing data used train automated dialogue agents performing customer care tasks enterprises organizations particular framework takes documents input generates tasks obtaining annotations simulating dialog flows dialog flows used guide collection utterances produced crowd workers outcomes include dialogue data grounded given documents well various types annotations help ensure quality data flexibility composite dialogues\n",
            "output sentence:  introduce doc dial end end framework generating conversational data grounded business documents via crowdsourcing train train train agents agents dialogue \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.3076923076923077, 'f': 0.10666666380088896}, 'rouge-2': {'r': 0.025974025974025976, 'p': 0.15384615384615385, 'f': 0.04444444197283965}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.23076923076923078, 'f': 0.07999999713422232}}\n",
            "pair:  sequential decision problems real world applications often need solved real time requiring algorithms perform well restricted computational budget width based lookaheads shown state art performance classical planning problems well atari games tight budgets work investigate width based lookaheads stochastic shortest paths ssp analyse width based algorithms perform poorly ssp problems overcome pitfalls proposing method estimate costs go formalize width based lookaheads instance rollout algorithm give definition width ssp problems explain sample complexity experimental results variety ssp benchmarks show algorithm outperform state art rollout algorithms uct rtdp\n",
            "output sentence:  propose new monte carlo tree search rollout algorithm relies width based search construct lookahead \n",
            "\n",
            "{'rouge-1': {'r': 0.08421052631578947, 'p': 0.6153846153846154, 'f': 0.1481481460305213}, 'rouge-2': {'r': 0.01652892561983471, 'p': 0.16666666666666666, 'f': 0.030075186328226675}, 'rouge-l': {'r': 0.06315789473684211, 'p': 0.46153846153846156, 'f': 0.11111110899348427}}\n",
            "pair:  deep learning achieved astonishing results many tasks large amounts data generalization within proximity training data many important real world applications requirements unfeasible additional prior knowledge task domain required overcome resulting problems particular learning physics models model based control requires robust extrapolation fewer samples often collected online real time model errors may lead drastic damages system directly incorporating physical insight enabled us obtain novel deep model learning approach extrapolates well requiring fewer samples first example propose deep lagrangian networks delan deep network structure upon lagrangian mechanics imposed delan learn equations motion mechanical system system dynamics deep network efficiently ensuring physical plausibility resulting delan network performs well robot tracking control proposed method outperform previous model learning approaches learning speed exhibits substantially improved robust extrapolation novel trajectories learns online real time\n",
            "output sentence:  paper introduces physics prior deep learning applies resulting network topology model based control \n",
            "\n",
            "{'rouge-1': {'r': 0.050505050505050504, 'p': 0.625, 'f': 0.09345794254170672}, 'rouge-2': {'r': 0.022388059701492536, 'p': 0.42857142857142855, 'f': 0.042553190545747216}, 'rouge-l': {'r': 0.04040404040404041, 'p': 0.5, 'f': 0.07476635375665999}}\n",
            "pair:  goal paper propose algorithm learning generalizable solution given training data shown bayesian approach leads solution dependent statistics training data particular samples solution stable perturbations training data defined integral contribution multiple maxima likelihood single global maximum specifically bayesian probability distribution parameters weights probabilistic model given neural network estimated via recurrent variational approximations derived recurrent update rules correspond sgd type rules finding minimum effective loss average original negative log likelihood gaussian distributions weights makes function means variances effective loss convex large variances non convex limit small variances among stationary solutions update rules trivial solutions zero variances local minima original loss single non trivial solution finite variances critical point end convexity effective loss mean variance space critical point first second order gradients effective loss means zero empirical study confirms critical point represents generalizable solution location critical point weight space depends specifics used probabilistic model properties critical point universal model independent\n",
            "output sentence:  proposed method finding generalizable solution stable perturbations trainig data \n",
            "\n",
            "{'rouge-1': {'r': 0.045454545454545456, 'p': 0.5555555555555556, 'f': 0.08403361204717184}, 'rouge-2': {'r': 0.014285714285714285, 'p': 0.25, 'f': 0.0270270260043828}, 'rouge-l': {'r': 0.02727272727272727, 'p': 0.3333333333333333, 'f': 0.050420166669020584}}\n",
            "pair:  counterfactual regret minimization cfr fundamental effective technique solving imperfect information games iig however original cfr algorithm works discrete states action spaces resulting strategy maintained tabular representation tabular representation limits method directly applied large games paper propose double neural representation iigs one neural network represents cumulative regret represents average strategy neural representations allow us avoid manual game abstraction carry end end optimization make learning efficient also developed several novel techniques including robust sampling method mini batch monte carlo counterfactual regret minimization mccfr method may independent interests empirically games tractable tabular approaches neural strategies trained algorithm converge comparably tabular counterparts significantly outperform based deep reinforcement learning extremely large games billions decision nodes approach achieved strong performance using hundreds times less memory tabular cfr head head matches hands limit texas hold em neural agent beat strong agent abs cfr pm chips per game successful application neural cfr large games\n",
            "output sentence:  proposed double neural framework solve large scale imperfect information game \n",
            "\n",
            "{'rouge-1': {'r': 0.10465116279069768, 'p': 0.9, 'f': 0.1874999981336806}, 'rouge-2': {'r': 0.08247422680412371, 'p': 0.8888888888888888, 'f': 0.1509433946724813}, 'rouge-l': {'r': 0.10465116279069768, 'p': 0.9, 'f': 0.1874999981336806}}\n",
            "pair:  propose robust bayesian deep learning algorithm infer complex posteriors latent variables inspired dropout popular tool regularization model ensemble assign sparse priors weights deep neural networks dnn order achieve automatic dropout avoid fitting alternatively sampling posterior distribution stochastic gradient markov chain monte carlo sg mcmc optimizing latent variables via stochastic approximation sa trajectory target weights proved converge true posterior distribution conditioned optimal latent variables ensures stronger regularization fitted parameter space accurate uncertainty quantification decisive variables simulations large small regressions showcase robustness method applied models latent variables additionally application convolutional neural networks cnn leads state art performance mnist fashion mnist datasets improved resistance adversarial attacks\n",
            "output sentence:  robust bayesian deep learning algorithm infer complex posteriors latent variables \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.5454545454545454, 'f': 0.14814814580094499}, 'rouge-2': {'r': 0.0125, 'p': 0.09090909090909091, 'f': 0.02197801985267501}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.45454545454545453, 'f': 0.12345678777625364}}\n",
            "pair:  designing architectures deep neural networks requires expert knowledge substantial computation time propose technique accelerate architecture selection learning auxiliary hypernet generates weights main model conditioned model architecture comparing relative validation performance networks hypernet generated weights effectively search wide range architectures cost single training run facilitate search develop flexible mechanism based memory read writes allows us define wide range network connectivity patterns resnet densenet fractalnet blocks special cases validate method smash cifar cifar stl modelnet imagenet achieving competitive performance similarly sized hand designed networks\n",
            "output sentence:  technique accelerating neural architecture selection approximating weights candidate architecture instead training individually \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.7, 'f': 0.2692307661242604}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.3, 'f': 0.11538461227810658}}\n",
            "pair:  peripheral nervous system represents input output system brain cuff electrodes implanted peripheral nervous system allow observation control system however data produced electrodes low signal noise ratio complex signal content paper consider analysis neural data recorded vagus nerve animal models develop unsupervised learner based convolutional neural networks able simultaneously de noise cluster regions data signal content\n",
            "output sentence:  unsupervised analysis data recorded peripheral nervous system denoises categorises signals \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.875, 'f': 0.19718309659194602}, 'rouge-2': {'r': 0.08974358974358974, 'p': 0.875, 'f': 0.16279069598702}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.875, 'f': 0.19718309659194602}}\n",
            "pair:  propose bayesian hypernetworks framework approximate bayesian inference neural networks bayesian hypernetwork neural network learns transform simple noise distribution distribution parameters another neural network primary network train variational inference using invertible enable efficient estimation variational lower bound posterior via sampling contrast methods bayesian deep learning bayesian hypernets represent complex multimodal approximate posterior correlations parameters enabling cheap iid sampling practice bayesian hypernets provide better defense adversarial examples dropout also exhibit competitive performance suite tasks evaluate model uncertainty including regularization active learning anomaly detection\n",
            "output sentence:  propose bayesian hypernetworks framework approximate bayesian inference neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07272727272727272, 'p': 0.5714285714285714, 'f': 0.12903225606139437}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05454545454545454, 'p': 0.42857142857142855, 'f': 0.09677419154526538}}\n",
            "pair:  abstract work describe set rules design initialization well conditioned neural networks guided goal naturally balancing diagonal blocks hessian start training show measure conditioning block relates another natural measure conditioning ratio weight gradients weights prove relu based deep multilayer perceptron simple initialization scheme using geometric mean fan fan satisfies scaling rule sophisticated architectures show scaling principle used guide design choices produce well conditioned neural networks reducing guess work\n",
            "output sentence:  theory initialization scaling relu neural network layers \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.6666666666666666, 'f': 0.1739130412098299}, 'rouge-2': {'r': 0.02912621359223301, 'p': 0.23076923076923078, 'f': 0.05172413594084432}, 'rouge-l': {'r': 0.05, 'p': 0.3333333333333333, 'f': 0.0869565194706995}}\n",
            "pair:  referential games offer grounded learning environment neural agents accounts fact language functionally used communicate however take account second constraint considered fundamental shape human language must learnable new language learners thus overcome transmission bottleneck work insert bottleneck referential game introducing changing population agents new agents learn playing experienced agents show mere cultural transmission results substantial improvement language efficiency communicative success measured convergence speed degree structure emerged languages within population consistency language however core contribution show optimal situation co evolve language agents allow agent population evolve genotypical evolution achieve across board improvements considered metrics results stress language emergence studies cultural evolution important also suitability architecture considered\n",
            "output sentence:  enable cultural evolution language genetic evolution agents referential game using new language transmission engine \n",
            "\n",
            "{'rouge-1': {'r': 0.16901408450704225, 'p': 0.8, 'f': 0.2790697645619254}, 'rouge-2': {'r': 0.13253012048192772, 'p': 0.7857142857142857, 'f': 0.2268041212413647}, 'rouge-l': {'r': 0.14084507042253522, 'p': 0.6666666666666666, 'f': 0.23255813665494862}}\n",
            "pair:  social dilemmas situations individuals face temptation increase payoffs cost total welfare building artificially intelligent agents achieve good outcomes situations important many real world interactions include tension selfish interests welfare others show modify modern reinforcement learning methods construct agents act ways simple understand nice begin cooperating provokable try avoid exploited forgiving try return mutual cooperation show theoretically experimentally agents maintain cooperation markov social dilemmas construction require training methods beyond modification self play thus environment good strategies constructed zero sum case eg atari construct agents solve social dilemmas environment\n",
            "output sentence:  build artificial agents solve social dilemmas situations individuals face temptation increase payoffs cost total welfare \n",
            "\n",
            "{'rouge-1': {'r': 0.05154639175257732, 'p': 0.4166666666666667, 'f': 0.09174311730662406}, 'rouge-2': {'r': 0.00847457627118644, 'p': 0.09090909090909091, 'f': 0.01550387440899}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.3333333333333333, 'f': 0.07339449345341306}}\n",
            "pair:  last decade two competing control strategies emerged solving complex control tasks high efficacy model based control algorithms model predictive control mpc trajectory optimization peer gradients underlying system dynamics order solve control tasks high sample efficiency however like gradient based numerical optimization methods model based control methods sensitive intializations prone becoming trapped local minima deep reinforcement learning drl hand somewhat alleviate issues exploring solution space sampling expense computational cost paper present hybrid method combines best aspects gradient based methods drl base algorithm deep deterministic policy gradients ddpg algorithm propose simple modification uses true gradients differentiable physical simulator increase convergence rate actor critic demonstrate algorithm seven robot control tasks complex one differentiable half cheetah hard contact constraints empirical results show method boosts performance ddpgwithout sacrificing robustness local minima\n",
            "output sentence:  propose novel method leverages gradients differentiable simulators improve performance rl robotics control \n",
            "\n",
            "{'rouge-1': {'r': 0.06930693069306931, 'p': 0.7, 'f': 0.12612612448664884}, 'rouge-2': {'r': 0.025, 'p': 0.3333333333333333, 'f': 0.04651162660897786}, 'rouge-l': {'r': 0.06930693069306931, 'p': 0.7, 'f': 0.12612612448664884}}\n",
            "pair:  graph neural networks shown promising results representing analyzing diverse graph structured data social citation protein interaction networks existing approaches commonly suffer oversmoothing issue regardless whether policies edge based node based neighborhood aggregation methods also focus transductive scenarios fixed graphs leading poor generalization performance unseen graphs address issues propose new graph neural network model considers edge based neighborhood relationships node based entity features graph entities step mixture via random walk gesm gesm employs mixture various steps random walk alleviate oversmoothing problem attention use node information explicitly two mechanisms allow weighted neighborhood aggregation considers properties entities relations intensive experiments show proposed gesm achieves state art comparable performances four benchmark graph datasets comprising transductive inductive learning tasks furthermore empirically demonstrate significance considering global information source code publicly available near future\n",
            "output sentence:  simple effective graph neural network mixture random walk steps attention \n",
            "\n",
            "{'rouge-1': {'r': 0.14583333333333334, 'p': 0.7777777777777778, 'f': 0.24561403242843954}, 'rouge-2': {'r': 0.05357142857142857, 'p': 0.375, 'f': 0.09374999781250005}, 'rouge-l': {'r': 0.125, 'p': 0.6666666666666666, 'f': 0.21052631313019393}}\n",
            "pair:  existing cnn structures video representation learning clip based methods consider video level temporal evolution spatio temporal features paper propose video level convolutional neural networks namely model evolution long range spatio temporal representation convolutions well preserving spatio temporal representations residual connections introduce training inference methods proposed extensive experiments conducted three video recognition benchmarks achieves excellent results surpassing recent cnns large margin\n",
            "output sentence:  novel cnn structure video level representation learning surpassing recent cnns \n",
            "\n",
            "{'rouge-1': {'r': 0.11363636363636363, 'p': 0.625, 'f': 0.192307689704142}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.5, 'f': 0.1538461512426036}}\n",
            "pair:  work propose self supervised method learn sentence representations injection linguistic knowledge multiple linguistic frameworks propose diverse sentence structures semantic meaning might expressed compositional words operations aim take advantage linguist diversity learn represent sentences contrasting diverse views formally multiple views sentence mapped close representations contrary views sentences mapped contrasting different linguistic views aim building embeddings better capture semantic less sensitive sentence outward form\n",
            "output sentence:  aim exploit diversity linguistic structures build sentence representations \n",
            "\n",
            "{'rouge-1': {'r': 0.10126582278481013, 'p': 0.38095238095238093, 'f': 0.15999999668200007}, 'rouge-2': {'r': 0.020618556701030927, 'p': 0.1, 'f': 0.034188031353641846}, 'rouge-l': {'r': 0.08860759493670886, 'p': 0.3333333333333333, 'f': 0.13999999668200008}}\n",
            "pair:  important detect anomalous inputs deploying machine learning systems use larger complex inputs deep learning magnifies difficulty distinguishing anomalous distribution examples time diverse image text data available enormous quantities propose leveraging data improve deep anomaly detection training anomaly detectors auxiliary dataset outliers approach call outlier exposure oe enables anomaly detectors generalize detect unseen anomalies extensive experiments natural language processing small large scale vision tasks find outlier exposure significantly improves detection performance also observe cutting edge generative models trained cifar may assign higher likelihoods svhn images cifar images use oe mitigate issue also analyze flexibility robustness outlier exposure identify characteristics auxiliary dataset improve performance\n",
            "output sentence:  oe teaches anomaly detectors learn heuristics detecting unseen anomalies experiments classification density estimation calibration nlp vision settings tune test distribution samples \n",
            "\n",
            "{'rouge-1': {'r': 0.12698412698412698, 'p': 0.5714285714285714, 'f': 0.2077922048170012}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.15384615384615385, 'f': 0.044943817730084726}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.42857142857142855, 'f': 0.1558441528689493}}\n",
            "pair:  visual attention mechanisms widely used image captioning models paper better link image structure generated text replace traditional softmax attention mechanism two alternative sparsity promoting transformations sparsemax total variation sparse attention tvmax sparsemax obtain sparse attention weights selecting relevant features order promote sparsity encourage fusing related adjacent spatial locations propose tvmax selecting relevant groups features tvmax transformation improves interpretability present results microsoft coco flickr datasets obtaining gains comparison softmax tvmax outperforms compared attention mechanisms terms human rated caption quality attention relevance\n",
            "output sentence:  propose new sparse structured attention mechanism tvmax promotes sparsity encourages weight related adjacent locations \n",
            "\n",
            "{'rouge-1': {'r': 0.09574468085106383, 'p': 0.75, 'f': 0.16981131874688504}, 'rouge-2': {'r': 0.03571428571428571, 'p': 0.36363636363636365, 'f': 0.06504064877784392}, 'rouge-l': {'r': 0.06382978723404255, 'p': 0.5, 'f': 0.11320754516197937}}\n",
            "pair:  predictive models generalize well distributional shift often desirable sometimes crucial machine learning applications one example estimation treatment effects observational data subtask predict effect treatment subjects systematically different received treatment data related kind distributional shift appears unsupervised domain adaptation tasked generalizing distribution inputs different one observe labels pose problems prediction shift design popular methods overcoming distributional shift often heuristic rely assumptions rarely true practice well specified model knowing policy gave rise observed data methods hindered need pre specified metric comparing observations poor asymptotic properties work devise bound generalization error design shift based integral probability metrics sample weighting combine idea representation learning generalizing tightening existing results space finally propose algorithmic framework inspired bound verify effectiveness causal effect estimation\n",
            "output sentence:  theory algorithmic framework prediction distributional shift including causal effect estimation domain adaptation \n",
            "\n",
            "{'rouge-1': {'r': 0.1595744680851064, 'p': 1.0, 'f': 0.2752293554246276}, 'rouge-2': {'r': 0.07751937984496124, 'p': 0.625, 'f': 0.1379310325193817}, 'rouge-l': {'r': 0.10638297872340426, 'p': 0.6666666666666666, 'f': 0.1834862361585725}}\n",
            "pair:  investigate task clustering deep learning based multi task shot learning settings large numbers diverse tasks method measures task similarities using cross task transfer performance matrix although matrix provides us critical information regarding similarities tasks uncertain task pairs ones extremely asymmetric transfer scores may collectively mislead clustering algorithms output inaccurate task partition moreover number tasks large generating full transfer performance matrix time consuming overcome limitations propose novel task clustering algorithm estimate similarity matrix based theory matrix completion proposed algorithm work partially observed similarity matrices based sampled task pairs reliable scores ensuring efficiency robustness theoretical analysis shows mild assumptions reconstructed matrix perfectly matches underlying true similarity matrix overwhelming probability final task partition computed applying efficient spectral clustering algorithm recovered matrix results show new task clustering method discover task clusters benefit multi task learning shot learning setups sentiment classification dialog intent classification tasks\n",
            "output sentence:  propose matrix completion based task clustering algorithm deep multi task shot learning settings large numbers numbers diverse tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.09302325581395349, 'p': 0.7272727272727273, 'f': 0.16494845159740676}, 'rouge-2': {'r': 0.009259259259259259, 'p': 0.1, 'f': 0.016949150991094657}, 'rouge-l': {'r': 0.046511627906976744, 'p': 0.36363636363636365, 'f': 0.08247422479328309}}\n",
            "pair:  deep neural networks achieved impressive performance handling complicated semantics natural language mostly treated black boxes explain model handles compositional semantics words phrases study hierarchical explanation problem highlight key challenge compute non additive context independent importance individual words phrases show prior efforts hierarchical explanations contextual decomposition satisfy desired properties mathematically leading inconsistent explanation quality different models paper propose formal way quantify importance word phrase generate hierarchical explanations modify contextual decomposition algorithms according formulation propose model agnostic explanation algorithm competitive performance human evaluation automatic metrics evaluation lstm models fine tuned bert transformer models multiple datasets show algorithms robustly outperform prior works hierarchical explanations show algorithms help explain compositionality semantics extract classification rules improve human trust models\n",
            "output sentence:  propose measurement phrase importance algorithms hierarchical explanation neural sequence model predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5294117647058824, 'f': 0.18367346652019997}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.23529411764705882, 'f': 0.07079645762080046}, 'rouge-l': {'r': 0.08641975308641975, 'p': 0.4117647058823529, 'f': 0.14285713998958774}}\n",
            "pair:  semi supervised learning jointly learning labeled unlabeled samples active research topic due key role relaxing human annotation constraints context image classification recent advances learn unlabeled samples mainly focused consistency regularization methods encourage invariant predictions different perturbations unlabeled samples conversely propose learn unlabeled data generating soft pseudo labels using network predictions show naive pseudo labeling overfits incorrect pseudo labels due called confirmation bias demonstrate mixup augmentation setting minimum number labeled samples per mini batch effective regularization techniques reducing proposed approach achieves state art results cifar mini imagenet despite much simpler state art results demonstrate pseudo labeling outperform consistency regularization methods opposite supposed previous work code made available\n",
            "output sentence:  pseudo labeling shown weak alternative semi supervised learning conversely demonstrate dealing confirmation bias several regularizations makes pseudo labeling \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5384615384615384, 'f': 0.18421052347991693}, 'rouge-2': {'r': 0.013157894736842105, 'p': 0.08333333333333333, 'f': 0.022727270371901073}, 'rouge-l': {'r': 0.07936507936507936, 'p': 0.38461538461538464, 'f': 0.13157894453254854}}\n",
            "pair:  visual attention mechanisms widely used image captioning models paper better link image structure generated text replace traditional softmax attention mechanism two alternative sparsity promoting transformations sparsemax total variation sparse attention tvmax sparsemax obtain sparse attention weights selecting relevant features order promote sparsity encourage fusing related adjacent spatial locations propose tvmax selecting relevant groups features tvmax transformation improves interpretability present results microsoft coco flickr datasets obtaining gains comparison softmax tvmax outperforms compared attention mechanisms terms human rated caption quality attention relevance\n",
            "output sentence:  propose new sparse structured attention mechanism tvmax promotes sparsity encourages weight related adjacent \n",
            "\n",
            "{'rouge-1': {'r': 0.05084745762711865, 'p': 0.5, 'f': 0.0923076906319527}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.3333333333333333, 'f': 0.06153845986272194}}\n",
            "pair:  work offers new method domain translation semantic label maps computer graphic cg simulation edge map images photo realistic im ages train generative adversarial network gan conditional way generate photo realistic version given cg scene existing architectures gans still lack photo realism capabilities needed train dnns computer vision tasks address issue embedding edge maps training adversarial mode also offer extension model uses gan architecture create visually appealing temporally coherent videos\n",
            "output sentence:  simulation real images translation video generation \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.5714285714285714, 'f': 0.1702127634223631}, 'rouge-2': {'r': 0.031578947368421054, 'p': 0.16666666666666666, 'f': 0.053097342454381836}, 'rouge-l': {'r': 0.0875, 'p': 0.5, 'f': 0.14893616767768222}}\n",
            "pair:  conditional generative adversarial networks cgan led large improvements task conditional image generation lies heart computer vision major focus far performance improvement little effort making cgan robust noise regression generator might lead arbitrarily large errors output makes cgan unreliable real world applications work introduce novel conditional gan model called rocgan leverages structure target space model address issue model augments generator unsupervised pathway promotes outputs generator span target manifold even presence intense noise prove rocgan share similar theoretical properties gan experimentally verify model outperforms existing state art cgan architectures large margin variety domains including images natural scenes faces\n",
            "output sentence:  introduce new type conditional gan aims leverage structure target space generator augment generator new new pathway target structure target \n",
            "\n",
            "{'rouge-1': {'r': 0.12698412698412698, 'p': 0.8, 'f': 0.21917807982735973}, 'rouge-2': {'r': 0.04938271604938271, 'p': 0.4, 'f': 0.08791208595580248}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.6, 'f': 0.16438355927941453}}\n",
            "pair:  introduce mtlab new algorithm learning multiple related tasks strong theoretical guarantees key idea perform learning sequentially data tasks without interruptions restarts task boundaries predictors individual tasks derived process additional online batch conversion step learning across task boundaries mtlab achieves sublinear regret true risks number tasks lifelong learning setting leads improved generalization bound converges total number samples across observed tasks instead number examples per tasks number tasks independently time widely applicable handle finite sets tasks common multi task learning well stochastic task sequences studied lifelong learning\n",
            "output sentence:  new algorithm online multi task learning learns without restarts task borders \n",
            "\n",
            "{'rouge-1': {'r': 0.1797752808988764, 'p': 0.8421052631578947, 'f': 0.29629629339677643}, 'rouge-2': {'r': 0.09649122807017543, 'p': 0.6111111111111112, 'f': 0.1666666643112948}, 'rouge-l': {'r': 0.16853932584269662, 'p': 0.7894736842105263, 'f': 0.27777777487825794}}\n",
            "pair:  paper propose novel approach improve given surface mapping local refinement approach receives established mapping two surfaces follows four phases inspection mapping creation sparse nset landmarks mismatching regions ii segmentation low distortion region growing process based flattening nsegmented parts iii optimization deformation segmented parts align landmarks planar parameterization domain nand iv aggregation mappings segments update surface mapping addition propose new method deform mesh order meet constraints case landmark alignment phase iii incrementally adjust cotangent weights constraints apply deformation fashion guarantees deformed mesh free flipped faces low conformal distortion new deformation approach iterative least squares conformal mapping ilscm outperforms low distortion deformation methods approach general tested improving mappings different existing surface mapping methods also tested effectiveness editing mappings variety objects\n",
            "output sentence:  propose novel approach improve given cross surface mapping local refinement new iterative method deform mesh order meet user constraints \n",
            "\n",
            "{'rouge-1': {'r': 0.07920792079207921, 'p': 0.5, 'f': 0.136752134391117}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.125, 'f': 0.027027025098612267}, 'rouge-l': {'r': 0.06930693069306931, 'p': 0.4375, 'f': 0.1196581172970999}}\n",
            "pair:  capsule networks constrained parameter expensive nature layers general lack provable equivariance guarantees present variation capsule networks aims remedy identify learning pair wise part whole relationships capsules successive layers inefficient also realise choice prediction networks routing mechanism key equivariance based propose alternative framework capsule networks learns projectively encode manifold pose variations termed space variation sov every capsule type layer done using trainable equivariant function defined grid group transformations thus prediction phase routing involves projection sov deeper capsule using corresponding function specific instantiation idea also order reap benefits increased parameter sharing use type homogeneous group equivariant convolutions shallower capsules phase also introduce equivariant routing mechanism based degree centrality show particular instance general model equivariant hence preserves compositional representation input transformations conduct several experiments standard object classification datasets showcase increased transformation robustness well general performance model several capsule baselines\n",
            "output sentence:  new scalable group equivariant model capsule networks preserves compositionality transformations empirically transformation robust older capsule network models \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.23076923076923078, 'f': 0.10526315437365354}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.23076923076923078, 'f': 0.10526315437365354}}\n",
            "pair:  concerns interpretability computational resources principled inductive priors motivated efforts engineer sparse neural models nlp tasks sparsity important nlp might well trained neural models naturally become roughly sparse using taxi euclidean norm measure sparsity find frequent input words associated concentrated sparse activations frequent target words associated dispersed activations concentrated gradients find gradients associated function words concentrated gradients content words even controlling word frequency\n",
            "output sentence:  study natural emergence sparsity activations gradients layers dense lstm language model course training \n",
            "\n",
            "{'rouge-1': {'r': 0.07954545454545454, 'p': 0.875, 'f': 0.14583333180555555}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.7142857142857143, 'f': 0.07874015643871288}, 'rouge-l': {'r': 0.07954545454545454, 'p': 0.875, 'f': 0.14583333180555555}}\n",
            "pair:  emerging topic face recognition designing margin based loss functions increase feature margin different classes enhanced discriminability recently absorbing idea mining based strategies adopted emphasize misclassified samples achieve promising results however entire training process prior methods either explicitly emphasize sample based importance renders hard samples fully exploited explicitly emphasize effects semi hard hard samples even early training stage may lead convergence issues work propose novel adaptive curriculum learning loss curricularface embeds idea curriculum learning loss function achieve novel training strategy deep face recognition mainly addresses easy samples early training stage hard ones later stage specifically curricularface adaptively adjusts relative importance easy hard samples different training stages stage different samples assigned different importance according corresponding difficultness extensive experimental results popular benchmarks demonstrate superiority curricularface state art competitors code available upon publication\n",
            "output sentence:  novel adaptive curriculum learning loss deep face recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.8, 'f': 0.21621621387874362}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.25, 'f': 0.057142855118367426}, 'rouge-l': {'r': 0.09375, 'p': 0.6, 'f': 0.16216215982468957}}\n",
            "pair:  machine learning algorithms controlling devices need learn quickly trials goal attained concepts borrowed continental philosophy formalized using tools mathematical theory categories illustrations approach presented cyberphysical system slot car game also atari games\n",
            "output sentence:  continental philosophy inspired approach learn data \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.4166666666666667, 'f': 0.09345794193379338}, 'rouge-2': {'r': 0.015037593984962405, 'p': 0.18181818181818182, 'f': 0.027777776366705316}, 'rouge-l': {'r': 0.042105263157894736, 'p': 0.3333333333333333, 'f': 0.07476635314874666}}\n",
            "pair:  recent improvements generative adversarial networks gans made possible generate realistic images high resolution based natural language descriptions image captions furthermore conditional gans allow us control image generation process labels even natural language descriptions however fine grained control image layout image specific objects located still difficult achieve especially true images contain multiple distinct objects different spatial locations introduce new approach allows us control location arbitrarily many objects within image adding object pathway generator discriminator approach need detailed semantic layout bounding boxes respective labels desired objects needed object pathway focuses solely individual objects iteratively applied locations specified bounding boxes global pathway focuses image background general image layout perform experiments multi mnist clevr complex ms coco data set experiments show use object pathway control object locations within images model complex scenes multiple objects various locations show object pathway focuses individual objects learns features relevant global pathway focuses global image characteristics image background\n",
            "output sentence:  extend gan architecture obtain control locations identities multiple objects within generated images \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.5555555555555556, 'f': 0.1666666641166667}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.125, 'f': 0.02739725832238708}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}}\n",
            "pair:  develop new algorithm imitation learning single expert demonstration contrast many previous one shot imitation learning approaches algorithm assume access one expert demonstration training phase instead leverage exploration policy acquire unsupervised trajectories used train encoder context aware imitation policy optimization procedures encoder imitation learner exploration policy tightly linked linking creates feedback loop wherein exploration policy collects new demonstrations challenge imitation learner encoder attempts help imitation policy best abilities evaluate algorithm mujoco robotics tasks\n",
            "output sentence:  unsupervised self imitation algorithm capable inference single expert demonstration \n",
            "\n",
            "{'rouge-1': {'r': 0.08928571428571429, 'p': 0.5, 'f': 0.15151514894398535}, 'rouge-2': {'r': 0.015873015873015872, 'p': 0.1111111111111111, 'f': 0.027777775590277953}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.4, 'f': 0.12121211864095506}}\n",
            "pair:  great progress made making neural networks effective across wide range tasks many surprisingly vulnerable small carefully chosen perturbations input known adversarial examples paper advocate experimentally investigate use logit regularization techniques adversarial defense used conjunction methods creating adversarial robustness little cost demonstrate much effectiveness one recent adversarial defense mechanism attributed logit regularization show improve defense white box black box attacks process creating stronger black box attacks pgd based models\n",
            "output sentence:  logit regularization methods help explain improve state art adversarial defenses \n",
            "\n",
            "{'rouge-1': {'r': 0.09876543209876543, 'p': 0.5714285714285714, 'f': 0.1684210501185596}, 'rouge-2': {'r': 0.021052631578947368, 'p': 0.15384615384615385, 'f': 0.037037034919410274}, 'rouge-l': {'r': 0.06172839506172839, 'p': 0.35714285714285715, 'f': 0.1052631553817175}}\n",
            "pair:  many notions fairness may expressed linear constraints resulting constrained objective often optimized transforming problem lagrangian dual additive linear penalties non convex settings resulting problem may difficult solve lagrangian guaranteed deterministic saddle point equilibrium paper propose modify linear penalties second order ones argue results practical training procedure non convex large data settings one use second order penalties allows training penalized objective fixed value penalty coefficient thus avoiding instability potential lack convergence associated two player min max games secondly derive method efficiently computing gradients associated second order penalties stochastic mini batch settings resulting algorithm performs well empirically learning appropriately fair classifier number standard benchmarks\n",
            "output sentence:  propose method stochastically optimize second order penalties show may apply training fairness aware classifiers \n",
            "\n",
            "{'rouge-1': {'r': 0.11475409836065574, 'p': 0.7, 'f': 0.1971830961713946}, 'rouge-2': {'r': 0.057971014492753624, 'p': 0.4444444444444444, 'f': 0.10256410052268247}, 'rouge-l': {'r': 0.08196721311475409, 'p': 0.5, 'f': 0.1408450680023805}}\n",
            "pair:  paper propose view acceptance rate metropolis hastings algorithm universal objective learning sample target distribution given either set samples form unnormalized density point view unifies goals approaches markov chain monte carlo mcmc generative adversarial networks gans variational inference reveal connection derive lower bound acceptance rate treat objective learning explicit implicit samplers form lower bound allows doubly stochastic gradient optimization case target distribution factorizes data points empirically validate approach bayesian inference neural networks generative models images\n",
            "output sentence:  learning sample via lower bounding acceptance rate metropolis hastings algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.5555555555555556, 'f': 0.1388888867013889}, 'rouge-2': {'r': 0.025, 'p': 0.25, 'f': 0.04545454380165295}, 'rouge-l': {'r': 0.07936507936507936, 'p': 0.5555555555555556, 'f': 0.1388888867013889}}\n",
            "pair:  flow based models real nvp extremely powerful approach density estimation however existing flow based models restricted transforming continuous densities continuous input space similarly continuous distributions continuous latent variables makes poorly suited modeling representing discrete structures data distributions example class membership discrete symmetries address difficulty present normalizing flow architecture relies domain partitioning using locally invertible functions possesses real discrete valued latent variables real discrete rad approach retains desirable normalizing flow properties exact sampling exact inference analytically computable probabilities time allowing simultaneous modeling continuous discrete structure data distribution\n",
            "output sentence:  flow based models non invertible also learn discrete variables \n",
            "\n",
            "{'rouge-1': {'r': 0.140625, 'p': 0.75, 'f': 0.23684210260387814}, 'rouge-2': {'r': 0.06329113924050633, 'p': 0.45454545454545453, 'f': 0.11111110896543214}, 'rouge-l': {'r': 0.0625, 'p': 0.3333333333333333, 'f': 0.10526315523545714}}\n",
            "pair:  past years various advancements made generative models owing formulation generative adversarial networks gans gans shown perform exceedingly well wide variety tasks pertaining image generation style transfer field natural language processing word embeddings word vec glove state art methods applying neural network models textual data attempts made utilizing gans word embeddings text generation work presents approach text generation using skip thought sentence embeddings conjunction gans based gradient penalty functions measures results using sentence embeddings gans generating text conditioned input information comparable approaches word embeddings used\n",
            "output sentence:  generating text using sentence embeddings skip thought vectors help generative adversarial networks \n",
            "\n",
            "{'rouge-1': {'r': 0.06557377049180328, 'p': 0.4, 'f': 0.1126760539178735}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.1111111111111111, 'f': 0.02631578738573424}, 'rouge-l': {'r': 0.06557377049180328, 'p': 0.4, 'f': 0.1126760539178735}}\n",
            "pair:  propose new perspective adversarial attacks deep reinforcement learning agents main contribution copycat targeted attack able consistently lure agent following outsider policy pre computed therefore fast inferred could thus usable real time scenario show effectiveness atari games novel read setting latter adversary cannot directly modify agent state representation environment attack agent observation perception environment directly modifying agent state would require write access agent inner workings argue assumption strong realistic settings\n",
            "output sentence:  propose new attack taking full control neural policies realistic settings \n",
            "\n",
            "{'rouge-1': {'r': 0.0975609756097561, 'p': 0.6666666666666666, 'f': 0.17021276373019467}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.4, 'f': 0.07547169640441441}, 'rouge-l': {'r': 0.0975609756097561, 'p': 0.6666666666666666, 'f': 0.17021276373019467}}\n",
            "pair:  consider problem information compression high dimensional data many studies consider problem compression non invertible trans formations emphasize importance invertible compression introduce new class likelihood based auto encoders pseudo bijective architecture call pseudo invertible encoders provide theoretical explanation principles evaluate gaussian pseudo invertible encoder mnist model outperform wae vae sharpness generated images\n",
            "output sentence:  new class autoencoders pseudo invertible architecture \n",
            "\n",
            "{'rouge-1': {'r': 0.14864864864864866, 'p': 0.6111111111111112, 'f': 0.2391304316351607}, 'rouge-2': {'r': 0.047058823529411764, 'p': 0.2222222222222222, 'f': 0.07766990002827799}, 'rouge-l': {'r': 0.10810810810810811, 'p': 0.4444444444444444, 'f': 0.1739130403308129}}\n",
            "pair:  recent advances generative adversarial networks facilitated improvements framework successful application various problems resulted extensions multiple domains irgan attempts leverage framework information retrieval ir task described modeling correct conditional probability distribution documents given query work proposes irgan claims optimizing minimax loss function result generator learn distribution setup baseline term steer model away exact adversarial formulation work attempts point certain inaccuracies formulation analyzing loss curves gives insight possible mistakes loss functions better performance obtained using co training like setup propose two models trained co operative rather adversarial fashion\n",
            "output sentence:  points problems loss function used irgan recently proposed gan framework information retrieval model motivated co training proposed better achieves \n",
            "\n",
            "{'rouge-1': {'r': 0.1875, 'p': 0.9473684210526315, 'f': 0.31304347550245754}, 'rouge-2': {'r': 0.1328125, 'p': 0.9444444444444444, 'f': 0.2328767101670107}, 'rouge-l': {'r': 0.1875, 'p': 0.9473684210526315, 'f': 0.31304347550245754}}\n",
            "pair:  propose model able perform physical parameter estimation systems video differential equations governing scene dynamics known labeled states objects available existing physical scene understanding methods require either object state supervision integrate differentiable physics learn interpretable system parameters states address problem textit physics inverse graphics approach brings together vision inverse graphics differentiable physics engines objects explicit state velocity representations discovered model framework allows us perform long term extrapolative video prediction well vision based model predictive control approach significantly outperforms related unsupervised methods long term future frame prediction systems interacting objects ball spring body gravitational systems due ability build dynamics model inductive bias show value tight vision physics integration demonstrating data efficient learning vision actuated model based control pendulum system also show controller interpretability provides unique capabilities goal driven control physical reasoning zero data adaptation\n",
            "output sentence:  propose model able perform physical parameter estimation systems video differential equations governing scene dynamics known labeled states objects available \n",
            "\n",
            "{'rouge-1': {'r': 0.07608695652173914, 'p': 1.0, 'f': 0.14141414009998982}, 'rouge-2': {'r': 0.025, 'p': 0.5, 'f': 0.04761904671201815}, 'rouge-l': {'r': 0.07608695652173914, 'p': 1.0, 'f': 0.14141414009998982}}\n",
            "pair:  knowledge bases kb often represented collection facts form head predicate tail head tail entities predicate binary relationship links two well known fact knowledge bases far complete hence plethora research kb completion methods specifically link prediction however though frequently ignored repositories also contain numerical facts numerical facts link entities numerical values via numerical predicates paris latitude likewise numerical facts also suffer incompleteness problem address issue introduce numerical attribute prediction problem problem involves new type query relationship numerical predicate consequently contrary link prediction answer query numerical value argue numerical values associated entities explain extent relational structure knowledge base therefore leverage knowledge base embedding methods learn representations useful predictors numerical attributes extensive set experiments benchmark versions freebase yago show approaches largely outperform sensible baselines make datasets available permissive bsd license\n",
            "output sentence:  prediction numerical attribute values associated entities knowledge bases \n",
            "\n",
            "{'rouge-1': {'r': 0.2708333333333333, 'p': 0.7222222222222222, 'f': 0.39393938997245176}, 'rouge-2': {'r': 0.14516129032258066, 'p': 0.47368421052631576, 'f': 0.22222221863130623}, 'rouge-l': {'r': 0.1875, 'p': 0.5, 'f': 0.2727272687603306}}\n",
            "pair:  difficult beginners etching latte art make well balanced patterns using two fluids different viscosities foamed milk syrup even though making etching latte art watching making videos show procedure difficult keep balance thus well balanced etching latte art cannot made easily paper propose system supports beginners make well balanced etching latte art projecting making procedure etching latte art directly onto cappuccino experiment results show progress using system also discuss similarity etching latte art design templates using background subtraction\n",
            "output sentence:  developed etching latte art support system projects making procedure directly onto cappuccino help beginners make well balanced balanced etching latte art \n",
            "\n",
            "{'rouge-1': {'r': 0.1506849315068493, 'p': 0.55, 'f': 0.23655913640883344}, 'rouge-2': {'r': 0.037037037037037035, 'p': 0.14285714285714285, 'f': 0.0588235261418687}, 'rouge-l': {'r': 0.136986301369863, 'p': 0.5, 'f': 0.21505376006474738}}\n",
            "pair:  consistently checking statistical significance experimental results first mandatory step towards reproducible science paper presents hitchhiker guide rigorous comparisons reinforcement learning algorithms introducing concepts statistical testing review relevant statistical tests compare empirically terms false positive rate statistical power function sample size number seeds effect size investigate robustness tests violations common hypotheses normal distributions distributions equal variances beside simulations compare empirical distributions obtained running soft actor critic twin delayed deep deterministic policy gradient half cheetah conclude providing guidelines code perform rigorous comparisons rl algorithm performances\n",
            "output sentence:  paper compares statistical tests rl comparisons false positive statistical power checks robustness assumptions using simulated distributions empirical td sac provides rl provides \n",
            "\n",
            "{'rouge-1': {'r': 0.2857142857142857, 'p': 0.8333333333333334, 'f': 0.4255319110909914}, 'rouge-2': {'r': 0.20454545454545456, 'p': 0.75, 'f': 0.3214285680612245}, 'rouge-l': {'r': 0.2857142857142857, 'p': 0.8333333333333334, 'f': 0.4255319110909914}}\n",
            "pair:  show information whether neural network output correct incorrect present outputs network intermediate layers demonstrate effect train new meta network predict either final output underlying base network output one base network intermediate layers whether base network correct incorrect particular input find wide range tasks base networks meta network achieve accuracies ranging making determination\n",
            "output sentence:  information whether neural network output correct incorrect somewhat present outputs network intermediate layers \n",
            "\n",
            "{'rouge-1': {'r': 0.12, 'p': 0.5, 'f': 0.19354838397502605}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.09090909090909091, 'f': 0.027777775189043456}, 'rouge-l': {'r': 0.08, 'p': 0.3333333333333333, 'f': 0.12903225494276802}}\n",
            "pair:  training neural networks certifiably robust critical ensure safety adversarial attacks however currently difficult train neural network accurate certifiably robust work take step towards addressing challenge prove every continuous function exists network approximates arbitrarily close ii simple interval bound propagation region yields result arbitrarily close optimal output result seen universal approximation theorem interval certified relu networks best knowledge first work prove existence accurate interval certified networks\n",
            "output sentence:  prove large class functions exists interval certified robust network approximating arbitrary precision \n",
            "\n",
            "{'rouge-1': {'r': 0.030612244897959183, 'p': 0.3, 'f': 0.05555555387517152}, 'rouge-2': {'r': 0.008695652173913044, 'p': 0.1, 'f': 0.015999998528000135}, 'rouge-l': {'r': 0.02040816326530612, 'p': 0.2, 'f': 0.03703703535665302}}\n",
            "pair:  recent advances recurrent neural nets rnns shown much promise many applications natural language processing tasks sentiment analysis customer reviews recurrent neural net model parses entire review forming decision argue reading entire input always necessary practice since lot reviews often easy classify decision formed reading crucial sentences words provided text paper present approach fast reading text classification inspired several well known human reading techniques approach implements intelligent recurrent agent evaluates importance current snippet order decide whether make prediction skip texts read part sentence agent uses rnn module encode information past current tokens applies policy module form decisions end end training algorithm based policy gradient train test agent several text classification datasets achieve higher efficiency better accuracy compared previous approaches\n",
            "output sentence:  develop end end trainable approach skimming rereading early stopping applicable classification tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.13513513513513514, 'p': 0.6666666666666666, 'f': 0.22471909832091908}, 'rouge-2': {'r': 0.049019607843137254, 'p': 0.35714285714285715, 'f': 0.08620689442925095}, 'rouge-l': {'r': 0.12162162162162163, 'p': 0.6, 'f': 0.20224718820855958}}\n",
            "pair:  present new latent model natural images learned large scale datasets learning process provides latent embedding every image training dataset well deep convolutional network maps latent space image space training new model provides strong universal image prior variety image restoration tasks large hole inpainting superresolution colorization model high resolution natural images approach uses latent spaces high dimensionality one two orders magnitude higher previous latent image models tackle high dimensionality use latent spaces special manifold structure convolutional manifolds parameterized convnet certain architecture experiments compare learned latent models latent models learned autoencoders advanced variants generative adversarial networks strong baseline system using simpler parameterization latent space model outperforms competing approaches range restoration tasks\n",
            "output sentence:  present new deep latent model natural images trained unlabeled datasets utilized solve various image restoration tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.0975609756097561, 'p': 0.5, 'f': 0.1632653033902541}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.3125, 'f': 0.10204081359433577}}\n",
            "pair:  lifelong machine learning focuses adapting novel tasks without forgetting old tasks whereas shot learning strives learn single task given small amount data two different research areas crucial artificial general intelligence however existing studies somehow assumed impractical settings training models lifelong learning nature quantity incoming tasks inference time assumed known training time shot learning commonly assumed large number tasks available training humans hand perform learning tasks without regard aforementioned assumptions inspired human brain works propose novel model called slow thinking learn stl makes sophisticated slightly slower predictions iteratively considering interactions current previously seen tasks runtime conducted experiments results empirically demonstrate effectiveness stl realistic lifelong shot learning settings\n",
            "output sentence:  paper studies interactions fast learning slow prediction models demonstrate interactions improve machine capability solve joint lifelong learning \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.5714285714285714, 'f': 0.20512820218277453}, 'rouge-2': {'r': 0.08860759493670886, 'p': 0.5, 'f': 0.15053763185108107}, 'rouge-l': {'r': 0.125, 'p': 0.5714285714285714, 'f': 0.20512820218277453}}\n",
            "pair:  study problem multiset prediction goal multiset prediction train predictor maps input multiset consisting multiple items unlike existing problems supervised learning classification ranking sequence generation known order among items target multiset item multiset may appear making problem extremely challenging paper propose novel multiset loss function viewing problem perspective sequential decision making proposed multiset loss function empirically evaluated two families datasets one synthetic real varying levels difficulty various baseline loss functions including reinforcement learning sequence aggregated distribution matching loss functions experiments reveal effectiveness proposed loss function others\n",
            "output sentence:  study problem multiset prediction propose novel multiset loss function providing analysis empirical evidence demonstrates effectiveness \n",
            "\n",
            "{'rouge-1': {'r': 0.08108108108108109, 'p': 0.5454545454545454, 'f': 0.14117646833494812}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.2, 'f': 0.04166666480034731}, 'rouge-l': {'r': 0.04054054054054054, 'p': 0.2727272727272727, 'f': 0.07058823304083052}}\n",
            "pair:  many biological learning systems mushroom body hippocampus cerebellum built sparsely connected networks neurons new understanding networks study function spaces induced sparse random features characterize functions may may learned network inputs per neuron found equivalent additive model order whereas degree distribution network combines additive terms different orders identify three specific advantages sparsity additive function approximation powerful inductive bias limits curse dimensionality sparse networks stable outlier noise inputs sparse random features scalable thus even simple brain architectures powerful function approximators finally hope work helps popularize kernel theories networks among computational neuroscientists\n",
            "output sentence:  advocate random features theory biological neural networks focusing sparsely connected networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.6666666666666666, 'f': 0.13333333153333335}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.2, 'f': 0.03030302890266306}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  hypernetworks meta neural networks generate weights main neural network end end differentiable manner despite extensive applications ranging multi task learning bayesian deep learning problem optimizing hypernetworks studied date observe classical weight initialization methods like glorot bengio et al applied directly hypernet fail produce weights mainnet correct scale develop principled techniques weight initialization hypernets show lead stable mainnet weights lower training loss faster convergence\n",
            "output sentence:  first principled weight initialization method hypernetworks \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.8, 'f': 0.22222221983024693}, 'rouge-2': {'r': 0.04065040650406504, 'p': 0.3333333333333333, 'f': 0.07246376617832394}, 'rouge-l': {'r': 0.10752688172043011, 'p': 0.6666666666666666, 'f': 0.1851851827932099}}\n",
            "pair:  mode connectivity provides novel geometric insights analyzing loss landscapes enables building high accuracy pathways well trained neural networks work propose employ mode connectivity loss landscapes study adversarial robustness deep neural networks provide novel methods improving robustness experiments cover various types adversarial attacks applied different network architectures datasets network models tampered backdoor error injection attacks results demonstrate path connection learned using limited amount bonafide data effectively mitigate adversarial effects maintaining original accuracy clean data therefore mode connectivity provides users power repair backdoored error injected models also use mode connectivity investigate loss landscapes regular robust models evasion attacks experiments show exists barrier adversarial robustness loss path connecting regular adversarially trained models high correlation observed adversarial robustness loss largest eigenvalue input hessian matrix theoretical justifications provided results suggest mode connectivity offers holistic tool practical means evaluating improving adversarial robustness\n",
            "output sentence:  novel approach using mode connectivity loss landscapes mitigate adversarial effects repair tampered models evaluate adversarial robustness \n",
            "\n",
            "{'rouge-1': {'r': 0.14814814814814814, 'p': 0.4444444444444444, 'f': 0.22222221847222226}, 'rouge-2': {'r': 0.06451612903225806, 'p': 0.25, 'f': 0.10256409930309017}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.3333333333333333, 'f': 0.16666666291666676}}\n",
            "pair:  classic papers zellner demonstrated bayesian inference could derived solution information theoretic functional derive generalized form functional variational lower bound predictive information bottleneck objective generalized functional encompasses modern inference procedures suggests novel ones\n",
            "output sentence:  rederive wide class inference procedures global information bottleneck objective \n",
            "\n",
            "{'rouge-1': {'r': 0.09836065573770492, 'p': 0.6666666666666666, 'f': 0.1714285691877551}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}, 'rouge-l': {'r': 0.09836065573770492, 'p': 0.6666666666666666, 'f': 0.1714285691877551}}\n",
            "pair:  users tremendous potential aid construction maintenance knowledges bases kbs contribution feedback identifies incorrect missing entity attributes relations however new data added kb kb entities constructed running entity resolution er change rendering intended targets user feedback unknown problem term identity uncertainty work present framework integrating user feedback kbs presence identity uncertainty approach based user feedback participate alongside mentions er propose specific representation user feedback feedback mentions introduce new online algorithm integrating mentions existing kb experiments demonstrate proposed approach outperforms baselines experimental conditions\n",
            "output sentence:  paper develops framework integrating user feedback identity uncertainty knowledge bases \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.625, 'f': 0.2197802168820191}, 'rouge-2': {'r': 0.052083333333333336, 'p': 0.3125, 'f': 0.08928571183673477}, 'rouge-l': {'r': 0.10666666666666667, 'p': 0.5, 'f': 0.17582417292597516}}\n",
            "pair:  many approaches causal discovery limited inability discriminate markov equivalent graphs given observational data formulate causal discovery marginal likelihood based bayesian model selection problem adopt parameterization based notion independence causal mechanisms renders markov equivalent graphs distinguishable complement empirical bayesian approach setting priors actual underlying causal graph assigned higher marginal likelihood alternatives adopting bayesian approach also allows straightforward modeling unobserved confounding variables provide variational algorithm approximate marginal likelihood since desirable feat renders computation marginal likelihood intractable believe bayesian approach causal discovery allows rich methodology bayesian inference used various difficult aspects problem provides unifying framework causal discovery research demonstrate promising results experiments conducted real data supporting modeling approach inference methodology\n",
            "output sentence:  cast causal structure discovery bayesian model selection way allows us discriminate markov equivalent graphs identify unique causal graph \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.4, 'f': 0.13793103162901313}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.4, 'f': 0.13793103162901313}}\n",
            "pair:  order choose neural network architecture effective particular modeling problem one must understand limitations imposed potential options limitations typically described terms information theoretic bounds comparing relative complexity needed approximate example functions different architectures paper examine topological constraints architecture neural network imposes level sets functions able approximate approach novel nature limitations fact independent network depth broad family activation functions\n",
            "output sentence:  paper proves skinny neural networks cannot approximate certain functions matter deep \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.6428571428571429, 'f': 0.1682242967909861}, 'rouge-2': {'r': 0.05128205128205128, 'p': 0.42857142857142855, 'f': 0.09160305152613488}, 'rouge-l': {'r': 0.06451612903225806, 'p': 0.42857142857142855, 'f': 0.11214953043584597}}\n",
            "pair:  travelling salesman problem tsp well known combinatorial optimization problem variety real life applications tackle tsp incorporating machine learning methodology leveraging variable neighborhood search strategy precisely search process considered markov decision process mdp opt local search used search within small neighborhood monte carlo tree search mcts method iterates simulation selection back propagation steps used sample number targeted actions within enlarged neighborhood new paradigm clearly distinguishes existing machine learning ml based paradigms solving tsp either uses end end ml model simply applies traditional techniques ml post optimization experiments based two public data sets show approach clearly dominates existing learning based tsp algorithms terms performance demonstrating high potential tsp importantly general framework without complicated hand crafted rules readily extended many combinatorial optimization problems\n",
            "output sentence:  paper combines monte carlo tree search opt local search variable neighborhood mode solve tsp effectively \n",
            "\n",
            "{'rouge-1': {'r': 0.0958904109589041, 'p': 0.5, 'f': 0.16091953752939625}, 'rouge-2': {'r': 0.03529411764705882, 'p': 0.23076923076923078, 'f': 0.06122448749479393}, 'rouge-l': {'r': 0.0821917808219178, 'p': 0.42857142857142855, 'f': 0.13793103178226984}}\n",
            "pair:  algorithms representation learning link prediction relational data designed static data however data applied usually evolves time friend graphs social networks user interactions items recommender systems also case knowledge bases contain facts us president obama valid certain points time problem link prediction temporal constraints answering queries form us president propose solution inspired canonical decomposition tensors order introduce new regularization schemes present extension complex achieves state art performance additionally propose new dataset knowledge base completion constructed wikidata larger previous benchmarks order magnitude new reference evaluating temporal non temporal link prediction methods\n",
            "output sentence:  propose new tensor decompositions associated regularizers obtain state art performances temporal knowledge base completion \n",
            "\n",
            "{'rouge-1': {'r': 0.10204081632653061, 'p': 0.7142857142857143, 'f': 0.1785714263839286}, 'rouge-2': {'r': 0.05, 'p': 0.5, 'f': 0.09090908925619837}, 'rouge-l': {'r': 0.10204081632653061, 'p': 0.7142857142857143, 'f': 0.1785714263839286}}\n",
            "pair:  present simple approach based pixel wise nearest neighbors understand interpret functioning state art neural networks pixel level tasks aim understand uncover synthesis prediction mechanisms state art convolutional neural networks end primarily analyze synthesis process generative models prediction mechanism discriminative models main hypothesis work convolutional neural networks pixel level tasks learn fast compositional nearest neighbor synthesis prediction function experiments semantic segmentation image image translation show qualitative quantitative evidence supporting hypothesis\n",
            "output sentence:  convolutional neural networks behave compositional nearest neighbors \n",
            "\n",
            "{'rouge-1': {'r': 0.15254237288135594, 'p': 0.5, 'f': 0.23376623018384218}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.05555555555555555, 'f': 0.02409638214544975}, 'rouge-l': {'r': 0.06779661016949153, 'p': 0.2222222222222222, 'f': 0.1038961003137124}}\n",
            "pair:  deep reinforcement learning rl methods generally engage exploratory behavior noise injection action space alternative add noise directly agent parameters lead consistent exploration richer set behaviors methods evolutionary strategies use parameter perturbations discard temporal structure process require significantly samples combining parameter noise traditional rl methods allows combine best worlds demonstrate policy methods benefit approach experimental comparison dqn ddpg trpo high dimensional discrete action environments well continuous control tasks\n",
            "output sentence:  parameter space noise allows reinforcement learning algorithms explore perturbing parameters instead actions often leading significantly improved exploration exploration performance \n",
            "\n",
            "{'rouge-1': {'r': 0.14772727272727273, 'p': 0.7647058823529411, 'f': 0.24761904490521544}, 'rouge-2': {'r': 0.05357142857142857, 'p': 0.375, 'f': 0.09374999781250005}, 'rouge-l': {'r': 0.10227272727272728, 'p': 0.5294117647058824, 'f': 0.17142856871473924}}\n",
            "pair:  neural networks could misclassify inputs slightly different training data indicates small margin decision boundaries training dataset work study binary classification linearly separable datasets show linear classifiers could also decision boundaries lie close training dataset cross entropy loss used training particular show features training dataset lie low dimensional affine subspace cross entropy loss minimized using gradient method margin training points decision boundary could much smaller optimal value result contrary conclusions recent related works soudry et al identify reason contradiction order improve margin introduce differential training training paradigm uses loss function defined pairs points class show decision boundary linear classifier trained differential training indeed achieves maximum margin results reveal use cross entropy loss one hidden culprits adversarial examples introduces new direction make neural networks robust\n",
            "output sentence:  show minimizing cross entropy loss using gradient method could lead poor margin features dataset lie low dimensional \n",
            "\n",
            "{'rouge-1': {'r': 0.05102040816326531, 'p': 0.45454545454545453, 'f': 0.091743117451393}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.030612244897959183, 'p': 0.2727272727272727, 'f': 0.05504586974497103}}\n",
            "pair:  neural networks trained map one specific dataset another usually learn generalized transformation extrapolate accurately outside space training instance generative adversarial network gan exclusively trained transform images cars light dark might effect images horses neural networks good generation within manifold data trained however generating new samples outside manifold extrapolating sample much harder problem less well studied address introduce technique called neuron editing learns neurons encode edit particular transformation latent space use autoencoder decompose variation within dataset activations different neurons generate transformed data defining editing transformation neurons performing transformation latent trained space encode fairly complex non linear transformations data much simpler distribution shifts neuron activations showcase technique image domain style transfer two biological applications removal batch artifacts representing unwanted noise modeling effect drug treatments predict synergy drugs\n",
            "output sentence:  reframe generation problem one editing existing points result extrapolate better traditional gans \n",
            "\n",
            "{'rouge-1': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.16666666666666666, 'f': 0.04081632438150782}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}}\n",
            "pair:  driving force behind recent success lstms ability learn complex non linear relationships consequently inability describe relationships led lstms characterized black boxes end introduce contextual decomposition cd interpretation algorithm analysing individual predictions made standard lstms without changes underlying model decomposing output lstm cd captures contributions combinations words variables final prediction lstm task sentiment analysis yelp sst data sets show cd able reliably identify words phrases contrasting sentiment combined yield lstm final prediction using phrase level labels sst also demonstrate cd able successfully extract positive negative negations lstm something previously done\n",
            "output sentence:  introduce contextual decompositions interpretation algorithm lstms capable extracting word phrase interaction level importance score \n",
            "\n",
            "{'rouge-1': {'r': 0.2857142857142857, 'p': 0.9230769230769231, 'f': 0.43636363275371903}, 'rouge-2': {'r': 0.24444444444444444, 'p': 0.9166666666666666, 'f': 0.3859649089566021}, 'rouge-l': {'r': 0.2857142857142857, 'p': 0.9230769230769231, 'f': 0.43636363275371903}}\n",
            "pair:  propose new anytime neural network allows partial evaluation subnetworks different widths well depths compared conventional anytime networks depth controllability increased architectural diversity leads higher resource utilization consequent performance improvement various dynamic resource budgets highlight architectural features make scheme feasible well efficient show effectiveness image classification tasks\n",
            "output sentence:  propose new anytime neural network allows partial evaluation subnetworks different widths well depths \n",
            "\n",
            "{'rouge-1': {'r': 0.2159090909090909, 'p': 0.7916666666666666, 'f': 0.3392857109183674}, 'rouge-2': {'r': 0.088, 'p': 0.4074074074074074, 'f': 0.14473683918369118}, 'rouge-l': {'r': 0.13636363636363635, 'p': 0.5, 'f': 0.21428571091836737}}\n",
            "pair:  state art unsupervised domain adaptation uda methods learn transferable features minimizing feature distribution discrepancy source target domains different methods model feature distributions explicitly paper explore explicit feature distribution modeling uda particular propose distribution matching prototypical network dmpn model deep features domain gaussian mixture distributions explicit feature distribution modeling easily measure discrepancy two domains dmpn propose two new domain discrepancy losses probabilistic interpretations first one minimizes distances corresponding gaussian component means source target data second one minimizes pseudo negative log likelihood generating target features source feature distribution learn discriminative domain invariant features dmpn trained minimizing classification loss labeled source data domain discrepancy losses together extensive experiments conducted two uda tasks approach yields large margin digits image transfer task state art approaches remarkably dmpn obtains mean accuracy visda dataset hyper parameter sensitivity analysis shows approach robust hyper parameter changes\n",
            "output sentence:  propose explicitly model deep feature distributions source target data gaussian mixture distributions unsupervised domain adaptation uda achieve superior results multiple uda tasks state art methods tasks tasks methods \n",
            "\n",
            "{'rouge-1': {'r': 0.17346938775510204, 'p': 0.7391304347826086, 'f': 0.28099173245816544}, 'rouge-2': {'r': 0.09230769230769231, 'p': 0.46153846153846156, 'f': 0.15384615106837612}, 'rouge-l': {'r': 0.1326530612244898, 'p': 0.5652173913043478, 'f': 0.2148760299788266}}\n",
            "pair:  recent work modeling neural responses primate visual system benefited deep neural networks trained large scale object recognition found hierarchical correspondence layers artificial neural network brain areas along ventral visual stream however neither know whether task optimized networks enable equally good models rodent visual system similar hierarchical correspondence exists address questions mouse visual system extracting features several layers convolutional neural network cnn trained imagenet predict responses thousands neurons four visual areas lm al rl natural images found cnn features outperform classical subunit energy models found evidence order areas recorded via correspondence hierarchy cnn layers moreover cnn random weights provided equivalently useful feature space predicting neural responses results suggest object recognition high level task provide discriminative features characterize mouse visual system random network unlike primate training ethologically relevant visually guided behaviors beyond static object recognition may needed unveil functional organization mouse visual cortex\n",
            "output sentence:  goal driven approach model four mouse visual areas lm al rl based deep neural networks trained static object recognition functional functional organization visual visual cortex cortex unlike \n",
            "\n",
            "{'rouge-1': {'r': 0.047058823529411764, 'p': 0.36363636363636365, 'f': 0.08333333130425352}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.18181818181818182, 'f': 0.032520323574591926}, 'rouge-l': {'r': 0.047058823529411764, 'p': 0.36363636363636365, 'f': 0.08333333130425352}}\n",
            "pair:  propose active learning algorithmic architecture capable organizing learning process order achieve field complex tasks learning sequences primitive motor policies socially guided intrinsic motivation procedure babbling sgim pb learner generalize experience continuously learn new outcomes choosing actively learn guided empirical measures progress paper considering learning set interrelated complex outcomes hierarchically organized introduce new framework called procedures enables autonomous discovery combine previously learned skills order learn increasingly complex motor policies combinations primitive motor policies architecture actively decide outcome focus exploration strategy apply strategies could autonomous exploration active social guidance relies expertise human teacher providing demonstrations learner request show simulated environment new architecture capable tackling learning complex motor policies adapt complexity policies task hand also show procedures increases agent capability learn complex tasks\n",
            "output sentence:  paper describes strategic intrinsically motivated learning algorithm tackles learning complex motor policies \n",
            "\n",
            "{'rouge-1': {'r': 0.10810810810810811, 'p': 0.8888888888888888, 'f': 0.1927710824038322}, 'rouge-2': {'r': 0.0625, 'p': 0.75, 'f': 0.11538461396449705}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.7777777777777778, 'f': 0.16867469686166353}}\n",
            "pair:  propose algorithm guided variational autoencoder guided vae able learn controllable generative model performing latent representation disentanglement learning learning objective achieved providing signal latent encoding embedding vae without changing main backbone architecture hence retaining desirable properties vae design unsupervised supervised strategy guided vae observe enhanced modeling controlling capability vanilla vae unsupervised strategy guide vae learning introducing lightweight decoder learns latent geometric transformation principal components supervised strategy use adversarial excitation inhibition mechanism encourage disentanglement latent variables guided vae enjoys transparency simplicity general representation learning task well disentanglement learning number experiments representation learning improved synthesis sampling better disentanglement classification reduced classification errors meta learning observed\n",
            "output sentence:  learning controllable generative model performing latent representation disentanglement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.23214285714285715, 'p': 0.9285714285714286, 'f': 0.37142856822857145}, 'rouge-2': {'r': 0.2, 'p': 0.6363636363636364, 'f': 0.30434782244801517}, 'rouge-l': {'r': 0.23214285714285715, 'p': 0.9285714285714286, 'f': 0.37142856822857145}}\n",
            "pair:  present graph wavelet neural network gwnn novel graph convolutional neural network cnn leveraging graph wavelet transform address shortcomings previous spectral graph cnn methods depend graph fourier transform different graph fourier transform graph wavelet transform obtained via fast algorithm without requiring matrix eigendecomposition high computational cost moreover graph wavelets sparse localized vertex domain offering high efficiency good interpretability graph convolution proposed gwnn significantly outperforms previous spectral graph cnns task graph based semi supervised classification three benchmark datasets cora citeseer pubmed\n",
            "output sentence:  present graph wavelet neural network gwnn novel graph convolutional neural network cnn leveraging graph wavelet previous cnn transform graph transform spectral graph graph spectral graph spectral graph spectral \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.6, 'f': 0.11650485261570365}, 'rouge-2': {'r': 0.008547008547008548, 'p': 0.1111111111111111, 'f': 0.015873014546485376}, 'rouge-l': {'r': 0.043010752688172046, 'p': 0.4, 'f': 0.07766990115939301}}\n",
            "pair:  knowledge extraction techniques used convert neural networks symbolic descriptions objective producing comprehensible learning models central challenge find explanation comprehensible original model still representing model faithfully distributed nature deep networks led many believe hidden features neural network cannot explained logical descriptions simple enough understood humans decompositional knowledge extraction abandoned favour methods paper examine question systematically proposing knowledge extraction method using textit rules allows us map complexity accuracy landscape rules describing hidden features convolutional neural network cnn experiments reported paper show shape landscape reveals optimal trade comprehensibility accuracy showing latent variable optimal textit rule describe behaviour find rules optimal tradeoff first final layer high degree explainability whereas rules optimal tradeoff second third layer less explainable results shed light feasibility rule extraction deep networks point value decompositional knowledge extraction method explainability\n",
            "output sentence:  systematically examines well explain hidden features deep network terms logical rules \n",
            "\n",
            "{'rouge-1': {'r': 0.06944444444444445, 'p': 0.625, 'f': 0.12499999820000002}, 'rouge-2': {'r': 0.011627906976744186, 'p': 0.14285714285714285, 'f': 0.021505374952017663}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  paper presents novel two step approach fundamental problem learning optimal map one distribution another first learn optimal transport ot plan thought one many map two distributions end propose stochastic dual approach regularized ot show empirically scales better recent related approach amount samples large second estimate monge map deep neural network learned approximating barycentric projection previously obtained ot plan parameterization allows generalization mapping outside support input measure prove two theoretical stability results regularized ot show estimations converge ot monge map underlying continuous measures showcase proposed approach two applications domain adaptation generative modeling\n",
            "output sentence:  learning optimal mapping deepnn distributions along theoretical guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.03, 'p': 0.2727272727272727, 'f': 0.054054052268484754}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03, 'p': 0.2727272727272727, 'f': 0.054054052268484754}}\n",
            "pair:  innovations architecture design deeper wider neural network models deliver improved performance diverse variety tasks increased memory footprint models presents challenge training intermediate layer activations need stored back propagation limited gpu memory forces practitioners make sub optimal choices either train inefficiently smaller batches examples limit architecture lower depth width fewer layers higher spatial resolutions work introduces approximation strategy significantly reduces network memory footprint training negligible effect training performance computational expense forward pass replace activations lower precision approximations immediately used subsequent layers thus freeing memory approximate activations used backward pass approach limits accumulation errors across forward backward pass forward computation across network still happens full precision approximation limited effect computing gradients layer input experiments cifar imagenet show using approach even bit fixed point approximations bit floating point activations minor effect training validation performance affording significant savings memory usage\n",
            "output sentence:  algorithm reduce amount memory required training deep networks based approximation strategy \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.6, 'f': 0.15584415358407824}, 'rouge-2': {'r': 0.025, 'p': 0.2222222222222222, 'f': 0.0449438184067669}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.4, 'f': 0.10389610163602636}}\n",
            "pair:  optimization manifold widely used machine learning handle optimization problems constraint previous works focus case single manifold however practice quite common optimization problem involves one constraints constraint corresponding one manifold clear general optimize multiple manifolds effectively provably especially intersection multiple manifolds manifold cannot easily calculated propose unified algorithm framework handle optimization multiple manifolds specifically integrate information multiple manifolds move along ensemble direction viewing information manifold drift adding together prove convergence properties proposed algorithms also apply algorithms training neural network batch normalization layers achieve preferable empirical results\n",
            "output sentence:  paper introduces algorithm handle optimization problem multiple constraints vision manifold \n",
            "\n",
            "{'rouge-1': {'r': 0.21212121212121213, 'p': 1.0, 'f': 0.3499999971125}, 'rouge-2': {'r': 0.1728395061728395, 'p': 1.0, 'f': 0.29473683959224384}, 'rouge-l': {'r': 0.21212121212121213, 'p': 1.0, 'f': 0.3499999971125}}\n",
            "pair:  propose novel projection based way incorporate conditional information discriminator gans respects role conditional information underlining probabilistic model approach contrast frameworks conditional gans used application today use conditional information concatenating embedded conditional vector feature vectors modification able significantly improve quality class conditional image generation ilsvrc imagenet dataset current state art result achieved single pair discriminator generator also able extend application super resolution succeeded producing highly discriminative super resolution images new structure also enabled high quality category transformation based parametric functional transformation conditional batch normalization layers generator\n",
            "output sentence:  propose novel projection based way incorporate conditional information discriminator gans respects role conditional information underlining probabilistic model \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.5333333333333333, 'f': 0.20253164249319022}, 'rouge-2': {'r': 0.04054054054054054, 'p': 0.2, 'f': 0.06741572753440234}, 'rouge-l': {'r': 0.125, 'p': 0.5333333333333333, 'f': 0.20253164249319022}}\n",
            "pair:  conversational machine comprehension requires deep understanding conversation history enable traditional single turn models encode history comprehensively introduce flow mechanism incorporate intermediate representations generated process answering previous questions alternating parallel processing structure compared shallow approaches concatenate previous questions answers input flow integrates latent semantics conversation history deeply model flowqa shows superior performance two recently proposed conversational challenges coqa quac effectiveness flow also shows tasks reducing sequential instruction understanding conversational machine comprehension flowqa outperforms best models three domains scone improvement accuracy\n",
            "output sentence:  propose flow mechanism end end architecture flowqa achieves sota two conversational qa datasets sequential instruction understanding task \n",
            "\n",
            "{'rouge-1': {'r': 0.14634146341463414, 'p': 0.7058823529411765, 'f': 0.24242423957963474}, 'rouge-2': {'r': 0.0196078431372549, 'p': 0.11764705882352941, 'f': 0.033613442929171844}, 'rouge-l': {'r': 0.13414634146341464, 'p': 0.6470588235294118, 'f': 0.22222221937761455}}\n",
            "pair:  large pre trained transformers bert tremendously effective many nlp tasks however inference large capacity models prohibitively slow expensive transformers essentially stack self attention layers encode input position using entire input sequence context however find may necessary apply expensive sequence wide self attention layers based observation propose decomposition pre trained transformer allows lower layers process segments input independently enabling parallelism caching show information loss due decomposition recovered upper layers auxiliary supervision fine tuning evaluate de composition pre trained bert models five different paired input tasks question answering sentence similarity natural language inference results show decomposition enables faster inference significant memory reduction retaining original performance release code anonymized url\n",
            "output sentence:  inference large transformers expensive due self attention multiple layers show simple decomposition technique yield faster memory memory model model \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.5, 'f': 0.1555555529283951}, 'rouge-2': {'r': 0.010526315789473684, 'p': 0.07142857142857142, 'f': 0.0183486216143425}, 'rouge-l': {'r': 0.07894736842105263, 'p': 0.42857142857142855, 'f': 0.1333333307061729}}\n",
            "pair:  provide novel perspective forward pass block layers deep network particular show forward pass standard dropout layer followed linear layer non linear activation equivalent optimizing convex objective single iteration tau nice proximal stochastic gradient method show replacing standard bernoulli dropout additive dropout equivalent optimizing convex objective variance reduced proximal method expressing fully connected convolutional layers special cases high order tensor product unify underlying convex optimization problem tensor setting derive formula lipschitz constant used determine optimal step size proximal methods conduct experiments standard convolutional networks applied cifar cifar datasets show replacing block layers multiple iterations corresponding solver step size set via consistently improves classification accuracy\n",
            "output sentence:  framework links deep network layers stochastic optimization algorithms used improve model accuracy inform network design \n",
            "\n",
            "{'rouge-1': {'r': 0.043478260869565216, 'p': 0.75, 'f': 0.0821917797860762}, 'rouge-2': {'r': 0.022222222222222223, 'p': 0.5, 'f': 0.042553190674513366}, 'rouge-l': {'r': 0.028985507246376812, 'p': 0.5, 'f': 0.054794519512103596}}\n",
            "pair:  research lifelong learning applies images games language present lamol simple yet effective method lifelong language learning lll based language modeling lamol replays pseudo samples previous tasks requiring extra memory model capacity specifically lamol language model simultaneously learns solve tasks generate training samples model trained new task generates pseudo samples previous tasks training alongside data new task results show lamol prevents catastrophic forgetting without sign intransigence perform five different language tasks sequentially one model overall lamol outperforms previous methods considerable margin worse multitasking usually considered lll upper bound source code available https github com jojotenya lamol\n",
            "output sentence:  language modeling lifelong language learning \n",
            "\n",
            "{'rouge-1': {'r': 0.16071428571428573, 'p': 0.5, 'f': 0.24324323956172395}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.05555555555555555, 'f': 0.023809520442177348}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.4444444444444444, 'f': 0.21621621253469692}}\n",
            "pair:  paper propose new control framework called moving endpoint control restore images corrupted different degradation levels one model proposed control problem contains restoration dynamics modeled rnn moving endpoint essentially terminal time associated dynamics determined policy network call proposed model dynamically unfolding recurrent restorer durr numerical experiments show durr able achieve state art performances blind image denoising jpeg image deblocking furthermore durr well generalize images higher degradation levels included training stage\n",
            "output sentence:  propose novel method handle image degradations different levels learning diffusion terminal time model generalize unseen degradation level different statistic \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.5833333333333334, 'f': 0.25925925580246917}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.18181818181818182, 'f': 0.07142856827168381}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.5833333333333334, 'f': 0.25925925580246917}}\n",
            "pair:  propose single neural probabilistic model based variational autoencoder conditioned arbitrary subset observed features sample remaining features one shot features may real valued categorical training model performed stochastic variational bayes experimental evaluation synthetic data well feature imputation image inpainting problems shows effectiveness proposed approach diversity generated samples\n",
            "output sentence:  propose extension conditional variational autoencoder allows conditioning arbitrary subset features sampling remaining ones \n",
            "\n",
            "{'rouge-1': {'r': 0.14754098360655737, 'p': 0.9, 'f': 0.2535211243404087}, 'rouge-2': {'r': 0.075, 'p': 0.6666666666666666, 'f': 0.13483145885620504}, 'rouge-l': {'r': 0.14754098360655737, 'p': 0.9, 'f': 0.2535211243404087}}\n",
            "pair:  give new algorithm learning two layer neural network general class input distributions assuming ground truth two layer network sigma wx xi weight matrices xi represents noise number neurons hidden layer larger input output algorithm guaranteed recover parameters ground truth network requirement input symmetric still allows highly complicated structured input algorithm based method moments framework extends several results tensor decompositions use spectral algorithms avoid complicated non convex optimization learning neural networks experiments show algorithm robustly learn ground truth neural network small number samples many symmetric input distributions\n",
            "output sentence:  give algorithm learning two layer neural network symmetric input distribution \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.7777777777777778, 'f': 0.2121212097658402}, 'rouge-2': {'r': 0.030303030303030304, 'p': 0.2222222222222222, 'f': 0.05333333122133342}, 'rouge-l': {'r': 0.12280701754385964, 'p': 0.7777777777777778, 'f': 0.2121212097658402}}\n",
            "pair:  introduce adaptive input representations neural language modeling extend adaptive softmax grave et al input representations variable capacity several choices factorize input output layers whether model words characters sub word units perform systematic comparison popular choices self attentional architecture experiments show models equipped adaptive embeddings twice fast train popular character input cnn lower number parameters wikitext benchmark achieve perplexity improvement perplexity compared previously best published result billion word benchmark achieve perplexity\n",
            "output sentence:  variable capacity input word embeddings sota wikitext billion word benchmarks \n",
            "\n",
            "{'rouge-1': {'r': 0.14545454545454545, 'p': 0.6666666666666666, 'f': 0.23880596720873248}, 'rouge-2': {'r': 0.0821917808219178, 'p': 0.46153846153846156, 'f': 0.13953488115467824}, 'rouge-l': {'r': 0.10909090909090909, 'p': 0.5, 'f': 0.17910447467141904}}\n",
            "pair:  verification planning domain models crucial ensure safety integrity correctness planning based automated systems task usually performed using model checking techniques however directly applying model checkers verify planning domain models result false positives counterexamples unreachable sound planner using domain verification planning task paper discuss downside unconstrained planning domain model verification propose fail safe practice designing planning domain models inherently guarantee safety produced plans case undetected errors domain models addition demonstrate model checkers well state trajectory constraints planning techniques used verify planning domain models unreachable counterexamples returned\n",
            "output sentence:  constrain planning domain model verification planning goals avoid unreachable counterexamples false positives verification outcomes \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.6923076923076923, 'f': 0.19780219535321825}, 'rouge-2': {'r': 0.04395604395604396, 'p': 0.3333333333333333, 'f': 0.07766990085399195}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.38461538461538464, 'f': 0.10989010744113034}}\n",
            "pair:  deep generative models advanced state art semi supervised classification however capacity deriving useful discriminative features completely unsupervised fashion classification difficult real world data sets adequate manifold separation required adequately explored methods rely defining pipeline deriving features via generative modeling applying clustering algorithms separating modeling discriminative processes propose deep hierarchical generative model uses mixture discrete continuous distributions learn effectively separate different data manifolds trainable end end show specifying form discrete variable distribution imposing specific structure model latent representations test model discriminative performance task cll diagnosis baselines field computational fc well variational autoencoder literature\n",
            "output sentence:  unsupervised classification via deep generative modeling controllable feature learning evaluated difficult real world task \n",
            "\n",
            "{'rouge-1': {'r': 0.05714285714285714, 'p': 0.4444444444444444, 'f': 0.10126582076590292}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}, 'rouge-l': {'r': 0.05714285714285714, 'p': 0.4444444444444444, 'f': 0.10126582076590292}}\n",
            "pair:  understanding flow information deep neural networks dnns challenging problem gain increasing attention last years several methods proposed explain network predictions attempts compare theoretical perspective exhaustive empirical comparison performed past work analyze four gradient based attribution methods formally prove conditions equivalence approximation reformulating two methods construct unified framework enables direct comparison well easier implementation finally propose novel evaluation metric called sensitivity test gradient based attribution methods alongside simple perturbation based attribution method several datasets domains image text classification using various network architectures\n",
            "output sentence:  four existing backpropagation based attribution methods fundamentally similar assess \n",
            "\n",
            "{'rouge-1': {'r': 0.12643678160919541, 'p': 0.6470588235294118, 'f': 0.2115384588036243}, 'rouge-2': {'r': 0.04716981132075472, 'p': 0.29411764705882354, 'f': 0.08130081062595025}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.35294117647058826, 'f': 0.11538461264977817}}\n",
            "pair:  recent success neural networks solving difficult decision tasks incentivized incorporating smart decision making edge however work traditionally focused neural network inference rather training due memory compute limitations especially emerging non volatile memory systems writes energetically costly reduce lifespan yet ability train edge becoming increasingly important enables applications real time adaptability device drift environmental variation user customization federated learning across devices work address four key challenges training edge devices non volatile memory low weight update density weight quantization low auxiliary memory online learning present low rank training scheme addresses four challenges maintaining computational efficiency demonstrate technique representative convolutional neural network across several adaptation problems performs standard sgd accuracy number weight updates\n",
            "output sentence:  use kronecker sum approximations low rank training address challenges training neural networks edge devices utilize emerging memory technologies \n",
            "\n",
            "{'rouge-1': {'r': 0.05084745762711865, 'p': 0.375, 'f': 0.08955223670305197}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.25, 'f': 0.05970149043439526}}\n",
            "pair:  introduce model learns convert simple hand drawings graphics programs written subset latex model combines techniques deep learning program synthesis learn convolutional neural network proposes plausible drawing primitives explain image drawing primitives like trace set primitive commands issued graphics program learn model uses program synthesis techniques recover graphics program trace programs constructs like variable bindings iterative loops simple kinds conditionals graphics program hand correct errors made deep network extrapolate drawings taken together results step towards agents induce useful human readable programs perceptual input\n",
            "output sentence:  learn convert hand drawn sketch high level program \n",
            "\n",
            "{'rouge-1': {'r': 0.1917808219178082, 'p': 0.7368421052631579, 'f': 0.3043478228095463}, 'rouge-2': {'r': 0.06382978723404255, 'p': 0.2608695652173913, 'f': 0.10256409940536206}, 'rouge-l': {'r': 0.1232876712328767, 'p': 0.47368421052631576, 'f': 0.1956521706356333}}\n",
            "pair:  end end task oriented dialogue challenging since knowledge bases usually large dynamic hard incorporate learning framework propose global local memory pointer glmp networks address issue model global memory encoder local memory decoder proposed share external knowledge encoder encodes dialogue history modifies global contextual representation generates global memory pointer decoder first generates sketch response unfilled slots next passes global memory pointer filter external knowledge relevant information instantiates slots via local memory pointers empirically show model improve copy accuracy mitigate common vocabulary problem result glmp able improve previous state art models simulated babi dialogue dataset human human stanford multi domain dialogue dataset automatic human evaluation\n",
            "output sentence:  glmp global memory encoder context rnn global pointer local memory decoder sketch rnn local pointer share external knowledge memnn proposed strengthen response task generation \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.875, 'f': 0.16470588064775088}, 'rouge-2': {'r': 0.030612244897959183, 'p': 0.42857142857142855, 'f': 0.057142855898412726}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.875, 'f': 0.16470588064775088}}\n",
            "pair:  learning learn powerful paradigm enabling models learn data effectively efficiently popular approach meta learning train recurrent model read training dataset input output parameters learned model output predictions new test inputs alternatively recent approach meta learning aims acquire deep representations effectively fine tuned via standard gradient descent new tasks paper consider meta learning problem perspective universality formalizing notion learning algorithm approximation comparing expressive power aforementioned recurrent models recent approaches embed gradient descent meta learner particular seek answer following question deep representation combined standard gradient descent sufficient capacity approximate learning algorithm find indeed true find experiments gradient based meta learning consistently leads learning strategies generalize widely compared represented recurrent models\n",
            "output sentence:  deep representations combined gradient descent approximate learning algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.07777777777777778, 'p': 0.4666666666666667, 'f': 0.13333333088435376}, 'rouge-2': {'r': 0.02830188679245283, 'p': 0.2, 'f': 0.04958677468752144}, 'rouge-l': {'r': 0.07777777777777778, 'p': 0.4666666666666667, 'f': 0.13333333088435376}}\n",
            "pair:  self training one earliest simplest semi supervised methods key idea augment original labeled dataset unlabeled data paired model prediction self training mostly well studied classification problems however complex sequence generation tasks machine translation still clear self training woks due compositionality target space work first show possible recommended apply self training sequence generation careful examination performance gains find noise added hidden states dropout critical success self training acts like regularizer forces model yield similar predictions similar inputs unlabeled data encourage mechanism propose inject noise input space resulting noisy version self training empirical study standard benchmarks across machine translation text summarization tasks different resource settings shows noisy self training able effectively utilize unlabeled data improve baseline performance large margin\n",
            "output sentence:  revisit self training semi supervised learning method neural sequence generation problem show self training quite successful injected noise \n",
            "\n",
            "{'rouge-1': {'r': 0.14444444444444443, 'p': 0.9285714285714286, 'f': 0.24999999767011838}, 'rouge-2': {'r': 0.10619469026548672, 'p': 0.9230769230769231, 'f': 0.19047618862559842}, 'rouge-l': {'r': 0.14444444444444443, 'p': 0.9285714285714286, 'f': 0.24999999767011838}}\n",
            "pair:  propose new form autoencoding model incorporates best properties variational autoencoders vae generative adversarial networks gan known gan produce realistic samples vae suffer mode collapsing problem model optimizes jeffreys divergence model distribution true data distribution show takes best properties vae gan objectives consists two parts one parts optimized using standard adversarial training second one objective vae model however straightforward way substituting vae loss work well use explicit likelihood gaussian laplace limited flexibility high dimensions unnatural modelling images space pixels tackle problem propose novel approach train vae model implicit likelihood adversarially trained discriminator extensive set experiments cifar tinyimagent datasets show model achieves state art generation reconstruction quality demonstrate balance mode seeking mode covering behaviour model adjusting weight objective\n",
            "output sentence:  propose new form autoencoding model incorporates best properties variational autoencoders vae generative adversarial networks gan \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.7, 'f': 0.16279069561925366}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.2222222222222222, 'f': 0.03883494986143847}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.7, 'f': 0.16279069561925366}}\n",
            "pair:  gradient clipping widely used technique training deep networks generally motivated optimisation lens informally controls dynamics iterates thus enhancing rate convergence local minimum intuition made precise line recent works show suitable clipping yield significantly faster convergence vanilla gradient descent paper propose new lens studying gradient clipping namely robustness informally one expects clipping provide robustness noise since one overly trust single sample surprisingly prove common problem label noise classification standard gradient clipping general provide robustness hand show simple variant gradient clipping provably robust corresponds suitably modifying underlying loss function yields simple noise robust alternative standard cross entropy loss performs well empirically\n",
            "output sentence:  gradient clipping endow robustness label noise simple loss based variant \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.4, 'f': 0.19999999625000006}, 'rouge-2': {'r': 0.02, 'p': 0.06666666666666667, 'f': 0.03076922721893532}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.3333333333333333, 'f': 0.16666666291666676}}\n",
            "pair:  work introduces simple network producing character aware word embeddings position agnostic position aware character embeddings combined produce embedding vector word learned word representations shown sparse facilitate improved results language modeling tasks despite using markedly fewer parameters without need apply dropout final experiment suggests weight sharing contributes sparsity increases performance prevents overfitting\n",
            "output sentence:  fully connected architecture used produce word embeddings character representations outperforms traditional embeddings provides insight sparsity dropout \n",
            "\n",
            "{'rouge-1': {'r': 0.07352941176470588, 'p': 0.8333333333333334, 'f': 0.13513513364499638}, 'rouge-2': {'r': 0.044444444444444446, 'p': 0.8, 'f': 0.08421052531855956}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.8333333333333334, 'f': 0.13513513364499638}}\n",
            "pair:  scarcity labeled training data often prohibits internationalization nlp models multiple languages cross lingual understanding made progress area using language universal representations however current approaches focus problem one aligning language address natural domain drift across languages cultures paper address domain gap setting semi supervised cross lingual document classification labeled data available source language unlabeled data available target language combine state art unsupervised learning method masked language modeling pre training recent method semi supervised learning unsupervised data augmentation uda simultaneously close language domain gap show addressing domain gap cross lingual tasks crucial improve strong baselines achieve new state art cross lingual document classification\n",
            "output sentence:  semi supervised cross lingual document classification \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.8888888888888888, 'f': 0.22535211046220993}, 'rouge-2': {'r': 0.09722222222222222, 'p': 0.875, 'f': 0.1749999982}, 'rouge-l': {'r': 0.12903225806451613, 'p': 0.8888888888888888, 'f': 0.22535211046220993}}\n",
            "pair:  paper propose nonlinear unsupervised metric learning framework boost performance clustering algorithms framework nonlinear distance metric learning manifold embedding integrated conducted simultaneously increase natural separations among data samples metric learning component implemented feature space transformations regulated nonlinear deformable model called coherent point drifting cpd driven cpd data points get higher level linear separability subsequently picked manifold embedding component generate well separable sample projections clustering experimental results synthetic benchmark datasets show effectiveness proposed approach state art solutions unsupervised metric learning\n",
            "output sentence:  nonlinear unsupervised metric learning framework boost performance clustering algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.04285714285714286, 'p': 0.5, 'f': 0.07894736696675902}, 'rouge-2': {'r': 0.011363636363636364, 'p': 0.2, 'f': 0.021505375326627406}, 'rouge-l': {'r': 0.02857142857142857, 'p': 0.3333333333333333, 'f': 0.05263157749307483}}\n",
            "pair:  neural networks learn extract statistical properties data seldom make use structured information label space help representation learning although label structure implicitly obtained training huge amounts data shot learning context little data available making explicit use label structure inform model reshape representation space reflect global sense class dependencies propose meta learning framework conditional class aware meta learning caml conditionally transforms feature representations based metric space trained capture inter class dependencies enables conditional modulation feature representations base learner impose regularities informed label space experiments show conditional transformation caml leads disentangled representations achieves competitive results miniimagenet benchmark\n",
            "output sentence:  caml instance maml conditional class dependencies \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.375, 'f': 0.1071428546938776}, 'rouge-2': {'r': 0.015503875968992248, 'p': 0.11764705882352941, 'f': 0.027397258216363452}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.25, 'f': 0.07142856897959192}}\n",
            "pair:  deploying machine learning systems real world requires high accuracy clean data robustness naturally occurring corruptions architectural advances led improved accuracy building robust models remains challenging involving major changes training procedure datasets prior work argued inherent trade robustness accuracy exemplified standard data augmentation techniques cutout improves clean accuracy robustness additive gaussian noise improves robustness hurts accuracy introduce patch gaussian simple augmentation scheme adds noise randomly selected patches input image models trained patch gaussian achieve state art cifar imagenet common corruptions benchmarks also maintaining accuracy clean data find augmentation leads reduced sensitivity high frequency noise similar gaussian retaining ability take advantage relevant high frequency information image similar cutout show used conjunction regularization methods data augmentation policies autoaugment finally find idea restricting perturbations patches also useful context adversarial learning yielding models without loss accuracy found unconstrained adversarial training\n",
            "output sentence:  simple augmentation method overcomes robustness accuracy trade observed literature opens questions effect training distribution distribution generalization generalization generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.13432835820895522, 'p': 0.6428571428571429, 'f': 0.222222219362902}, 'rouge-2': {'r': 0.04878048780487805, 'p': 0.2857142857142857, 'f': 0.08333333084201397}, 'rouge-l': {'r': 0.08955223880597014, 'p': 0.42857142857142855, 'f': 0.14814814528882797}}\n",
            "pair:  consider task shot link prediction goal predict missing edges across multiple graphs using small sample known edges show current link prediction methods generally ill equipped handle task cannot effectively transfer knowledge graphs multi graph setting unable effectively learn sparse data address challenge introduce new gradient based meta learning framework meta graph leverages higher order gradients along learned graph signature function conditionally generates graph neural network initialization using novel set shot link prediction benchmarks show meta graph enables fast adaptation also better final convergence effectively learn using small sample true edges\n",
            "output sentence:  apply gradient based meta learning graph domain introduce new graph specific transfer function bootstrap process \n",
            "\n",
            "{'rouge-1': {'r': 0.18947368421052632, 'p': 0.9473684210526315, 'f': 0.3157894709064328}, 'rouge-2': {'r': 0.15454545454545454, 'p': 0.7727272727272727, 'f': 0.2575757547979798}, 'rouge-l': {'r': 0.18947368421052632, 'p': 0.9473684210526315, 'f': 0.3157894709064328}}\n",
            "pair:  despite impressive performance deep neural networks exhibit striking failures distribution inputs one core idea adversarial example research reveal neural network errors distribution shifts decompose errors two complementary sources sensitivity invariance show deep networks sensitive task irrelevant changes input well known epsilon adversarial examples also invariant wide range task relevant changes thus making vast regions input space vulnerable adversarial attacks show excessive invariance occurs across various tasks architecture types mnist imagenet one manipulate class specific content almost image without changing hidden activations identify insufficiency standard cross entropy loss reason failures extend objective based information theoretic analysis encourages model consider task dependent features decision provides first approach tailored explicitly overcome excessive invariance resulting vulnerabilities\n",
            "output sentence:  show deep networks sensitive task irrelevant changes input also invariant wide range task relevant changes thus making vast regions regions adversarial input regions \n",
            "\n",
            "{'rouge-1': {'r': 0.10204081632653061, 'p': 0.9090909090909091, 'f': 0.18348623671744804}, 'rouge-2': {'r': 0.04065040650406504, 'p': 0.5, 'f': 0.07518796853411727}, 'rouge-l': {'r': 0.04081632653061224, 'p': 0.36363636363636365, 'f': 0.07339449359818201}}\n",
            "pair:  ai systems garner widespread public acceptance must develop methods capable explaining decisions black box models neural networks work identify two issues current explanatory methods first show two prevalent perspectives explanations feature additivity feature selection lead fundamentally different instance wise explanations literature explainers different perspectives currently directly compared despite distinct explanation goals second issue current post hoc explainers thoroughly validated simple models linear regression applied real world neural networks explainers commonly evaluated assumption learned models behave reasonably however neural networks often rely unreasonable correlations even producing correct decisions introduce verification framework explanatory methods feature selection perspective framework based non trivial neural network architecture trained real world task able provide guarantees inner workings validate efficacy evaluation showing failure modes current explainers aim framework provide publicly available shelf evaluation feature selection perspective explanations needed\n",
            "output sentence:  evaluation framework based real world neural network post hoc explanatory methods \n",
            "\n",
            "{'rouge-1': {'r': 0.12698412698412698, 'p': 0.8, 'f': 0.21917807982735973}, 'rouge-2': {'r': 0.04938271604938271, 'p': 0.4, 'f': 0.08791208595580248}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.6, 'f': 0.16438355927941453}}\n",
            "pair:  introduce mtlab new algorithm learning multiple related tasks strong theoretical guarantees key idea perform learning sequentially data tasks without interruptions restarts task boundaries predictors individual tasks derived process additional online batch conversion step learning across task boundaries mtlab achieves sublinear regret true risks number tasks lifelong learning setting leads improved generalization bound converges total number samples across observed tasks instead number examples per tasks number tasks independently time widely applicable handle finite sets tasks common multi task learning well stochastic task sequences studied lifelong learning\n",
            "output sentence:  new algorithm online multi task learning learns without restarts task borders \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.6666666666666666, 'f': 0.24999999695312503}, 'rouge-2': {'r': 0.09195402298850575, 'p': 0.5, 'f': 0.15533980320105573}, 'rouge-l': {'r': 0.13846153846153847, 'p': 0.6, 'f': 0.224999996953125}}\n",
            "pair:  paper describe implicit autoencoder iae generative autoencoder generative path recognition path parametrized implicit distributions use two generative adversarial networks define reconstruction regularization cost functions implicit autoencoder derive learning rules based maximum likelihood learning using implicit distributions allows us learn expressive posterior conditional likelihood distributions autoencoder learning expressive conditional likelihood distribution enables latent code capture abstract high level information data remaining information captured implicit conditional likelihood distribution example show implicit autoencoders disentangle global local information perform deterministic stochastic reconstructions images show implicit autoencoders disentangle discrete underlying factors variation continuous factors unsupervised fashion perform clustering semi supervised learning\n",
            "output sentence:  propose generative autoencoder learn expressive posterior conditional likelihood distributions using implicit distributions train model using new formulation elbo \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.8, 'f': 0.2758620661117717}, 'rouge-2': {'r': 0.08888888888888889, 'p': 0.5333333333333333, 'f': 0.15238094993197282}, 'rouge-l': {'r': 0.1527777777777778, 'p': 0.7333333333333333, 'f': 0.25287356036464526}}\n",
            "pair:  answering questions require multi hop reasoning web scale necessitates retrieving multiple evidence documents one often little lexical semantic relationship question paper introduces new graph based recurrent retrieval approach learns retrieve reasoning paths wikipedia graph answer multi hop open domain questions retriever model trains recurrent neural network learns sequentially retrieve evidence paragraphs reasoning path conditioning previously retrieved documents reader model ranks reasoning paths extracts answer span included best reasoning path experimental results show state art results three open domain qa datasets showcasing effectiveness robustness method notably method achieves significant improvement hotpotqa outperforming previous best model points\n",
            "output sentence:  graph based recurrent retriever learns retrieve reasoning paths wikipedia graph outperforms recent state art hotpotqa points \n",
            "\n",
            "{'rouge-1': {'r': 0.12941176470588237, 'p': 0.8461538461538461, 'f': 0.22448979361724286}, 'rouge-2': {'r': 0.0660377358490566, 'p': 0.5833333333333334, 'f': 0.11864406596954902}, 'rouge-l': {'r': 0.08235294117647059, 'p': 0.5384615384615384, 'f': 0.14285714055601834}}\n",
            "pair:  despite advances deep learning artificial neural networks learn way humans today neural networks learn multiple tasks trained jointly cannot maintain performance learnt tasks tasks presented one time phenomenon called catastrophic forgetting fundamental challenge overcome neural networks learn continually incoming data work derive inspiration human memory develop architecture capable learning continuously sequentially incoming tasks averting catastrophic forgetting specifically model consists dual memory architecture emulate complementary learning systems hippocampus neocortex human brain maintains consolidated long term memory via generative replay past experiences substantiate claim replay generative ii show benefits generative replay dual memory via experiments iii demonstrate improved performance retention even small models low capacity architecture displays many important characteristics human memory provides insights connection sleep learning humans\n",
            "output sentence:  dual memory architecture inspired human brain learn sequentially incoming tasks averting catastrophic forgetting \n",
            "\n",
            "{'rouge-1': {'r': 0.171875, 'p': 0.9166666666666666, 'f': 0.2894736815512466}, 'rouge-2': {'r': 0.0963855421686747, 'p': 0.5714285714285714, 'f': 0.16494845113827192}, 'rouge-l': {'r': 0.15625, 'p': 0.8333333333333334, 'f': 0.26315789207756235}}\n",
            "pair:  multi agent systems complex interacting behaviors arise due high correlations among agents however previous work modeling multi agent interactions demonstrations primarily constrained assuming independence among policies reward structures paper cast multi agent interactions modeling problem multi agent imitation learning framework explicit modeling correlated policies approximating opponents policies recover agents policies regenerate similar interactions consequently develop decentralized adversarial imitation learning algorithm correlated policies codail allows decentralized training execution various experiments demonstrate codail better regenerate complex interactions close demonstrators outperforms state art multi agent imitation learning methods code available url https github com apexrl codail\n",
            "output sentence:  modeling complex multi agent interactions multi agent imitation learning framework explicit modeling policies approximating approximating policies \n",
            "\n",
            "{'rouge-1': {'r': 0.045454545454545456, 'p': 0.5714285714285714, 'f': 0.08421052495069253}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03409090909090909, 'p': 0.42857142857142855, 'f': 0.06315789337174517}}\n",
            "pair:  deep neural networks dnns attained surprising achievement last decade due advantages automatic feature learning freedom expressiveness however interpretability remains mysterious dnns complex combinations linear nonlinear transformations even though many models proposed explore interpretability dnns several challenges remain unsolved lack interpretability quantity measures dnns lack theory stability dnns difficulty solve nonconvex dnn problems interpretability constraints address challenges simultaneously paper presents novel intrinsic interpretability evaluation framework dnns specifically four independent properties interpretability defined based existing works moreover investigate theory stability dnns important aspect interpretability prove dnns generally stable given different activation functions finally extended version deep learning alternating direction method multipliers dladmm proposed solve dnn problems interpretability constraints efficiently accurately extensive experiments several benchmark datasets validate several dnns proposed interpretability framework\n",
            "output sentence:  propose novel framework evaluate interpretability neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.6666666666666666, 'f': 0.1304347808435728}, 'rouge-2': {'r': 0.00980392156862745, 'p': 0.125, 'f': 0.018181816833057952}, 'rouge-l': {'r': 0.024096385542168676, 'p': 0.2222222222222222, 'f': 0.04347825910444242}}\n",
            "pair:  counterfactual regret minimization cfr successful algorithm finding approximate nash equilibria imperfect information games however cfr reliance full game tree traversals limits scalability generality therefore game state action space often abstracted simplified cfr resulting strategy mapped back full game requires extensive expert knowledge practical many games outside poker often converges highly exploitable policies recently proposed method deep cfr applies deep learning directly cfr allowing agent intrinsically abstract generalize state space samples without requiring expert knowledge paper introduce single deep cfr sd cfr variant deep cfr lower overall approximation error avoiding training average strategy network show sd cfr attractive theoretical perspective empirically outperforms deep cfr respect exploitability one one play poker\n",
            "output sentence:  better deep reinforcement learning algorithm approximate counterfactual regret minimization \n",
            "\n",
            "{'rouge-1': {'r': 0.10294117647058823, 'p': 0.7777777777777778, 'f': 0.18181817975375275}, 'rouge-2': {'r': 0.011111111111111112, 'p': 0.125, 'f': 0.020408161765930976}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.5555555555555556, 'f': 0.1298701278057008}}\n",
            "pair:  interpretability ai agent behavior utmost importance effective human ai interaction end increasing interest characterizing generating interpretable behavior agent alternative approach guarantee agent generates interpretable behavior would design agent environment uninterpretable behaviors either prohibitively expensive unavailable agent date work umbrella goal plan recognition design exploring notion environment redesign specific instances interpretable behavior position paper scope landscape interpretable behavior environment redesign different flavors specifically focus three specific types interpretable behaviors explicability legibility predictability present general framework problem environment design instantiated achieve three interpretable behaviors also discuss specific instantiations framework correspond prior works environment design identify exciting opportunities future work\n",
            "output sentence:  present approach redesign environment uninterpretable agent behaviors minimized eliminated \n",
            "\n",
            "{'rouge-1': {'r': 0.03296703296703297, 'p': 0.21428571428571427, 'f': 0.05714285483174612}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03296703296703297, 'p': 0.21428571428571427, 'f': 0.05714285483174612}}\n",
            "pair:  study problem model extraction natural language processing adversary query access victim model attempts reconstruct local copy model assuming adversary victim model fine tune large pretrained language model bert devlin et al show adversary need real training data successfully mount attack fact attacker need even use grammatical semantically meaningful queries show random sequences words coupled task specific heuristics form effective queries model extraction diverse set nlp tasks including natural language inference question answering work thus highlights exploit made feasible shift towards transfer learning methods within nlp community query budget hundred dollars attacker extract model performs slightly worse victim model finally study two defense strategies model extraction membership classification api watermarking successful adversaries also circumvented clever ones\n",
            "output sentence:  outputs modern nlp apis nonsensical text provide strong signals model internals allowing adversaries steal apis \n",
            "\n",
            "{'rouge-1': {'r': 0.10465116279069768, 'p': 0.9, 'f': 0.1874999981336806}, 'rouge-2': {'r': 0.08247422680412371, 'p': 0.8888888888888888, 'f': 0.1509433946724813}, 'rouge-l': {'r': 0.10465116279069768, 'p': 0.9, 'f': 0.1874999981336806}}\n",
            "pair:  propose robust bayesian deep learning algorithm infer complex posteriors latent variables inspired dropout popular tool regularization model ensemble assign sparse priors weights deep neural networks dnn order achieve automatic dropout avoid fitting alternatively sampling posterior distribution stochastic gradient markov chain monte carlo sg mcmc optimizing latent variables via stochastic approximation sa trajectory target weights proved converge true posterior distribution conditioned optimal latent variables ensures stronger regularization fitted parameter space accurate uncertainty quantification decisive variables simulations large small regressions showcase robustness method applied models latent variables additionally application convolutional neural networks cnn leads state art performance mnist fashion mnist datasets improved resistance adversarial attacks\n",
            "output sentence:  robust bayesian deep learning algorithm infer complex posteriors latent variables \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.7857142857142857, 'f': 0.2749999971125}, 'rouge-2': {'r': 0.13513513513513514, 'p': 0.7692307692307693, 'f': 0.22988505492931696}, 'rouge-l': {'r': 0.16666666666666666, 'p': 0.7857142857142857, 'f': 0.2749999971125}}\n",
            "pair:  propose new output layer deep neural networks permits use logged contextual bandit feedback training contextual bandit feedback available huge quantities logs search engines recommender systems little cost opening path training deep networks orders magnitude data effect propose counterfactual risk minimization crm approach training deep networks using equivariant empirical risk estimator variance regularization banditnet show resulting objective decomposed way allows stochastic gradient descent sgd training empirically demonstrate effectiveness method showing deep networks resnets particular trained object recognition without conventionally labeled images\n",
            "output sentence:  paper proposes new output layer deep networks permits use logged contextual bandit feedback training \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.5, 'f': 0.16216215944485027}, 'rouge-2': {'r': 0.036585365853658534, 'p': 0.2727272727272727, 'f': 0.06451612694646787}, 'rouge-l': {'r': 0.0967741935483871, 'p': 0.5, 'f': 0.16216215944485027}}\n",
            "pair:  comparing inferences diverse candidate models essential part model checking escaping local optima enable efficient comparison introduce amortized variational inference framework perform fast reliable posterior estimation across models architecture parameter encoder ape extends encoder neural network common amortized inference take data feature vector model parameter vector input ape thus reduces posterior inference across unseen data models single forward pass experiments comparing candidate topic models synthetic data product reviews parameter encoder yields comparable posteriors expensive methods far less time especially encoder architecture designed model aware fashion\n",
            "output sentence:  develop vaes encoder takes model parameter vector input rapid inference many models \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.6666666666666666, 'f': 0.19047618802721092}, 'rouge-2': {'r': 0.04854368932038835, 'p': 0.45454545454545453, 'f': 0.08771929650200062}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5833333333333334, 'f': 0.1666666642176871}}\n",
            "pair:  existing public face image datasets strongly biased toward caucasian faces races latino significantly underrepresented models trained datasets suffer inconsistent classification accuracy limits applicability face analytic systems non white race groups mitigate race bias problem datasets constructed novel face image dataset containing images balanced race define race groups white black indian east asian southeast asian middle eastern latino images collected yfcc flickr dataset labeled race gender age groups evaluations performed existing face attribute datasets well novel image datasets measure generalization performance find model trained dataset substantially accurate novel datasets accuracy consistent across race gender groups also compare several commercial computer vision apis report balanced accuracy across gender race age groups\n",
            "output sentence:  new face image dataset balanced race gender age used bias measurement mitigation \n",
            "\n",
            "{'rouge-1': {'r': 0.03389830508474576, 'p': 0.2857142857142857, 'f': 0.06060605870982558}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.16666666666666666, 'f': 0.028169012537195087}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.2857142857142857, 'f': 0.06060605870982558}}\n",
            "pair:  framework efficient bayesian inference probabilistic programs introduced embedding sampler inside variational posterior approximation strength lies ease implementation automatically tuning sampler parameters speed mixing time several strategies approximate evidence lower bound elbo computation introduced including rewriting elbo objective experimental evidence shown performing experiments unconditional vae density estimation tasks solving influence diagram high dimensional space conditional variational autoencoder cvae deep bayes classifier state space models time series data\n",
            "output sentence:  embed sg mcmc samplers inside variational approximation \n",
            "\n",
            "{'rouge-1': {'r': 0.09859154929577464, 'p': 0.3888888888888889, 'f': 0.15730336755965163}, 'rouge-2': {'r': 0.022988505747126436, 'p': 0.11764705882352941, 'f': 0.03846153572670138}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.3333333333333333, 'f': 0.13483145744729208}}\n",
            "pair:  paper explore meta learning shot text classification meta learning shown strong performance computer vision low level patterns transferable across learning tasks however directly applying approach text challenging lexical features highly informative one task maybe insignificant another thus rather learning solely words model also leverages distributional signatures encode pertinent word occurrence patterns model trained within meta learning framework map signatures attention scores used weight lexical representations words demonstrate model consistently outperforms prototypical networks learned lexical knowledge snell et al shot text classification relation classification significant margin across six benchmark datasets average shot classification\n",
            "output sentence:  meta learning methods used vision directly applied nlp perform worse nearest neighbors new classes better distributional signatures signatures \n",
            "\n",
            "{'rouge-1': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.4444444444444444, 'f': 0.09411764516539796}}\n",
            "pair:  central goal study primate visual cortex hierarchical models object recognition understanding single units trade invariance versus sensitivity image transformations example deep networks visual cortex substantial variation layer layer unit unit degree translation invariance provide theoretical insight variation consequences encoding deep network critical insight comes fact rectification simultaneously decreases response variance correlation across responses transformed stimuli naturally inducing positive relationship invariance dynamic range invariant input units tend drive network sensitive small image transformations discuss consequences relationship ai deep nets naturally weight invariant units sensitive units strengthened training perhaps contributing generalization performance results predict signature relationship invariance dynamic range tested future neurophysiological studies\n",
            "output sentence:  rectification deep neural networks naturally leads favor invariant representation \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.6666666666666666, 'f': 0.1791044752862553}, 'rouge-2': {'r': 0.04225352112676056, 'p': 0.3, 'f': 0.07407407190976992}, 'rouge-l': {'r': 0.08620689655172414, 'p': 0.5555555555555556, 'f': 0.14925372901759862}}\n",
            "pair:  search space key consideration neural architecture search recently xie et al found randomly generated networks distribution perform similarly suggest search random graph distributions instead graphs propose graphon new search space graphon limit cauchy sequence graphs scale free probabilistic distribution graphs different number vertices drawn property enables us perform nas using fast low capacity models scale found models necessary develop algorithm nas space graphons empirically demonstrate find stage wise graphs outperform densenet baselines imagenet\n",
            "output sentence:  graphon good search space neural architecture search empirically produces good networks \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.75, 'f': 0.1935483848491155}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.42857142857142855, 'f': 0.08219177908800905}, 'rouge-l': {'r': 0.09259259259259259, 'p': 0.625, 'f': 0.1612903203329865}}\n",
            "pair:  community detection graphs central importance graph mining machine learning network science detecting overlapping communities especially challenging remains open problem motivated success graph based deep learning graph related tasks study applicability framework overlapping community detection propose probabilistic model overlapping community detection based graph neural network architecture despite simplicity model outperforms existing approaches community recovery task large margin moreover due inductive formulation proposed model able perform sample community detection nodes present training time\n",
            "output sentence:  detecting overlapping communities graphs using graph neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07142857142857142, 'p': 0.5555555555555556, 'f': 0.12658227646210546}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.125, 'f': 0.020618555187586464}, 'rouge-l': {'r': 0.05714285714285714, 'p': 0.4444444444444444, 'f': 0.10126582076590292}}\n",
            "pair:  two major paradigms white box adversarial attacks attempt impose input perturbations first paradigm called fix perturbation attack crafts adversarial samples within given perturbation level second paradigm called zero confidence attack finds smallest perturbation needed cause misclassification also known margin input feature former paradigm well resolved latter existing zero confidence attacks either introduce significant approximation errors time consuming therefore propose marginattack zero confidence attack framework able compute margin improved accuracy efficiency experiments show marginattack able compute smaller margin state art zero confidence attacks matches state art fix perturbation attacks addition runs significantly faster carlini wagner attack currently accurate zero confidence attack algorithm\n",
            "output sentence:  paper introduces marginattack stronger faster zero confidence adversarial attack \n",
            "\n",
            "{'rouge-1': {'r': 0.0891089108910891, 'p': 0.75, 'f': 0.15929203349988255}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.3333333333333333, 'f': 0.058823527802768207}, 'rouge-l': {'r': 0.07920792079207921, 'p': 0.6666666666666666, 'f': 0.14159291845563476}}\n",
            "pair:  recent development natural language processing nlp achieved great success using large pre trained models hundreds millions parameters however models suffer heavy model size high latency cannot directly deploy resource limited mobile devices paper propose mobilebert compressing accelerating popular bert model like bert mobilebert task agnostic universally applied various downstream nlp tasks via fine tuning mobilebert slimmed version bert large augmented bottleneck structures carefully designed balance self attentions feed forward networks train mobilebert use bottom top progressive scheme transfer intrinsic knowledge specially designed inverted bottleneck bert large teacher empirical studies show mobilebert smaller faster original bert base achieving competitive results well known nlp benchmarks natural language inference tasks glue mobilebert achieves glue score performance degradation ms latency pixel phone squad question answering task mobilebert achieves dev score higher bert base\n",
            "output sentence:  develop task agnosticlly compressed bert smaller faster bert base achieving competitive performance glue squad \n",
            "\n",
            "{'rouge-1': {'r': 0.12962962962962962, 'p': 0.6363636363636364, 'f': 0.21538461257278108}, 'rouge-2': {'r': 0.04918032786885246, 'p': 0.3, 'f': 0.08450703983336647}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.2727272727272727, 'f': 0.09230768949585806}}\n",
            "pair:  rectified linear units relus become preferred activation function artificial neural networks paper consider problem learning generative model presence nonlinearity modeled relu functions given set signal vectors mathbf mathbb dots aim learn network parameters times matrix model mathbf mathrm relu mathbf mathbf mathbf mathbb random bias vector mathbf mathbb arbitrary unknown latent vectors show possible recover column space within error frobenius norm certain conditions distribution mathbf\n",
            "output sentence:  show possible recover parameters layer relu generative model looking samples generated \n",
            "\n",
            "{'rouge-1': {'r': 0.10714285714285714, 'p': 0.5, 'f': 0.17647058532871976}, 'rouge-2': {'r': 0.03, 'p': 0.17647058823529413, 'f': 0.05128204879830533}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.3333333333333333, 'f': 0.11764705591695508}}\n",
            "pair:  checkerboard phenomenon one well known visual artifacts computer vision field origins solutions checkerboard artifacts pixel space studied long time effects gradient space rarely investigated paper revisit checkerboard artifacts gradient space turn weak point network architecture explore image agnostic property gradient checkerboard artifacts propose simple yet effective defense method utilizing artifacts introduce defense module dubbed artificial checkerboard enhancer ace induces adversarial attacks designated pixels enables model deflect attacks shifting single pixel image remarkable defense rate provide extensive experiments support effectiveness work various attack scenarios using state art attack methods furthermore show ace even applicable large scale datasets including imagenet dataset easily transferred various pretrained networks\n",
            "output sentence:  propose novel aritificial checkerboard enhancer ace module guides attacks pre specified pixel space successfully defends simple padding operation \n",
            "\n",
            "{'rouge-1': {'r': 0.12121212121212122, 'p': 0.7272727272727273, 'f': 0.2077922053432282}, 'rouge-2': {'r': 0.024691358024691357, 'p': 0.2, 'f': 0.04395604199975857}, 'rouge-l': {'r': 0.10606060606060606, 'p': 0.6363636363636364, 'f': 0.18181817936920225}}\n",
            "pair:  current work neural code synthesis consists increasingly sophisticated architectures trained highly simplified domain specific languages using uniform sampling across program space languages training comparison program space like language vast extremely sparsely populated terms useful functionalities requires far intelligent approach corpus generation effective training use genetic programming approach using iteratively retrained discriminator produce population suitable labelled training data neural code synthesis architecture demonstrate use discriminator based training corpus generator trained using unlabelled problem specifications classic programming example format greatly improves network performance compared current uniform sampling techniques\n",
            "output sentence:  way generate training corpora neural code synthesis using discriminator trained unlabelled data \n",
            "\n",
            "{'rouge-1': {'r': 0.13793103448275862, 'p': 1.0, 'f': 0.24242424029384757}, 'rouge-2': {'r': 0.10144927536231885, 'p': 1.0, 'f': 0.18421052464335183}, 'rouge-l': {'r': 0.13793103448275862, 'p': 1.0, 'f': 0.24242424029384757}}\n",
            "pair:  cost annotating training data traditionally bottleneck supervised learning approaches problem exacerbated supervised learning applied number correlated tasks simultaneously since amount labels required scales number tasks mitigate concern propose active multitask learning algorithm achieves knowledge transfer tasks approach forms called committee task jointly makes decisions directly shares data across similar tasks approach reduces number queries needed training maintaining high accuracy test data empirical results benchmark datasets show significant improvements accuracy number query requests\n",
            "output sentence:  propose active multitask learning algorithm achieves knowledge transfer tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.6666666666666666, 'f': 0.12371133852269103}, 'rouge-2': {'r': 0.010101010101010102, 'p': 0.125, 'f': 0.018691587401519886}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.4444444444444444, 'f': 0.08247422512062921}}\n",
            "pair:  recent progress physics based character animation shown impressive breakthroughs human motion synthesis imitating motion capture data via deep reinforcement learning however results mostly demonstrated imitating single distinct motion pattern generalize interactive tasks require flexible motion patterns due varying human object spatial configurations bridge gap focus one class interactive tasks sitting onto chair propose hierarchical reinforcement learning framework relies collection subtask controllers trained imitate simple reusable mocap motions meta controller trained execute subtasks properly complete main task experimentally demonstrate strength approach different single level hierarchical baselines also show approach applied motion prediction given image input video highlight found https youtu xwu wzz ip\n",
            "output sentence:  synthesizing human motions interactive tasks using mocap data hierarchical rl \n",
            "\n",
            "{'rouge-1': {'r': 0.12195121951219512, 'p': 0.7692307692307693, 'f': 0.21052631342714684}, 'rouge-2': {'r': 0.019801980198019802, 'p': 0.16666666666666666, 'f': 0.03539822819014812}, 'rouge-l': {'r': 0.08536585365853659, 'p': 0.5384615384615384, 'f': 0.14736841869030476}}\n",
            "pair:  work provides automatic machine learning automl modelling architecture called autostacker autostacker improves prediction accuracy machine learning baselines utilizing innovative hierarchical stacking architecture efficient parameter search algorithm neither prior domain knowledge data feature preprocessing needed significantly reduce time automl naturally inspired algorithm parallel hill climbing phc parallelizing phc autostacker provide candidate pipelines sufficient prediction accuracy within short amount time pipelines used starting point human experts build focusing modelling process autostacker breaks tradition following fixed order pipelines exploring single model pipeline also innovative combinations structures show experiment section autostacker achieves significantly better performance terms test accuracy time cost comparing human initial trials recent popular automl system\n",
            "output sentence:  automate machine learning system efficient search algorithm innovative structure provide better model baselines \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.8, 'f': 0.22222221983024693}, 'rouge-2': {'r': 0.06944444444444445, 'p': 0.5555555555555556, 'f': 0.12345678814814819}, 'rouge-l': {'r': 0.12903225806451613, 'p': 0.8, 'f': 0.22222221983024693}}\n",
            "pair:  describe techniques training high quality image denoising models require single instances corrupted images training data inspired recent technique removes need supervision image pairs employing networks blind spot receptive field address two shortcomings inefficient training poor final denoising performance achieved novel blind spot convolutional network architecture allows efficient self supervised training well application bayesian distribution prediction output colors together bring self supervised model par fully supervised deep learning techniques terms quality training speed case gaussian noise\n",
            "output sentence:  learn high quality denoising using single instances corrupted images training data \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.4166666666666667, 'f': 0.12987012723899483}, 'rouge-2': {'r': 0.012987012987012988, 'p': 0.09090909090909091, 'f': 0.02272727053977294}, 'rouge-l': {'r': 0.06153846153846154, 'p': 0.3333333333333333, 'f': 0.10389610126496887}}\n",
            "pair:  ability generalize quickly observations crucial intelligent systems paper introduce apl algorithm approximates probability distributions remembering surprising observations encountered past observations recalled external memory module processed decoder network combine information different memory slots generalize beyond direct recall show algorithm perform well state art baselines shot classification benchmarks smaller memory footprint addition memory compression allows scale thousands unknown labels finally introduce meta learning reasoning task challenging direct classification setting apl able generalize fewer one example per class via deductive reasoning\n",
            "output sentence:  introduce model generalizes quickly observations storing surprising information attending relevant data time point \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.8333333333333334, 'f': 0.24691357772290812}, 'rouge-2': {'r': 0.07142857142857142, 'p': 0.5454545454545454, 'f': 0.1263157874260388}, 'rouge-l': {'r': 0.13043478260869565, 'p': 0.75, 'f': 0.22222221969821673}}\n",
            "pair:  natural language inference nli task requires agent determine logical relationship natural language premise natural language hypothesis introduce interactive inference network iin novel class neural network architectures able achieve high level understanding sentence pair hierarchically extracting semantic features interaction space show interaction tensor attention weight contains semantic information solve natural language inference denser interaction tensor contains richer semantic information one instance architecture densely interactive inference network diin demonstrates state art performance large scale nli copora large scale nli alike corpus noteworthy diin achieve greater error reduction challenging multi genre nli multinli dataset respect strongest published system\n",
            "output sentence:  show multi channel attention weight contains semantic feature solve natural language inference task \n",
            "\n",
            "{'rouge-1': {'r': 0.175, 'p': 0.7368421052631579, 'f': 0.28282827972655855}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.3888888888888889, 'f': 0.11382113571286938}, 'rouge-l': {'r': 0.1125, 'p': 0.47368421052631576, 'f': 0.18181817871645756}}\n",
            "pair:  non autoregressive machine translation nat systems predict sequence output tokens parallel achieving substantial improvements generation speed compared autoregressive models existing nat models usually rely technique knowledge distillation creates training data pretrained autoregressive model better performance knowledge distillation empirically useful leading large gains accuracy nat models reason success yet unclear paper first design systematic experiments investigate knowledge distillation crucial nat training find knowledge distillation reduce complexity data sets help nat model variations output data furthermore strong correlation observed capacity nat model optimal complexity distilled data best translation quality based findings propose several approaches alter complexity data sets improve performance nat models achieve state art performance nat based models close gap autoregressive baseline wmt en de benchmark\n",
            "output sentence:  systematically examine knowledge distillation crucial training non autoregressive translation nat models propose methods improve distilled data best match capacity nat \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5714285714285714, 'f': 0.1951219483878644}, 'rouge-2': {'r': 0.0375, 'p': 0.23076923076923078, 'f': 0.06451612662735585}, 'rouge-l': {'r': 0.08823529411764706, 'p': 0.42857142857142855, 'f': 0.14634146058298636}}\n",
            "pair:  likelihood based generative models promising resource detect distribution ood inputs could compromise robustness reliability machine learning system however likelihoods derived models shown problematic detecting certain types inputs significantly differ training data paper pose problem due excessive influence input complexity generative models likelihoods report set experiments supporting hypothesis use estimate input complexity derive efficient parameter free ood score seen likelihood ratio akin bayesian model comparison find score perform comparably even better existing ood detection approaches wide range data sets models model sizes complexity estimates\n",
            "output sentence:  pose generative models likelihoods excessively influenced input complexity propose way compensate detecting distribution inputs \n",
            "\n",
            "{'rouge-1': {'r': 0.028169014084507043, 'p': 0.2857142857142857, 'f': 0.05128204964825777}, 'rouge-2': {'r': 0.012658227848101266, 'p': 0.16666666666666666, 'f': 0.023529410452595226}, 'rouge-l': {'r': 0.028169014084507043, 'p': 0.2857142857142857, 'f': 0.05128204964825777}}\n",
            "pair:  introduce lyceum high performance computational ecosystem robotlearning lyceum built top julia programming language themujoco physics simulator combining ease use high level program ming language performance native lyceum xfaster compared popular abstractions like openai sgymand deep mind sdm control substantially reduces training time various inforcement learning algorithms also fast enough support real timemodel predictive control physics simulators lyceum straightfor ward api supports parallel computation across multiple cores machines code base tutorials demonstration videos found https sites google com view lyceum anon\n",
            "output sentence:  high performance robotics simulation algorithm development framework \n",
            "\n",
            "{'rouge-1': {'r': 0.05357142857142857, 'p': 0.6666666666666666, 'f': 0.09917355234205315}, 'rouge-2': {'r': 0.016260162601626018, 'p': 0.25, 'f': 0.0305343499982519}, 'rouge-l': {'r': 0.044642857142857144, 'p': 0.5555555555555556, 'f': 0.08264462672221845}}\n",
            "pair:  field medical diagnostics contains wealth challenges closely resemble classical machine learning problems practical constraints however complicate translation endpoints naively classical architectures many tasks radiology example largely problems multi label classification wherein medical images interpreted indicate multiple present suspected pathologies clinical settings drive necessity high accuracy simultaneously across multitude pathological outcomes greatly limit utility tools consider subset issue exacerbated general scarcity training data maximizes need extract clinically relevant features available samples ideally without use pre trained models may carry forward undesirable biases tangentially related tasks present evaluate partial solution constraints using lstms leverage interdependencies among target labels predicting pathologic patterns chest rays establish state art results largest publicly available chest ray dataset nih without pre training furthermore propose discuss alternative evaluation metrics relevance clinical practice\n",
            "output sentence:  present state art results using neural networks diagnose chest rays \n",
            "\n",
            "{'rouge-1': {'r': 0.13157894736842105, 'p': 0.7692307692307693, 'f': 0.224719098628961}, 'rouge-2': {'r': 0.05319148936170213, 'p': 0.38461538461538464, 'f': 0.09345794179054943}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.5384615384615384, 'f': 0.15730336829188238}}\n",
            "pair:  develop reinforcement learning based search assistant assist users set actions sequence interactions enable realize intent approach caters subjective search user seeking digital assets images fundamentally different tasks objective limited search modalities labeled conversational data generally available search tasks training agent human interactions time consuming propose stochastic virtual user impersonates real user used sample user behavior efficiently train agent accelerates bootstrapping agent develop algorithm based context preserving architecture enables agent provide contextual assistance user compare agent learning evaluate performance average rewards state values obtains virtual user validation episodes experiments show agent learns achieve higher rewards better states\n",
            "output sentence:  reinforcement learning based conversational search assistant provides contextual assistance subjective search like digital assets \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.5555555555555556, 'f': 0.1666666641166667}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.125, 'f': 0.02739725832238708}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}}\n",
            "pair:  develop new algorithm imitation learning single expert demonstration contrast many previous one shot imitation learning approaches algorithm assume access one expert demonstration training phase instead leverage exploration policy acquire unsupervised trajectories used train encoder context aware imitation policy optimization procedures encoder imitation learner exploration policy tightly linked linking creates feedback loop wherein exploration policy collects new demonstrations challenge imitation learner encoder attempts help imitation policy best abilities evaluate algorithm mujoco robotics tasks\n",
            "output sentence:  unsupervised self imitation algorithm capable inference single expert demonstration \n",
            "\n",
            "{'rouge-1': {'r': 0.06741573033707865, 'p': 0.4, 'f': 0.11538461291605034}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.13333333333333333, 'f': 0.03149606090892196}, 'rouge-l': {'r': 0.056179775280898875, 'p': 0.3333333333333333, 'f': 0.09615384368528113}}\n",
            "pair:  prohibitive energy cost running high performance convolutional neural networks cnns limiting deployment resource constrained platforms including mobile wearable devices propose cnn energy aware dynamic routing called energynet achieves adaptive complexity inference based inputs leading overall reduction run time energy cost without noticeably losing even improving accuracy achieved proposing energy loss captures computational data movement costs combine accuracy oriented loss learn dynamic routing policy skipping certain layers networks optimizes hybrid loss empirical results demonstrate compared baseline cnns energynetcan trim energy cost inference cifar tiny imagenet testing sets respectively maintaining testing accuracies encouraging observe energy awareness might serve training regularization even improve prediction accuracy models achieve higher top testing accuracy baseline cifar saving energy higher top testing accuracy tiny imagenet saving energy respectively\n",
            "output sentence:  paper proposes new cnn model combines energy cost dynamic routing strategy enable adaptive energy efficient inference \n",
            "\n",
            "{'rouge-1': {'r': 0.059322033898305086, 'p': 0.4666666666666667, 'f': 0.10526315589349317}, 'rouge-2': {'r': 0.02112676056338028, 'p': 0.21428571428571427, 'f': 0.038461536827744974}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.4, 'f': 0.09022556190853077}}\n",
            "pair:  large amount interest past particularly recently relative advantage different families universal function approximators instance neural networks polynomials rational functions etc however current research focused almost exclusively understanding problem worst case setting characterizing best infty approximation box sometimes even adversarially constructed data distribution setting many classical tools approximation theory effectively used however typical applications expect data high dimensional structured would important approximate desired function well relevant part domain small manifold real input data actually lies moreover even within domain desired quality approximation may uniform instance classification problems approximation needs accurate near decision boundary issues best knowledge remain unexplored mind analyze performance neural networks polynomial kernels natural regression setting data enjoys sparse latent structure labels depend simple way latent variables give almost tight theoretical analysis performance neural networks polynomials problem well verify theory simulations results involve new complex analytic techniques may independent interest show substantial qualitative differences known worst case setting\n",
            "output sentence:  beyond worst case analysis representational power relu nets polynomial kernels particular presence sparse latent structure \n",
            "\n",
            "{'rouge-1': {'r': 0.10576923076923077, 'p': 0.9166666666666666, 'f': 0.18965517055885853}, 'rouge-2': {'r': 0.06451612903225806, 'p': 0.7272727272727273, 'f': 0.11851851702167354}, 'rouge-l': {'r': 0.08653846153846154, 'p': 0.75, 'f': 0.15517241193816883}}\n",
            "pair:  reduce memory footprint run time latency techniques neural net work pruning binarization explored separately however un clear combine best two worlds get extremely small efficient models paper first time define filter level pruning problem binary neural networks cannot solved simply migrating existing structural pruning methods full precision models novel learning based approach proposed prune filters main subsidiary network frame work main network responsible learning representative features optimize prediction performance subsidiary component works filter selector main network avoid gradient mismatch training subsidiary component propose layer wise bottom scheme also provide theoretical experimental comparison learning based greedy rule based methods finally empirically demonstrate effectiveness approach applied several binary models including binarizednin vgg resnet various image classification datasets bi nary resnet imagenet use filters achieve slightly better test error original model\n",
            "output sentence:  define filter level pruning problem binary neural networks first time propose method solve \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.7692307692307693, 'f': 0.24390243635633554}, 'rouge-2': {'r': 0.0449438202247191, 'p': 0.3333333333333333, 'f': 0.0792079186981669}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.6153846153846154, 'f': 0.19512194855145748}}\n",
            "pair:  graphs fundamental data structures required model many important real world data knowledge graphs physical social interactions molecules proteins paper study problem learning generative models graphs dataset graphs interest learning models used generate samples similar properties ones dataset models useful lot applications drug discovery knowledge graph construction task learning generative models graphs however unique challenges particular handle symmetries graphs ordering elements generation process important issues propose generic graph neural net based model capable generating arbitrary graph study performance graph generation tasks compared baselines exploit domain knowledge discuss potential issues open problems generative models going forward\n",
            "output sentence:  study graph generation problem propose powerful deep generative model capable generating arbitrary graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.6666666666666666, 'f': 0.27586206568370986}, 'rouge-2': {'r': 0.054945054945054944, 'p': 0.2777777777777778, 'f': 0.09174311650871148}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.4444444444444444, 'f': 0.1839080426952042}}\n",
            "pair:  information bottleneck ib problem tackles issue obtaining relevant compressed representations random variable task predicting defined constrained optimization problem maximizes information representation task ensuring minimum level compression achieved practical reasons problem usually solved maximizing ib lagrangian many values lagrange multiplier therefore drawing ib curve curve maximal given selecting representation desired predictability compression known deterministic function ib curve cannot explored lagrangians proposed tackle problem squared ib lagrangian paper present general family lagrangians allow exploration ib curve scenarios ii prove lagrangians used one one mapping lagrange multiplier desired compression rate known ib curve shapes hence freeing burden solving optimization problem many values lagrange multiplier\n",
            "output sentence:  introduce general family lagrangians allow exploring ib curve scenarios used ib curve known one optimize directly performance compression level directly \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.625, 'f': 0.11363636198347109}, 'rouge-2': {'r': 0.010752688172043012, 'p': 0.14285714285714285, 'f': 0.019999998698000086}, 'rouge-l': {'r': 0.05, 'p': 0.5, 'f': 0.09090908925619837}}\n",
            "pair:  click rate ctr prediction critical task industrial applications especially online social commerce applications challenging find proper way automatically discover effective cross features ctr tasks propose novel model ctr tasks called deep neural networks encoder enhanced factorization machine deepenfm instead learning cross features directly deepenfm adopts transformer encoder backbone align feature embeddings clues fields embeddings generated encoder beneficial feature interactions particularly deepenfm utilizes bilinear approach generate different similarity functions respect different field pairs furthermore max pooling method makes deepenfm feasible capture supplementary suppressing information among different attention heads model validated criteo avazu datasets achieves state art performance\n",
            "output sentence:  dnn encoder enhanced fm bilinear attention max pooling ctr \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.6428571428571429, 'f': 0.19565217133270324}, 'rouge-2': {'r': 0.02727272727272727, 'p': 0.21428571428571427, 'f': 0.04838709477107188}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.5, 'f': 0.15217391046313802}}\n",
            "pair:  derive new intrinsic social motivation multi agent reinforcement learning marl agents rewarded causal influence another agent actions causal influence assessed using counterfactual reasoning reward depend observing another agent reward function thus realistic approach marl taken previous work show causal influence reward related maximizing mutual information agents actions test approach challenging social dilemma environments consistently leads enhanced cooperation agents higher collective reward moreover find rewarding influence lead agents develop emergent communication protocols therefore also employ influence train agents use explicit communication channel find leads effective communication higher collective reward finally show influence computed equipping agent internal model predicts actions agents allows social influence reward computed without use centralised controller represents significantly general scalable inductive bias marl independent agents\n",
            "output sentence:  reward agents causal influence actions agents show gives rise better cooperation meaningful emergent communication protocols \n",
            "\n",
            "{'rouge-1': {'r': 0.061855670103092786, 'p': 0.5454545454545454, 'f': 0.11111110928155009}, 'rouge-2': {'r': 0.01652892561983471, 'p': 0.2, 'f': 0.030534349734864002}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.36363636363636365, 'f': 0.07407407224451307}}\n",
            "pair:  distributed optimization essential training large models large datasets multiple approaches proposed reduce communication overhead distributed training synchronizing performing multiple local sgd steps decentralized methods using gossip algorithms decouple communications among workers although methods run faster allreduce based methods use blocking communication every update resulting models may less accurate number updates inspired bmuf method chen huo propose slow momentum slomo framework workers periodically synchronize perform momentum update multiple iterations base optimization algorithm experiments image classification machine translation tasks demonstrate slomo consistently yields improvements optimization generalization performance relative base optimizer even additional overhead amortized many updates slomo runtime par base optimizer provide theoretical convergence guarantees showing slomo converges stationary point smooth non convex losses since bmuf particular instance slomo framework results also correspond first theoretical convergence guarantees bmuf\n",
            "output sentence:  slowmo improves optimization generalization performance communication efficient decentralized algorithms without sacrificing speed \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.5882352941176471, 'f': 0.25974025629954467}, 'rouge-2': {'r': 0.06172839506172839, 'p': 0.29411764705882354, 'f': 0.10204081345897552}, 'rouge-l': {'r': 0.1, 'p': 0.35294117647058826, 'f': 0.1558441524034408}}\n",
            "pair:  log linear models models widely used machine learning particular ubiquitous deep learning architectures form softmax exact inference learning requires linear time done approximately sub linear time strong concentrations guarantees work present lsh softmax method perform sub linear learning inference softmax layer deep learning setting method relies popular locality sensitive hashing build well concentrated gradient estimator using nearest neighbors uniform samples also present inference scheme sub linear time lsh softmax using gumbel distribution language modeling show recurrent neural networks trained lsh softmax perform par computing exact softmax requiring sub linear computations\n",
            "output sentence:  present lsh softmax softmax approximation layer sub linear learning inference strong theoretical guarantees showcase applicability efficiency evaluating real task \n",
            "\n",
            "{'rouge-1': {'r': 0.06779661016949153, 'p': 0.3333333333333333, 'f': 0.11267605352906178}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.09090909090909091, 'f': 0.025974023525046613}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.25, 'f': 0.08450703944455476}}\n",
            "pair:  edge intelligence especially binary neural network bnn attracted considerable attention artificial intelligence community recently bnns significantly reduce computational cost model size memory footprint however still performance gap successful full precision neural network relu activation bnns argue accuracy drop bnns due geometry analyze behaviour full precision neural network relu activation compare binarized counterpart comparison suggests random bias initialization remedy activation saturation full precision networks leads us towards improved bnn training numerical experiments confirm geometric intuition\n",
            "output sentence:  improve saturating activations sigmoid tanh htanh etc binarized neural network bias initialization \n",
            "\n",
            "{'rouge-1': {'r': 0.14084507042253522, 'p': 1.0, 'f': 0.2469135780826094}, 'rouge-2': {'r': 0.10112359550561797, 'p': 1.0, 'f': 0.18367346771970014}, 'rouge-l': {'r': 0.14084507042253522, 'p': 1.0, 'f': 0.2469135780826094}}\n",
            "pair:  deep neural networks use deeper broader structures achieve better performance consequently use increasingly gpu memory well however limited gpu memory restricts many potential designs neural networks paper propose reinforcement learning based variable swapping recomputation algorithm reduce memory cost without sacrificing accuracy models variable swapping transfer variables cpu gpu memory reduce variables stored gpu memory recomputation trade time space removing feature maps forward propagation forward functions executed get feature maps reuse however automatically decide variables swapped recomputed remains challenging problem address issue propose use deep network dqn make plans combining variable swapping recomputation results outperform several well known benchmarks\n",
            "output sentence:  propose reinforcement learning based variable swapping recomputation algorithm reduce memory cost \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.5, 'f': 0.13698629900544196}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.07936507936507936, 'p': 0.5, 'f': 0.13698629900544196}}\n",
            "pair:  capturing high level structure audio waveforms challenging single second audio spans tens thousands timesteps long range dependencies difficult model directly time domain show tractably modelled two dimensional time frequency representations spectrograms leveraging representational advantage conjunction highly expressive probabilistic model multiscale generation procedure design model capable generating high fidelity audio samples capture structure timescales time domain models yet achieve demonstrate model captures longer range dependencies time domain models wavenet across diverse set unconditional generation tasks including single speaker speech generation multi speaker speech generation music generation\n",
            "output sentence:  introduce autoregressive generative model spectrograms demonstrate applications speech music generation \n",
            "\n",
            "{'rouge-1': {'r': 0.08163265306122448, 'p': 0.47058823529411764, 'f': 0.13913043226313804}, 'rouge-2': {'r': 0.03571428571428571, 'p': 0.25, 'f': 0.062499997812500076}, 'rouge-l': {'r': 0.061224489795918366, 'p': 0.35294117647058826, 'f': 0.10434782356748588}}\n",
            "pair:  designing rewards reinforcement learning rl challenging needs convey desired task efficient optimize easy compute latter particularly problematic applying rl robotics detecting whether desired configuration reached might require considerable supervision instrumentation furthermore often interested able reach wide range configurations hence setting different reward every time might unpractical methods like hindsight experience replay recently shown promise learn policies able reach many goals without need reward unfortunately without tricks like resetting points along trajectory might take long time discover reach certain areas state space work investigate different approaches incorporate demonstrations drastically speed convergence policy able reach goal also surpassing performance agent trained imitation learning algorithms furthermore method used trajectories without expert actions available leverage kinestetic third person demonstration\n",
            "output sentence:  tackle goal conditioned tasks combining hindsight experience replay imitation learning algorithms showing faster convergence first higher second \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.8181818181818182, 'f': 0.20224718884484283}, 'rouge-2': {'r': 0.04854368932038835, 'p': 0.5, 'f': 0.08849557360795679}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.6363636363636364, 'f': 0.15730336862012376}}\n",
            "pair:  widely recognized adversarial examples easily crafted fool deep networks mainly root locally non linear behavior nearby input examples applying mixup training provides effective mechanism improve generalization performance model robustness adversarial perturbations introduces globally linear behavior training examples however previous work mixup trained models passively defend adversarial attacks inference directly classifying inputs induced global linearity well exploited namely since locality adversarial perturbations would efficient actively break locality via globality model predictions inspired simple geometric intuition develop inference principle named mixup inference mi mixup trained models mi mixups input random clean samples shrink transfer equivalent perturbation input adversarial experiments cifar cifar demonstrate mi improve adversarial robustness models trained mixup variants\n",
            "output sentence:  exploit global linearity mixup trained models inference break locality adversarial perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.20270270270270271, 'p': 1.0, 'f': 0.3370786488827169}, 'rouge-2': {'r': 0.1320754716981132, 'p': 1.0, 'f': 0.23333333127222225}, 'rouge-l': {'r': 0.20270270270270271, 'p': 1.0, 'f': 0.3370786488827169}}\n",
            "pair:  mixup data augmentation scheme pairs training samples corresponding labels mixed using linear coefficients without label mixing mixup becomes conventional scheme input samples moved original labels retained samples preferentially moved direction classes iffalse typically clustered input space fi refer method directional adversarial training dat show two mild conditions mixup asymptotically convergences subset dat define untied mixup umixup superset mixup wherein training labels mixed different linear coefficients corresponding samples show mild conditions untied mixup converges entire class dat schemes motivated understanding umixup generalization mixup form adversarial training experiment different datasets loss functions show umixup provides improved performance mixup short present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup\n",
            "output sentence:  present novel interpretation mixup belonging class highly analogous adversarial training basis introduce simple generalization outperforms mixup \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.5, 'f': 0.1555555529283951}, 'rouge-2': {'r': 0.010526315789473684, 'p': 0.07142857142857142, 'f': 0.0183486216143425}, 'rouge-l': {'r': 0.07894736842105263, 'p': 0.42857142857142855, 'f': 0.1333333307061729}}\n",
            "pair:  provide novel perspective forward pass block layers deep network particular show forward pass standard dropout layer followed linear layer non linear activation equivalent optimizing convex objective single iteration tau nice proximal stochastic gradient method show replacing standard bernoulli dropout additive dropout equivalent optimizing convex objective variance reduced proximal method expressing fully connected convolutional layers special cases high order tensor product unify underlying convex optimization problem tensor setting derive formula lipschitz constant used determine optimal step size proximal methods conduct experiments standard convolutional networks applied cifar cifar datasets show replacing block layers multiple iterations corresponding solver step size set via consistently improves classification accuracy\n",
            "output sentence:  framework links deep network layers stochastic optimization algorithms used improve model accuracy inform network design \n",
            "\n",
            "{'rouge-1': {'r': 0.15053763440860216, 'p': 0.7777777777777778, 'f': 0.25225224953494035}, 'rouge-2': {'r': 0.0546875, 'p': 0.3888888888888889, 'f': 0.09589040879714773}, 'rouge-l': {'r': 0.12903225806451613, 'p': 0.6666666666666666, 'f': 0.21621621349890432}}\n",
            "pair:  zero shot learning zsl classification task classes referred unseen classes labeled training images instead side information description seen unseen classes often form semantic descriptive attributes lack training images set classes restricts use standard classification techniques losses including popular cross entropy loss key step tackling zsl problem bridging visual semantic space via learning nonlinear embedding well established approach obtain semantic representation visual information perform classification semantic space paper propose novel architecture casting zsl fully connected neural network cross entropy loss embed visual space semantic space training order introduce unseen visual information network utilize soft labeling based semantic similarities seen unseen classes best knowledge similarity based soft labeling explored cross modal transfer zsl evaluate proposed model five benchmark datasets zero shot learning awa awa apy sun cub datasets show despite simplicity approach achieves state art performance generalized zsl setting datasets outperforms state art datasets\n",
            "output sentence:  use cross entropy loss zero shot learning soft labeling unseen classes simple effective solution achieves state performance performance performance \n",
            "\n",
            "{'rouge-1': {'r': 0.06578947368421052, 'p': 0.5555555555555556, 'f': 0.11764705693010381}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.4444444444444444, 'f': 0.09411764516539796}}\n",
            "pair:  central goal study primate visual cortex hierarchical models object recognition understanding single units trade invariance versus sensitivity image transformations example deep networks visual cortex substantial variation layer layer unit unit degree translation invariance provide theoretical insight variation consequences encoding deep network critical insight comes fact rectification simultaneously decreases response variance correlation across responses transformed stimuli naturally inducing positive relationship invariance dynamic range invariant input units tend drive network sensitive small image transformations discuss consequences relationship ai deep nets naturally weight invariant units sensitive units strengthened training perhaps contributing generalization performance results predict signature relationship invariance dynamic range tested future neurophysiological studies\n",
            "output sentence:  rectification deep neural networks naturally leads favor invariant representation \n",
            "\n",
            "{'rouge-1': {'r': 0.1232876712328767, 'p': 0.6428571428571429, 'f': 0.2068965490236491}, 'rouge-2': {'r': 0.043478260869565216, 'p': 0.26666666666666666, 'f': 0.0747663527294961}, 'rouge-l': {'r': 0.1095890410958904, 'p': 0.5714285714285714, 'f': 0.1839080432765227}}\n",
            "pair:  rate medical questions asked online significantly exceeds capacity qualified people answer leaving many questions unanswered inadequately answered many questions unique reliable identification similar questions would enable efficient effective question answering schema many research efforts focused problem general question similarity approaches generalize well medical domain medical expertise often required determine semantic similarity paper show semi supervised approach pre training neural network medical question answer pairs particularly useful intermediate task ultimate goal determining medical question similarity pre training tasks yield accuracy task model achieves accuracy number training examples accuracy much smaller training set accuracy full corpus medical question answer data used\n",
            "output sentence:  show question answer matching particularly good pre training task question similarity release dataset medical question similarity \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.5555555555555556, 'f': 0.11235954874384549}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.125, 'f': 0.01999999852800011}, 'rouge-l': {'r': 0.05, 'p': 0.4444444444444444, 'f': 0.08988763863148597}}\n",
            "pair:  point clouds form lagrangian representation allow powerful flexible applications large number computational disciplines propose novel deep learning method learn stable temporally coherent feature spaces points clouds change time identify set inherent problems approaches without knowledge time dimension inferred solutions exhibit strong flickering easy solutions suppress flickering result undesirable local minima manifest halo structures propose novel temporal loss function takes account higher time derivatives point positions encourages mingling prevent aforementioned halos combine techniques super resolution method truncation approach flexibly adapt size generated positions show method works large deforming point sets different sources demonstrate flexibility approach\n",
            "output sentence:  propose generative neural network approach temporally coherent point clouds \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.7058823529411765, 'f': 0.2790697642698756}, 'rouge-2': {'r': 0.0375, 'p': 0.1875, 'f': 0.06249999722222234}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.35294117647058826, 'f': 0.13953488054894544}}\n",
            "pair:  deep learning computer vision depends mainly source supervision photo realistic simulators generate large scale automatically labeled synthetic data introduce domain gap negatively impacting performance propose new unsupervised domain adaptation algorithm called spigan relying simulator privileged information pi generative adversarial networks gan use internal data simulator pi training target task network experimentally evaluate approach semantic segmentation train networks real world cityscapes vistas datasets using unlabeled real world images synthetic labeled data buffer depth pi synthia dataset method improves adaptation state art unsupervised domain adaptation techniques\n",
            "output sentence:  unsupervised sim real domain adaptation method semantic segmentation using privileged information simulator gan based image translation translation \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}}\n",
            "pair:  deep learning nlp represents word single point single mode region semantic space existing multi mode word embeddings cannot represent longer word sequences like phrases sentences introduce phrase representation also applicable sentences phrase distinct set multi mode codebook embeddings capture different semantic facets phrase meaning codebook embeddings viewed cluster centers summarize distribution possibly co occurring words pre trained word embedding space propose end end trainable neural model directly predicts set cluster centers input text sequence phrase sentence test time find per phrase sentence codebook embeddings provide interpretable semantic representation also outperform strong baselines large margin tasks benchmark datasets unsupervised phrase similarity sentence similarity hypernym detection extractive summarization\n",
            "output sentence:  propose unsupervised way learn multiple embeddings sentences phrases \n",
            "\n",
            "{'rouge-1': {'r': 0.11235955056179775, 'p': 0.6666666666666666, 'f': 0.19230768983912724}, 'rouge-2': {'r': 0.056074766355140186, 'p': 0.4, 'f': 0.09836065358102665}, 'rouge-l': {'r': 0.0898876404494382, 'p': 0.5333333333333333, 'f': 0.15384615137758878}}\n",
            "pair:  conditional generative adversarial networks cgans finding increasingly widespread use many application domains despite outstanding progress quantitative evaluation models often involves multiple distinct metrics assess different desirable properties image quality conditional consistency intra conditioning diversity setting model benchmarking becomes challenge metric may indicate different best model paper propose frechet joint distance fjd defined frechet distance joint distributions images conditioning allowing implicitly capture aforementioned properties single metric conduct proof concept experiments controllable synthetic dataset consistently highlight benefits fjd compared currently established metrics moreover use newly introduced metric compare existing cgan based models variety conditioning modalities class labels object masks bounding boxes images text captions show fjd used promising single metric model benchmarking\n",
            "output sentence:  propose new metric evaluating conditional gans captures image quality conditional consistency intra conditioning diversity single measure \n",
            "\n",
            "{'rouge-1': {'r': 0.10416666666666667, 'p': 0.7142857142857143, 'f': 0.18181817959669425}, 'rouge-2': {'r': 0.017543859649122806, 'p': 0.16666666666666666, 'f': 0.03174603002267583}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.5714285714285714, 'f': 0.14545454323305787}}\n",
            "pair:  paper propose perform model ensembling multiclass multilabel learning setting using wasserstein barycenters optimal transport metrics wasserstein distance allow incorporating semantic side information word embeddings using barycenters find consensus models allows us balance confidence semantics finding agreement models show applications wasserstein ensembling attribute based classification multilabel learning image captioning generation results show ensembling viable alternative basic geometric arithmetic mean ensembling\n",
            "output sentence:  propose use wasserstein barycenters semantic model ensembling \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.625, 'f': 0.16949152307957485}, 'rouge-2': {'r': 0.031496062992125984, 'p': 0.26666666666666666, 'f': 0.0563380262795081}, 'rouge-l': {'r': 0.06862745098039216, 'p': 0.4375, 'f': 0.11864406545245625}}\n",
            "pair:  state art deep neural networks dnns typically tens millions parameters might fit upper levels memory hierarchy thus increasing inference time energy consumption significantly prohibiting use edge devices mobile phones compression dnn models therefore become active area research recently emph connection pruning emerging one successful strategies natural approach prune connections dnns via ell regularization recent empirical investigations suggested work well context dnn compression work revisit simple strategy analyze rigorously show emph stationary point ell regularized layerwise pruning objective number non zero elements bounded number penalized prediction logits regardless strength regularization successful pruning highly relies accurate optimization solver trade compression speed distortion prediction accuracy controlled strength regularization theoretical results thus suggest ell pruning could successful provided use accurate optimization solver corroborate experiments show simple ell regularization adamax cumulative solver gives pruning ratio competitive state art\n",
            "output sentence:  revisit simple idea pruning connections dnns ell regularization achieving state art results multiple datasets theoretic guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.07317073170731707, 'p': 0.4, 'f': 0.1237113375916676}, 'rouge-2': {'r': 0.021052631578947368, 'p': 0.14285714285714285, 'f': 0.03669724546755337}, 'rouge-l': {'r': 0.07317073170731707, 'p': 0.4, 'f': 0.1237113375916676}}\n",
            "pair:  many partially observable scenarios reinforcement learning rl agents must rely long term memory order learn optimal policy demonstrate using techniques nlp supervised learning fails rl tasks due stochasticity environment exploration utilizing insights limitations traditional memory methods rl propose amrl class models learn better policies greater sample efficiency resilient noisy inputs specifically models use standard memory module summarize short term context aggregate prior states standard model without respect order show provides advantages terms gradient decay signal noise ratio time evaluating minecraft maze environments test long term memory find model improves average return baseline number parameters stronger baseline far parameters\n",
            "output sentence:  deep rl order invariant functions used conjunction standard memory modules improve gradient decay resilience resilience \n",
            "\n",
            "{'rouge-1': {'r': 0.05813953488372093, 'p': 0.7142857142857143, 'f': 0.10752688032836168}, 'rouge-2': {'r': 0.01904761904761905, 'p': 0.3333333333333333, 'f': 0.0360360350133918}, 'rouge-l': {'r': 0.03488372093023256, 'p': 0.42857142857142855, 'f': 0.06451612764018964}}\n",
            "pair:  many real world learning scenarios features acquirable cost constrained budget paper propose novel approach cost sensitive feature acquisition prediction time suggested method acquires features incrementally based context aware feature value function formulate problem reinforcement learning paradigm introduce reward function based utility feature specifically mc dropout sampling used measure expected variations model uncertainty used feature value function furthermore suggest sharing representations class predictor value function estimator networks suggested approach completely online readily applicable stream learning setups solution evaluated three different datasets including well known mnist dataset benchmark well two cost sensitive datasets yahoo learning rank dataset medical domain diabetes classification according results proposed method able efficiently acquire features make accurate predictions\n",
            "output sentence:  online algorithm cost aware feature acquisition prediction \n",
            "\n",
            "{'rouge-1': {'r': 0.11842105263157894, 'p': 0.9, 'f': 0.20930232352623043}, 'rouge-2': {'r': 0.034482758620689655, 'p': 0.3333333333333333, 'f': 0.0624999983007813}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.7, 'f': 0.16279069561925366}}\n",
            "pair:  semi supervised learning ssl approaches influential framework usage unlabeled data sufficient amount labeled data available course training ssl methods based convolutional neural networks cnns recently provided successful results standard benchmark tasks image classification work consider general setting ssl problem labeled unlabeled data come underlying probability distribution propose new approach adopts optimal transport ot technique serving metric similarity discrete empirical probability measures provide pseudo labels unlabeled data used conjunction initial labeled data train cnn model ssl manner evaluated compared proposed method state art ssl algorithms standard datasets demonstrate superiority effectiveness ssl algorithm\n",
            "output sentence:  propose new algorithm based optimal transport train cnn ssl fashion \n",
            "\n",
            "{'rouge-1': {'r': 0.06349206349206349, 'p': 0.36363636363636365, 'f': 0.10810810557706362}, 'rouge-2': {'r': 0.013888888888888888, 'p': 0.09090909090909091, 'f': 0.024096383242851138}, 'rouge-l': {'r': 0.06349206349206349, 'p': 0.36363636363636365, 'f': 0.10810810557706362}}\n",
            "pair:  paper concerns dictionary learning sparse coding fundamental representation learning problem show subgradient descent algorithm random initialization recover orthogonal dictionaries natural nonsmooth nonconvex minimization formulation problem mild statistical assumption data contrast previous provable methods require either expensive computation delicate initialization schemes analysis develops several tools characterizing landscapes nonsmooth functions might independent interest provable training deep networks nonsmooth activations relu among applications preliminary synthetic real experiments corroborate analysis show algorithm works well empirically recovering orthogonal dictionaries\n",
            "output sentence:  efficient dictionary learning minimization via novel analysis non convex non smooth geometry \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.7692307692307693, 'f': 0.24390243635633554}, 'rouge-2': {'r': 0.0449438202247191, 'p': 0.3333333333333333, 'f': 0.0792079186981669}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.6153846153846154, 'f': 0.19512194855145748}}\n",
            "pair:  graphs fundamental data structures required model many important real world data knowledge graphs physical social interactions molecules proteins paper study problem learning generative models graphs dataset graphs interest learning models used generate samples similar properties ones dataset models useful lot applications drug discovery knowledge graph construction task learning generative models graphs however unique challenges particular handle symmetries graphs ordering elements generation process important issues propose generic graph neural net based model capable generating arbitrary graph study performance graph generation tasks compared baselines exploit domain knowledge discuss potential issues open problems generative models going forward\n",
            "output sentence:  study graph generation problem propose powerful deep generative model capable generating arbitrary graphs \n",
            "\n",
            "{'rouge-1': {'r': 0.05813953488372093, 'p': 0.7142857142857143, 'f': 0.10752688032836168}, 'rouge-2': {'r': 0.01904761904761905, 'p': 0.3333333333333333, 'f': 0.0360360350133918}, 'rouge-l': {'r': 0.03488372093023256, 'p': 0.42857142857142855, 'f': 0.06451612764018964}}\n",
            "pair:  many real world learning scenarios features acquirable cost constrained budget paper propose novel approach cost sensitive feature acquisition prediction time suggested method acquires features incrementally based context aware feature value function formulate problem reinforcement learning paradigm introduce reward function based utility feature specifically mc dropout sampling used measure expected variations model uncertainty used feature value function furthermore suggest sharing representations class predictor value function estimator networks suggested approach completely online readily applicable stream learning setups solution evaluated three different datasets including well known mnist dataset benchmark well two cost sensitive datasets yahoo learning rank dataset medical domain diabetes classification according results proposed method able efficiently acquire features make accurate predictions\n",
            "output sentence:  online algorithm cost aware feature acquisition prediction \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.6363636363636364, 'f': 0.1590909069034091}, 'rouge-2': {'r': 0.038834951456310676, 'p': 0.36363636363636365, 'f': 0.07017543685287785}, 'rouge-l': {'r': 0.07792207792207792, 'p': 0.5454545454545454, 'f': 0.13636363417613637}}\n",
            "pair:  investigate multi task learning approaches use shared feature representation tasks better understand transfer task information study architecture shared module tasks separate output module task study theory setting linear relu activated models key observation whether tasks data well aligned significantly affect performance multi task learning show misalignment task data cause negative transfer hurt performance provide sufficient conditions positive transfer inspired theoretical insights show aligning tasks embedding layers leads performance gains multi task training transfer learning glue benchmark sentiment analysis tasks example obtained glue score average improvement glue tasks bert large using alignment method also design svd based task weighting scheme show improves robustness multi task training multi label image dataset\n",
            "output sentence:  theoretical study multi task learning practical implications improving multi task training transfer learning \n",
            "\n",
            "{'rouge-1': {'r': 0.14925373134328357, 'p': 0.9090909090909091, 'f': 0.2564102539875082}, 'rouge-2': {'r': 0.1, 'p': 0.8, 'f': 0.17777777580246917}, 'rouge-l': {'r': 0.13432835820895522, 'p': 0.8181818181818182, 'f': 0.23076922834648256}}\n",
            "pair:  learnability different neural architectures characterized directly computable measures data complexity paper reframe problem architecture selection understanding data determines expressive generalizable architectures suited data beyond inductive bias suggesting algebraic topology measure data complexity show power network express topological complexity dataset decision boundary strictly limiting factor ability generalize provide first empirical characterization topological capacity neural networks empirical analysis shows every level dataset complexity neural networks exhibit topological phase transitions stratification observation allowed us connect existing theory empirically driven conjectures choice architectures single hidden layer neural networks\n",
            "output sentence:  show learnability different neural architectures characterized directly computable measures data complexity \n",
            "\n",
            "{'rouge-1': {'r': 0.03669724770642202, 'p': 0.4444444444444444, 'f': 0.06779660876041371}, 'rouge-2': {'r': 0.0070921985815602835, 'p': 0.1111111111111111, 'f': 0.013333332205333429}, 'rouge-l': {'r': 0.027522935779816515, 'p': 0.3333333333333333, 'f': 0.050847456218040835}}\n",
            "pair:  artificial intelligence ai becomes integral part life development explainable ai embodied decision making process ai robotic agent becomes imperative robotic teammate ability generate explanations explain behavior one key requirements explainable agency prior work explanation generation focuses supporting reasoning behind robot behavior approaches however fail consider mental workload needed understand received explanation words human teammate expected understand explanation provided often task execution matter much information presented explanation work argue explanation especially complex ones made online fashion execution helps spread information explained thus reducing mental workload humans however challenge different parts explanation dependent must taken account generating online explanations end general formulation online explanation generation presented along three different implementations satisfying different online properties base explanation generation method model reconciliation setting introduced prior work approaches evaluated human subjects standard planning competition ipc domain using nasa task load index tlx well simulation ten different problems across two ipc domains\n",
            "output sentence:  introduce online explanation consider cognitive requirement human understanding generated explanation agent \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.4166666666666667, 'f': 0.15873015564625853}, 'rouge-2': {'r': 0.01818181818181818, 'p': 0.09090909090909091, 'f': 0.03030302752525278}, 'rouge-l': {'r': 0.09803921568627451, 'p': 0.4166666666666667, 'f': 0.15873015564625853}}\n",
            "pair:  deep neural networks proven powerful tool many recognition classification tasks stability properties still well understood past image classifiers shown vulnerable called adversarial attacks created additively perturbing correctly classified image paper propose adef algorithm construct different kind adversarial attack created iteratively applying small deformations image found gradient descent step demonstrate results mnist convolutional neural networks imagenet inception resnet\n",
            "output sentence:  propose new efficient algorithm construct adversarial examples means deformations rather additive perturbations \n",
            "\n",
            "{'rouge-1': {'r': 0.0784313725490196, 'p': 0.5333333333333333, 'f': 0.1367521345167653}, 'rouge-2': {'r': 0.024793388429752067, 'p': 0.2, 'f': 0.04411764509623711}, 'rouge-l': {'r': 0.049019607843137254, 'p': 0.3333333333333333, 'f': 0.08547008323471406}}\n",
            "pair:  present simple nearest neighbor nn approach synthesizes high frequency photorealistic images incomplete signal low resolution image surface normal map edges current state art deep generative models designed conditional image synthesis lack two important things unable generate large set diverse outputs due mode collapse problem interpretable making difficult control synthesized output demonstrate nn approaches potentially address limitations suffer accuracy small datasets design simple pipeline combines best worlds first stage uses convolutional neural network cnn map input overly smoothed image second stage uses pixel wise nearest neighbor method map smoothed output multiple high quality high frequency outputs controllable manner importantly pixel wise matching allows method compose novel high frequency content cutting pasting pixels different training exemplars demonstrate approach various input modalities various domains ranging human faces pets shoes handbags\n",
            "output sentence:  pixel wise nearest neighbors used generating multiple images incomplete priors low res images surface normals edges etc \n",
            "\n",
            "{'rouge-1': {'r': 0.05357142857142857, 'p': 0.6666666666666666, 'f': 0.09917355234205315}, 'rouge-2': {'r': 0.016260162601626018, 'p': 0.25, 'f': 0.0305343499982519}, 'rouge-l': {'r': 0.044642857142857144, 'p': 0.5555555555555556, 'f': 0.08264462672221845}}\n",
            "pair:  field medical diagnostics contains wealth challenges closely resemble classical machine learning problems practical constraints however complicate translation endpoints naively classical architectures many tasks radiology example largely problems multi label classification wherein medical images interpreted indicate multiple present suspected pathologies clinical settings drive necessity high accuracy simultaneously across multitude pathological outcomes greatly limit utility tools consider subset issue exacerbated general scarcity training data maximizes need extract clinically relevant features available samples ideally without use pre trained models may carry forward undesirable biases tangentially related tasks present evaluate partial solution constraints using lstms leverage interdependencies among target labels predicting pathologic patterns chest rays establish state art results largest publicly available chest ray dataset nih without pre training furthermore propose discuss alternative evaluation metrics relevance clinical practice\n",
            "output sentence:  present state art results using neural networks diagnose chest rays \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.4444444444444444, 'f': 0.2051282015779093}, 'rouge-2': {'r': 0.013333333333333334, 'p': 0.058823529411764705, 'f': 0.021739127422023104}, 'rouge-l': {'r': 0.11666666666666667, 'p': 0.3888888888888889, 'f': 0.1794871759368837}}\n",
            "pair:  introduce es maml new framework solving model agnostic meta learning maml problem based evolution strategies es existing algorithms maml based policy gradients incur significant difficulties attempting estimate second derivatives using backpropagation stochastic policies show es applied maml obtain algorithm avoids problem estimating second derivatives also conceptually simple easy implement moreover es maml handle new types nonsmooth adaptation operators techniques improving performance estimation es methods become applicable show empirically es maml competitive existing methods often yields better adaptation fewer queries\n",
            "output sentence:  provide new framework maml es blackbox setting show allows deterministic linear policies better exploration non differentiable adaptation operators \n",
            "\n",
            "{'rouge-1': {'r': 0.04, 'p': 0.5, 'f': 0.07407407270233198}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.2, 'f': 0.02061855572324375}, 'rouge-l': {'r': 0.04, 'p': 0.5, 'f': 0.07407407270233198}}\n",
            "pair:  single cell rna sequencing scrnaseq technology enables quantifying gene expression profiles individual cells within cancer dimension reduction methods commonly used cell clustering analysis visualization data current dimension reduction methods tend overly eliminate expression variations correspond less dominating characteristics fail find homogenious properties cancer development paper proposed new clustering analysis method scrnaseq data namely bbsc via implementing binarization gene expression profile frequency changes boolean matrix factorization low rank representation expression matrix recovered bbsc increase resolution identifying distinct cell types functions application bbsc two cancer scrnaseq data successfully discovered homogeneous heterogeneous cancer cell clusters finding showed potential preventing cancer progression\n",
            "output sentence:  finding shed lights preventing cancer progression \n",
            "\n",
            "{'rouge-1': {'r': 0.1643835616438356, 'p': 0.9230769230769231, 'f': 0.2790697648756085}, 'rouge-2': {'r': 0.11702127659574468, 'p': 0.9166666666666666, 'f': 0.20754716780348884}, 'rouge-l': {'r': 0.1643835616438356, 'p': 0.9230769230769231, 'f': 0.2790697648756085}}\n",
            "pair:  work study generalization neural networks gradient based meta learning analyzing various properties objective landscapes experimentally demonstrate meta training progresses meta test solutions obtained adapting meta train solution model new tasks via steps gradient based fine tuning become flatter lower loss away meta train solution also show meta test solutions become flatter even generalization starts degrade thus providing experimental evidence correlation generalization flat minima paradigm gradient based meta leaning furthermore provide empirical evidence generalization new tasks correlated coherence adaptation trajectories parameter space measured average cosine similarity task specific trajectory directions starting meta train solution also show coherence meta test gradients measured average inner product task specific gradient vectors evaluated meta train solution also correlated generalization\n",
            "output sentence:  study generalization neural networks gradient based meta learning analyzing various properties objective landscape \n",
            "\n",
            "{'rouge-1': {'r': 0.08, 'p': 0.5, 'f': 0.1379310321046374}, 'rouge-2': {'r': 0.021052631578947368, 'p': 0.15384615384615385, 'f': 0.037037034919410274}, 'rouge-l': {'r': 0.08, 'p': 0.5, 'f': 0.1379310321046374}}\n",
            "pair:  paper present approach learn recomposable motor primitives across large scale diverse manipulation demonstrations current approaches decomposing demonstrations primitives often assume manually defined primitives bypass difficulty discovering primitives hand approaches primitive discovery put restrictive assumptions complexity primitive limit applicability narrow tasks approach attempts circumvent challenges jointly learning underlying motor primitives recomposing primitives form original demonstration constraints parsimony primitive decomposition simplicity given primitive able learn diverse set motor primitives well coherent latent representation primitives demonstrate qualitatively quantitatively learned primitives capture semantically meaningful aspects demonstration allows us compose primitives hierarchical reinforcement learning setup efficiently solve robotic manipulation tasks like reaching pushing\n",
            "output sentence:  learn space motor primitives unannotated robot demonstrations show primitives semantically meaningful composed new robot tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.4444444444444444, 'f': 0.2051282015779093}, 'rouge-2': {'r': 0.013333333333333334, 'p': 0.058823529411764705, 'f': 0.021739127422023104}, 'rouge-l': {'r': 0.11666666666666667, 'p': 0.3888888888888889, 'f': 0.1794871759368837}}\n",
            "pair:  introduce es maml new framework solving model agnostic meta learning maml problem based evolution strategies es existing algorithms maml based policy gradients incur significant difficulties attempting estimate second derivatives using backpropagation stochastic policies show es applied maml obtain algorithm avoids problem estimating second derivatives also conceptually simple easy implement moreover es maml handle new types nonsmooth adaptation operators techniques improving performance estimation es methods become applicable show empirically es maml competitive existing methods often yields better adaptation fewer queries\n",
            "output sentence:  provide new framework maml es blackbox setting show allows deterministic linear policies better exploration non differentiable adaptation operators \n",
            "\n",
            "{'rouge-1': {'r': 0.20238095238095238, 'p': 0.8947368421052632, 'f': 0.33009708436987467}, 'rouge-2': {'r': 0.09615384615384616, 'p': 0.5263157894736842, 'f': 0.16260162340405845}, 'rouge-l': {'r': 0.17857142857142858, 'p': 0.7894736842105263, 'f': 0.29126213291356395}}\n",
            "pair:  recent powerful pre trained language models achieved remarkable performance popular datasets reading comprehension time introduce challenging datasets push development field towards comprehensive reasoning text paper introduce new reading comprehension dataset requiring logical reasoning reclor extracted standardized graduate admission examinations earlier studies suggest human annotated datasets usually contain biases often exploited models achieve high accuracy without truly understanding text order comprehensively evaluate logical reasoning ability models reclor propose identify biased data points separate easy set rest hard set empirical results show state art models outstanding ability capture biases contained dataset high accuracy easy set however struggle hard set poor performance near random guess indicating research needed essentially enhance logical reasoning ability current models\n",
            "output sentence:  introduce reclor reading comprehension dataset requiring logical reasoning find current state art models struggle real logical reasoning poor performance near random guess \n",
            "\n",
            "{'rouge-1': {'r': 0.04878048780487805, 'p': 0.5, 'f': 0.08888888726913582}, 'rouge-2': {'r': 0.02040816326530612, 'p': 0.25, 'f': 0.0377358476610894}, 'rouge-l': {'r': 0.036585365853658534, 'p': 0.375, 'f': 0.06666666504691361}}\n",
            "pair:  multi task learning promises use less data parameters time training separate single task models realizing benefits practice challenging particular difficult define suitable architecture enough capacity support many tasks requiring excessive compute individual task difficult trade offs deciding allocate parameters layers across large set tasks address propose method automatically searching multi task architectures accounts resource constraints define parameterization feature sharing strategies effective coverage sampling architectures also present method quick evaluation architectures feature distillation together contributions allow us quickly optimize parameter efficient multi task models benchmark visual decathlon demonstrating automatically search identify architectures effectively make trade offs task resource requirements maintaining high level final performance\n",
            "output sentence:  automatic search multi task architectures reduce per task feature use \n",
            "\n",
            "{'rouge-1': {'r': 0.07547169811320754, 'p': 0.3333333333333333, 'f': 0.12307692006627227}, 'rouge-2': {'r': 0.015873015873015872, 'p': 0.09090909090909091, 'f': 0.027027024495982706}, 'rouge-l': {'r': 0.07547169811320754, 'p': 0.3333333333333333, 'f': 0.12307692006627227}}\n",
            "pair:  inference models replace optimization based inference procedure learned model fundamental advancing bayesian deep learning notable example variational auto encoders vaes paper propose iterative inference models learn optimize variational lower bound repeatedly encoding gradients approach generalizes vaes certain conditions viewing vaes context iterative inference provide insight several recent empirical findings demonstrate inference optimization capabilities iterative inference models explore unique aspects models show outperform standard inference models typical benchmark data sets\n",
            "output sentence:  propose new class inference models iteratively encode gradients estimate approximate posterior distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.10989010989010989, 'p': 0.5882352941176471, 'f': 0.18518518253257893}, 'rouge-2': {'r': 0.05042016806722689, 'p': 0.35294117647058826, 'f': 0.0882352919301471}, 'rouge-l': {'r': 0.08791208791208792, 'p': 0.47058823529411764, 'f': 0.1481481454955419}}\n",
            "pair:  deep generative models variational autoencoder vae generative adversarial network gan play increasingly important role machine learning computer vision however two fundamental issues hindering real world applications difficulty conducting variational inference vae functional absence encoding real world samples gan paper propose novel algorithm named latently invertible autoencoder lia address two issues one framework invertible network inverse mapping symmetrically embedded latent space vae thus partial encoder first transforms input feature vectors distribution feature vectors reshaped fit prior invertible network decoder proceeds reverse order encoder composite mappings two stage stochasticity free training scheme designed train lia via adversarial learning sense decoder lia first trained standard gan invertible network partial encoder learned autoencoder detaching invertible network lia experiments conducted ffhq face dataset three lsun datasets validate effectiveness lia inference generation\n",
            "output sentence:  new model latently invertible autoencoder proposed solve problem variational inference vae using invertible network two stage adversarial training \n",
            "\n",
            "{'rouge-1': {'r': 0.1016949152542373, 'p': 0.75, 'f': 0.1791044755090221}, 'rouge-2': {'r': 0.029850746268656716, 'p': 0.2857142857142857, 'f': 0.05405405234112497}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.375, 'f': 0.08955223670305197}}\n",
            "pair:  search engine become fundamental component various web mobile applications retrieving relevant documents massive datasets challenging search engine system especially faced verbose tail queries paper explore vector space search framework document retrieval specifically trained deep semantic matching model query document encoded low dimensional embedding model trained based bert architecture deployed fast nearest neighbor index service online serving offline online metrics demonstrate method improved retrieval performance search quality considerably particularly tail queries\n",
            "output sentence:  deep semantic framework textual search engine document retrieval \n",
            "\n",
            "{'rouge-1': {'r': 0.11494252873563218, 'p': 0.9090909090909091, 'f': 0.20408163066014165}, 'rouge-2': {'r': 0.05454545454545454, 'p': 0.6, 'f': 0.09999999847222223}, 'rouge-l': {'r': 0.10344827586206896, 'p': 0.8181818181818182, 'f': 0.18367346739483548}}\n",
            "pair:  robustness neural networks adversarial examples received great attention due security implications despite various attack approaches crafting visually imperceptible adversarial examples little developed towards comprehensive measure robustness paper provide theoretical justification converting robustness analysis local lipschitz constant estimation problem propose use extreme value theory efficient evaluation analysis yields novel robustness metric called clever short cross lipschitz extreme value network robustness proposed clever score attack agnostic computationally feasible large neural networks experimental results various networks including resnet inception mobilenet show clever aligned robustness indication measured ell ell infty norms adversarial examples powerful attacks ii defended networks using defensive distillation bounded relu indeed give better clever scores best knowledge clever first attack independent robustness metric applied neural network classifiers\n",
            "output sentence:  propose first attack independent robustness metric clever applied neural network classifier \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 1.0, 'f': 0.2121212102249771}, 'rouge-2': {'r': 0.07746478873239436, 'p': 0.8461538461538461, 'f': 0.14193548233423517}, 'rouge-l': {'r': 0.0847457627118644, 'p': 0.7142857142857143, 'f': 0.15151514961891646}}\n",
            "pair:  inspired neurophysiological discoveries navigation cells mammalian brain introduce first deep neural network architecture modeling egocentric spatial memory esm learns estimate pose agent progressively construct top global maps egocentric views spatially extended environment exploration proposed esm network model updates belief global map based local observations using recurrent neural network also augments local mapping novel external memory encode store latent representations visited places based corresponding locations egocentric coordinate enables agents perform loop closure mapping correction work contributes following aspects first proposed esm network provides accurate mapping ability vitally important embodied agents navigate goal locations experiments demonstrate functionalities esm network random walks complicated mazes comparing several competitive baselines state art simultaneous localization mapping slam algorithms secondly faithfully hypothesize functionality working mechanism navigation cells brain comprehensive analysis model suggests essential role individual modules proposed architecture demonstrates efficiency communications among modules hope work would advance research collaboration communications fields computer science computational neuroscience\n",
            "output sentence:  first deep neural network modeling egocentric spatial memory inspired neurophysiological discoveries navigation cells mammalian brain \n",
            "\n",
            "{'rouge-1': {'r': 0.12631578947368421, 'p': 0.8, 'f': 0.21818181582644633}, 'rouge-2': {'r': 0.08333333333333333, 'p': 0.7142857142857143, 'f': 0.14925372947204277}, 'rouge-l': {'r': 0.12631578947368421, 'p': 0.8, 'f': 0.21818181582644633}}\n",
            "pair:  program verification offers framework ensuring program correctness therefore systematically eliminating different classes bugs inferring loop invariants one main challenges behind automated verification real world programs often contain many loops paper present continuous logic network cln novel neural architecture automatically learning loop invariants directly program execution traces unlike existing neural networks clns learn precise explicit representations formulas satisfiability modulo theories smt loop invariants program execution traces develop new sound complete semantic mapping assigning smt formulas continuous truth values allows clns trained efficiently use clns implement new inference system loop invariants cln inv significantly outperforms existing approaches popular code inv dataset cln inv first tool solve theoretically solvable problems code inv dataset moreover cln inv takes second average problem times faster existing approaches demonstrate cln inv even learn significantly complex loop invariants ones required code inv dataset\n",
            "output sentence:  introduce continuous logic network cln novel neural architecture automatically learning loop invariants general smt formulas \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}, 'rouge-2': {'r': 0.03409090909090909, 'p': 0.25, 'f': 0.059999997888000076}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5384615384615384, 'f': 0.16470587976193776}}\n",
            "pair:  goal imitation learning il learn good policy high quality demonstrations however quality demonstrations reality diverse since easier cheaper collect demonstrations mix experts amateurs il situations challenging especially level demonstrators expertise unknown propose new il paradigm called variational imitation learning diverse quality demonstrations vild explicitly model level demonstrators expertise probabilistic graphical model estimate along reward function show naive estimation approach suitable large state action spaces fix issue using variational approach easily implemented using existing reinforcement learning methods experiments continuous control benchmarks demonstrate vild outperforms state art methods work enables scalable data efficient il realistic settings\n",
            "output sentence:  propose imitation learning method learn diverse quality demonstrations collected demonstrators different level expertise \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.46153846153846156, 'f': 0.15189873142765586}, 'rouge-2': {'r': 0.04, 'p': 0.25, 'f': 0.06896551486325811}, 'rouge-l': {'r': 0.07575757575757576, 'p': 0.38461538461538464, 'f': 0.12658227573145336}}\n",
            "pair:  engineered proteins offer potential solve many problems biomedicine energy materials science creating designs succeed difficult practice significant aspect challenge complex coupling protein sequence structure task finding viable design often referred inverse protein folding problem develop generative models protein sequences conditioned graph structured specification design target approach efficiently captures complex dependencies proteins focusing long range sequence local space framework significantly improves upon prior parametric models protein sequences given structure takes step toward rapid targeted biomolecular design aid deep generative models\n",
            "output sentence:  learn conditionally generate protein sequences given structures model captures sparse long range dependencies \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.375, 'f': 0.1071428546938776}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0625, 'p': 0.375, 'f': 0.1071428546938776}}\n",
            "pair:  key equatorial climate phenomena qbo enso never adequately explained deterministic processes spite recent research showing growing evidence predictable behavior study applies fundamental laplace tidal equations simplifying assumptions along equator coriolis force small angle approximation solutions partial differential equations highly non linear related navier stokes search approaches used fit data\n",
            "output sentence:  analytical formulation equatorial standing wave phenomena application qbo enso \n",
            "\n",
            "{'rouge-1': {'r': 0.10588235294117647, 'p': 0.9, 'f': 0.18947368232686984}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.7777777777777778, 'f': 0.12280701608956603}, 'rouge-l': {'r': 0.09411764705882353, 'p': 0.8, 'f': 0.16842105074792246}}\n",
            "pair:  weakly supervised temporal action localization previous works failed locate dense integral regions entire action due overestimation salient regions alleviate issue propose marginalized average attentional network maan suppress dominant response salient regions principled manner maan employs novel marginalized average aggregation maa module learns set latent discriminative probabilities end end fashion maa samples multiple subsets video snippet features according set latent discriminative probabilities takes expectation averaged subset features theoretically prove maa module learned latent discriminative probabilities successfully reduces difference responses salient regions others therefore maan able generate better class activation sequences identify dense integral action regions videos moreover propose fast algorithm reduce complexity constructing maa extensive experiments two large scale video datasets show maan achieves superior performance weakly supervised temporal action localization\n",
            "output sentence:  novel marginalized average attentional network weakly supervised temporal action localization \n",
            "\n",
            "{'rouge-1': {'r': 0.042105263157894736, 'p': 0.6666666666666666, 'f': 0.07920791967454172}, 'rouge-2': {'r': 0.008620689655172414, 'p': 0.2, 'f': 0.016528924827539142}, 'rouge-l': {'r': 0.042105263157894736, 'p': 0.6666666666666666, 'f': 0.07920791967454172}}\n",
            "pair:  energy based models outputs unmormalized log probability values given datasamples estimation essential variety application problems suchas sample generation denoising sample restoration outlier detection bayesianreasoning many however standard maximum likelihood training iscomputationally expensive due requirement sampling model distribution score matching potentially alleviates problem denoising score matching vincent particular convenient version however previous attemptsfailed produce models capable high quality sample synthesis believethat performed denoising score matching singlenoise scale overcome limitation instead learn energy functionusing noise scales sampled using annealed langevin dynamics andsingle step denoising jump model produced high quality samples comparableto state art techniques gans addition assigning likelihood totest data comparable previous likelihood models model set new sam ple quality baseline likelihood based models demonstrate model learns sample distribution generalize well image inpainting tasks\n",
            "output sentence:  learned energy based model score matching \n",
            "\n",
            "{'rouge-1': {'r': 0.2159090909090909, 'p': 0.9047619047619048, 'f': 0.34862385010015995}, 'rouge-2': {'r': 0.1523809523809524, 'p': 0.8, 'f': 0.255999997312}, 'rouge-l': {'r': 0.17045454545454544, 'p': 0.7142857142857143, 'f': 0.2752293546873159}}\n",
            "pair:  stochastic neural net weights used variety contexts including regularization bayesian neural nets exploration reinforcement learning evolution strategies unfortunately due large number weights examples mini batch typically share weight perturbation thereby limiting variance reduction effect large mini batches introduce flipout efficient method decorrelating gradients within mini batch implicitly sampling pseudo independent weight perturbations example empirically flipout achieves ideal linear variance reduction fully connected networks convolutional networks rnns find significant speedups training neural networks multiplicative gaussian perturbations show flipout effective regularizing lstms outperforms previous methods flipout also enables us vectorize evolution strategies experiments single gpu flipout handle throughput least cpu cores using existing methods equivalent factor cost reduction amazon web services\n",
            "output sentence:  introduce flipout efficient method decorrelating gradients computed stochastic neural net weights within mini batch implicitly sampling pseudo independent weight perturbations example \n",
            "\n",
            "{'rouge-1': {'r': 0.09615384615384616, 'p': 0.5, 'f': 0.16129031987513012}, 'rouge-2': {'r': 0.03571428571428571, 'p': 0.2222222222222222, 'f': 0.06153845915266282}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.4, 'f': 0.1290322553590011}}\n",
            "pair:  present new family objective functions term conditional entropy bottleneck ceb objectives motivated minimum necessary information mni criterion demonstrate application ceb classification tasks show ceb gives well calibrated predictions strong detection challenging distribution examples powerful whitebox adversarial examples substantial robustness adversaries finally report ceb fails learn information free datasets providing possible resolution problem generalization observed zhang et al\n",
            "output sentence:  conditional entropy bottleneck information theoretic objective function learning optimal representations \n",
            "\n",
            "{'rouge-1': {'r': 0.10810810810810811, 'p': 0.8, 'f': 0.19047618837868482}, 'rouge-2': {'r': 0.043010752688172046, 'p': 0.4444444444444444, 'f': 0.0784313709400231}, 'rouge-l': {'r': 0.06756756756756757, 'p': 0.5, 'f': 0.11904761695011341}}\n",
            "pair:  learning knowledge graph embeddings kges efficient approach knowledge graph completion conventional kges often suffer limited knowledge representation causes less accuracy especially training sparse knowledge graphs remedy present pretrain kges training framework learning better knowledgeable entity relation embeddings leveraging abundant linguistic knowledge pretrained language models specifically propose unified approach first learn entity relation representations via pretrained language models use representations initialize entity relation embeddings training kge models proposed method model agnostic sense applied variant kge models experimental results show method consistently improve results achieve state art performance using different kge models transe quate across four benchmark kg datasets link prediction triplet classification tasks\n",
            "output sentence:  propose learn knowledgeable entity relation representations bert knowledge graph embeddings \n",
            "\n",
            "{'rouge-1': {'r': 0.06481481481481481, 'p': 0.6363636363636364, 'f': 0.11764705714568183}, 'rouge-2': {'r': 0.031496062992125984, 'p': 0.4, 'f': 0.05839415923064631}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5454545454545454, 'f': 0.10084033445660619}}\n",
            "pair:  crafting adversarial examples discrete inputs like text sequences fundamentally different generating examples continuous inputs like images paper tries answer question black box setting create adversarial examples automatically effectively fool deep learning classifiers texts making imperceptible changes answer firm yes previous efforts mostly replied using gradient evidence less effective either due finding nearest neighbor word wrt meaning automatically difficult relying heavily hand crafted linguistic rules instead use monte carlo tree search mcts finding important words perturb perform homoglyph attack replacing one character selected word symbol identical shape novel algorithm call mctsbug black box extremely effective time experimental results indicate mctsbug fool deep learning classifiers success rates seven large scale benchmark datasets perturbing characters surprisingly mctsbug without relying gradient information effective gradient based white box baseline thanks nature homoglyph attack generated adversarial perturbations almost imperceptible human eyes\n",
            "output sentence:  use monte carlo tree search homoglyphs generate indistinguishable adversarial samples text data \n",
            "\n",
            "{'rouge-1': {'r': 0.11235955056179775, 'p': 0.7142857142857143, 'f': 0.19417475493260442}, 'rouge-2': {'r': 0.05504587155963303, 'p': 0.42857142857142855, 'f': 0.09756097359243841}, 'rouge-l': {'r': 0.07865168539325842, 'p': 0.5, 'f': 0.1359223277481384}}\n",
            "pair:  multi label classification mlc task assigning set target labels given sample modeling combinatorial label interactions mlc long haul challenge recurrent neural network rnn based encoder decoder models shown state art performance solving mlc however sequential nature modeling label dependencies rnn limits ability parallel computation predicting dense labels providing interpretable results paper propose message passing encoder decoder mped networks aiming provide fast accurate interpretable mlc mped networks model joint prediction labels replacing rnns encoder decoder architecture message passing mechanisms dispense autoregressive inference entirely proposed models simple fast accurate interpretable structure agnostic used known unknown structured data experiments seven real world mlc datasets show proposed models outperform autoregressive rnn models across five different metrics significant speedup training testing time\n",
            "output sentence:  propose message passing encoder decode networks fast accurate way modelling label dependencies multi label classification \n",
            "\n",
            "{'rouge-1': {'r': 0.05333333333333334, 'p': 0.4, 'f': 0.09411764498269902}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.1111111111111111, 'f': 0.02040816159725128}, 'rouge-l': {'r': 0.05333333333333334, 'p': 0.4, 'f': 0.09411764498269902}}\n",
            "pair:  convolutional neural networks continuously advance progress image object classification steadfast usage algorithm requires constant evaluation upgrading foundational concepts maintain progress network regularization techniques typically focus convolutional layer operations leaving pooling layer operations without suitable options introduce wavelet pooling another alternative traditional neighborhood pooling method decomposes features second level decomposition discards first level subbands reduce feature dimensions method addresses overfitting problem encountered max pooling reducing features structurally compact manner pooling via neighborhood regions experimental results four benchmark classification datasets demonstrate proposed method outperforms performs comparatively methods like max mean mixed stochastic pooling\n",
            "output sentence:  pooling achieved using wavelets instead traditional neighborhood approaches max average etc \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.6666666666666666, 'f': 0.09302325451595458}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0375, 'p': 0.5, 'f': 0.06976744056246621}}\n",
            "pair:  several first order stochastic optimization methods commonly used euclidean domain stochastic gradient descent sgd accelerated gradient descent variance reduced methods already adapted certain riemannian settings however popular optimization tools namely adam adagrad recent amsgrad remain generalized riemannian manifolds discuss difficulty generalizing adaptive schemes agnostic riemannian setting provide algorithms convergence proofs geodesically convex objectives particular case product riemannian manifolds adaptivity implemented across manifolds cartesian product generalization tight sense choosing euclidean space riemannian manifold yields algorithms regret bounds already known standard algorithms experimentally show faster convergence lower train loss value riemannian adaptive methods corresponding baselines realistic task embedding wordnet taxonomy poincare ball\n",
            "output sentence:  adapting adam amsgrad adagrad riemannian manifolds \n",
            "\n",
            "{'rouge-1': {'r': 0.234375, 'p': 0.8823529411764706, 'f': 0.37037036705380283}, 'rouge-2': {'r': 0.11363636363636363, 'p': 0.47619047619047616, 'f': 0.18348623542126086}, 'rouge-l': {'r': 0.234375, 'p': 0.8823529411764706, 'f': 0.37037036705380283}}\n",
            "pair:  due success deep learning solving variety challenging machine learning tasks rising interest understanding loss functions training neural networks theoretical aspect particularly properties critical points landscape around importance determine convergence performance optimization algorithms paper provide necessary sufficient characterization analytical forms critical points well global minimizers square loss functions linear neural networks show analytical forms critical points characterize values corresponding loss functions well necessary sufficient conditions achieve global minimum furthermore exploit analytical forms critical points characterize landscape properties loss functions linear neural networks shallow relu networks one particular conclusion loss function linear networks spurious local minimum loss function one hidden layer nonlinear networks relu activation function local minimum global minimum\n",
            "output sentence:  provide necessary sufficient analytical forms critical points square loss functions various neural networks exploit analytical forms landscape landscape properties properties neural networks networks networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.45454545454545453, 'f': 0.13157894489265934}, 'rouge-2': {'r': 0.012987012987012988, 'p': 0.09090909090909091, 'f': 0.02272727053977294}, 'rouge-l': {'r': 0.046153846153846156, 'p': 0.2727272727272727, 'f': 0.07894736594529093}}\n",
            "pair:  aim build complex humanoid agents integrate perception motor control memory work partly factor problem low level motor control proprioception high level coordination low level skills informed vision develop architecture capable surprisingly flexible task directed motor control relatively high dof humanoid body combining pre training low level motor controllers high level task focused controller switches among low level sub policies resulting system able control physically simulated humanoid body solve tasks require coupling visual perception unstabilized egocentric rgb camera locomotion environment supplementary video link https youtu fboir pnxpk\n",
            "output sentence:  solve tasks involving vision guided humanoid locomotion reusing locomotion behavior motion capture \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.8, 'f': 0.21621621387874362}, 'rouge-2': {'r': 0.0958904109589041, 'p': 0.7, 'f': 0.16867469667586008}, 'rouge-l': {'r': 0.125, 'p': 0.8, 'f': 0.21621621387874362}}\n",
            "pair:  recent efforts combine representation learning formal methods commonly known neuro symbolic methods given rise new trend applying rich neural architectures solve classical combinatorial optimization problems paper propose neural framework learn solve circuit satisfiability problem framework built upon two fundamental contributions rich embedding architecture encodes problem structure end end differentiable training procedure mimics reinforcement learning trains model directly toward solving sat problem experimental results show superior sample generalization performance framework compared recently developed neurosat method\n",
            "output sentence:  propose neural framework learn solve circuit satisfiability problem unlabeled circuit instances \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.7777777777777778, 'f': 0.15053763266042317}, 'rouge-2': {'r': 0.019801980198019802, 'p': 0.25, 'f': 0.0366972463462672}, 'rouge-l': {'r': 0.05952380952380952, 'p': 0.5555555555555556, 'f': 0.10752687997225113}}\n",
            "pair:  deep generative neural networks proven effective conditional unconditional modeling complex data distributions conditional generation enables interactive control creating new controls often requires expensive retraining paper develop method condition generation without retraining model post hoc learning latent constraints value functions identify regions latent space generate outputs desired attributes conditionally sample regions gradient based optimization amortized actor functions combining attribute constraints universal realism constraint enforces similarity data distribution generate realistic conditional images unconditional variational autoencoder using gradient based optimization demonstrate identity preserving transformations make minimal adjustment latent space modify attributes image finally discrete sequences musical notes demonstrate zero shot conditional generation learning latent constraints absence labeled data differentiable reward function\n",
            "output sentence:  new approach conditional generation constraining latent space unconditional generative model \n",
            "\n",
            "{'rouge-1': {'r': 0.0784313725490196, 'p': 0.4444444444444444, 'f': 0.13333333078333337}, 'rouge-2': {'r': 0.016129032258064516, 'p': 0.125, 'f': 0.02857142654693892}, 'rouge-l': {'r': 0.0392156862745098, 'p': 0.2222222222222222, 'f': 0.06666666411666677}}\n",
            "pair:  present probabilistic framework session based recommendation latent variable user state updated user views items learn interests provide computational solutions using parameterization trick using bouchard bound softmax function explore employing variational auto encoder variational expectation maximization algorithm tightening variational bound finally show bouchard bound causes denominator softmax decompose sum enabling fast noisy gradients bound giving fully probabilistic algorithm reminiscent word vec fast online em algorithm\n",
            "output sentence:  fast variational approximations approximating user state learning product embeddings \n",
            "\n",
            "{'rouge-1': {'r': 0.0392156862745098, 'p': 0.2857142857142857, 'f': 0.06896551511890613}, 'rouge-2': {'r': 0.009009009009009009, 'p': 0.07692307692307693, 'f': 0.01612903038111364}, 'rouge-l': {'r': 0.029411764705882353, 'p': 0.21428571428571427, 'f': 0.05172413580856132}}\n",
            "pair:  brain computer interfaces bci may help patients faltering communication abilities due neurodegenerative diseases produce text speech direct neural processing however practical realization proven difficult due limitations speed accuracy generalizability existing interfaces end aim create bci decodes text directly neural signals implement framework initially isolates frequency bands input signal encapsulating differential information regarding production various phonemic classes bands form feature set feeds lstm discerns time point probability distributions across phonemes uttered subject finally particle filtering algorithm temporally smooths probabilities incorporating prior knowledge english language output text corresponding decoded word producing output abstain constraining reconstructed word given bag words unlike previous studies empirical success proposed approach offers promise employment interface patients unfettered naturalistic environments\n",
            "output sentence:  present open loop brain machine interface whose performance unconstrained traditionally used bag words approach \n",
            "\n",
            "{'rouge-1': {'r': 0.0963855421686747, 'p': 0.6153846153846154, 'f': 0.16666666432508684}, 'rouge-2': {'r': 0.01834862385321101, 'p': 0.16666666666666666, 'f': 0.033057849452906324}, 'rouge-l': {'r': 0.08433734939759036, 'p': 0.5384615384615384, 'f': 0.14583333099175347}}\n",
            "pair:  model distillation aims distill knowledge complex model simpler one paper consider alternative formulation called dataset distillation keep model fixed instead attempt distill knowledge large training dataset small one idea synthesize small number data points need come correct data distribution given learning algorithm training data approximate model trained original data example show possible compress mnist training images synthetic distilled images one per class achieve close original performance given fixed network initialization evaluate method various initialization settings experiments multiple datasets mnist cifar pascal voc cub demonstrate ad vantage approach compared alternative methods finally include real world application dataset distillation continual learning setting show storing distilled images episodic memory previous tasks alleviate forgetting effectively real images\n",
            "output sentence:  propose distill large dataset small set synthetic data train networks close original performance \n",
            "\n",
            "{'rouge-1': {'r': 0.16455696202531644, 'p': 0.9285714285714286, 'f': 0.2795698899155972}, 'rouge-2': {'r': 0.11320754716981132, 'p': 0.9230769230769231, 'f': 0.20168067032271736}, 'rouge-l': {'r': 0.16455696202531644, 'p': 0.9285714285714286, 'f': 0.2795698899155972}}\n",
            "pair:  examine techniques combining generalized policies search algorithms exploit strengths overcome weaknesses solving probabilistic planning problems action schema network asnet recent contribution planning uses deep learning neural networks learn generalized policies probabilistic planning problems asnets well suited problems local knowledge environment exploited improve performance may fail generalize problems trained monte carlo tree search mcts forward chaining state space search algorithm optimal decision making performs simulations incrementally build search tree estimate values state although mcts achieve state art results paired domain specific knowledge without knowledge mcts requires large number simulations order obtain reliable estimates search tree combining asnets mcts able improve capability asnet generalize beyond distribution problems trained well enhance navigation search space mcts\n",
            "output sentence:  techniques combining generalized policies search algorithms exploit strengths overcome weaknesses solving probabilistic planning problems \n",
            "\n",
            "{'rouge-1': {'r': 0.16326530612244897, 'p': 0.7272727272727273, 'f': 0.2666666636722222}, 'rouge-2': {'r': 0.08461538461538462, 'p': 0.4230769230769231, 'f': 0.1410256382478633}, 'rouge-l': {'r': 0.12244897959183673, 'p': 0.5454545454545454, 'f': 0.1999999970055556}}\n",
            "pair:  recent work modeling neural responses primate visual system benefited deep neural networks trained large scale object recognition found hierarchical correspondence layers artificial neural network brain areas along ventral visual stream however neither know whether task optimized networks enable equally good models rodent visual system similar hierarchical correspondence exists address questions mouse visual system extracting features several layers convolutional neural network cnn trained imagenet predict responses thousands neurons four visual areas lm al rl natural images found cnn features outperform classical subunit energy models found evidence order areas recorded via correspondence hierarchy cnn layers moreover cnn random weights provided equivalently useful feature space predicting neural responses results suggest object recognition high level task provide discriminative features characterize mouse visual system random network unlike primate training ethologically relevant visually guided behaviors beyond static object recognition may needed unveil functional organization mouse visual cortex\n",
            "output sentence:  goal driven approach model four mouse visual areas lm al rl based deep neural networks trained static object recognition functional functional functional cortex visual visual cortex cortex unlike \n",
            "\n",
            "{'rouge-1': {'r': 0.19480519480519481, 'p': 0.8333333333333334, 'f': 0.31578947061274243}, 'rouge-2': {'r': 0.14772727272727273, 'p': 0.7222222222222222, 'f': 0.24528301604841585}, 'rouge-l': {'r': 0.18181818181818182, 'p': 0.7777777777777778, 'f': 0.29473683903379505}}\n",
            "pair:  common sense physical reasoning essential ingredient intelligent agent operating real world example used simulate environment infer state parts world currently unobserved order match real world conditions causal knowledge must learned without access supervised data address problem present novel method learns discover objects model physical interactions raw visual images purely unsupervised fashion incorporates prior knowledge compositional nature human perception factor interactions object pairs learn efficiently videos bouncing balls show superior modelling capabilities method compared unsupervised neural approaches incorporate prior knowledge demonstrate ability handle occlusion show extrapolate learned knowledge scenes different numbers objects\n",
            "output sentence:  introduce novel approach common sense physical reasoning learns discover objects model physical interactions raw visual images purely unsupervised fashion \n",
            "\n",
            "{'rouge-1': {'r': 0.08235294117647059, 'p': 0.5833333333333334, 'f': 0.14432989473907962}, 'rouge-2': {'r': 0.020202020202020204, 'p': 0.18181818181818182, 'f': 0.036363634563636456}, 'rouge-l': {'r': 0.047058823529411764, 'p': 0.3333333333333333, 'f': 0.08247422463598689}}\n",
            "pair:  sequence generation models recurrent networks trained diverse set learning algorithms example maximum likelihood learning simple efficient yet suffers exposure bias problem reinforcement learning like policy gradient addresses problem prohibitively poor exploration efficiency variety algorithms raml spg data noising also developed different perspectives paper establishes formal connection algorithms present generalized entropy regularized policy optimization formulation show apparently divergent algorithms reformulated special instances framework difference configurations reward function couple hyperparameters unified interpretation offers systematic view varying properties exploration learning efficiency besides based framework present new algorithm dynamically interpolates among existing algorithms improved learning experiments machine translation text summarization demonstrate superiority proposed algorithm\n",
            "output sentence:  unified perspective various learning algorithms sequence generation mle rl raml data noising etc \n",
            "\n",
            "{'rouge-1': {'r': 0.0379746835443038, 'p': 0.42857142857142855, 'f': 0.06976744036506224}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02531645569620253, 'p': 0.2857142857142857, 'f': 0.04651162641157387}}\n",
            "pair:  variational auto encoders vae capable generating realistic images sounds video sequences practitioners point view usually interested solving problems tasks learned sequentially way avoids revisiting previous data stage address problem introducing conceptually simple scalable end end approach incorporating past knowledge learning prior directly data consider scalable boosting like approximation intractable theoretical optimal prior provide empirical studies two commonly used benchmarks namely mnist fashion mnist disjoint sequential image generation tasks dataset proposed method delivers best results among comparable approaches avoiding catastrophic forgetting fully automatic way fixed model architecture\n",
            "output sentence:  novel algorithm incremental learning vae fixed architecture \n",
            "\n",
            "{'rouge-1': {'r': 0.08045977011494253, 'p': 0.5384615384615384, 'f': 0.13999999773800004}, 'rouge-2': {'r': 0.008695652173913044, 'p': 0.08333333333333333, 'f': 0.015748029784859757}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.46153846153846156, 'f': 0.11999999773800002}}\n",
            "pair:  cold start efficiency issues top recommendation critical large scale recommender systems previous hybrid recommendation methods effective deal cold start issues extracting real latent factors cold start items users side information still suffer low efficiency online recommendation caused expensive similarity search real latent space paper presents collaborative generated hashing cgh improve efficiency denoting users items binary codes applies various settings cold start users cold start items warm start ones specifically cgh designed learn hash functions users items minimum description length mdl principle thus deal various recommendation settings addition cgh initiates new marketing strategy mining potential users generative step reconstruct effective users mdl principle used learn compact informative binary codes content data extensive experiments two public datasets show advantages recommendations various settings competing baselines analyze feasibility application marketing\n",
            "output sentence:  generate effective hash codes efficient cold start recommendation meanwhile provide feasible marketing strategy \n",
            "\n",
            "{'rouge-1': {'r': 0.075, 'p': 0.42857142857142855, 'f': 0.1276595719330014}, 'rouge-2': {'r': 0.030303030303030304, 'p': 0.23076923076923078, 'f': 0.053571426519451605}, 'rouge-l': {'r': 0.05, 'p': 0.2857142857142857, 'f': 0.08510638044363973}}\n",
            "pair:  given variety visual world one true scale recognition objects may appear drastically different sizes across visual field rather enumerate variations across filter channels pyramid levels dynamic models locally predict scale adapt receptive fields accordingly degree variation diversity inputs makes difficult task existing methods either learn feedforward predictor totally immune scale variation meant counter select scales fixed algorithm cannot learn given task data extend dynamic scale inference feedforward prediction iterative optimization adaptivity propose novel entropy minimization objective inference optimize task structure parameters tune model input optimization inference improves semantic segmentation accuracy generalizes better extreme scale variations cause feedforward dynamic inference falter\n",
            "output sentence:  unsupervised optimization inference gives top feedback iteratively adjust feedforward prediction scale variation equivariant recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.07142857142857142, 'p': 0.875, 'f': 0.1320754703025988}, 'rouge-2': {'r': 0.015503875968992248, 'p': 0.2857142857142857, 'f': 0.02941176372945505}, 'rouge-l': {'r': 0.05102040816326531, 'p': 0.625, 'f': 0.09433962124599504}}\n",
            "pair:  objective deep extreme multi label learning jointly learn feature representations classifiers automatically tag data points relevant subset labels extremely large label set unfortunately state art deep extreme classifiers either scalable inaccurate short text documents paper develops deepxml algorithm addresses limitations introducing novel architecture splits training head tail labels deepxml increases accuracy learning word embeddings head labels transferring novel residual connection data impoverished tail labels increasing amount negative training data available extending state art negative sub sampling techniques ranking set predicted labels eliminate hardest negatives original classifier contributions implemented efficiently extending highly scalable slice algorithm pretrained embeddings learn proposed deepxml architecture result deepxml could efficiently scale problems involving millions labels beyond pale state art deep extreme classifiers could faster training xml cnn attentionxml time deepxml also empirically determined accurate leading techniques matching search engine queries advertiser bid phrases\n",
            "output sentence:  scalable accurate deep multi label learning millions labels \n",
            "\n",
            "{'rouge-1': {'r': 0.05102040816326531, 'p': 0.625, 'f': 0.09433962124599504}, 'rouge-2': {'r': 0.02654867256637168, 'p': 0.375, 'f': 0.04958677562461584}, 'rouge-l': {'r': 0.04081632653061224, 'p': 0.5, 'f': 0.07547169671769313}}\n",
            "pair:  work present novel upper bound target error address problem unsupervised domain adaptation recent studies reveal deep neural network learn transferable features generalize well novel tasks furthermore ben david et al provide upper bound target error transferring knowledge summarized minimizing source error distance marginal distributions simultaneously however common methods based theory usually ignore joint error samples different classes might mixed together matching marginal distribution case matter minimize marginal discrepancy target error bounded due increasing joint error address problem propose general upper bound taking joint error account undesirable case properly penalized addition utilize constrained hypothesis space formalize tighter bound well novel cross margin discrepancy measure dissimilarity hypotheses alleviates instability adversarial learning extensive empirical evidence shows proposal outperforms related approaches image classification error rates standard domain adaptation benchmarks\n",
            "output sentence:  joint error matters unsupervised domain adaptation especially domain shift huge \n",
            "\n",
            "{'rouge-1': {'r': 0.17894736842105263, 'p': 0.85, 'f': 0.2956521710396976}, 'rouge-2': {'r': 0.07692307692307693, 'p': 0.45, 'f': 0.13138685882039536}, 'rouge-l': {'r': 0.14736842105263157, 'p': 0.7, 'f': 0.24347825799621928}}\n",
            "pair:  ability decompose complex multi object scenes meaningful abstractions like objects fundamental achieve higher level cognition previous approaches unsupervised object oriented scene representation learning either based spatial attention scene mixture approaches limited scalability main obstacle towards modeling real world scenes paper propose generative latent variable model called space provides uni ed probabilistic modeling framework combines best spatial attention scene mixture approaches space explicitly provide factorized object representations foreground objects also decomposing background segments complex morphology previous models good either space also resolves scalability problems previous methods incorporating parallel spatial attention thus applicable scenes large number objects without performance degradations show experiments atari rooms space achieves properties consistently comparison spair iodine genesis results experiments found project website https sites google com view space project page\n",
            "output sentence:  propose generative latent variable model unsupervised scene decomposition provides factorized object representation per foreground object also decomposing background segments complex morphology \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.5833333333333334, 'f': 0.13592232803845794}, 'rouge-2': {'r': 0.017094017094017096, 'p': 0.18181818181818182, 'f': 0.03124999842895516}, 'rouge-l': {'r': 0.054945054945054944, 'p': 0.4166666666666667, 'f': 0.09708737658214729}}\n",
            "pair:  general problem received considerable recent attention perform multiple tasks network maximizing efficiency prediction accuracy popular approach consists multi branch architecture top shared backbone jointly trained weighted sum losses however many cases shared representation results non optimal performance mainly due interference conflicting gradients uncorrelated tasks recent approaches address problem channel wise modulation feature maps along shared backbone task specific vectors manually dynamically tuned taking approach step propose novel architecture modulate recognition network channel wise well spatial wise efficient top image dependent computation scheme architecture uses task specific branches task specific modules instead uses top modulation network shared tasks show effectiveness scheme achieving par better results alternative approaches correlated uncorrelated sets tasks also demonstrate advantages terms model size addition novel tasks interpretability code released\n",
            "output sentence:  propose top modulation network multi task learning applications several advantages current schemes \n",
            "\n",
            "{'rouge-1': {'r': 0.3953488372093023, 'p': 1.0, 'f': 0.5666666626055555}, 'rouge-2': {'r': 0.32, 'p': 1.0, 'f': 0.4848484811753903}, 'rouge-l': {'r': 0.3953488372093023, 'p': 1.0, 'f': 0.5666666626055555}}\n",
            "pair:  point important problems common practice using best single model performance comparing deep learning architectures propose method corrects flaws time model trained one gets different result due random factors training process include random parameter initialization random data shuffling reporting best single model performance appropriately address stochasticity propose normalized expected best performance boo way correct problems\n",
            "output sentence:  point important problems common practice using best single model performance comparing deep learning architectures propose method corrects flaws \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.5, 'f': 0.12121211908172637}, 'rouge-2': {'r': 0.021897810218978103, 'p': 0.2, 'f': 0.03947368243161366}, 'rouge-l': {'r': 0.04310344827586207, 'p': 0.3125, 'f': 0.07575757362718095}}\n",
            "pair:  owing connection generative adversarial networks gans saddle point problems recently attracted considerable interest machine learning beyond necessity theoretical guarantees revolve around convex concave even linear problems however making theoretical inroads towards efficient gan training depends crucially moving beyond classic framework make piecemeal progress along lines analyze behavior mirror descent md class non monotone problems whose solutions coincide naturally associated variational inequality property call coherence first show ordinary vanilla md converges strict version condition otherwise particular may fail converge even bilinear models unique solution show deficiency mitigated optimism taking extra gradient step optimistic mirror descent omd converges coherent problems analysis generalizes extends results daskalakis et al optimistic gradient descent ogd bilinear problems makes concrete headway provable convergence beyond convex concave games also provide stochastic analogues results validate analysis numerical experiments wide array gan models including gaussian mixture models celeba cifar datasets\n",
            "output sentence:  show inclusion extra gradient step first order gan training methods improve stability lead improved convergence results \n",
            "\n",
            "{'rouge-1': {'r': 0.04040404040404041, 'p': 0.8, 'f': 0.07692307600776628}, 'rouge-2': {'r': 0.007692307692307693, 'p': 0.2, 'f': 0.014814814101508953}, 'rouge-l': {'r': 0.04040404040404041, 'p': 0.8, 'f': 0.07692307600776628}}\n",
            "pair:  recent studies show convolutional neural networks cnns vulnerable various settings including adversarial examples backdoor attacks distribution shifting motivated findings human visual system pays attention global structure shape recognition cnns biased towards local texture features images propose unified framework edgeganrob based robust edge features improve robustness cnns general first explicitly extracts shape structure features given image reconstructs new image refilling texture information trained generative adversarial network gan addition reduce sensitivity edge detection algorithm adversarial perturbation propose robust edge detection approach robust canny based vanilla canny algorithm gain insights also compare edgeganrob simplified backbone procedure edgenetrob performs learning tasks directly extracted robust edge features find edgenetrob help boost model robustness significantly cost clean model accuracy edgeganrob hand able improve clean model accuracy compared edgenetrob without losing robustness benefits introduced edgenetrob extensive experiments show edgeganrob resilient different learning tasks diverse settings\n",
            "output sentence:  unified model improve model robustness multiple tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.8571428571428571, 'f': 0.11650485310208314}, 'rouge-2': {'r': 0.024193548387096774, 'p': 0.42857142857142855, 'f': 0.04580152570596121}, 'rouge-l': {'r': 0.0625, 'p': 0.8571428571428571, 'f': 0.11650485310208314}}\n",
            "pair:  deep neural networks dnns witnessed powerful approach year solving long standing artificial intelligence ai supervised unsupervised tasks exists natural language processing speech processing computer vision others paper attempt apply dnns three different cyber security use cases android malware classification incident detection fraud detection data set use case contains real known benign malicious activities samples use cases part cybersecurity data mining competition cdmc efficient network architecture dnns chosen conducting various trails experiments network parameters network structures experiments chosen efficient configurations dnns run epochs learning rate set range experiments dnns performed well comparison classical machine learning algorithm cases experiments cyber security use cases due fact dnns implicitly extract build better features identifies characteristics data lead better accuracy best accuracy obtained dnns xgboost android malware classification incident detection fraud detection respectively accuracy obtained dnns varies top scored system cdmc tasks\n",
            "output sentence:  deep net deep neural network cyber security use cases \n",
            "\n",
            "{'rouge-1': {'r': 0.07954545454545454, 'p': 0.5384615384615384, 'f': 0.13861385914322127}, 'rouge-2': {'r': 0.018518518518518517, 'p': 0.16666666666666666, 'f': 0.033333331533333434}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.46153846153846156, 'f': 0.11881187894520145}}\n",
            "pair:  using class labels represent class similarity typical approach training deep hashing systems retrieval samples different classes take binary similarity values similarity model full rich knowledge semantic relations may present data points work build upon idea using semantic hierarchies form distance metrics available sample labels example cat dog smaller distance cat guitar combine type semantic distance loss function promote similar distances deep neural network embeddings also introduce empirical kullback leibler divergence loss term promote binarization uniformity embeddings test resulting shrewd method demonstrate improvements hierarchical retrieval scores using compact binary hash codes instead real valued ones show weakly supervised hashing setting able learn competitively without explicitly relying class labels instead similarities labels\n",
            "output sentence:  propose new method training deep hashing image retrieval using relational distance metric samples \n",
            "\n",
            "{'rouge-1': {'r': 0.07462686567164178, 'p': 0.7142857142857143, 'f': 0.13513513342220598}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.16666666666666666, 'f': 0.022988504462941015}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.5714285714285714, 'f': 0.108108106395179}}\n",
            "pair:  learning gaussian process models occurs adaptation hyperparameters mean covariance function classical approach entails maximizing marginal likelihood yielding fixed point estimates approach called type ii maximum likelihood ml ii alternative learning procedure infer posterior hyperparameters hierarchical specification gps call fully bayesian gaussian process regression gpr work considers two approximations intractable hyperparameter posterior hamiltonian monte carlo hmc yielding sampling based approximation variational inference vi posterior hyperparameters approximated factorized gaussian mean field full rank gaussian accounting correlations hyperparameters analyse predictive performance fully bayesian gpr range benchmark data sets\n",
            "output sentence:  analysis bayesian hyperparameter inference gaussian process regression \n",
            "\n",
            "{'rouge-1': {'r': 0.20833333333333334, 'p': 0.5882352941176471, 'f': 0.30769230382958584}, 'rouge-2': {'r': 0.10909090909090909, 'p': 0.375, 'f': 0.16901408101567159}, 'rouge-l': {'r': 0.14583333333333334, 'p': 0.4117647058823529, 'f': 0.21538461152189353}}\n",
            "pair:  gaussian processes ubiquitous nature engineering case point class neural networks infinite width limit whose priors correspond gaussian processes perturbatively extend correspondence finite width neural networks yielding non gaussian processes priors methodology developed herein allows us track flow preactivation distributions progressively integrating random variables lower higher layers reminiscent renormalization group flow develop perturbative prescription perform bayesian inference weakly non gaussian priors\n",
            "output sentence:  develop analytical method study bayesian inference finite width neural networks find renormalization group flow picture naturally emerges \n",
            "\n",
            "{'rouge-1': {'r': 0.0975609756097561, 'p': 0.5, 'f': 0.1632653033902541}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.3125, 'f': 0.10204081359433577}}\n",
            "pair:  lifelong machine learning focuses adapting novel tasks without forgetting old tasks whereas shot learning strives learn single task given small amount data two different research areas crucial artificial general intelligence however existing studies somehow assumed impractical settings training models lifelong learning nature quantity incoming tasks inference time assumed known training time shot learning commonly assumed large number tasks available training humans hand perform learning tasks without regard aforementioned assumptions inspired human brain works propose novel model called slow thinking learn stl makes sophisticated slightly slower predictions iteratively considering interactions current previously seen tasks runtime conducted experiments results empirically demonstrate effectiveness stl realistic lifelong shot learning settings\n",
            "output sentence:  paper studies interactions fast learning slow prediction models demonstrate interactions improve machine capability solve joint lifelong learning \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.45454545454545453, 'f': 0.10416666463758685}, 'rouge-2': {'r': 0.018867924528301886, 'p': 0.2, 'f': 0.03448275704518438}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.45454545454545453, 'f': 0.10416666463758685}}\n",
            "pair:  backpropagation algorithm popular algorithm training neural networks nowadays however suffers forward locking backward locking update locking problems especially neural network large layers distributed across multiple devices existing solutions either handle one locking problem lead severe accuracy loss memory inefficiency moreover none consider straggler problem among devices paper propose textbf layer wise staleness novel efficient training algorithm textbf diversely stale parameters dsp address challenges without loss accuracy memory issue also analyze convergence dsp two popular gradient based methods prove guaranteed converge critical points non convex problems finally extensive experimental results training deep convolutional neural networks demonstrate proposed dsp algorithm achieve significant training speedup stronger robustness better generalization compared methods\n",
            "output sentence:  propose diversely stale parameters break lockings backpropoagation algorithm train cnn parallel \n",
            "\n",
            "{'rouge-1': {'r': 0.1016949152542373, 'p': 0.5454545454545454, 'f': 0.1714285687795919}, 'rouge-2': {'r': 0.013333333333333334, 'p': 0.1, 'f': 0.023529409688581502}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.2727272727272727, 'f': 0.08571428306530622}}\n",
            "pair:  consider task program synthesis presence reward function output programs goal find programs maximal rewards introduce novel iterative optimization scheme train rnn dataset best programs priority queue generated programs far synthesize new programs add priority queue sampling rnn benchmark algorithm called priority queue training pqt genetic algorithm reinforcement learning baselines simple expressive turing complete programming language called bf experimental results show deceptively simple pqt algorithm significantly outperforms baselines adding program length penalty reward function able synthesize short human readable programs\n",
            "output sentence:  use simple search algorithm involving rnn priority queue find solutions coding tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.03409090909090909, 'p': 0.3333333333333333, 'f': 0.061855668419598296}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.022727272727272728, 'p': 0.2222222222222222, 'f': 0.04123711171856739}}\n",
            "pair:  nnew types compute hardware development entering market hold promise revolutionizing deep learning manner profound gpus however existing software frameworks training algorithms deep learning yet evolve fully leverage capability new wave silicon particular models exploit structured input via complex instance dependent control flow difficult accelerate using existing algorithms hardware typically rely minibatching present asynchronous model parallel amp training algorithm specifically motivated training networks interconnected devices implementation multi core cpus show amp training converges accuracy conventional synchronous training algorithms similar number epochs utilizes available hardware efficiently even small minibatch sizes resulting shorter overall training times framework opens door scaling new class deep learning models cannot efficiently trained today\n",
            "output sentence:  using asynchronous gradient updates accelerate dynamic neural network training \n",
            "\n",
            "{'rouge-1': {'r': 0.02197802197802198, 'p': 0.15384615384615385, 'f': 0.038461536274038595}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02197802197802198, 'p': 0.15384615384615385, 'f': 0.038461536274038595}}\n",
            "pair:  propose study method learning interpretable representations task regression features represented networks multi type expression trees comprised activation functions common neural networks addition elementary functions differentiable features trained via gradient descent performance features linear model used weight rate change among subcomponents representation search process maintains archive representations accuracy complexity trade offs assist generalization interpretation compare several stochastic optimization approaches within framework benchmark variants open source regression problems comparison state art machine learning approaches main finding approach produces highest average test scores across problems producing representations orders magnitude smaller next best performing method gradient boosting also report negative result attempts directly optimize disentanglement representation result highly correlated features\n",
            "output sentence:  representing network architecture set syntax trees optimizing structure leads accurate concise regression models \n",
            "\n",
            "{'rouge-1': {'r': 0.11842105263157894, 'p': 0.5294117647058824, 'f': 0.1935483841091456}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.1875, 'f': 0.060606057896133166}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.4117647058823529, 'f': 0.15053763142097357}}\n",
            "pair:  domain time series forecasting extensively studied fundamental importance many real life applications weather prediction traffic flow forecasting sales compelling examples sequential phenomena predictive models generally make use relations past future values however case stationary time series observed values also drastically depend number exogenous features used improve forecasting quality work propose change paradigm consists learning features embeddings vectors within recurrent neural networks apply framework forecast smart cards tap logs parisian subway network results show context embedded models perform quantitatively better one step ahead multi step ahead forecasting\n",
            "output sentence:  order forecast multivariate stationary time series learn embeddings containing contextual features within rnn apply framework public transportation data \n",
            "\n",
            "{'rouge-1': {'r': 0.05434782608695652, 'p': 0.625, 'f': 0.09999999852800001}, 'rouge-2': {'r': 0.007874015748031496, 'p': 0.14285714285714285, 'f': 0.014925372144130162}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.5, 'f': 0.07999999852800002}}\n",
            "pair:  graph neural networks gnns received tremendous attention recently due power handling graph data different downstream tasks across different application domains key gnn graph convolutional filters recently various kinds filters designed however still lacks depth analysis whether exists best filter perform best graph data graph properties influence optimal choice graph filter design appropriate filter adaptive graph data paper focus addressing three questions first propose novel assessment tool evaluate effectiveness graph convolutional filters given graph using assessment tool find single filter silver bullet perform best possible graphs addition different graph structure properties influence optimal graph convolutional filter design choice based findings develop adaptive filter graph neural network afgnn simple powerful model adaptively learn task specific filter given graph leverages graph filter assessment regularization learns combine set base filters experiments synthetic real world benchmark datasets demonstrate proposed model indeed learn appropriate filter perform well graph tasks\n",
            "output sentence:  propose assessment framework analyze learn graph convolutional filter \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.6666666666666666, 'f': 0.24999999695312503}, 'rouge-2': {'r': 0.07936507936507936, 'p': 0.4166666666666667, 'f': 0.13333333064533337}, 'rouge-l': {'r': 0.1346153846153846, 'p': 0.5833333333333334, 'f': 0.21874999695312503}}\n",
            "pair:  although word analogy problems become standard tool evaluating word vectors little known word vectors good solving problems paper attempt understanding subject developing simple highly accurate generative approach solve word analogy problem case terms involved problem nouns results demonstrate ambiguities associated learning relationship word pair role training dataset determining relationship gets highlighted furthermore results show ability model accurately solve word analogy problem may indicative model ability learn relationship word pair way human\n",
            "output sentence:  simple generative approach solve word analogy problem yields insights word relationships problems estimating \n",
            "\n",
            "{'rouge-1': {'r': 0.1232876712328767, 'p': 0.6428571428571429, 'f': 0.2068965490236491}, 'rouge-2': {'r': 0.042105263157894736, 'p': 0.2857142857142857, 'f': 0.07339449317397531}, 'rouge-l': {'r': 0.0958904109589041, 'p': 0.5, 'f': 0.16091953752939625}}\n",
            "pair:  compare model free reinforcement learning model based approaches lens expressive power neural networks policies functions dynamics show theoretically empirically even one dimensional continuous state space many mdps whose optimal functions policies much complex dynamics hypothesize many real world mdps also similar property mdps model based planning favorable algorithm resulting policies approximate optimal policy significantly better neural network parameterization model free model based policy optimization rely policy parameterization motivated theory apply simple multi step model based bootstrapping planner boots bootstrap weak function stronger policy empirical results show applying boots top model based model free policy optimization algorithms test time improves performance mujoco benchmark tasks\n",
            "output sentence:  compare deep model based model free rl algorithms studying approximability functions policies dynamics neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.7222222222222222, 'f': 0.27083333028645834}, 'rouge-2': {'r': 0.06818181818181818, 'p': 0.3333333333333333, 'f': 0.11320754435030267}, 'rouge-l': {'r': 0.15384615384615385, 'p': 0.6666666666666666, 'f': 0.24999999695312503}}\n",
            "pair:  paper introduce symplectic ode net symoden deep learning framework infer dynamics physical system observed state trajectories achieve better generalization fewer training samples symoden incorporates appropriate inductive bias designing associated computation graph physics informed manner particular enforce hamiltonian dynamics control learn underlying dynamics transparent way leveraged draw insight relevant physical aspects system mass potential energy addition propose parametrization enforce hamiltonian formalism even generalized coordinate data embedded high dimensional space access velocity data instead generalized momentum framework offering interpretable physically consistent models physical systems opens new possibilities synthesizing model based control strategies\n",
            "output sentence:  work enforces hamiltonian dynamics control learn system models embedded position velocity data exploits physically consistent dynamics model based control via \n",
            "\n",
            "{'rouge-1': {'r': 0.13793103448275862, 'p': 1.0, 'f': 0.24242424029384757}, 'rouge-2': {'r': 0.10144927536231885, 'p': 1.0, 'f': 0.18421052464335183}, 'rouge-l': {'r': 0.13793103448275862, 'p': 1.0, 'f': 0.24242424029384757}}\n",
            "pair:  cost annotating training data traditionally bottleneck supervised learning approaches problem exacerbated supervised learning applied number correlated tasks simultaneously since amount labels required scales number tasks mitigate concern propose active multitask learning algorithm achieves knowledge transfer tasks approach forms called committee task jointly makes decisions directly shares data across similar tasks approach reduces number queries needed training maintaining high accuracy test data empirical results benchmark datasets show significant improvements accuracy number query requests\n",
            "output sentence:  propose active multitask learning algorithm achieves knowledge transfer tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.08196721311475409, 'p': 0.5555555555555556, 'f': 0.14285714061632654}, 'rouge-2': {'r': 0.028985507246376812, 'p': 0.25, 'f': 0.051948050086017945}, 'rouge-l': {'r': 0.08196721311475409, 'p': 0.5555555555555556, 'f': 0.14285714061632654}}\n",
            "pair:  size complexity models datasets grow need communication efficient variants stochastic gradient descent deployed clusters perform model fitting parallel alistarh et al describe two variants data parallel sgd quantize encode gradients lessen communication costs first variant qsgd provide strong theoretical guarantees second variant call qsgdinf demonstrate impressive empirical gains distributed training large neural networks building work propose alternative scheme quantizing gradients show yields stronger theoretical guarantees exist qsgd matching empirical performance qsgdinf\n",
            "output sentence:  nuqsgd closes gap theoretical guarantees qsgd empirical performance qsgdinf \n",
            "\n",
            "{'rouge-1': {'r': 0.11224489795918367, 'p': 0.7333333333333333, 'f': 0.19469026318427443}, 'rouge-2': {'r': 0.01652892561983471, 'p': 0.14285714285714285, 'f': 0.029629627770644835}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.4666666666666667, 'f': 0.1238938030072833}}\n",
            "pair:  information bottleneck ib tuning relative strength compression prediction terms two terms behave relationship dataset learned representation paper set answer questions studying multiple phase transitions ib objective ib defined encoding distribution input target representation sudden jumps di prediction accuracy observed increasing introduce definition ib phase transitions qualitative change ib loss landscape show transitions correspond onset learning new classes using second order calculus variations derive formula provides practical condition ib phase transitions draw connection fisher information matrix parameterized models provide two perspectives understand formula revealing ib phase transition finding component maximum nonlinear correlation orthogonal learned representation close analogy canonical correlation analysis cca linear settings based theory present algorithm discovering phase transition points finally verify theory algorithm accurately predict phase transitions categorical datasets predict onset learning new classes class difficulty mnist predict prominent phase transitions cifar\n",
            "output sentence:  give theoretical analysis information bottleneck objective understand predict observed phase transitions prediction vs compression tradeoff \n",
            "\n",
            "{'rouge-1': {'r': 0.10112359550561797, 'p': 0.6923076923076923, 'f': 0.17647058601114957}, 'rouge-2': {'r': 0.064, 'p': 0.6666666666666666, 'f': 0.11678831956950293}, 'rouge-l': {'r': 0.10112359550561797, 'p': 0.6923076923076923, 'f': 0.17647058601114957}}\n",
            "pair:  paper addresses unsupervised shot object recognition training images unlabeled share classes labeled support images shot recognition testing use new gan like deep architecture aimed unsupervised learning image representation encode latent object parts thus generalize well unseen classes shot recognition task unsupervised training integrates adversarial self supervision deep metric learning make two contributions first extend vanilla gan reconstruction loss enforce discriminator capture relevant characteristics fake images generated randomly sampled codes second compile training set triplet image examples estimating triplet loss metric learning using image masking procedure suitably designed identify latent object parts hence metric learning ensures deep representation images showing similar object classes share parts closer representations images common parts results show significantly outperform state art well get similar performance common episodic training fully supervised shot learning mini imagenet tiered imagenet datasets\n",
            "output sentence:  address problem unsupervised shot object recognition training images unlabeled share classes test images \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.46153846153846156, 'f': 0.19672130812147276}, 'rouge-2': {'r': 0.0392156862745098, 'p': 0.16666666666666666, 'f': 0.06349206040816341}, 'rouge-l': {'r': 0.0625, 'p': 0.23076923076923078, 'f': 0.09836065238376793}}\n",
            "pair:  propose effective multitask learning setup reducing distant supervision noise leveraging sentence level supervision show sentence level supervision used improve encoding individual sentences learn input sentences likely express relationship pair entities also introduce novel neural architecture collecting signals multiple input sentences combines benefits attention maxpooling proposed method increases auc outperforms recently published results fb nyt dataset\n",
            "output sentence:  new form attention works well distant supervision setting multitask learning approach add level level \n",
            "\n",
            "{'rouge-1': {'r': 0.029850746268656716, 'p': 0.5, 'f': 0.056338027105733005}, 'rouge-2': {'r': 0.011904761904761904, 'p': 0.3333333333333333, 'f': 0.022988505081252497}, 'rouge-l': {'r': 0.029850746268656716, 'p': 0.5, 'f': 0.056338027105733005}}\n",
            "pair:  propose pure capsnets capsnets without routing procedures specifically make three modifications capsnets first remove routing procedures capsnets based observation coupling coefficients learned implicitly second replace convolutional layers capsnets improve efficiency third package capsules rank tensors improve efficiency experiment shows capsnets achieve better performance capsnets varied routine procedures using significantly fewer parameters mnist cifar high efficiency capsnets even comparable deep compressing models example achieve percent accuracy mnist using parameters visualize capsules well corresponding correlation matrix show possible way initializing capsnets future also explore adversarial robustness capsnets compared cnns\n",
            "output sentence:  routing procedures necessary capsnets \n",
            "\n",
            "{'rouge-1': {'r': 0.08421052631578947, 'p': 0.5714285714285714, 'f': 0.1467889885868193}, 'rouge-2': {'r': 0.008064516129032258, 'p': 0.07692307692307693, 'f': 0.014598538428259568}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.35714285714285715, 'f': 0.09174311702718631}}\n",
            "pair:  weight pruning introduced efficient model compression technique even though pruning removes significant amount weights network memory requirement reduction limited since conventional sparse matrix formats require significant amount memory store index related information moreover computations associated sparse matrix formats slow sequential sparse matrix decoding process utilize highly parallel computing systems efficiently attempt compress index information keeping decoding process parallelizable viterbi based pruning suggested decoding non zero weights however still sequential viterbi based pruning paper propose new sparse matrix format order enable highly parallel decoding process entire sparse matrix proposed sparse matrix constructed combining pruning weight quantization latest rnn models ptb wikitext corpus lstm parameter storage requirement compressed using proposed sparse matrix format compared baseline model compressed weight indices reconstructed dense matrix fast using viterbi encoders simulation results show proposed scheme feed parameters processing elements faster case dense matrix values directly come dram\n",
            "output sentence:  present new weight encoding scheme enables high compression ratio fast sparse dense matrix conversion \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.5714285714285714, 'f': 0.14545454323305787}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.3333333333333333, 'f': 0.06060605895316809}, 'rouge-l': {'r': 0.0625, 'p': 0.42857142857142855, 'f': 0.10909090686942154}}\n",
            "pair:  magnitude based pruning one simplest methods pruning neural networks despite simplicity magnitude based pruning variants demonstrated remarkable performances pruning modern architectures based observation magnitude based pruning indeed minimizes frobenius distortion linear operator corresponding single layer develop simple pruning method coined lookahead pruning extending single layer optimization multi layer optimization experimental results demonstrate proposed method consistently outperforms magnitude pruning various networks including vgg resnet particularly high sparsity regime\n",
            "output sentence:  study multi layer generalization magnitude based pruning \n",
            "\n",
            "{'rouge-1': {'r': 0.03571428571428571, 'p': 0.42857142857142855, 'f': 0.06593406451394762}, 'rouge-2': {'r': 0.009900990099009901, 'p': 0.16666666666666666, 'f': 0.01869158772643905}, 'rouge-l': {'r': 0.03571428571428571, 'p': 0.42857142857142855, 'f': 0.06593406451394762}}\n",
            "pair:  paper propose novel kind kernel random forest kernel enhance empirical performance mmd gan different common forests deterministic routings probabilistic routing variant used innovated random forest kernel possible merge cnn frameworks proposed random forest kernel following advantages perspective random forest output gan discriminator viewed feature inputs forest tree gets access merely fraction features thus entire forest benefits ensemble learning aspect kernel method random forest kernel proved characteristic therefore suitable mmd structure besides asymmetric kernel random forest kernel much flexible terms capturing differences distributions sharing advantages cnn kernel method ensemble learning random forest kernel based mmd gan obtains desirable empirical performances cifar celeba lsun bedroom data sets furthermore sake completeness also put forward comprehensive theoretical analysis support experimental results\n",
            "output sentence:  equip mmd gans new random forest kernel \n",
            "\n",
            "{'rouge-1': {'r': 0.1262135922330097, 'p': 0.6190476190476191, 'f': 0.20967741654136318}, 'rouge-2': {'r': 0.02586206896551724, 'p': 0.15, 'f': 0.044117644550173155}, 'rouge-l': {'r': 0.0970873786407767, 'p': 0.47619047619047616, 'f': 0.16129031976716965}}\n",
            "pair:  much work design convolutional networks last five years revolved around empirical investigation importance depth filter sizes number feature channels recent studies shown branching splitting computation along parallel distinct threads aggregating outputs represents new promising dimension significant improvements performance combat complexity design choices multi branch architectures prior work adopted simple strategies fixed branching factor input fed parallel branches additive combination outputs produced branches aggregation points work remove predefined choices propose algorithm learn connections branches network instead chosen priori human designer multi branch connectivity learned simultaneously weights network optimizing single loss function defined respect end task demonstrate approach problem multi class image classification using four different datasets yields consistently higher accuracy compared state art resnext multi branch network given learning capacity\n",
            "output sentence:  paper introduced algorithm learn connectivity deep multi branch networks approach evaluated image categorization consistently yields accuracy gains state art models use fixed \n",
            "\n",
            "{'rouge-1': {'r': 0.0784313725490196, 'p': 0.4444444444444444, 'f': 0.13333333078333337}, 'rouge-2': {'r': 0.016129032258064516, 'p': 0.125, 'f': 0.02857142654693892}, 'rouge-l': {'r': 0.0392156862745098, 'p': 0.2222222222222222, 'f': 0.06666666411666677}}\n",
            "pair:  present probabilistic framework session based recommendation latent variable user state updated user views items learn interests provide computational solutions using parameterization trick using bouchard bound softmax function explore employing variational auto encoder variational expectation maximization algorithm tightening variational bound finally show bouchard bound causes denominator softmax decompose sum enabling fast noisy gradients bound giving fully probabilistic algorithm reminiscent word vec fast online em algorithm\n",
            "output sentence:  fast variational approximations approximating user state learning product embeddings \n",
            "\n",
            "{'rouge-1': {'r': 0.10526315789473684, 'p': 0.625, 'f': 0.180180177712848}, 'rouge-2': {'r': 0.04132231404958678, 'p': 0.3125, 'f': 0.07299269866695089}, 'rouge-l': {'r': 0.06315789473684211, 'p': 0.375, 'f': 0.10810810564077598}}\n",
            "pair:  importance weighted autoencoder iwae burda et al popular variational inference method achieves tighter evidence bound hence lower bias standard variational autoencoders optimising multi sample objective objective expressible integral monte carlo samples unfortunately iwae crucially relies availability reparametrisations even exist multi sample objective leads inference network gradients break increased rainforth et al breakdown circumvented removing high variance score function terms either heuristically ignoring yields sticking landing iwae iwae stl gradient roeder et al identity tucker et al yields doubly reparametrised iwae iwae dreg gradient work argue directly optimising proposal distribution importance sampling reweighted wake sleep rws algorithm bornschein bengio preferable optimising iwae type multi sample objectives formalise argument introduce adaptive importance sampling framework termed adaptive importance sampling learning aisle slightly generalises rws algorithm show aisle admits iwae stl iwae dreg iwae gradients avoid breakdown special cases\n",
            "output sentence:  show variants importance weighted autoencoders derived principled manner special cases adaptive importance sampling approaches like reweighted wake sleep \n",
            "\n",
            "{'rouge-1': {'r': 0.13, 'p': 0.7222222222222222, 'f': 0.22033898046538355}, 'rouge-2': {'r': 0.048, 'p': 0.3333333333333333, 'f': 0.08391608171548738}, 'rouge-l': {'r': 0.09, 'p': 0.5, 'f': 0.15254237029589202}}\n",
            "pair:  study problem designing provably optimal adversarial noise algorithms induce misclassification settings learner aggregates decisions multiple classifiers given demonstrated vulnerability state art models adversarial examples recent efforts within field robust machine learning focused use ensemble classifiers way boosting robustness individual models paper design provably optimal attacks set classifiers demonstrate problem framed finding strategies equilibrium two player zero sum game learner adversary consequently illustrate need randomization adversarial attacks main technical challenge consider design best response oracles implemented multiplicative weight updates framework find equilibrium strategies zero sum game develop series scalable noise generation algorithms deep neural networks show outperforms state art attacks various image classification tasks although generally guarantees deep learning show well principled approach provably optimal linear classifiers main insight geometric characterization decision space reduces problem designing best response oracles minimizing quadratic function set convex polytopes\n",
            "output sentence:  paper analyzes problem designing adversarial attacks multiple classifiers introducing algorithms optimal linear classifiers provide state art results deep learning \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.9, 'f': 0.17999999820000004}, 'rouge-2': {'r': 0.02631578947368421, 'p': 0.3, 'f': 0.048387095291363195}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5, 'f': 0.09999999820000001}}\n",
            "pair:  recent years deep reinforcement learning shown adept solving sequential decision processes high dimensional state spaces atari games many reinforcement learning problems however involve high dimensional discrete action spaces well high dimensional state spaces paper develop novel policy gradient methodology case large multidimensional discrete action spaces propose two approaches creating parameterized policies lstm parameterization modified mdp mmdp giving rise feed forward network ffn parameterization approaches provide expressive models backpropagation applied training consider entropy bonus typically added reward function enhance exploration case high dimensional action spaces calculating entropy gradient entropy requires enumerating actions action space running forward backpropagation action may computationally infeasible develop several novel unbiased estimators entropy bonus gradient finally test algorithms two environments multi hunter multi rabbit grid game multi agent multi arm bandit problem\n",
            "output sentence:  policy parameterizations unbiased policy entropy estimators mdp large multidimensional discrete action space \n",
            "\n",
            "{'rouge-1': {'r': 0.037383177570093455, 'p': 0.2857142857142857, 'f': 0.06611570043303058}, 'rouge-2': {'r': 0.00819672131147541, 'p': 0.07692307692307693, 'f': 0.014814813074348631}, 'rouge-l': {'r': 0.028037383177570093, 'p': 0.21428571428571427, 'f': 0.0495867748131959}}\n",
            "pair:  model pruning become useful technique improves computational efficiency deep learning making possible deploy solutions resource limited scenarios widely used practice relevant work assumes smaller norm parameter feature plays less informative role inference time paper propose channel pruning technique accelerating computations deep convolutional neural networks cnns critically rely assumption instead focuses direct simplification channel channel computation graph cnn without need performing computationally difficult always useful task making high dimensional tensors cnn structured sparse approach takes two stages first adopt end end stochastic training method eventually forces outputs channels constant prune constant channels original neural network adjusting biases impacting layers resulting compact model quickly fine tuned approach mathematically appealing optimization perspective easy reproduce experimented approach several image learning benchmarks demonstrate interest ing aspects competitive performance\n",
            "output sentence:  cnn model pruning method using ista rescaling trick enforce sparsity scaling parameters batch normalization \n",
            "\n",
            "{'rouge-1': {'r': 0.09859154929577464, 'p': 0.7, 'f': 0.1728395040085353}, 'rouge-2': {'r': 0.03488372093023256, 'p': 0.3333333333333333, 'f': 0.0631578930216067}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.6, 'f': 0.14814814598384393}}\n",
            "pair:  present real time method synthesizing highly complex human motions using novel training regime call auto conditioned recurrent neural network acrnn recently researchers attempted synthesize new motion using autoregressive techniques existing methods tend freeze diverge couple seconds due accumulation errors fed back network furthermore methods shown reliable relatively simple human motions walking running contrast approach synthesize arbitrary motions highly complex styles including dances martial arts addition locomotion acrnn able accomplish explicitly accommodating autoregressive noise accumulation training work first knowledge demonstrates ability generate continuous frames seconds new complex human motion different styles\n",
            "output sentence:  synthesize complex extended human motions using auto conditioned lstm network \n",
            "\n",
            "{'rouge-1': {'r': 0.09302325581395349, 'p': 1.0, 'f': 0.1702127644001811}, 'rouge-2': {'r': 0.06481481481481481, 'p': 0.875, 'f': 0.1206896538882283}, 'rouge-l': {'r': 0.09302325581395349, 'p': 1.0, 'f': 0.1702127644001811}}\n",
            "pair:  message passing neural networks mpnns successfully applied wide variety applications real world however two fundamental weaknesses mpnns aggregators limit ability represent graph structured data losing structural information nodes neighborhoods lacking ability capture long range dependencies disassortative graphs studies noticed weaknesses different perspectives observations classical neural network network geometry propose novel geometric aggregation scheme graph neural networks overcome two weaknesses behind basic idea aggregation graph benefit continuous space underlying graph proposed aggregation scheme permutation invariant consists three modules node embedding structural neighborhood bi level aggregation also present implementation scheme graph convolutional networks termed geom gcn perform transductive learning graphs experimental results show proposed geom gcn achieved state art performance wide range open datasets graphs\n",
            "output sentence:  graph neural networks aggregation graph benefit continuous space underlying graph \n",
            "\n",
            "{'rouge-1': {'r': 0.08045977011494253, 'p': 0.875, 'f': 0.14736841951024932}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.5714285714285714, 'f': 0.07079645901480149}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.75, 'f': 0.12631578793130194}}\n",
            "pair:  worst case training principle minimizes maximal adversarial loss also known adversarial training shown state art approach enhancing adversarial robustness norm ball bounded input perturbations nonetheless min max optimization beyond purpose rigorously explored research adversarial attack defense particular given set risk sources domains minimizing maximal loss induced domain set reformulated general min max problem different examples general formulation include attacking model ensembles devising universal perturbation multiple inputs data transformations generalized different types attack models show problems solved unified theoretically principled min max optimization framework also show self adjusted domain weights learned method provides means explain difficulty level attack defense multiple domains extensive experiments show approach leads substantial performance improvement conventional averaging strategy\n",
            "output sentence:  unified min max optimization framework adversarial attack defense \n",
            "\n",
            "{'rouge-1': {'r': 0.06481481481481481, 'p': 0.5, 'f': 0.11475409632894386}, 'rouge-2': {'r': 0.022556390977443608, 'p': 0.23076923076923078, 'f': 0.041095888788703384}, 'rouge-l': {'r': 0.037037037037037035, 'p': 0.2857142857142857, 'f': 0.06557376846009144}}\n",
            "pair:  domain specific goal oriented dialogue systems typically require modeling three types inputs viz knowledge base associated domain ii history conversation sequence utterances iii current utterance response needs generated modeling inputs current state art models mem seq typically ignore rich structure inherent knowledge graph sentences conversation context inspired recent success structure aware graph convolutional networks gcns various nlp tasks machine translation semantic role labeling document dating propose memory augmented gcn goal oriented dialogues model exploits entity relation graph knowledge base ii dependency graph associated utterance compute richer representations words entities take cognizance fact certain situations conversation code mixed language dependency parsers may available show situations could use global word co occurrence graph use enrich representations utterances experiment modified dstc dataset recently released code mixed versions four languages show method outperforms existing state art methods using wide range evaluation metrics\n",
            "output sentence:  propose graph convolutional network based encoder decoder model sequential attention goal oriented dialogue systems \n",
            "\n",
            "{'rouge-1': {'r': 0.1134020618556701, 'p': 0.8461538461538461, 'f': 0.19999999791570247}, 'rouge-2': {'r': 0.022727272727272728, 'p': 0.25, 'f': 0.04166666513888895}, 'rouge-l': {'r': 0.07216494845360824, 'p': 0.5384615384615384, 'f': 0.12727272518842978}}\n",
            "pair:  recurrent neural networks rnns learn continuous vector representations symbolic structures sequences sentences representations often exhibit linear regularities analogies regularities motivate hypothesis rnns show regularities implicitly compile symbolic structures tensor product representations tprs smolensky additively combine tensor products vectors representing roles sequence positions vectors representing fillers particular words test hypothesis introduce tensor product decomposition networks tpdns use tprs approximate existing vector representations demonstrate using synthetic data tpdns successfully approximate linear tree based rnn autoencoder representations suggesting representations exhibit interpretable compositional structure explore settings lead rnns induce structure sensitive representations contrast tpdn experiments show representations four models trained encode naturally occurring sentences largely approximated bag words marginal improvements sophisticated structures conclude tpdns provide powerful method interpreting vector representations standard rnns induce compositional sequence representations remarkably well approximated bytprs time existing training tasks sentence representation learning may sufficient inducing robust structural representations\n",
            "output sentence:  rnns implicitly implement tensor product representations principled interpretable method representing symbolic structures continuous space \n",
            "\n",
            "{'rouge-1': {'r': 0.1038961038961039, 'p': 0.6666666666666666, 'f': 0.17977527856583767}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.09090909090909091, 'f': 0.019999998042000193}, 'rouge-l': {'r': 0.07792207792207792, 'p': 0.5, 'f': 0.13483145834111857}}\n",
            "pair:  reinforcement learning rl typically defines discount factor part markov decision process discount factor values future rewards exponential scheme leads theoretical convergence guarantees bellman equation however evidence psychology economics neuroscience suggests humans animals instead hyperbolic time preferences extend earlier work kurth nelson redish propose efficient deep reinforcement learning agent acts via hyperbolic discounting non exponential discount mechanisms demonstrate simple approach approximates hyperbolic discount functions still using familiar temporal difference learning techniques rl additionally independent hyperbolic discounting make surprising discovery simultaneously learning value functions multiple time horizons effective auxiliary task often improves state art methods\n",
            "output sentence:  deep rl agent learns hyperbolic non exponential values new multi horizon auxiliary task \n",
            "\n",
            "{'rouge-1': {'r': 0.12631578947368421, 'p': 0.8, 'f': 0.21818181582644633}, 'rouge-2': {'r': 0.043478260869565216, 'p': 0.35714285714285715, 'f': 0.07751937790998141}, 'rouge-l': {'r': 0.07368421052631578, 'p': 0.4666666666666667, 'f': 0.1272727249173554}}\n",
            "pair:  network embedding ne methods aim learn low dimensional representations network nodes vectors typically euclidean space representations used variety downstream prediction tasks link prediction one popular choices assessing performance ne methods however complexity link prediction requires carefully designed evaluation pipeline provide consistent reproducible comparable results argue considered sufficiently recent works main goal paper overcome difficulties associated evaluation pipelines reproducibility results introduce evalne evaluation framework transparently assess compare performance ne methods link prediction evalne provides automation abstraction tasks hyper parameter tuning model validation edge sampling computation edge embeddings model validation framework integrates efficient procedures edge non edge sampling used easily evaluate shelf embedding method framework freely available python toolbox finally demonstrating usefulness evalne practice conduct empirical study try replicate analyse experimental sections several influential papers\n",
            "output sentence:  paper introduce evalne python toolbox automating evaluation network embedding methods link prediction ensuring reproducibility results \n",
            "\n",
            "{'rouge-1': {'r': 0.08, 'p': 0.75, 'f': 0.14457831151110467}, 'rouge-2': {'r': 0.03296703296703297, 'p': 0.42857142857142855, 'f': 0.061224488469387786}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.625, 'f': 0.12048192596893599}}\n",
            "pair:  present deep generative model named monge amp ere flow builds continuous time gradient flow arising monge amp ere equation optimal transport theory generative map latent space data space follows dynamical system learnable potential function guides compressible fluid flow towards target density distribution training model amounts solving optimal control problem monge amp ere flow tractable likelihoods supports efficient sampling inference one easily impose symmetry constraints generative model designing suitable scalar potential functions apply approach unsupervised density estimation mnist dataset variational calculation two dimensional ising model critical point approach brings insights techniques monge amp ere equation optimal transport fluid dynamics reversible flow based generative models\n",
            "output sentence:  gradient flow based dynamical system invertible generative modeling \n",
            "\n",
            "{'rouge-1': {'r': 0.13793103448275862, 'p': 0.6666666666666666, 'f': 0.2285714257306123}, 'rouge-2': {'r': 0.09836065573770492, 'p': 0.5454545454545454, 'f': 0.16666666407793213}, 'rouge-l': {'r': 0.1206896551724138, 'p': 0.5833333333333334, 'f': 0.1999999971591837}}\n",
            "pair:  study problem fitting task specific learning rate schedules perspective hyperparameter optimization allows us explicitly search schedules achieve good generalization describe structure gradient validation error learning rates hypergradient based introduce novel online algorithm method adaptively interpolates two recently proposed techniques franceschi et al baydin et al featuring increased stability faster convergence show empirically proposed technique compares favorably baselines related methodsin terms final test accuracy\n",
            "output sentence:  marthe new method fit task specific learning rate schedules perspective hyperparameter optimization \n",
            "\n",
            "{'rouge-1': {'r': 0.029850746268656716, 'p': 0.4, 'f': 0.05555555426311731}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.014925373134328358, 'p': 0.2, 'f': 0.027777776485339568}}\n",
            "pair:  neural conversational models widely used applications like personal assistants chat bots models seem give better performance operating word level however fusion languages like french russian polish vocabulary size sometimes become infeasible since words lots word forms propose neural network architecture transforming normalized text grammatically correct one model efficiently employs correspondence normalized target words significantly outperforms character level models faster training faster evaluation also propose new pipeline building conversational models first generate normalized answer transform grammatically correct one using network proposed pipeline gives better performance character level conversational models according assessor testing\n",
            "output sentence:  proposed architecture solve morphological agreement task \n",
            "\n",
            "{'rouge-1': {'r': 0.2708333333333333, 'p': 0.7222222222222222, 'f': 0.39393938997245176}, 'rouge-2': {'r': 0.12903225806451613, 'p': 0.4444444444444444, 'f': 0.19999999651250003}, 'rouge-l': {'r': 0.1875, 'p': 0.5, 'f': 0.2727272687603306}}\n",
            "pair:  difficult beginners etching latte art make well balanced patterns using two fluids different viscosities foamed milk syrup even though making etching latte art watching making videos show procedure difficult keep balance thus well balanced etching latte art cannot made easily paper propose system supports beginners make well balanced etching latte art projecting making procedure etching latte art directly onto cappuccino experiment results show progress using system also discuss similarity etching latte art design templates using background subtraction\n",
            "output sentence:  developed etching latte art support system projects making procedure directly onto cappuccino help beginners make well balanced balanced etching art \n",
            "\n",
            "{'rouge-1': {'r': 0.1891891891891892, 'p': 0.5, 'f': 0.27450979993848523}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.23076923076923078, 'f': 0.10344827238406672}, 'rouge-l': {'r': 0.13513513513513514, 'p': 0.35714285714285715, 'f': 0.19607842738946568}}\n",
            "pair:  saliency maps often used suggest explanations behavior deep rein forcement learning rl agents however explanations derived saliency maps often unfalsifiable highly subjective introduce empirical approach grounded counterfactual reasoning test hypotheses generated saliency maps show explanations suggested saliency maps often supported experiments experiments suggest saliency maps best viewed exploratory tool rather explanatory tool\n",
            "output sentence:  proposing new counterfactual based methodology evaluate hypotheses generated saliency maps deep rl agent behavior \n",
            "\n",
            "{'rouge-1': {'r': 0.03260869565217391, 'p': 0.3333333333333333, 'f': 0.0594059389706892}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03260869565217391, 'p': 0.3333333333333333, 'f': 0.0594059389706892}}\n",
            "pair:  generative adversarial networks gans trained large datasets diverse modes known produce conflated images distinctly belong modes hypothesize problem occurs due interaction two facts datasets large variety likely modes lie separate manifolds generator formulated continuous function input noise derived connected set due output connected set covers modes must portion output connects corresponds undesirable conflated images develop theoretical arguments support intuitions propose novel method break second assumption via learnable discontinuities latent noise space equivalently viewed training several generators thus creating discontinuities function also augment gan formulation classifier predicts noise partition generator produced output images encouraging diversity partition generator experiment mnist celeba stl difficult dataset clearly distinct modes show noise partitions correspond different modes data distribution produce images superior quality\n",
            "output sentence:  introduce theory explain failure gans complex datasets propose solution fix \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5, 'f': 0.18181817884297521}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.2222222222222222, 'f': 0.06451612655046836}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.5, 'f': 0.18181817884297521}}\n",
            "pair:  recent years several adversarial attacks defenses proposed often seemingly robust models turn non robust sophisticated attacks used one way dilemma provable robustness guarantees provably robust models specific perturbation models developed show come guarantee perturbations propose new regularization scheme mmr universal relu networks enforces robustness wrt textit infty perturbations show leads first provably robust models wrt norm geq\n",
            "output sentence:  introduce method train models provable robustness wrt norms geq simultaneously \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.375, 'f': 0.1071428546938776}, 'rouge-2': {'r': 0.015503875968992248, 'p': 0.11764705882352941, 'f': 0.027397258216363452}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.25, 'f': 0.07142856897959192}}\n",
            "pair:  deploying machine learning systems real world requires high accuracy clean data robustness naturally occurring corruptions architectural advances led improved accuracy building robust models remains challenging involving major changes training procedure datasets prior work argued inherent trade robustness accuracy exemplified standard data augmentation techniques cutout improves clean accuracy robustness additive gaussian noise improves robustness hurts accuracy introduce patch gaussian simple augmentation scheme adds noise randomly selected patches input image models trained patch gaussian achieve state art cifar imagenet common corruptions benchmarks also maintaining accuracy clean data find augmentation leads reduced sensitivity high frequency noise similar gaussian retaining ability take advantage relevant high frequency information image similar cutout show used conjunction regularization methods data augmentation policies autoaugment finally find idea restricting perturbations patches also useful context adversarial learning yielding models without loss accuracy found unconstrained adversarial training\n",
            "output sentence:  simple augmentation method overcomes robustness accuracy trade observed literature opens questions effect training distribution distribution generalization generalization generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.017857142857142856, 'p': 0.2857142857142857, 'f': 0.03361344427088486}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.008928571428571428, 'p': 0.14285714285714285, 'f': 0.016806721581809268}}\n",
            "pair:  generative models shown great success generating high dimensional samples conditional low dimensional descriptors learning stroke thickness mnist hair color celeba speaker identity wavenet generation sample poses fundamental problems conditional variational autoencoder cvae simple conditional generative model explicitly relate conditions training hence incentive learning compact joint distribution across conditions overcome limitation matching distributions using maximum mean discrepancy mmd decoder layer follows bottleneck introduces strong regularization reconstructing samples within condition transforming samples across conditions resulting much improved generalization refer architecture transformer vae trvae benchmarking trvae high dimensional image tabular data demonstrate higher robustness higher accuracy existing approaches particular show qualitatively improved predictions cellular perturbation response treatment disease based high dimensional single cell gene expression data tackling previously problematic minority classes multiple conditions generic tasks improve pearson correlations high dimensional estimated means variances ground truths respectively\n",
            "output sentence:  generates never seen data training desired condition \n",
            "\n",
            "{'rouge-1': {'r': 0.14814814814814814, 'p': 0.6666666666666666, 'f': 0.24242423944903582}, 'rouge-2': {'r': 0.0625, 'p': 0.3333333333333333, 'f': 0.10526315523545714}, 'rouge-l': {'r': 0.12962962962962962, 'p': 0.5833333333333334, 'f': 0.21212120914600552}}\n",
            "pair:  neural population responses sensory stimuli exhibit nonlinear stimulus dependence richly structured shared variability show adversarial training used optimize neural encoding models capture deterministic stochastic components neural population data account discrete nature neural spike trains use rebar method estimate unbiased gradients adversarial optimization neural encoding models illustrate approach population recordings primary visual cortex show adding latent noise sources convolutional neural network yields model captures stimulus dependence noise correlations population activity\n",
            "output sentence:  show neural encoding models trained capture signal spiking variability neural population data using gans \n",
            "\n",
            "{'rouge-1': {'r': 0.13432835820895522, 'p': 0.6, 'f': 0.21951219213265916}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.2, 'f': 0.06122448720324875}, 'rouge-l': {'r': 0.1044776119402985, 'p': 0.4666666666666667, 'f': 0.17073170432778112}}\n",
            "pair:  generative models variational auto encoders vaes generative adversarial networks gans typically trained fixed prior distribution latent space uniform gaussian trained model obtained one sample generator various forms exploration understanding interpolating two samples sampling vicinity sample exploring differences pair samples applied third sample paper show latent space operations used literature far induce distribution mismatch resulting outputs prior distribution model trained address propose use distribution matching transport maps ensure latent space operations preserve prior distribution minimally modifying original operation experimental results validate proposed operations give higher quality samples compared original operations\n",
            "output sentence:  operations gan latent space induce distribution mismatch compared training distribution address using optimal transport match distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.075, 'p': 0.5454545454545454, 'f': 0.1318681297427847}, 'rouge-2': {'r': 0.031578947368421054, 'p': 0.3, 'f': 0.0571428554195012}, 'rouge-l': {'r': 0.075, 'p': 0.5454545454545454, 'f': 0.1318681297427847}}\n",
            "pair:  deep reinforcement learning rl policies known vulnerable adversarial perturbations observations similar adversarial examples classifiers however attacker usually able directly modify another agent observations might lead one wonder possible attack rl agent simply choosing adversarial policy acting multi agent environment create natural observations adversarial demonstrate existence adversarial policies zero sum games simulated humanoid robots proprioceptive observations state art victims trained via self play robust opponents adversarial policies reliably win victims generate seemingly random uncoordinated behavior find policies successful high dimensional environments induce substantially different activations victim policy network victim plays normal opponent videos available https attackingrl github io\n",
            "output sentence:  deep rl policies attacked agents taking actions create natural observations adversarial \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.8, 'f': 0.22222221983024693}, 'rouge-2': {'r': 0.06756756756756757, 'p': 0.5555555555555556, 'f': 0.1204819257773262}, 'rouge-l': {'r': 0.12903225806451613, 'p': 0.8, 'f': 0.22222221983024693}}\n",
            "pair:  present neural rendering architecture helps variational autoencoders vaes learn disentangled representations instead deconvolutional network typically used decoder vaes tile broadcast latent vector across space concatenate fixed coordinate channels apply fully convolutional network stride provides architectural prior dissociating positional non positional features latent space yet without providing explicit supervision effect show architecture term spatial broadcast decoder improves disentangling reconstruction accuracy generalization held regions data space show spatial broadcast decoder complementary state art sota disentangling techniques incorporated improves performance\n",
            "output sentence:  introduce neural rendering architecture helps vaes learn disentangled latent representations \n",
            "\n",
            "{'rouge-1': {'r': 0.05970149253731343, 'p': 0.5714285714285714, 'f': 0.108108106395179}, 'rouge-2': {'r': 0.022988505747126436, 'p': 0.3333333333333333, 'f': 0.043010751481096114}, 'rouge-l': {'r': 0.04477611940298507, 'p': 0.42857142857142855, 'f': 0.08108107936815197}}\n",
            "pair:  credit assignment meta reinforcement learning meta rl still poorly understood existing methods either neglect credit assignment pre adaptation behavior implement naively leads poor sample efficiency meta training well ineffective task identification strategies paper provides theoretical analysis credit assignment gradient based meta rl building gained insights develop novel meta learning algorithm overcomes issue poor credit assignment previous difficulties estimating meta policy gradients controlling statistical distance pre adaptation adapted policies meta policy search proposed algorithm endows efficient stable meta learning approach leads superior pre adaptation policy behavior consistently outperforms previous meta rl algorithms sample efficiency wall clock time asymptotic performance\n",
            "output sentence:  novel theoretically grounded meta reinforcement learning algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.10416666666666667, 'p': 0.625, 'f': 0.17857142612244903}, 'rouge-2': {'r': 0.034482758620689655, 'p': 0.2857142857142857, 'f': 0.06153845961656811}, 'rouge-l': {'r': 0.10416666666666667, 'p': 0.625, 'f': 0.17857142612244903}}\n",
            "pair:  paper develops variational continual learning vcl simple general framework continual learning fuses online variational inference vi recent advances monte carlo vi neural networks framework successfully train deep discriminative models deep generative models complex continual learning settings existing tasks evolve time entirely new tasks emerge experimental results show vcl outperforms state art continual learning methods variety tasks avoiding catastrophic forgetting fully automatic way\n",
            "output sentence:  paper develops principled method continual learning deep models \n",
            "\n",
            "{'rouge-1': {'r': 0.057971014492753624, 'p': 0.23529411764705882, 'f': 0.09302325264196874}, 'rouge-2': {'r': 0.02531645569620253, 'p': 0.125, 'f': 0.04210526035678689}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.17647058823529413, 'f': 0.0697674386884804}}\n",
            "pair:  outline new approaches incorporate ideas deep learning wave based least squares imaging aim main contribution work combination handcrafted constraints deep convolutional neural networks way harness remarkable ease generating natural images mathematical basis underlying method expectation maximization framework data divided batches coupled additional latent unknowns unknowns pairs elements original unknown space coupled specific data batch network inputs setting neural network controls similarity additional parameters acting center variable resulting problem amounts maximum likelihood estimation network parameters augmented data model marginalized latent variables\n",
            "output sentence:  combine hard handcrafted constraints deep prior weak constraint perform seismic imaging reap information posterior distribution leveraging multiplicity data \n",
            "\n",
            "{'rouge-1': {'r': 0.12345679012345678, 'p': 0.7692307692307693, 'f': 0.21276595506337712}, 'rouge-2': {'r': 0.019230769230769232, 'p': 0.16666666666666666, 'f': 0.03448275676575515}, 'rouge-l': {'r': 0.08641975308641975, 'p': 0.5384615384615384, 'f': 0.14893616782933455}}\n",
            "pair:  paper propose continuous graph flow generative continuous flow based method aims model complex distributions graph structured data learned model applied arbitrary graph defining probability density random variables represented graph formulated ordinary differential equation system shared reusable functions operate graphs leads new type neural graph message passing scheme performs continuous message passing time class models offers several advantages flexible representation generalize variable data dimensions ability model dependencies complex data distributions reversible memory efficient exact efficient computation likelihood data demonstrate effectiveness model diverse set generation tasks across different domains graph generation image puzzle generation layout generation scene graphs proposed model achieves significantly better performance compared state art models\n",
            "output sentence:  graph generative models based generalization message passing continuous time using ordinary differential equations \n",
            "\n",
            "{'rouge-1': {'r': 0.32432432432432434, 'p': 0.9230769230769231, 'f': 0.47999999615200006}, 'rouge-2': {'r': 0.2558139534883721, 'p': 0.9166666666666666, 'f': 0.39999999658842983}, 'rouge-l': {'r': 0.32432432432432434, 'p': 0.9230769230769231, 'f': 0.47999999615200006}}\n",
            "pair:  propose approach generate realistic high fidelity stock market data based generative adversarial networks model order stream stochastic process finite history dependence employ conditional wasserstein gan capture history dependence orders stock market test approach actual market synthetic data number different statistics find generated data close real data\n",
            "output sentence:  propose approach generate realistic high fidelity stock market data based generative adversarial networks \n",
            "\n",
            "{'rouge-1': {'r': 0.14606741573033707, 'p': 0.8125, 'f': 0.24761904503582766}, 'rouge-2': {'r': 0.05714285714285714, 'p': 0.4, 'f': 0.09999999781250005}, 'rouge-l': {'r': 0.1348314606741573, 'p': 0.75, 'f': 0.22857142598820862}}\n",
            "pair:  introduce two approaches conducting efficient bayesian inference stochastic simulators containing nested stochastic sub procedures internal procedures density cannot calculated directly rejection sampling loops resulting class simulators used extensively throughout sciences interpreted probabilistic generative models however drawing inferences poses substantial challenge due inability evaluate even unnormalised density preventing use many standard inference procedures like markov chain monte carlo mcmc address introduce inference algorithms based two step approach first approximates conditional densities individual sub procedures using approximations run mcmc methods full program sub procedures dealt separately lower dimensional overall problem two step process allows isolated thus tractably dealt without placing restrictions overall dimensionality problem demonstrate utility approach simple artificially constructed simulator\n",
            "output sentence:  introduce two approaches efficient scalable inference stochastic simulators density cannot evaluated directly due example rejection sampling loops \n",
            "\n",
            "{'rouge-1': {'r': 0.06349206349206349, 'p': 0.36363636363636365, 'f': 0.10810810557706362}, 'rouge-2': {'r': 0.013888888888888888, 'p': 0.09090909090909091, 'f': 0.024096383242851138}, 'rouge-l': {'r': 0.06349206349206349, 'p': 0.36363636363636365, 'f': 0.10810810557706362}}\n",
            "pair:  paper concerns dictionary learning sparse coding fundamental representation learning problem show subgradient descent algorithm random initialization recover orthogonal dictionaries natural nonsmooth nonconvex minimization formulation problem mild statistical assumption data contrast previous provable methods require either expensive computation delicate initialization schemes analysis develops several tools characterizing landscapes nonsmooth functions might independent interest provable training deep networks nonsmooth activations relu among applications preliminary synthetic real experiments corroborate analysis show algorithm works well empirically recovering orthogonal dictionaries\n",
            "output sentence:  efficient dictionary learning minimization via novel analysis non convex non smooth geometry \n",
            "\n",
            "{'rouge-1': {'r': 0.0410958904109589, 'p': 0.375, 'f': 0.07407407229385768}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0410958904109589, 'p': 0.375, 'f': 0.07407407229385768}}\n",
            "pair:  federated learning recent advance privacy protection context trusted curator aggregates parameters optimized decentralized fashion multiple clients resulting model distributed back clients ultimately converging joint representative model without explicitly share data however protocol vulnerable differential attacks could originate party contributing federated optimization attack client contribution training information data set revealed analyzing distributed model tackle problem propose algorithm client sided differential privacy preserving federated optimization aim hide clients contributions training balancing trade privacy loss model performance empirical studies suggest given sufficiently large number participating clients proposed procedure maintain client level differential privacy minor cost model performance\n",
            "output sentence:  ensuring models learned federated fashion reveal client participation \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.46153846153846156, 'f': 0.16901408151557235}, 'rouge-2': {'r': 0.03125, 'p': 0.16666666666666666, 'f': 0.05263157628808878}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.3076923076923077, 'f': 0.11267605334655831}}\n",
            "pair:  application deep recurrent networks audio transcription led impressive gains automatic speech recognition asr systems many demonstrated small adversarial perturbations fool deep neural networks incorrectly predicting specified target high confidence current work fooling asr systems focused white box attacks model architecture parameters known paper adopt black box approach adversarial generation combining approaches genetic algorithms gradient estimation solve task achieve targeted attack similarity generations maintaining audio file similarity\n",
            "output sentence:  present novel black box targeted attack able fool state art speech text transcription \n",
            "\n",
            "{'rouge-1': {'r': 0.109375, 'p': 0.5384615384615384, 'f': 0.18181817901163774}, 'rouge-2': {'r': 0.054945054945054944, 'p': 0.38461538461538464, 'f': 0.0961538439663462}, 'rouge-l': {'r': 0.09375, 'p': 0.46153846153846156, 'f': 0.15584415303761176}}\n",
            "pair:  although languages world pronunciations many phonemes sound similar across languages people learn foreign language pronunciation often reflect native language characteristics motivates us investigate speech synthesis network learns pronunciation multi lingual dataset given study train speech synthesis network bilingually english korean analyze network learns relations phoneme pronunciation languages experimental result shows learned phoneme embedding vectors located closer pronunciations similar across languages based result also show possible train networks synthesize english speaker korean speech vice versa another experiment train network limited amount english dataset large korean dataset analyze required amount dataset train resource poor language help resource rich languages\n",
            "output sentence:  learned phoneme embeddings multilingual neural speech synthesis network could represent relations phoneme pronunciation languages \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5, 'f': 0.19047618739229027}, 'rouge-2': {'r': 0.031746031746031744, 'p': 0.16666666666666666, 'f': 0.05333333064533347}, 'rouge-l': {'r': 0.09803921568627451, 'p': 0.4166666666666667, 'f': 0.15873015564625853}}\n",
            "pair:  paper introduces framework solving combinatorial optimization problems learning input output examples optimization problems introduce new memory augmented neural model memory resettable information stored memory processing input example kept next seen examples used deep reinforcement learning train memory controller agent store useful memories model able outperform hand crafted solver binary linear programming binary lp proposed model tested different binary lp instances large number variables variables constrains constrains\n",
            "output sentence:  propose memory network model solve binary lp instances memory information perseved long term use \n",
            "\n",
            "{'rouge-1': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}, 'rouge-2': {'r': 0.03529411764705882, 'p': 0.25, 'f': 0.06185566793495597}, 'rouge-l': {'r': 0.06756756756756757, 'p': 0.38461538461538464, 'f': 0.11494252619368482}}\n",
            "pair:  dense word vectors proven values many downstream nlp tasks past years however dimensions embeddings easily interpretable dimensions word vector would able understand high low values mean previous approaches addressing issue mainly focused either training sparse non negative constrained word embeddings post processing standard pre trained word embeddings hand analyze conventional word embeddings trained singular value decomposition reveal similar interpretability use novel eigenvector analysis method inspired random matrix theory show semantically coherent groups form row space also column space allows us view individual word vector dimensions human interpretable semantic features\n",
            "output sentence:  without requiring constraints post processing show salient dimensions word vectors interpreted semantic features \n",
            "\n",
            "{'rouge-1': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}, 'rouge-2': {'r': 0.016, 'p': 0.25, 'f': 0.03007518683927869}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}}\n",
            "pair:  domain adaptation tackles problem transferring knowledge label rich source domain unlabeled label scarce target domain recently domain adversarial training dat shown promising capacity learn domain invariant feature space reversing gradient propagation domain classifier however dat still vulnerable several aspects including training instability due overwhelming discriminative ability domain classifier adversarial training restrictive feature level alignment lack interpretability systematic explanation learned feature space paper propose novel max margin domain adversarial training mdat designing adversarial reconstruction network arn proposed mdat stabilizes gradient reversing arn replacing domain classifier reconstruction network manner arn conducts feature level pixel level domain alignment without involving extra network structures furthermore arn demonstrates strong robustness wide range hyper parameters settings greatly alleviating task model selection extensive empirical results validate approach outperforms state art domain alignment methods additionally reconstructed target samples visualized interpret domain invariant feature space conforms intuition\n",
            "output sentence:  stable domain adversarial training approach robust comprehensive domain adaptation \n",
            "\n",
            "{'rouge-1': {'r': 0.24390243902439024, 'p': 0.9523809523809523, 'f': 0.3883495113168065}, 'rouge-2': {'r': 0.18269230769230768, 'p': 0.9047619047619048, 'f': 0.30399999720448007}, 'rouge-l': {'r': 0.24390243902439024, 'p': 0.9523809523809523, 'f': 0.3883495113168065}}\n",
            "pair:  present novel approach spike sorting high density multielectrode probes using neural clustering process ncp recently introduced neural architecture performs scalable amortized approximate bayesian inference efficient probabilistic clustering optimally encode spike waveforms clustering extended ncp adding convolutional spike encoder learned end end ncp network trained purely labeled synthetic spikes simple generative model ncp spike sorting model shows promising performance clustering multi channel spike waveforms model provides higher clustering quality alternative bayesian algorithm finds spike templates clear receptive fields real data recovers ground truth neurons hybrid test data compared recent spike sorting algorithm furthermore ncp able handle clustering uncertainty ambiguous small spikes gpu parallelized posterior sampling source code publicly available\n",
            "output sentence:  present novel approach spike sorting using neural clustering process ncp recently introduced neural architecture performs scalable amortized approximate bayesian inference efficient probabilistic \n",
            "\n",
            "{'rouge-1': {'r': 0.0379746835443038, 'p': 0.42857142857142855, 'f': 0.06976744036506224}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02531645569620253, 'p': 0.2857142857142857, 'f': 0.04651162641157387}}\n",
            "pair:  rise graph structured data social networks regulatory networks citation graphs functional brain networks combination resounding success deep learning various applications brought interest generalizing deep learning models non euclidean domains paper introduce new spectral domain convolutional architecture deep learning graphs core ingredient model new class parametric rational complex functions cayley polynomials allowing efficiently compute spectral filters graphs specialize frequency bands interest model generates rich spectral filters localized space scales linearly size input data sparsely connected graphs handle different constructions laplacian operators extensive experimental results show superior performance approach spectral image classification community detection vertex classification matrix completion tasks\n",
            "output sentence:  spectral graph convolutional neural network spectral zoom properties \n",
            "\n",
            "{'rouge-1': {'r': 0.11363636363636363, 'p': 0.5555555555555556, 'f': 0.18867924246351017}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.11363636363636363, 'p': 0.5555555555555556, 'f': 0.18867924246351017}}\n",
            "pair:  propose stochastic weight averaging parallel swap algorithm accelerate dnn training algorithm uses large mini batches compute approximate solution quickly refines averaging weights multiple models computed independently parallel resulting models generalize equally well trained small mini batches produced substantially shorter time demonstrate reduction training time good generalization performance resulting models computer vision datasets cifar cifar imagenet\n",
            "output sentence:  propose swap distributed algorithm large batch training neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.06930693069306931, 'p': 0.7, 'f': 0.12612612448664884}, 'rouge-2': {'r': 0.025, 'p': 0.3333333333333333, 'f': 0.04651162660897786}, 'rouge-l': {'r': 0.06930693069306931, 'p': 0.7, 'f': 0.12612612448664884}}\n",
            "pair:  graph neural networks shown promising results representing analyzing diverse graph structured data social citation protein interaction networks existing approaches commonly suffer oversmoothing issue regardless whether policies edge based node based neighborhood aggregation methods also focus transductive scenarios fixed graphs leading poor generalization performance unseen graphs address issues propose new graph neural network model considers edge based neighborhood relationships node based entity features graph entities step mixture via random walk gesm gesm employs mixture various steps random walk alleviate oversmoothing problem attention use node information explicitly two mechanisms allow weighted neighborhood aggregation considers properties entities relations intensive experiments show proposed gesm achieves state art comparable performances four benchmark graph datasets comprising transductive inductive learning tasks furthermore empirically demonstrate significance considering global information source code publicly available near future\n",
            "output sentence:  simple effective graph neural network mixture random walk steps attention \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.5, 'f': 0.09090908925619837}, 'rouge-2': {'r': 0.022222222222222223, 'p': 0.2857142857142857, 'f': 0.04123711206291853}, 'rouge-l': {'r': 0.0375, 'p': 0.375, 'f': 0.06818181652892566}}\n",
            "pair:  many domains especially enterprise text analysis abundance data used development new ai powered intelligent experiences improve people productivity however strong guarantees privacy prevent broad sampling labeling personal text data learn evaluate models interest fortunately cases like enterprise email manual annotation possible certain public datasets hope models trained public datasets would perform well target private datasets interest paper study challenges transferring information one email dataset another predicting user intent particular present approaches characterizing transfer gap text corpora intrinsic extrinsic point view evaluate several proposed methods literature bridging gap conclude raising issues discussion arena\n",
            "output sentence:  insights domain adaptation challenge predicting user intent enterprise email \n",
            "\n",
            "{'rouge-1': {'r': 0.06172839506172839, 'p': 0.4166666666666667, 'f': 0.10752687947277147}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04938271604938271, 'p': 0.3333333333333333, 'f': 0.08602150312868545}}\n",
            "pair:  paper introduces new neural structure called fusionnet extends existing attention approaches three perspectives first puts forward novel concept history word characterize attention information lowest word level embedding highest semantic level representation second identifies attention scoring function better utilizes history word concept third proposes fully aware multi level attention mechanism capture complete information one text question exploit counterpart context passage layer layer apply fusionnet stanford question answering dataset squad achieves first position single ensemble model official squad leaderboard time writing oct th meanwhile verify generalization fusionnet two adversarial squad datasets sets new state art datasets addsent fusionnet increases best metric addonesent fusionnet boosts best metric\n",
            "output sentence:  propose light weight enhancement attention neural architecture fusionnet achieve sota squad adversarial squad \n",
            "\n",
            "{'rouge-1': {'r': 0.16923076923076924, 'p': 0.7333333333333333, 'f': 0.27499999695312505}, 'rouge-2': {'r': 0.06329113924050633, 'p': 0.35714285714285715, 'f': 0.10752687916290908}, 'rouge-l': {'r': 0.15384615384615385, 'p': 0.6666666666666666, 'f': 0.24999999695312503}}\n",
            "pair:  propose new learning based approach solve ill posed inverse problems imaging address case ground truth training samples rare problem severely ill posed underlying physics get measurements setting common geophysical imaging remote sensing show case common approach directly learn mapping measured data reconstruction becomes unstable instead propose first learn ensemble simpler mappings data projections unknown image random piecewise constant subspaces combine projections form final reconstruction solving deconvolution like problem show experimentally proposed method robust measurement noise corruptions seen training directly learned inverse\n",
            "output sentence:  solve ill posed inverse problems scarce ground truth examples estimating ensemble random projections model instead model \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.5333333333333333, 'f': 0.20253164249319022}, 'rouge-2': {'r': 0.04054054054054054, 'p': 0.2, 'f': 0.06741572753440234}, 'rouge-l': {'r': 0.125, 'p': 0.5333333333333333, 'f': 0.20253164249319022}}\n",
            "pair:  conversational machine comprehension requires deep understanding conversation history enable traditional single turn models encode history comprehensively introduce flow mechanism incorporate intermediate representations generated process answering previous questions alternating parallel processing structure compared shallow approaches concatenate previous questions answers input flow integrates latent semantics conversation history deeply model flowqa shows superior performance two recently proposed conversational challenges coqa quac effectiveness flow also shows tasks reducing sequential instruction understanding conversational machine comprehension flowqa outperforms best models three domains scone improvement accuracy\n",
            "output sentence:  propose flow mechanism end end architecture flowqa achieves sota two conversational qa datasets sequential instruction understanding task \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.8333333333333334, 'f': 0.24390243652587743}, 'rouge-2': {'r': 0.11538461538461539, 'p': 0.8181818181818182, 'f': 0.20224718884484283}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.8333333333333334, 'f': 0.24390243652587743}}\n",
            "pair:  interpreting neural networks crucial challenging task machine learning paper develop novel framework detecting statistical interactions captured feedforward multilayer neural network directly interpreting learned weights depending desired interactions method achieve significantly better similar interaction detection performance compared state art without searching exponential solution space possible interactions obtain accuracy efficiency observing interactions input features created non additive effect nonlinear activation functions interacting paths encoded weight matrices demonstrate performance method importance discovered interactions via experimental results synthetic datasets real world application datasets\n",
            "output sentence:  detect statistical interactions captured feedforward multilayer neural network directly interpreting learned weights \n",
            "\n",
            "{'rouge-1': {'r': 0.03389830508474576, 'p': 0.2222222222222222, 'f': 0.05882352711505199}, 'rouge-2': {'r': 0.014492753623188406, 'p': 0.125, 'f': 0.02597402411199204}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.2222222222222222, 'f': 0.05882352711505199}}\n",
            "pair:  propose order learning determine order graph classes representing ranks priorities classify object instance one classes end design pairwise comparator categorize relationship two instances one three cases one instance greater similar smaller comparing input instance reference instances maximizing consistency among comparison results class input estimated reliably apply order learning develop facial age estimator provides state art performance moreover performance improved order graph divided disjoint chains using gender ethnic group information even unsupervised manner\n",
            "output sentence:  notion order learning proposed applied regression problems computer vision \n",
            "\n",
            "{'rouge-1': {'r': 0.07272727272727272, 'p': 0.5714285714285714, 'f': 0.12903225606139437}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05454545454545454, 'p': 0.42857142857142855, 'f': 0.09677419154526538}}\n",
            "pair:  abstract work describe set rules design initialization well conditioned neural networks guided goal naturally balancing diagonal blocks hessian start training show measure conditioning block relates another natural measure conditioning ratio weight gradients weights prove relu based deep multilayer perceptron simple initialization scheme using geometric mean fan fan satisfies scaling rule sophisticated architectures show scaling principle used guide design choices produce well conditioned neural networks reducing guess work\n",
            "output sentence:  theory initialization scaling relu neural network layers \n",
            "\n",
            "{'rouge-1': {'r': 0.10227272727272728, 'p': 0.6, 'f': 0.17475727906494487}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.26666666666666666, 'f': 0.06399999788800007}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.5333333333333333, 'f': 0.15533980333678954}}\n",
            "pair:  present approach anytime predictions deep neural networks dnns test sample anytime predictor produces coarse result quickly continues refine test time computational budget depleted predictors address growing computational problem dnns automatically adjusting varying test time budgets work study emph general augmentation feed forward networks form anytime neural networks anns via auxiliary predictions losses specifically point blind spot recent studies anns importance high final accuracy fact show multiple recognition data sets architectures near optimal final predictions small anytime models effectively double speed large ones reach corresponding accuracy level achieve speed simple weighting anytime losses oscillate training also assemble sequence exponentially deepening anns achieve theoretically practically near optimal anytime results budget cost constant fraction additional consumed budget\n",
            "output sentence:  focusing final predictions anytime predictors recent multi scale densenets make small anytime models outperform large ones focus \n",
            "\n",
            "{'rouge-1': {'r': 0.0759493670886076, 'p': 0.5, 'f': 0.13186812957855334}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05063291139240506, 'p': 0.3333333333333333, 'f': 0.08791208562250942}}\n",
            "pair:  deterministic models approximations reality often easier build interpret stochastic alternatives unfortunately nature capricious observational data never fully explained deterministic models practice observation process noise need added adapt deterministic models behave stochastically capable explaining extrapolating noisy data adding process noise deterministic simulators induce failure simulator resulting return value certain inputs property describe brittle investigate address wasted computation arises failures effect failures downstream inference tasks show performing inference space viewed rejection sampling train conditional normalizing flow proposal noise values low probability simulator crashes increasing computational efficiency inference fidelity fixed sample budget used proposal approximate inference algorithm\n",
            "output sentence:  learn conditional autoregressive flow propose perturbations induce simulator failure improving inference performance \n",
            "\n",
            "{'rouge-1': {'r': 0.015151515151515152, 'p': 0.2, 'f': 0.028169012775243075}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.015151515151515152, 'p': 0.2, 'f': 0.028169012775243075}}\n",
            "pair:  although stochastic gradient descent sgd driving force behind recent success deep learning understanding dynamics high dimensional parameter space limited recent years researchers used stochasticity minibatch gradients signal noise ratio better characterize learning dynamics sgd inspired work analyze sgd geometrical perspective inspecting stochasticity norms directions minibatch gradients propose model directional concentration minibatch gradients von mises fisher vmf distribution show directional uniformity minibatch gradients increases course sgd empirically verify result using deep convolutional networks observe higher correlation gradient stochasticity proposed directional uniformity gradient norm stochasticity suggesting directional statistics minibatch gradients major factor behind sgd\n",
            "output sentence:  one theoretical issues deep learning \n",
            "\n",
            "{'rouge-1': {'r': 0.05434782608695652, 'p': 0.625, 'f': 0.09999999852800001}, 'rouge-2': {'r': 0.007874015748031496, 'p': 0.14285714285714285, 'f': 0.014925372144130162}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.5, 'f': 0.07999999852800002}}\n",
            "pair:  graph neural networks gnns received tremendous attention recently due power handling graph data different downstream tasks across different application domains key gnn graph convolutional filters recently various kinds filters designed however still lacks depth analysis whether exists best filter perform best graph data graph properties influence optimal choice graph filter design appropriate filter adaptive graph data paper focus addressing three questions first propose novel assessment tool evaluate effectiveness graph convolutional filters given graph using assessment tool find single filter silver bullet perform best possible graphs addition different graph structure properties influence optimal graph convolutional filter design choice based findings develop adaptive filter graph neural network afgnn simple powerful model adaptively learn task specific filter given graph leverages graph filter assessment regularization learns combine set base filters experiments synthetic real world benchmark datasets demonstrate proposed model indeed learn appropriate filter perform well graph tasks\n",
            "output sentence:  propose assessment framework analyze learn graph convolutional filter \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.6363636363636364, 'f': 0.11382113658272194}, 'rouge-2': {'r': 0.036231884057971016, 'p': 0.5, 'f': 0.06756756630752375}, 'rouge-l': {'r': 0.0625, 'p': 0.6363636363636364, 'f': 0.11382113658272194}}\n",
            "pair:  multi view video summarization mvs lacks researchers attention due major challenges inter view correlations overlapping cameras prior mvs works offline relying summary needing extra communication bandwidth transmission time focus uncertain environments different existing methods propose edge intelligence based mvs spatio temporal features based activity recognition iot environments segment multi view videos slave device edge shots using light weight cnn object detection model compute mutual information among generate summary system rely summary encode transmit master device neural computing stick ncs intelligently computing inter view correlations efficiently recognizing activities thereby saving computation resources communication bandwidth transmission time experiments report increase measure score mvs office dataset well increase activity recognition accuracy ucf youtube datasets respectively lower storage transmission time compared state art time complexity decreased secs single frame processing thereby generating secs faster mvs furthermore made new dataset synthetically adding fog mvs dataset show adaptability system certain uncertain surveillance environments\n",
            "output sentence:  efficient multi view video summarization scheme advanced activity recognition iot environments \n",
            "\n",
            "{'rouge-1': {'r': 0.1746031746031746, 'p': 0.7857142857142857, 'f': 0.2857142827390791}, 'rouge-2': {'r': 0.10666666666666667, 'p': 0.6153846153846154, 'f': 0.18181817930010336}, 'rouge-l': {'r': 0.12698412698412698, 'p': 0.5714285714285714, 'f': 0.2077922048170012}}\n",
            "pair:  common sense background knowledge required understand natural language neural natural language understanding nlu systems requisite background knowledge indirectly acquired static corpora develop new reading architecture dynamic integration explicit background knowledge nlu models new task agnostic reading module provides refined word representations task specific nlu architecture processing background knowledge form free text statements together task specific inputs strong performance tasks document question answering dqa recognizing textual entailment rte demonstrate effectiveness flexibility approach analysis shows models learn exploit knowledge selectively semantically appropriate way\n",
            "output sentence:  paper present task agnostic reading architecture dynamic integration explicit background knowledge neural nlu models \n",
            "\n",
            "{'rouge-1': {'r': 0.0847457627118644, 'p': 0.3333333333333333, 'f': 0.13513513190284887}, 'rouge-2': {'r': 0.02702702702702703, 'p': 0.1111111111111111, 'f': 0.043478257722117435}, 'rouge-l': {'r': 0.0847457627118644, 'p': 0.3333333333333333, 'f': 0.13513513190284887}}\n",
            "pair:  learning rules neural networks necessarily include form regularization regularization techniques conceptualized implemented space parameters however also possible regularize space functions propose measure networks hilbert space test learning rule regularizes distance network travel space update approach inspired slow movement gradient descent parameter space well natural gradient derived regularization term upon functional change resulting learning rule call hilbert constrained gradient descent hcgd thus closely related natural gradient regularizes different calculable metric space functions experiments show hcgd efficient leads considerably better generalization\n",
            "output sentence:  important consider optimization function space parameter space introduce learning rule reduces distance traveled function space like sgd limits distance traveled parameter space \n",
            "\n",
            "{'rouge-1': {'r': 0.14754098360655737, 'p': 0.9, 'f': 0.2535211243404087}, 'rouge-2': {'r': 0.075, 'p': 0.6666666666666666, 'f': 0.13483145885620504}, 'rouge-l': {'r': 0.14754098360655737, 'p': 0.9, 'f': 0.2535211243404087}}\n",
            "pair:  give new algorithm learning two layer neural network general class input distributions assuming ground truth two layer network sigma wx xi weight matrices xi represents noise number neurons hidden layer larger input output algorithm guaranteed recover parameters ground truth network requirement input symmetric still allows highly complicated structured input algorithm based method moments framework extends several results tensor decompositions use spectral algorithms avoid complicated non convex optimization learning neural networks experiments show algorithm robustly learn ground truth neural network small number samples many symmetric input distributions\n",
            "output sentence:  give algorithm learning two layer neural network symmetric input distribution \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.23529411764705882, 'f': 0.10666666316088902}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.23529411764705882, 'f': 0.10666666316088902}}\n",
            "pair:  growing number available services slightly different parameters preconditions effects automated planning general semantic services become highly relevant however exiting planners consider pddl claim use owl usually translate pddl losing much semantics way paper propose new domain independent heuristic based semantic distance used generic planning algorithms automated planning semantic services described owl heuristic include relevant information calculate heuristic runtime using heuristic able produce better results fewer expanded states less time established techniques\n",
            "output sentence:  describing semantic heuristics builds upon owl service description uses word sentence distance measures evaluate usefulness services given goal \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.6363636363636364, 'f': 0.23333333033888887}, 'rouge-2': {'r': 0.0196078431372549, 'p': 0.1, 'f': 0.03278688250470327}, 'rouge-l': {'r': 0.10204081632653061, 'p': 0.45454545454545453, 'f': 0.16666666367222227}}\n",
            "pair:  propose vq wav vec learn discrete representations audio segments wav vec style self supervised context prediction task algorithm uses either gumbel softmax online means clustering quantize dense representations discretization enables direct application algorithms nlp community require discrete inputs experiments show bert pre training achieves new state art timit phoneme classification wsj speech recognition\n",
            "output sentence:  learn quantize speech signal apply algorithms requiring discrete inputs audio data bert \n",
            "\n",
            "{'rouge-1': {'r': 0.08421052631578947, 'p': 0.5714285714285714, 'f': 0.1467889885868193}, 'rouge-2': {'r': 0.008064516129032258, 'p': 0.07692307692307693, 'f': 0.014598538428259568}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.35714285714285715, 'f': 0.09174311702718631}}\n",
            "pair:  weight pruning introduced efficient model compression technique even though pruning removes significant amount weights network memory requirement reduction limited since conventional sparse matrix formats require significant amount memory store index related information moreover computations associated sparse matrix formats slow sequential sparse matrix decoding process utilize highly parallel computing systems efficiently attempt compress index information keeping decoding process parallelizable viterbi based pruning suggested decoding non zero weights however still sequential viterbi based pruning paper propose new sparse matrix format order enable highly parallel decoding process entire sparse matrix proposed sparse matrix constructed combining pruning weight quantization latest rnn models ptb wikitext corpus lstm parameter storage requirement compressed using proposed sparse matrix format compared baseline model compressed weight indices reconstructed dense matrix fast using viterbi encoders simulation results show proposed scheme feed parameters processing elements faster case dense matrix values directly come dram\n",
            "output sentence:  present new weight encoding scheme enables high compression ratio fast sparse dense matrix conversion \n",
            "\n",
            "{'rouge-1': {'r': 0.08571428571428572, 'p': 0.8571428571428571, 'f': 0.15584415419126327}, 'rouge-2': {'r': 0.023809523809523808, 'p': 0.3333333333333333, 'f': 0.04444444320000004}, 'rouge-l': {'r': 0.05714285714285714, 'p': 0.5714285714285714, 'f': 0.10389610224321134}}\n",
            "pair:  planning problems partially observable environments cannot solved directly convolutional networks require form memory even memory networks sophisticated addressing schemes unable learn intelligent reasoning satisfactorily due complexity simultaneously learning access memory plan mitigate challenges propose memory augmented control network macn network splits planning hierarchical process lower level learns plan locally observed space higher level uses collection policies computed locally observed spaces learn optimal plan global environment operating performance network evaluated path planning tasks environments presence simple complex obstacles addition tested ability generalize new environments seen training set\n",
            "output sentence:  memory augmented network plan partially observable environments \n",
            "\n",
            "{'rouge-1': {'r': 0.08620689655172414, 'p': 0.38461538461538464, 'f': 0.14084506743106534}, 'rouge-2': {'r': 0.014925373134328358, 'p': 0.08333333333333333, 'f': 0.02531645311969262}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.3076923076923077, 'f': 0.11267605334655831}}\n",
            "pair:  adversarial neural networks solve many important problems data science notoriously difficult train difficulties come fact optimal weights adversarial nets correspond saddle points minimizers loss function alternating stochastic gradient methods typically used problems reliably converge saddle points convergence happen often highly sensitive learning rates propose simple modification stochastic gradient descent stabilizes adversarial networks show theory practice proposed method reliably converges saddle points makes adversarial networks less likely collapse enables faster training larger learning rates\n",
            "output sentence:  present simple modification alternating sgd method called prediction step improves stability adversarial networks \n",
            "\n",
            "{'rouge-1': {'r': 0.08974358974358974, 'p': 0.7, 'f': 0.1590909070764463}, 'rouge-2': {'r': 0.03488372093023256, 'p': 0.3333333333333333, 'f': 0.0631578930216067}, 'rouge-l': {'r': 0.05128205128205128, 'p': 0.4, 'f': 0.09090908889462815}}\n",
            "pair:  learning mahalanobis metric spaces important problem found numerous applications several algorithms designed problem including information theoretic metric learning itml davis et al large margin nearest neighbor lmnn classification weinberger saul consider formulation mahalanobis metric learning optimization problem objective minimize number violated similarity dissimilarity constraints show fixed ambient dimension exists fully polynomial time approximation scheme fptas nearly linear running time result obtained using tools theory linear programming low dimensions also discuss improvements algorithm practice present experimental results synthetic real world data sets algorithm fully parallelizable performs favorably presence adversarial noise\n",
            "output sentence:  fully parallelizable adversarial noise resistant metric learning algorithm theoretical guarantees \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.6363636363636364, 'f': 0.1590909069034091}, 'rouge-2': {'r': 0.038834951456310676, 'p': 0.36363636363636365, 'f': 0.07017543685287785}, 'rouge-l': {'r': 0.07792207792207792, 'p': 0.5454545454545454, 'f': 0.13636363417613637}}\n",
            "pair:  investigate multi task learning approaches use shared feature representation tasks better understand transfer task information study architecture shared module tasks separate output module task study theory setting linear relu activated models key observation whether tasks data well aligned significantly affect performance multi task learning show misalignment task data cause negative transfer hurt performance provide sufficient conditions positive transfer inspired theoretical insights show aligning tasks embedding layers leads performance gains multi task training transfer learning glue benchmark sentiment analysis tasks example obtained glue score average improvement glue tasks bert large using alignment method also design svd based task weighting scheme show improves robustness multi task training multi label image dataset\n",
            "output sentence:  theoretical study multi task learning practical implications improving multi task training transfer learning \n",
            "\n",
            "{'rouge-1': {'r': 0.1511627906976744, 'p': 0.7222222222222222, 'f': 0.24999999713757398}, 'rouge-2': {'r': 0.02912621359223301, 'p': 0.15789473684210525, 'f': 0.04918032523918316}, 'rouge-l': {'r': 0.09302325581395349, 'p': 0.4444444444444444, 'f': 0.15384615098372786}}\n",
            "pair:  idea neural networks may exhibit bias towards simplicity long history simplicity bias provides way quantify intuition predicts broad class input output maps describe many systems science engineering simple outputs exponentially likely occur upon uniform random sampling inputs complex outputs simplicity bias behaviour observed systems ranging rna sequence secondary structure map systems coupled differential equations models plant growth deep neural networks viewed mapping space parameters weights space functions inputs get transformed outputs network show parameter function map obeys necessary conditions simplicity bias numerically show hugely biased towards functions low descriptional complexity also demonstrate zipf like power law probability rank relation bias towards simplicity may help explain neural nets generalize well\n",
            "output sentence:  strong bias towards simple outpouts observed many simple input ouput maps parameter function map deep networks found biased found biased \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.8, 'f': 0.1355932187877047}, 'rouge-2': {'r': 0.02459016393442623, 'p': 0.3333333333333333, 'f': 0.04580152543791158}, 'rouge-l': {'r': 0.06481481481481481, 'p': 0.7, 'f': 0.11864406624533182}}\n",
            "pair:  batch normalization bn often used attempt stabilize accelerate training deep neural networks many cases indeed decreases number parameter updates required achieve low training error however also reduces robustness small adversarial input perturbations common corruptions double digit percentages show five standard datasets furthermore find substituting weight decay bn sufficient nullify relationship adversarial vulnerability input dimension recent mean field analysis found bn induces gradient explosion used multiple layers cannot fully explain vulnerability observe given occurs already single bn layer argue actual cause tilting decision boundary respect nearest centroid classifier along input dimensions low variance result constant introduced numerical stability bn step acts important hyperparameter tuned recover robustness cost standard test accuracy explain mechanism explicitly linear toy model show experiments still holds nonlinear real world models\n",
            "output sentence:  batch normalization reduces robustness test time common corruptions adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.11, 'p': 0.6111111111111112, 'f': 0.18644067538063777}, 'rouge-2': {'r': 0.0390625, 'p': 0.29411764705882354, 'f': 0.06896551517146261}, 'rouge-l': {'r': 0.08, 'p': 0.4444444444444444, 'f': 0.13559321775351915}}\n",
            "pair:  domain transfer exciting challenging branch machine learning models must learn smoothly transfer domains preserving local variations capturing many aspects variation without labels however successful applications date require two domains closely related ex image image video video utilizing similar shared networks transform domain specific properties like texture coloring line shapes demonstrate possible transfer across modalities ex image audio first abstracting data latent generative models learning transformations latent spaces find simple variational autoencoder able learn shared latent space bridge two generative models unsupervised fashion even different types models ex variational autoencoder generative adversarial network impose desired semantic alignment attributes linear classifier shared latent space proposed variation autoencoder enables preserving locality semantic alignment transfer process shown qualitative quantitative evaluations finally hierarchical structure decouples cost training base generative models semantic alignments enabling computationally efficient data efficient retraining personalized mapping functions\n",
            "output sentence:  conditional vae top latent spaces pre trained generative models enables transfer drastically different domains preserving locality semantic alignment \n",
            "\n",
            "{'rouge-1': {'r': 0.07526881720430108, 'p': 0.4375, 'f': 0.12844036446763743}, 'rouge-2': {'r': 0.00819672131147541, 'p': 0.06666666666666667, 'f': 0.014598538195961689}, 'rouge-l': {'r': 0.07526881720430108, 'p': 0.4375, 'f': 0.12844036446763743}}\n",
            "pair:  several studies recently showing strong natural language understanding nlu models prone relying unwanted dataset biases without learning underlying task resulting models fail generalize domain datasets likely perform poorly real world scenarios propose several learning strategies train neural models robust biases transfer better domain datasets introduce additional lightweight bias model learns dataset biases uses prediction adjust loss base model reduce biases words methods weight importance biased examples focus training hard examples examples cannot correctly classified relying biases approaches model agnostic simple implement experiment large scale natural language inference fact verification datasets domain datasets show debiased models significantly improve robustness settings including gaining points fever symmetric evaluation dataset hans dataset points snli hard set datasets specifically designed assess robustness models domain setting typical biases training data exist evaluation set\n",
            "output sentence:  propose several general debiasing strategies address common biases seen different datasets obtain substantial domain performance settings \n",
            "\n",
            "{'rouge-1': {'r': 0.17142857142857143, 'p': 0.5, 'f': 0.2553191451335446}, 'rouge-2': {'r': 0.075, 'p': 0.2727272727272727, 'f': 0.11764705544021538}, 'rouge-l': {'r': 0.11428571428571428, 'p': 0.3333333333333333, 'f': 0.17021276215482128}}\n",
            "pair:  computer vision tasks image classification image retrieval shot learning currently dominated euclidean spherical embeddings final decisions class belongings degree similarity made using linear hyperplanes euclidean distances spherical geodesic distances cosine similarity work demonstrate many practical scenarios hyperbolic embeddings provide better alternative\n",
            "output sentence:  show hyperbolic embeddings useful high level computer vision tasks especially shot classification \n",
            "\n",
            "{'rouge-1': {'r': 0.09859154929577464, 'p': 0.3888888888888889, 'f': 0.15730336755965163}, 'rouge-2': {'r': 0.022988505747126436, 'p': 0.11764705882352941, 'f': 0.03846153572670138}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.3333333333333333, 'f': 0.13483145744729208}}\n",
            "pair:  paper explore meta learning shot text classification meta learning shown strong performance computer vision low level patterns transferable across learning tasks however directly applying approach text challenging lexical features highly informative one task maybe insignificant another thus rather learning solely words model also leverages distributional signatures encode pertinent word occurrence patterns model trained within meta learning framework map signatures attention scores used weight lexical representations words demonstrate model consistently outperforms prototypical networks learned lexical knowledge snell et al shot text classification relation classification significant margin across six benchmark datasets average shot classification\n",
            "output sentence:  meta learning methods used vision directly applied nlp perform worse nearest neighbors new classes better distributional signatures signatures \n",
            "\n",
            "{'rouge-1': {'r': 0.19101123595505617, 'p': 0.9444444444444444, 'f': 0.3177570065472967}, 'rouge-2': {'r': 0.13445378151260504, 'p': 0.8888888888888888, 'f': 0.23357664005327938}, 'rouge-l': {'r': 0.16853932584269662, 'p': 0.8333333333333334, 'f': 0.28037382897720325}}\n",
            "pair:  existing neural question answering qa models required reason draw complicated inferences long context large scale qa datasets however view qa combined retrieval reasoning task assume existence minimal context necessary sufficient answer given question recent work shown sentence selector module selects shorter context feeds downstream qa model achieves performance comparable qa model trained full context also interpretable recent work also shown state art qa models break adversarially generated sentences appended context humans immune distractor sentences qa models get easily misled selecting answers sentences hypothesize sentence selector module filter extraneous context thereby allowing downstream qa model focus reason parts context relevant question paper show sentence selector susceptible adversarial inputs however demonstrate pipeline consisting sentence selector module followed qa model made robust adversarial attacks comparison qa model trained full context thus provide evidence towards modular approach question answering robust interpretable\n",
            "output sentence:  modular approach consisting sentence selector module followed qa model made robust adversarial attacks comparison qa model trained full context context \n",
            "\n",
            "{'rouge-1': {'r': 0.17333333333333334, 'p': 0.7222222222222222, 'f': 0.2795698893513701}, 'rouge-2': {'r': 0.06593406593406594, 'p': 0.35294117647058826, 'f': 0.11111110845850486}, 'rouge-l': {'r': 0.09333333333333334, 'p': 0.3888888888888889, 'f': 0.15053763128685405}}\n",
            "pair:  estimating importance atom molecule one appealing challenging problems chemistry physics material engineering common way estimate atomic importance compute electronic structure using density functional theory dft interpret using domain knowledge human experts however conventional approach impractical large molecular database dft calculation requires huge computation specifically time complexity number electrons molecule furthermore calculation results interpreted human experts estimate atomic importance terms target molecular property tackle problem first exploit machine learning based approach atomic importance estimation end propose reverse self attention graph neural networks integrate graph based molecular description method provides efficiently automated target directed way estimate atomic importance without domain knowledge chemistry physics\n",
            "output sentence:  first propose fully automated target directed atomic importance estimator based graph neural networks new concept reverse self attention \n",
            "\n",
            "{'rouge-1': {'r': 0.1794871794871795, 'p': 0.7, 'f': 0.28571428246563935}, 'rouge-2': {'r': 0.0707070707070707, 'p': 0.3333333333333333, 'f': 0.11666666377916672}, 'rouge-l': {'r': 0.15384615384615385, 'p': 0.6, 'f': 0.2448979559350271}}\n",
            "pair:  training methods deep networks primarily variants stochastic gradient descent techniques use approximate second order information rarely used computational cost noise associated approaches deep learning contexts however paper show feedforward deep networks exhibit low rank derivative structure low rank structure makes possible use second order information without needing approximations without incurring significantly greater computational cost gradient descent demonstrate capability implement cubic regularization cr feedforward deep network stochastic gradient descent two variants use cr calculate learning rates per iteration basis training mnist cifar datasets cr proved particularly successful escaping plateau regions objective function also found approach requires less problem specific information optimal initial learning rate first order methods order perform well\n",
            "output sentence:  show deep learning network derivatives low rank structure structure allows us use second order derivative information calculate learning rates adaptively computationally feasible \n",
            "\n",
            "{'rouge-1': {'r': 0.16363636363636364, 'p': 0.6923076923076923, 'f': 0.2647058792603807}, 'rouge-2': {'r': 0.05, 'p': 0.3333333333333333, 'f': 0.0869565194706995}, 'rouge-l': {'r': 0.14545454545454545, 'p': 0.6153846153846154, 'f': 0.23529411455449828}}\n",
            "pair:  glove skip gram word embedding methods learn word vectors decomposing denoised matrix word co occurrences product low rank matrices work propose iterative algorithm computing word vectors based modeling word co occurrence matrices generalized low rank models algorithm generalizes skip gram glove well giving rise embedding methods based specified co occurrence matrix distribution co occurences number iterations iterative algorithm example using tweedie distribution one iteration results glove using multinomial distribution full convergence mode results skip gram experimental results demonstrate multiple iterations algorithm improves results glove method google word analogy similarity task\n",
            "output sentence:  present novel iterative algorithm based generalized low rank models computing interpreting word embedding \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.6, 'f': 0.19354838439125913}, 'rouge-2': {'r': 0.05172413793103448, 'p': 0.3333333333333333, 'f': 0.08955223648028521}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.4, 'f': 0.1290322553590011}}\n",
            "pair:  recent findings show deep generative models judge distribution samples likely drawn distribution training data work focus variational autoencoders vaes address problem misaligned likelihood estimates image data develop novel likelihood function based parameters returned vae also features data learned self supervised fashion way model additionally captures semantic information disregarded usual vae likelihood function demonstrate improvements reliability estimates experiments fashionmnist mnist datasets\n",
            "output sentence:  improved likelihood estimates variational autoencoders using self supervised feature learning \n",
            "\n",
            "{'rouge-1': {'r': 0.12941176470588237, 'p': 0.6470588235294118, 'f': 0.21568627173202617}, 'rouge-2': {'r': 0.037383177570093455, 'p': 0.25, 'f': 0.06504064814330102}, 'rouge-l': {'r': 0.11764705882352941, 'p': 0.5882352941176471, 'f': 0.19607842859477126}}\n",
            "pair:  convergence rate final performance common deep learning models significantly benefited recently proposed heuristics learning rate schedules knowledge distillation skip connections normalization layers absence theoretical underpinnings controlled experiments aimed explaining efficacy strategies aid understanding deep learning landscapes training dynamics existing approaches empirical analysis rely tools linear interpolation visualizations dimensionality reduction limitations instead revisit empirical analysis heuristics lens recently proposed methods loss surface representation analysis viz mode connectivity canonical correlation analysis cca hypothesize reasons heuristics succeed particular explore knowledge distillation learning rate heuristics cosine restarts warmup using mode connectivity cca empirical analysis suggests reasons often quoted success cosine annealing evidenced practice effect learning rate warmup prevent deeper layers creating training instability latent knowledge shared teacher primarily disbursed deeper layers\n",
            "output sentence:  use empirical tools mode connectivity svcca investigate neural network training heuristics learning rate restarts warmup knowledge distillation \n",
            "\n",
            "{'rouge-1': {'r': 0.24390243902439024, 'p': 0.45454545454545453, 'f': 0.31746031291509197}, 'rouge-2': {'r': 0.12244897959183673, 'p': 0.25, 'f': 0.1643835572302497}, 'rouge-l': {'r': 0.17073170731707318, 'p': 0.3181818181818182, 'f': 0.22222221767699685}}\n",
            "pair:  deep learning models efficiently optimized via stochastic gradient descent little theoretical evidence support key question optimization understand optimization landscape neural network amenable gradient based optimization focus simple neural network two layer relu network two hidden units show local minimizers global combined recent work lee et al lee et al show gradient descent converges global minimizer\n",
            "output sentence:  recovery guarantee stochastic gradient descent random initialization learning two layer neural network two hidden nodes unit norm weights relu activation activation functions functions functions gaussian inputs \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.5, 'f': 0.18181817884297521}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.2222222222222222, 'f': 0.06451612655046836}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.5, 'f': 0.18181817884297521}}\n",
            "pair:  recent years several adversarial attacks defenses proposed often seemingly robust models turn non robust sophisticated attacks used one way dilemma provable robustness guarantees provably robust models specific perturbation models developed show come guarantee perturbations propose new regularization scheme mmr universal relu networks enforces robustness wrt textit infty perturbations show leads first provably robust models wrt norm geq\n",
            "output sentence:  introduce method train models provable robustness wrt norms geq simultaneously \n",
            "\n",
            "{'rouge-1': {'r': 0.12149532710280374, 'p': 0.6842105263157895, 'f': 0.20634920378810784}, 'rouge-2': {'r': 0.04878048780487805, 'p': 0.3333333333333333, 'f': 0.08510638075147131}, 'rouge-l': {'r': 0.08411214953271028, 'p': 0.47368421052631576, 'f': 0.14285714029604438}}\n",
            "pair:  continual learning problem sequentially learning new tasks knowledge protecting previously acquired knowledge however catastrophic forgetting poses grand challenge neural networks performing learning process thus neural networks deployed real world often struggle scenarios data distribution non stationary concept drift imbalanced always fully available rare edge cases propose differentiable hebbian consolidation model composed differentiable hebbian plasticity dhp softmax layer adds rapid learning plastic component compressed episodic memory fixed slow changing parameters softmax output layer enabling learned representations retained longer timescale demonstrate flexibility method integrating well known task specific synaptic consolidation methods penalize changes slow weights important target task evaluate approach permuted mnist split mnist vision datasets mixture benchmarks introduce imbalanced variant permuted mnist dataset combines challenges class imbalance concept drift proposed model requires additional hyperparameters outperforms comparable baselines reducing forgetting\n",
            "output sentence:  hebbian plastic weights behave compressed episodic memory storage neural networks combination task specific synaptic consolidation improve ability learning learning \n",
            "\n",
            "{'rouge-1': {'r': 0.08771929824561403, 'p': 0.35714285714285715, 'f': 0.1408450672564968}, 'rouge-2': {'r': 0.029411764705882353, 'p': 0.15384615384615385, 'f': 0.04938271335467169}, 'rouge-l': {'r': 0.08771929824561403, 'p': 0.35714285714285715, 'f': 0.1408450672564968}}\n",
            "pair:  propose new notion non linearity network layer respect input batch based proximity linear system reflected non negative rank activation matrix measure non linearity applying non negative factorization activation matrix considering batches similar samples find high non linearity deep layers indicative memorization furthermore applying approach layer layer find mechanism memorization consists distinct phases perform experiments fully connected convolutional neural networks trained several image audio datasets results demonstrate indicator memorization technique used perform early stopping\n",
            "output sentence:  use non negative rank relu activation matrices complexity measure show negatively correlates good generalization \n",
            "\n",
            "{'rouge-1': {'r': 0.16470588235294117, 'p': 0.875, 'f': 0.277227720105872}, 'rouge-2': {'r': 0.09615384615384616, 'p': 0.625, 'f': 0.16666666435555558}, 'rouge-l': {'r': 0.15294117647058825, 'p': 0.8125, 'f': 0.25742573990785217}}\n",
            "pair:  important question task transfer learning determine task transferability given common input domain estimating extent representations learned source task help learning target task typically transferability either measured experimentally inferred task relatedness often defined without clear operational meaning paper present novel metric score easily computable evaluation function estimates performance transferred representations one task another classification problems inspired principled information theoretic approach score direct connection asymptotic error probability decision function based transferred feature formulation transferability used select suitable set source tasks task transfer learning problems devise efficient transfer learning policies experiments using synthetic real image data show formulation transferability meaningful practice also generalize inference problems beyond classification recognition tasks indoor scene understanding\n",
            "output sentence:  present provable easily computable evaluation function estimates performance transferred representations one learning task another task transfer learning \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.7777777777777778, 'f': 0.24137930772294885}, 'rouge-2': {'r': 0.08771929824561403, 'p': 0.625, 'f': 0.15384615168757396}, 'rouge-l': {'r': 0.12244897959183673, 'p': 0.6666666666666666, 'f': 0.20689654910225921}}\n",
            "pair:  propose method joint image per pixel annotation synthesis gan demonstrate gan good high level representation target data easily projected semantic segmentation masks method used create training dataset teaching separate semantic segmentation network experiments show segmentation network successfully generalizes real data additionally method outperforms supervised training number training samples small works variety different scenes classes source code proposed method publicly available\n",
            "output sentence:  gan based method joint image per pixel annotation synthesis \n",
            "\n",
            "{'rouge-1': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}, 'rouge-2': {'r': 0.023255813953488372, 'p': 0.16666666666666666, 'f': 0.04081632438150782}, 'rouge-l': {'r': 0.0945945945945946, 'p': 0.5384615384615384, 'f': 0.16091953768793768}}\n",
            "pair:  driving force behind recent success lstms ability learn complex non linear relationships consequently inability describe relationships led lstms characterized black boxes end introduce contextual decomposition cd interpretation algorithm analysing individual predictions made standard lstms without changes underlying model decomposing output lstm cd captures contributions combinations words variables final prediction lstm task sentiment analysis yelp sst data sets show cd able reliably identify words phrases contrasting sentiment combined yield lstm final prediction using phrase level labels sst also demonstrate cd able successfully extract positive negative negations lstm something previously done\n",
            "output sentence:  introduce contextual decompositions interpretation algorithm lstms capable extracting word phrase interaction level importance score \n",
            "\n",
            "{'rouge-1': {'r': 0.23076923076923078, 'p': 0.631578947368421, 'f': 0.33802816509422734}, 'rouge-2': {'r': 0.07246376811594203, 'p': 0.25, 'f': 0.11235954707738934}, 'rouge-l': {'r': 0.17307692307692307, 'p': 0.47368421052631576, 'f': 0.25352112284070627}}\n",
            "pair:  zoo deep nets available days almost given task increasingly unclear net start addressing new task net use initialization fine tuning new model address issue paper develop knowledge flow moves knowledge multiple deep nets referred teachers new deep net model called student structure teachers student differ arbitrarily trained entirely different tasks different output spaces upon training knowledge flow student independent teachers demonstrate approach variety supervised reinforcement learning tasks outperforming fine tuning knowledge exchange methods\n",
            "output sentence:  knowledge flow trains deep net student injecting information multiple nets teachers student independent upon training performs well tasks tasks irrespective irrespective \n",
            "\n",
            "{'rouge-1': {'r': 0.11458333333333333, 'p': 0.6875, 'f': 0.19642856897959182}, 'rouge-2': {'r': 0.01694915254237288, 'p': 0.13333333333333333, 'f': 0.030075185968681233}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.5, 'f': 0.14285714040816327}}\n",
            "pair:  investigate methods efficiently learn diverse strategies reinforcement learning generative structured prediction problem query reformulation proposed framework agent consists multiple specialized sub agents meta agent learns aggregate answers sub agents produce final answer sub agents trained disjoint partitions training data meta agent trained full training set method makes learning faster highly parallelizable better generalization performance strong baselines ensemble agents trained full data evaluate tasks document retrieval question answering improved performance seems due increased diversity reformulation strategies suggests multi agent hierarchical approaches might play important role structured prediction tasks kind however also find obvious characterize diversity context first attempt based clustering produce good results furthermore reinforcement learning reformulation task hard high performance regimes best marginally improves state art highlights complexity training models framework end end language understanding problems\n",
            "output sentence:  use reinforcement learning query reformulation two tasks surprisingly find training multiple agents diversity reformulations important specialisation \n",
            "\n",
            "{'rouge-1': {'r': 0.08823529411764706, 'p': 0.75, 'f': 0.15789473495844877}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.125, 'f': 0.022471908476202618}, 'rouge-l': {'r': 0.08823529411764706, 'p': 0.75, 'f': 0.15789473495844877}}\n",
            "pair:  regularizers critical tools machine learning due ability simplify solutions however imposing strong regularization gradient descent method easily fails limits generalization ability underlying neural networks understand phenomenon investigate training fails strong regularization specifically examine gradients change time different regularization strengths provide analysis gradients diminish fast find exists tolerance level regularization strength learning completely fails regularization strength goes beyond propose simple novel method delayed strong regularization order moderate tolerance level experiment results show proposed approach indeed achieves strong regularization regularizers improves accuracy sparsity public data sets source code published\n",
            "output sentence:  investigate strong regularization fails propose method achieve strong regularization \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.75, 'f': 0.1935483848491155}, 'rouge-2': {'r': 0.07547169811320754, 'p': 0.6153846153846154, 'f': 0.13445377956641483}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.75, 'f': 0.1935483848491155}}\n",
            "pair:  deep neural networks dnns typically enough capacity fit random data brute force even conventional data dependent regularizations focusing geometry features imposed find reason inconsistency enforced geometry standard softmax cross entropy loss resolve propose new framework data dependent dnn regularization geometrically regularized self validating neural networks grsvnet training geometry enforced one batch features simultaneously validated separate batch using validation loss consistent geometry study particular case grsvnet orthogonal low rank embedding ole grsvnet capable producing highly discriminative features residing orthogonal low rank subspaces numerical experiments show ole grsvnet outperforms dnns conventional regularization trained real data importantly unlike conventional dnns ole grsvnet refuses memorize random data random labels suggesting learns intrinsic patterns reducing memorizing capacity baseline dnn\n",
            "output sentence:  propose new framework data dependent dnn regularization prevent dnns overfitting random data random labels \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.75, 'f': 0.16216216023374724}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.5714285714285714, 'f': 0.08421052495069253}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.75, 'f': 0.16216216023374724}}\n",
            "pair:  common practice decay learning rate show one usually obtain learning curve training test sets instead increasing batch size training procedure successful stochastic gradient descent sgd sgd momentum nesterov momentum adam reaches equivalent test accuracies number training epochs fewer parameter updates leading greater parallelism shorter training times reduce number parameter updates increasing learning rate epsilon scaling batch size propto epsilon finally one increase momentum coefficient scale propto although tends slightly reduce test accuracy crucially techniques allow us repurpose existing training schedules large batch training hyper parameter tuning train resnet imagenet validation accuracy minutes\n",
            "output sentence:  decaying learning rate increasing batch size training equivalent \n",
            "\n",
            "{'rouge-1': {'r': 0.21568627450980393, 'p': 0.9166666666666666, 'f': 0.349206346122449}, 'rouge-2': {'r': 0.15873015873015872, 'p': 0.9090909090909091, 'f': 0.27027026773922574}, 'rouge-l': {'r': 0.21568627450980393, 'p': 0.9166666666666666, 'f': 0.349206346122449}}\n",
            "pair:  paper presents method explain knowledge encoded convolutional neural network cnn quantitatively semantically analyze specific rationale prediction made cnn presents one key issues understanding neural networks also significant practical values certain applications study propose distill knowledge cnn explainable additive model use explainable model provide quantitative explanation cnn prediction analyze typical bias interpreting problem explainable model develop prior losses guide learning explainable additive model experimental results demonstrated effectiveness method\n",
            "output sentence:  paper presents method explain knowledge encoded convolutional neural network cnn quantitatively semantically \n",
            "\n",
            "{'rouge-1': {'r': 0.09574468085106383, 'p': 0.9, 'f': 0.1730769213387574}, 'rouge-2': {'r': 0.056074766355140186, 'p': 0.6, 'f': 0.10256410100080358}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.8, 'f': 0.15384615210798816}}\n",
            "pair:  generative adversarial networks gans become gold standard comes learning generative models high dimensional distributions since advent numerous variations gans introduced literature primarily focusing utilization novel loss functions optimization regularization strategies network architectures paper turn attention generator investigate use high order polynomials alternative class universal function approximators concretely propose polygan model data generator means high order polynomial whose unknown parameters naturally represented high order tensors introduce two tensor decompositions significantly reduce number parameters show efficiently implemented hierarchical neural networks employ linear convolutional blocks exhibit first time using approach gan generator approximate data distribution without using activation functions thorough experimental evaluation synthetic real data images point clouds demonstrates merits polygan state art\n",
            "output sentence:  model data generator gan means high order polynomial represented high order tensors \n",
            "\n",
            "{'rouge-1': {'r': 0.028985507246376812, 'p': 0.4, 'f': 0.05405405279401025}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.028985507246376812, 'p': 0.4, 'f': 0.05405405279401025}}\n",
            "pair:  anatomical studies demonstrate brain reformats input information generate reliable responses performing computations however remains unclear neural circuits encode complex spatio temporal patterns show neural dynamics strongly influenced phase alignment input spontaneous chaotic activity input alignment along dominant chaotic projections causes chaotic trajectories become stable channels attractors hence improving computational capability recurrent network using mean field analysis derive impact input alignment overall stability attractors formed results indicate input alignment determines extent intrinsic noise suppression hence alters attractor state stability thereby controlling network inference ability\n",
            "output sentence:  input structuring along chaos stability \n",
            "\n",
            "{'rouge-1': {'r': 0.06666666666666667, 'p': 0.5, 'f': 0.11764705674740487}, 'rouge-2': {'r': 0.009708737864077669, 'p': 0.09090909090909091, 'f': 0.01754385790550956}, 'rouge-l': {'r': 0.044444444444444446, 'p': 0.3333333333333333, 'f': 0.0784313704728951}}\n",
            "pair:  discretizing floating point vectors fundamental step modern indexing methods state art techniques learn parameters quantizers training data optimal performance thus adapting quantizers data work propose reverse paradigm adapt data quantizer train neural net whose last layers form fixed parameter free quantizer pre defined points sphere proxy objective design train neural network favors uniformity spherical latent space preserving neighborhood structure mapping purpose propose new regularizer derived kozachenko leonenko differential entropy estimator combine locality aware triplet loss experiments show end end approach outperforms learned quantization methods competitive state art widely adopted benchmarks show training without quantization step results almost difference accuracy yields generic catalyser applied subsequent quantization technique\n",
            "output sentence:  learn neural network uniformizes input distribution leads competitive indexing performance high dimensional space \n",
            "\n",
            "{'rouge-1': {'r': 0.102803738317757, 'p': 0.5789473684210527, 'f': 0.17460317204207612}, 'rouge-2': {'r': 0.059322033898305086, 'p': 0.3888888888888889, 'f': 0.1029411741738755}, 'rouge-l': {'r': 0.102803738317757, 'p': 0.5789473684210527, 'f': 0.17460317204207612}}\n",
            "pair:  deep cnns achieved state art performance numerous machine learning computer vision tasks recent years become increasingly deep number parameters use also increased making hard deploy memory constrained environments difficult interpret machine learning theory implies networks highly parameterised possible reduce size without sacrificing accuracy indeed many recent studies begun highlight specific redundancies exploited achieve paper take step direction proposing filter sharing approach compressing deep cnns reduces memory footprint repeatedly applying single convolutional mapping learned filters simulate cnn pipeline show via experiments cifar cifar tiny imagenet imagenet allows us reduce parameter counts networks based common designs vggnet resnet factor proportional depth whilst leaving accuracy largely unaffected broader level approach also indicates scale space regularities found visual signals leveraged build neural architectures parsimonious interpretable\n",
            "output sentence:  compress deep cnns reusing single convolutional layer iterative manner thereby reducing parameter counts factor proportional depth whilst leaving unaffected \n",
            "\n",
            "{'rouge-1': {'r': 0.19117647058823528, 'p': 1.0, 'f': 0.3209876516262765}, 'rouge-2': {'r': 0.15584415584415584, 'p': 1.0, 'f': 0.2696629190152759}, 'rouge-l': {'r': 0.19117647058823528, 'p': 1.0, 'f': 0.3209876516262765}}\n",
            "pair:  graph classification currently dominated graph kernels powerful suffer significant limitations convolutional neural networks cnns offer appealing alternative however processing graphs cnns trivial address challenge many sophisticated extensions cnns recently proposed paper reverse problem rather proposing yet another graph cnn model introduce novel way represent graphs multi channel image like structures allows handled vanilla cnns despite simplicity method proves competitive state art graph kernels graph cnns outperforms wide margin datasets also preferable graph kernels terms time complexity code data publicly available\n",
            "output sentence:  introduce novel way represent graphs multi channel image like structures allows handled vanilla cnns \n",
            "\n",
            "{'rouge-1': {'r': 0.20238095238095238, 'p': 0.8947368421052632, 'f': 0.33009708436987467}, 'rouge-2': {'r': 0.09615384615384616, 'p': 0.5263157894736842, 'f': 0.16260162340405845}, 'rouge-l': {'r': 0.17857142857142858, 'p': 0.7894736842105263, 'f': 0.29126213291356395}}\n",
            "pair:  recent powerful pre trained language models achieved remarkable performance popular datasets reading comprehension time introduce challenging datasets push development field towards comprehensive reasoning text paper introduce new reading comprehension dataset requiring logical reasoning reclor extracted standardized graduate admission examinations earlier studies suggest human annotated datasets usually contain biases often exploited models achieve high accuracy without truly understanding text order comprehensively evaluate logical reasoning ability models reclor propose identify biased data points separate easy set rest hard set empirical results show state art models outstanding ability capture biases contained dataset high accuracy easy set however struggle hard set poor performance near random guess indicating research needed essentially enhance logical reasoning ability current models\n",
            "output sentence:  introduce reclor reading comprehension dataset requiring logical reasoning find current state art models struggle real logical reasoning poor performance near random guess \n",
            "\n",
            "{'rouge-1': {'r': 0.10294117647058823, 'p': 0.7777777777777778, 'f': 0.18181817975375275}, 'rouge-2': {'r': 0.0375, 'p': 0.3333333333333333, 'f': 0.06741572851912643}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.5555555555555556, 'f': 0.1298701278057008}}\n",
            "pair:  learning control environment without hand crafted rewards expert data remains challenging frontier reinforcement learning research present unsupervised learning algorithm train agents achieve perceptually specified goals using stream observations actions agent simultaneously learns goal conditioned policy goal achievement reward function measures similar state goal state dual optimization leads co operative game giving rise learned reward function reflects similarity controllable aspects environment instead distance space observations demonstrate efficacy agent learn unsupervised manner reach diverse set goals three domains atari deepmind control suite deepmind lab\n",
            "output sentence:  unsupervised reinforcement learning method learning policy robustly achieve perceptually specified goals \n",
            "\n",
            "{'rouge-1': {'r': 0.16, 'p': 0.6666666666666666, 'f': 0.2580645130072841}, 'rouge-2': {'r': 0.10344827586206896, 'p': 0.42857142857142855, 'f': 0.16666666353395063}, 'rouge-l': {'r': 0.08, 'p': 0.3333333333333333, 'f': 0.12903225494276802}}\n",
            "pair:  propose support guided adversarial imitation learning sail generic imitation learning framework unifies support estimation expert policy family adversarial imitation learning ail algorithms sail addresses two important challenges ail including implicit reward bias potential training instability also show sail least efficient standard ail extensive evaluation demonstrate proposed method effectively handles reward bias achieves better performance training stability baseline methods wide range benchmark control tasks\n",
            "output sentence:  unify support estimation family adversarial imitation learning algorithms support guided adversarial imitation learning robust stable imitation learning framework \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.5833333333333334, 'f': 0.13592232803845794}, 'rouge-2': {'r': 0.017094017094017096, 'p': 0.18181818181818182, 'f': 0.03124999842895516}, 'rouge-l': {'r': 0.054945054945054944, 'p': 0.4166666666666667, 'f': 0.09708737658214729}}\n",
            "pair:  general problem received considerable recent attention perform multiple tasks network maximizing efficiency prediction accuracy popular approach consists multi branch architecture top shared backbone jointly trained weighted sum losses however many cases shared representation results non optimal performance mainly due interference conflicting gradients uncorrelated tasks recent approaches address problem channel wise modulation feature maps along shared backbone task specific vectors manually dynamically tuned taking approach step propose novel architecture modulate recognition network channel wise well spatial wise efficient top image dependent computation scheme architecture uses task specific branches task specific modules instead uses top modulation network shared tasks show effectiveness scheme achieving par better results alternative approaches correlated uncorrelated sets tasks also demonstrate advantages terms model size addition novel tasks interpretability code released\n",
            "output sentence:  propose top modulation network multi task learning applications several advantages current schemes \n",
            "\n",
            "{'rouge-1': {'r': 0.07228915662650602, 'p': 0.5454545454545454, 'f': 0.12765957240153916}, 'rouge-2': {'r': 0.027777777777777776, 'p': 0.2727272727272727, 'f': 0.05042016638937934}, 'rouge-l': {'r': 0.07228915662650602, 'p': 0.5454545454545454, 'f': 0.12765957240153916}}\n",
            "pair:  deep neural networks known annotation hungry numerous efforts devoted reducing annotation cost learning deep networks two prominent directions include learning noisy labels semi supervised learning exploiting unlabeled data work propose dividemix novel framework learning noisy labels leveraging semi supervised learning techniques particular dividemix models per sample loss distribution mixture model dynamically divide training data labeled set clean samples unlabeled set noisy samples trains model labeled unlabeled data semi supervised manner avoid confirmation bias simultaneously train two diverged networks network uses dataset division network semi supervised training phase improve mixmatch strategy performing label co refinement label co guessing labeled unlabeled samples respectively experiments multiple benchmark datasets demonstrate substantial improvements state art methods code available https github com lijunnan dividemix\n",
            "output sentence:  propose novel semi supervised learning approach sota performance combating learning noisy labels \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.4, 'f': 0.11764705631487893}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.1111111111111111, 'f': 0.027027024890431142}, 'rouge-l': {'r': 0.05172413793103448, 'p': 0.3, 'f': 0.08823529160899661}}\n",
            "pair:  challenges natural sciences often phrased optimization problems machine learning techniques recently applied solve problems one example chemistry design tailor made organic materials molecules requires efficient methods explore chemical space present genetic algorithm ga enhanced neural network dnn based discriminator model improve diversity generated molecules time steer ga show algorithm outperforms generative models optimization tasks furthermore present way increase interpretability genetic algorithms helped us derive design principles\n",
            "output sentence:  tackling inverse design via genetic algorithms augmented deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.08888888888888889, 'p': 0.8, 'f': 0.1599999982}, 'rouge-2': {'r': 0.025, 'p': 0.3, 'f': 0.04615384473372785}, 'rouge-l': {'r': 0.07777777777777778, 'p': 0.7, 'f': 0.13999999820000003}}\n",
            "pair:  modern applications autonomous vehicles video surveillance generate massive amounts image data work propose novel image outlier detection approach iod short leverages cutting edge image classifier discover outliers without using labeled outlier observe although intuitively confidence convolutional neural network cnn image belongs particular class could serve outlierness measure image directly applying confidence detect outlier work well cnn often high confidence outlier image belong target class due generalization ability ensures high accuracy classification solve issue propose deep neural forest based approach harmonizes contradictory requirements accurately classifying images correctly detecting outlier images experiments using several benchmark image datasets including mnist cifar cifar svhn demonstrate effectiveness iod approach outlier detection capturing outliers generated injecting one image dataset another still preserving classification accuracy multi class classification problem\n",
            "output sentence:  novel approach detects outliers image data preserving classification accuracy image classification \n",
            "\n",
            "{'rouge-1': {'r': 0.1188118811881188, 'p': 0.8571428571428571, 'f': 0.20869565003553875}, 'rouge-2': {'r': 0.05426356589147287, 'p': 0.4666666666666667, 'f': 0.0972222203559028}, 'rouge-l': {'r': 0.09900990099009901, 'p': 0.7142857142857143, 'f': 0.17391304133988658}}\n",
            "pair:  existing sequence prediction methods mostly concerned time independent sequences actual time span events irrelevant distance events simply difference order positions sequence time independent view sequences applicable data natural languages dealing words sentence inappropriate inefficient many real world events observed collected unequally spaced points time naturally arise person goes grocery store makes phone call time span events carry important information sequence dependence human behaviors work propose set methods using time sequence prediction neural sequence models rnn amenable handling token like input propose two methods time dependent event representation based intuition time tokenized everyday life previous work embedding contextualization also introduce two methods using next event duration regularization training sequence prediction model discuss methods based recurrent neural nets evaluate methods well baseline models five datasets resemble variety sequence prediction tasks experiments revealed proposed methods offer accuracy gain baseline models range settings\n",
            "output sentence:  proposed methods time dependent event representation regularization sequence prediction evaluated methods five datasets involve range sequence prediction tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.18518518518518517, 'p': 0.9090909090909091, 'f': 0.3076923048804734}, 'rouge-2': {'r': 0.06756756756756757, 'p': 0.5, 'f': 0.11904761695011341}, 'rouge-l': {'r': 0.12962962962962962, 'p': 0.6363636363636364, 'f': 0.21538461257278108}}\n",
            "pair:  recent efforts combining deep models probabilistic graphical models promising providing flexible models also easy interpret propose variational message passing algorithm variational inference models make three contributions first propose structured inference networks incorporate structure graphical model inference network variational auto encoders vae second establish conditions inference networks enable fast amortized inference similar vae finally derive variational message passing algorithm perform efficient natural gradient inference retaining efficiency amortized inference simultaneously enabling structured amortized natural gradient inference deep structured models method simplifies generalizes existing methods\n",
            "output sentence:  propose variational message passing algorithm models contain deep model probabilistic graphical model \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.6363636363636364, 'f': 0.22047243808047617}, 'rouge-2': {'r': 0.05785123966942149, 'p': 0.30434782608695654, 'f': 0.09722221953800161}, 'rouge-l': {'r': 0.08571428571428572, 'p': 0.4090909090909091, 'f': 0.14173228060016127}}\n",
            "pair:  previous work shows adversarially robust generalization requires larger sample complexity dataset cifar enables good standard accuracy may suffice train robust models since collecting new training data could costly focus better utilizing given data inducing regions high sample density feature space could lead locally sufficient samples robust learning first formally show softmax cross entropy sce loss variants convey inappropriate supervisory signals encourage learned feature points spread space sparsely training inspires us propose max mahalanobis center mmc loss explicitly induce dense feature regions order benefit robustness namely mmc loss encourages model concentrate learning ordered compact representations gather around preset optimal centers different classes empirically demonstrate applying mmc loss significantly improve robustness even strong adaptive attacks keeping state art accuracy clean inputs little extra computation compared sce loss\n",
            "output sentence:  applying softmax function training leads indirect unexpected supervision features propose new training objective explicitly induce dense feature regions locally sufficient samples benefit samples benefit \n",
            "\n",
            "{'rouge-1': {'r': 0.1276595744680851, 'p': 0.6666666666666666, 'f': 0.2142857115880102}, 'rouge-2': {'r': 0.017241379310344827, 'p': 0.11764705882352941, 'f': 0.030075185740290743}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.4444444444444444, 'f': 0.14285714015943882}}\n",
            "pair:  previous work showed empirically large neural networks significantly reduced size preserving accuracy model compression became central research topic crucial deployment neural networks devices limited computational memory resources majority compression methods based heuristics offer worst case guarantees trade compression rate approximation error arbitrarily new sample propose first efficient data independent neural pruning algorithm provable trade compression rate approximation error future test sample method based coreset framework finds small weighted subset points provably approximates original inputs specifically approximate output layer neurons coreset neurons previous layer discard rest apply framework layer layer fashion top bottom unlike previous works coreset data independent meaning provably guarantees accuracy function input mathbb including adversarial one demonstrate effectiveness method popular network architectures particular coresets yield compression lenet architecture mnist improving accuracy\n",
            "output sentence:  propose efficient provable data independent method network compression via neural pruning using coresets neurons novel construction proposed paper paper \n",
            "\n",
            "{'rouge-1': {'r': 0.1388888888888889, 'p': 0.7692307692307693, 'f': 0.23529411505605538}, 'rouge-2': {'r': 0.041237113402061855, 'p': 0.3333333333333333, 'f': 0.07339449345341306}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.6153846153846154, 'f': 0.18823529152664362}}\n",
            "pair:  generative adversarial networks gans achieve state art sample quality generative modelling tasks suffer mode collapse problem variational autoencoders vae hand explicitly maximize reconstruction based data log likelihood forcing cover modes suffer poorer sample quality recent works proposed hybrid vae gan frameworks integrate gan based synthetic likelihood vae objective address mode collapse sample quality issues limited success vae objective forces trade data log likelihood divergence latent prior synthetic likelihood ratio term also shows instability training propose novel objective best many samples reconstruction cost stable direct estimate synthetic likelihood enables hybrid vae gan framework achieve high data log likelihood low divergence latent prior time shows significant improvement hybrid vae gans plain gans mode coverage quality\n",
            "output sentence:  propose new objective training hybrid vae gans lead significant improvement mode coverage quality \n",
            "\n",
            "{'rouge-1': {'r': 0.1125, 'p': 0.6923076923076923, 'f': 0.1935483846918719}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.3333333333333333, 'f': 0.06779660834243038}, 'rouge-l': {'r': 0.1, 'p': 0.6153846153846154, 'f': 0.1720430083477859}}\n",
            "pair:  present domain adaptation method transferring neural representations label rich source domains unlabeled target domains recent adversarial methods proposed task learn align features across domains fooling special domain classifier network however drawback approach domain classifier simply labels generated features domain without considering boundaries classes means ambiguous target features generated near class boundaries reducing target classification accuracy propose novel approach adversarial dropout regularization adr encourages generator output discriminative features target domain key idea replace traditional domain critic critic detects non discriminative features using dropout classifier network generator learns avoid areas feature space thus creates better features apply adr approach problem unsupervised domain adaptation image classification semantic segmentation tasks demonstrate significant improvements state art\n",
            "output sentence:  present new adversarial method adapting neural representations based critic detects non discriminative features \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.375, 'f': 0.1538461505851414}, 'rouge-2': {'r': 0.027777777777777776, 'p': 0.125, 'f': 0.045454542479339034}, 'rouge-l': {'r': 0.08064516129032258, 'p': 0.3125, 'f': 0.1282051249441158}}\n",
            "pair:  paper investigates strategies defend adversarial example attacks image classification systems transforming inputs feeding system specifically study applying image transformations bit depth reduction jpeg compression total variance minimization image quilting feeding image convolutional network classifier experiments imagenet show total variance minimization image quilting effective defenses practice particular network trained transformed images strength defenses lies non differentiable nature inherent randomness makes difficult adversary circumvent defenses best defense eliminates strong gray box strong black box attacks variety major attack methods\n",
            "output sentence:  apply model agnostic defense strategy adversarial examples achieve white box accuracy black box accuracy major attack algorithms algorithms \n",
            "\n",
            "{'rouge-1': {'r': 0.03614457831325301, 'p': 0.42857142857142855, 'f': 0.0666666652320988}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.16666666666666666, 'f': 0.019999998872000067}, 'rouge-l': {'r': 0.024096385542168676, 'p': 0.2857142857142857, 'f': 0.04444444300987659}}\n",
            "pair:  prepositions among frequent words good prepositional representation great syntactic semantic interest computational linguistics existing methods preposition representation either treat prepositions content words word vec glove depend heavily external linguistic resources including syntactic parsing training task dataset specific representations paper use word triple counts one words preposition capture preposition interaction head children prepositional embeddings derived via tensor decompositions large unlabeled corpus reveal new geometry involving hadamard products empirically demonstrate utility paraphrasing phrasal verbs furthermore prepositional embeddings used simple features two challenging downstream tasks preposition selection prepositional attachment disambiguation achieve comparable better results state art multiple standardized datasets\n",
            "output sentence:  work tensor based method preposition representation training \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.875, 'f': 0.16666666494331067}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.42857142857142855, 'f': 0.06185566876394944}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.5, 'f': 0.09523809351473926}}\n",
            "pair:  describe kernel rnn learning kernl reduced rank temporal eligibility trace based approximation backpropagation time bptt training recurrent neural networks rnns gives competitive performance bptt long time dependence tasks approximation replaces rank gradient learning tensor describes past hidden unit activations affect current state simple reduced rank product sensitivity weight temporal eligibility trace structured approximation motivated node perturbation sensitivity weights eligibility kernel time scales learned applying perturbations rule represents another step toward biologically plausible neurally inspired ml lower complexity terms relaxed architectural requirements symmetric return weights smaller memory demand unfolding storage states time shorter feedback time\n",
            "output sentence:  biologically plausible learning rule training recurrent neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.06481481481481481, 'p': 0.6363636363636364, 'f': 0.11764705714568183}, 'rouge-2': {'r': 0.031496062992125984, 'p': 0.4, 'f': 0.05839415923064631}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.5454545454545454, 'f': 0.10084033445660619}}\n",
            "pair:  crafting adversarial examples discrete inputs like text sequences fundamentally different generating examples continuous inputs like images paper tries answer question black box setting create adversarial examples automatically effectively fool deep learning classifiers texts making imperceptible changes answer firm yes previous efforts mostly replied using gradient evidence less effective either due finding nearest neighbor word wrt meaning automatically difficult relying heavily hand crafted linguistic rules instead use monte carlo tree search mcts finding important words perturb perform homoglyph attack replacing one character selected word symbol identical shape novel algorithm call mctsbug black box extremely effective time experimental results indicate mctsbug fool deep learning classifiers success rates seven large scale benchmark datasets perturbing characters surprisingly mctsbug without relying gradient information effective gradient based white box baseline thanks nature homoglyph attack generated adversarial perturbations almost imperceptible human eyes\n",
            "output sentence:  use monte carlo tree search homoglyphs generate indistinguishable adversarial samples text data \n",
            "\n",
            "{'rouge-1': {'r': 0.06593406593406594, 'p': 0.6666666666666666, 'f': 0.11999999836200001}, 'rouge-2': {'r': 0.023622047244094488, 'p': 0.375, 'f': 0.04444444332949248}, 'rouge-l': {'r': 0.054945054945054944, 'p': 0.5555555555555556, 'f': 0.09999999836200002}}\n",
            "pair:  spiking neural networks investigated biologically plausible models neural computation also potentially efficient type neural network convolutional spiking neural networks demonstrated achieve near state art performance one solution proposed convert gated recurrent neural networks far recurrent neural networks form networks gating memory cells central state art solutions problem domains involve sequence recognition generation design analog gated lstm cell neurons substituted efficient stochastic spiking neurons adaptive spiking neurons implement adaptive form sigma delta coding convert internally computed analog activation values spike trains neurons approximate effective activation function resembles sigmoid show analog neurons activation functions used create analog lstm cell networks cells trained standard backpropagation train lstm networks noisy noiseless version original sequence prediction task hochreiter schmidhuber also noisy noiseless version classical working memory reinforcement learning task maze substituting analog neurons corresponding adaptive spiking neurons show almost resulting spiking neural network equivalents correctly compute original tasks\n",
            "output sentence:  demonstrate gated recurrent asynchronous spiking neural network corresponds lstm unit \n",
            "\n",
            "{'rouge-1': {'r': 0.08163265306122448, 'p': 0.6666666666666666, 'f': 0.1454545435107438}, 'rouge-2': {'r': 0.024, 'p': 0.2727272727272727, 'f': 0.04411764557201562}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.5833333333333334, 'f': 0.12727272532892564}}\n",
            "pair:  federated learning fl refers learning high quality global model based decentralized data storage without ever copying raw data natural scenario arises data created mobile phones activity users given typical data heterogeneity situations natural ask global model personalized every device individually work point setting model agnostic meta learning maml one optimizes fast gradient based shot adaptation heterogeneous distribution tasks number similarities objective personalization fl present fl natural source practical applications maml algorithms make following observations popular fl algorithm federated averaging interpreted meta learning algorithm careful fine tuning yield global model higher accuracy time easier personalize however solely optimizing global model accuracy yields weaker personalization result model trained using standard datacenter optimization method much harder personalize compared one trained using federated averaging supporting first claim results raise new questions fl maml broader ml research\n",
            "output sentence:  federated averaging already meta learning algorithm datacenter trained methods significantly harder personalize \n",
            "\n",
            "{'rouge-1': {'r': 0.1518987341772152, 'p': 0.8571428571428571, 'f': 0.2580645135715112}, 'rouge-2': {'r': 0.11956521739130435, 'p': 0.8461538461538461, 'f': 0.20952380735419504}, 'rouge-l': {'r': 0.1518987341772152, 'p': 0.8571428571428571, 'f': 0.2580645135715112}}\n",
            "pair:  natural language processing models lack unified approach robustness testing paper introduce wildnlp framework testing model stability natural setting text corruptions keyboard errors misspelling occur compare robustness models popular nlp tasks nli ner sentiment analysis testing performance aspects introduced framework particular focus comparison recent state art text representations non contextualized word embeddings order improve robust ness perform adversarial training se lected aspects check transferability improvement models various cor ruption types find high perfor mance models ensure sufficient robustness although modern embedding tech niques help improve release cor rupted datasets code wildnlp frame work community\n",
            "output sentence:  compare robustness models popular nlp tasks nli ner sentiment analysis testing performance perturbed inputs \n",
            "\n",
            "{'rouge-1': {'r': 0.06153846153846154, 'p': 0.5714285714285714, 'f': 0.1111111093557099}, 'rouge-2': {'r': 0.023529411764705882, 'p': 0.3333333333333333, 'f': 0.0439560427243087}, 'rouge-l': {'r': 0.06153846153846154, 'p': 0.5714285714285714, 'f': 0.1111111093557099}}\n",
            "pair:  mixed precision arithmetic combining single half precision operands operation successfully applied train deep neural networks despite advantages mixed precision arithmetic terms reducing need key resources like memory bandwidth register file size limited capacity diminishing computing costs requires bits represent output operands paper proposes two approaches replace mixed precision half precision arithmetic large portion training first approach achieves accuracy ratios slightly slower state art using half precision arithmetic training second approach reaches accuracy state art dynamically switching half mixed precision arithmetic training uses half precision training process paper first demonstrating half precision used large portion dnns training still reach state art accuracy\n",
            "output sentence:  dynamic precision technique train deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.04040404040404041, 'p': 0.4444444444444444, 'f': 0.07407407254629635}, 'rouge-2': {'r': 0.008620689655172414, 'p': 0.1111111111111111, 'f': 0.015999998663680112}, 'rouge-l': {'r': 0.030303030303030304, 'p': 0.3333333333333333, 'f': 0.055555554027777815}}\n",
            "pair:  adaptive optimization methods adagrad rmsprop adam proposed achieve rapid training process element wise scaling term learning rates though prevailing observed generalize poorly compared sgd even fail converge due unstable extreme learning rates recent work put forward algorithms amsgrad tackle issue failed achieve considerable improvement existing methods paper demonstrate extreme learning rates lead poor performance provide new variants adam amsgrad called adabound amsbound respectively employ dynamic bounds learning rates achieve gradual smooth transition adaptive methods sgd give theoretical proof convergence conduct experiments various popular tasks models often insufficient previous work experimental results show new variants eliminate generalization gap adaptive methods sgd maintain higher learning speed early training time moreover bring significant improvement prototypes especially complex deep networks implementation algorithm found https github com luolc adabound\n",
            "output sentence:  novel variants optimization methods combine benefits adaptive non adaptive methods \n",
            "\n",
            "{'rouge-1': {'r': 0.09859154929577464, 'p': 0.3888888888888889, 'f': 0.15730336755965163}, 'rouge-2': {'r': 0.022988505747126436, 'p': 0.11764705882352941, 'f': 0.03846153572670138}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.3333333333333333, 'f': 0.13483145744729208}}\n",
            "pair:  paper explore meta learning shot text classification meta learning shown strong performance computer vision low level patterns transferable across learning tasks however directly applying approach text challenging lexical features highly informative one task maybe insignificant another thus rather learning solely words model also leverages distributional signatures encode pertinent word occurrence patterns model trained within meta learning framework map signatures attention scores used weight lexical representations words demonstrate model consistently outperforms prototypical networks learned lexical knowledge snell et al shot text classification relation classification significant margin across six benchmark datasets average shot classification\n",
            "output sentence:  meta learning methods used vision directly applied nlp perform worse nearest neighbors new classes better distributional signatures signatures \n",
            "\n",
            "{'rouge-1': {'r': 0.0875, 'p': 0.7777777777777778, 'f': 0.15730336896856456}, 'rouge-2': {'r': 0.030303030303030304, 'p': 0.375, 'f': 0.05607476497161328}, 'rouge-l': {'r': 0.0875, 'p': 0.7777777777777778, 'f': 0.15730336896856456}}\n",
            "pair:  introduce new routing algorithm capsule networks child capsule routed parent based agreement parent state child vote unlike previously proposed routing algorithms parent ability reconstruct child explicitly taken account update routing probabilities simplifies routing procedure improves performance benchmark datasets cifar cifar new mechanism designs routing via inverted dot product attention imposes layer normalization normalization replaces sequential iterative routing concurrent iterative routing besides outperforming existing capsule networks model performs par powerful cnn resnet using less parameters different task recognizing digits overlayed digit images proposed capsule model performs favorably cnns given number layers neurons per layer believe work raises possibility applying capsule networks complex real world tasks\n",
            "output sentence:  present new routing method capsule networks performs par resnet cifar \n",
            "\n",
            "{'rouge-1': {'r': 0.10784313725490197, 'p': 0.6875, 'f': 0.18644067562194772}, 'rouge-2': {'r': 0.0423728813559322, 'p': 0.3333333333333333, 'f': 0.07518796792356837}, 'rouge-l': {'r': 0.0784313725490196, 'p': 0.5, 'f': 0.13559321799482912}}\n",
            "pair:  counter machines received little attention theoretical computer science since recently achieved newfound relevance field natural language processing nlp recent work suggested strong performing recurrent neural networks utilize memory counters thus one potential way understand sucess networks revisit theory counter computation therefore choose study abilities real time counter machines formal grammars first show several variants counter machine converge express class formal languages also prove counter languages closed complement union intersection many common set operations next show counter machines cannot evaluate boolean expressions even though weakly validate syntax implications interpretability evaluation neural network systems successfully matching syntactic patterns guarantee counter like model accurately represents underlying semantic structures finally consider question whether counter languages semilinear work makes general contributions theory formal languages particular interest interpretability recurrent neural networks\n",
            "output sentence:  study class formal languages acceptable real time counter automata model computation related types recurrent neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.2222222222222222, 'p': 0.9411764705882353, 'f': 0.35955055870723396}, 'rouge-2': {'r': 0.17045454545454544, 'p': 0.8823529411764706, 'f': 0.28571428300045354}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.9411764705882353, 'f': 0.35955055870723396}}\n",
            "pair:  propose warped residual network warpnet using parallelizable warp operator forward backward propagation distant layers trains faster original residual neural network apply perturbation theory residual networks decouple interactions residual units resulting warp operator first order approximation output multiple layers first order perturbation theory exhibits properties binomial path lengths exponential gradient scaling found experimentally veit et al demonstrate extensive performance study proposed network achieves comparable predictive performance original residual network number parameters achieving significant speed total training time warpnet performs model parallelism residual network training weights distributed different gpus offers speed capability train larger networks compared original residual networks\n",
            "output sentence:  propose warped residual network using parallelizable warp operator forward backward propagation distant layers trains faster original residual network \n",
            "\n",
            "{'rouge-1': {'r': 0.0392156862745098, 'p': 0.25, 'f': 0.06779660782533764}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0196078431372549, 'p': 0.125, 'f': 0.03389830274059195}}\n",
            "pair:  propose end end trainable attention module convolutional neural network cnn architectures built image classification module takes input feature vector maps form intermediate representations input image different stages cnn pipeline outputs matrix scores map standard cnn architectures modified incorporation module trained constraint convex combination intermediate feature vectors parametrised score matrices must alone used classification incentivised amplify relevant suppress irrelevant misleading scores thus assume role attention values experimental observations provide clear evidence effect learned attention maps neatly highlight regions interest suppressing background clutter consequently proposed function able bootstrap standard cnn architectures task image classification demonstrating superior generalisation unseen benchmark datasets binarised attention maps outperform cnn based attention maps traditional saliency maps top object proposals weakly supervised segmentation demonstrated object discovery dataset also demonstrate improved robustness fast gradient sign method adversarial attack\n",
            "output sentence:  paper proposes method forcing cnns leverage spatial attention learning object centric representations perform better various respects \n",
            "\n",
            "{'rouge-1': {'r': 0.04504504504504504, 'p': 0.45454545454545453, 'f': 0.08196721147406615}, 'rouge-2': {'r': 0.014598540145985401, 'p': 0.18181818181818182, 'f': 0.027027025651022715}, 'rouge-l': {'r': 0.036036036036036036, 'p': 0.36363636363636365, 'f': 0.06557376885111534}}\n",
            "pair:  quality machine translation system depends largely availability sizable parallel corpora recently popular neural machine translation nmt framework data sparsity problem become even severe large amount tunable parameters nmt model may overfit existing language pairs failing understand general diversity language paper advocate broadcast every sentence pair two groups similar sentences incorporate diversity language expressions name parallel cluster define general cluster cluster correspondence score train model maximize score since direct maximization difficult derive lower bound surrogate objective found generalize point point maximum likelihood estimation mle point cluster reward augmented maximum likelihood raml algorithms special cases based novel objective function delineate four potential systems realize cluster cluster framework test performances three recognized translation tasks task forward reverse translation directions six experiments proposed four parallel systems consistently proved outperform mle baseline rl reinforcement learning raml systems significantly finally performed case study empirically analyze strength cluster cluster nmt framework\n",
            "output sentence:  invent novel cluster cluster framework nmt training better understand source target language \n",
            "\n",
            "{'rouge-1': {'r': 0.0958904109589041, 'p': 0.5, 'f': 0.16091953752939625}, 'rouge-2': {'r': 0.03529411764705882, 'p': 0.23076923076923078, 'f': 0.06122448749479393}, 'rouge-l': {'r': 0.0821917808219178, 'p': 0.42857142857142855, 'f': 0.13793103178226984}}\n",
            "pair:  algorithms representation learning link prediction relational data designed static data however data applied usually evolves time friend graphs social networks user interactions items recommender systems also case knowledge bases contain facts us president obama valid certain points time problem link prediction temporal constraints answering queries form us president propose solution inspired canonical decomposition tensors order introduce new regularization schemes present extension complex achieves state art performance additionally propose new dataset knowledge base completion constructed wikidata larger previous benchmarks order magnitude new reference evaluating temporal non temporal link prediction methods\n",
            "output sentence:  propose new tensor decompositions associated regularizers obtain state art performances temporal knowledge base completion \n",
            "\n",
            "{'rouge-1': {'r': 0.12, 'p': 0.6, 'f': 0.19999999722222223}, 'rouge-2': {'r': 0.0625, 'p': 0.4444444444444444, 'f': 0.10958903893413402}, 'rouge-l': {'r': 0.12, 'p': 0.6, 'f': 0.19999999722222223}}\n",
            "pair:  study bert language representation model sequence generation model bert encoder multi label text classification task experiment models explore special qualities setting also introduce examine experimentally mixed model ensemble multi label bert sequence generating bert models experiments demonstrated bert based models mixed model particular outperform current baselines several metrics achieving state art results three well studied multi label classification datasets english texts two private yandex taxi datasets russian texts\n",
            "output sentence:  using bert encoder sequential prediction labels multi label text classification task \n",
            "\n",
            "{'rouge-1': {'r': 0.13725490196078433, 'p': 0.7, 'f': 0.22950819398011293}, 'rouge-2': {'r': 0.04918032786885246, 'p': 0.3333333333333333, 'f': 0.08571428347346943}, 'rouge-l': {'r': 0.13725490196078433, 'p': 0.7, 'f': 0.22950819398011293}}\n",
            "pair:  clustering algorithms wide applications play important role data analysis fields including time series data analysis performance clustering algorithm depends features extracted data however time series analysis problem conventional methods based signal shape unstable phase shift amplitude signal length variations paper propose new clustering algorithm focused dynamical system aspect signal using recurrent neural network variational bayes method experiments show proposed algorithm robustness variations boost classification performance\n",
            "output sentence:  novel time series data clustring algorithm based dynamical system features \n",
            "\n",
            "{'rouge-1': {'r': 0.10294117647058823, 'p': 0.7777777777777778, 'f': 0.18181817975375275}, 'rouge-2': {'r': 0.011111111111111112, 'p': 0.125, 'f': 0.020408161765930976}, 'rouge-l': {'r': 0.07352941176470588, 'p': 0.5555555555555556, 'f': 0.1298701278057008}}\n",
            "pair:  interpretability ai agent behavior utmost importance effective human ai interaction end increasing interest characterizing generating interpretable behavior agent alternative approach guarantee agent generates interpretable behavior would design agent environment uninterpretable behaviors either prohibitively expensive unavailable agent date work umbrella goal plan recognition design exploring notion environment redesign specific instances interpretable behavior position paper scope landscape interpretable behavior environment redesign different flavors specifically focus three specific types interpretable behaviors explicability legibility predictability present general framework problem environment design instantiated achieve three interpretable behaviors also discuss specific instantiations framework correspond prior works environment design identify exciting opportunities future work\n",
            "output sentence:  present approach redesign environment uninterpretable agent behaviors minimized eliminated \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.23076923076923078, 'f': 0.10526315437365354}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.23076923076923078, 'f': 0.10526315437365354}}\n",
            "pair:  concerns interpretability computational resources principled inductive priors motivated efforts engineer sparse neural models nlp tasks sparsity important nlp might well trained neural models naturally become roughly sparse using taxi euclidean norm measure sparsity find frequent input words associated concentrated sparse activations frequent target words associated dispersed activations concentrated gradients find gradients associated function words concentrated gradients content words even controlling word frequency\n",
            "output sentence:  study natural emergence sparsity activations gradients layers dense lstm language model course training \n",
            "\n",
            "{'rouge-1': {'r': 0.057971014492753624, 'p': 0.23529411764705882, 'f': 0.09302325264196874}, 'rouge-2': {'r': 0.02531645569620253, 'p': 0.11764705882352941, 'f': 0.041666663752170346}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.17647058823529413, 'f': 0.0697674386884804}}\n",
            "pair:  outline new approaches incorporate ideas deep learning wave based least squares imaging aim main contribution work combination handcrafted constraints deep convolutional neural networks way harness remarkable ease generating natural images mathematical basis underlying method expectation maximization framework data divided batches coupled additional latent unknowns unknowns pairs elements original unknown space coupled specific data batch network inputs setting neural network controls similarity additional parameters acting center variable resulting problem amounts maximum likelihood estimation network parameters augmented data model marginalized latent variables\n",
            "output sentence:  combine hard handcrafted constraints deep prior weak constraint perform seismic imaging reap information posterior distribution leveraging leveraging multiplicity data \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.8571428571428571, 'f': 0.13636363489927686}, 'rouge-2': {'r': 0.017699115044247787, 'p': 0.3333333333333333, 'f': 0.0336134444205918}, 'rouge-l': {'r': 0.06172839506172839, 'p': 0.7142857142857143, 'f': 0.11363636217200414}}\n",
            "pair:  recently various neural networks proposed irregularly structured data graphs manifolds knowledge existing graph networks discrete depth inspired neural ordinary differential equation node data euclidean domain extend idea continuous depth models graph data propose graph ordinary differential equation gode derivative hidden node states parameterized graph neural network output states solution ordinary differential equation demonstrate two end end methods efficient training gode indirect back propagation adjoint method direct back propagation ode solver accurately computes gradient demonstrate direct backprop outperforms adjoint method experiments introduce family bijective blocks enables mathcal memory consumption demonstrate gode easily adapted different existing graph neural networks improve accuracy validate performance gode semi supervised node classification tasks graph classification tasks gode model achieves continuous model time memory efficiency accurate gradient estimation generalizability different graph networks\n",
            "output sentence:  apply ordinary differential equation model graph structured data \n",
            "\n",
            "{'rouge-1': {'r': 0.10256410256410256, 'p': 0.8888888888888888, 'f': 0.1839080441220769}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.5, 'f': 0.07017543729147431}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.7777777777777778, 'f': 0.16091953837495046}}\n",
            "pair:  paper present general framework distilling expectations respect bayesian posterior distribution deep neural network significantly extending prior work method known bayesian dark knowledge generalized framework applies case classification models takes input architecture teacher network general posterior expectation interest architecture student network distillation method performs online compression selected posterior expectation using iteratively generated monte carlo samples parameter posterior teacher model consider problem optimizing student model architecture respect accuracy speed storage trade present experimental results investigating multiple data sets distillation targets teacher model architectures approaches searching student model architectures establish key result distilling student model architecture matches teacher done bayesian dark knowledge lead sub optimal performance lastly show student architecture search methods identify student models significantly improved performance\n",
            "output sentence:  general framework distilling bayesian posterior expectations deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.45, 'f': 0.16822429602585381}, 'rouge-2': {'r': 0.010101010101010102, 'p': 0.05263157894736842, 'f': 0.01694914984056349}, 'rouge-l': {'r': 0.05747126436781609, 'p': 0.25, 'f': 0.09345794088566697}}\n",
            "pair:  beyond understanding discussed human communication requires awareness someone feeling one challenge dialogue agents recognizing feelings conversation partner replying accordingly key communicative skill trivial humans research area made difficult paucity suitable publicly available datasets emotion dialogues work proposes new task empathetic dialogue generation empatheticdialogues dataset conversations grounded emotional situations facilitate training evaluating dialogue systems experiments indicate dialogue models use dataset perceived empathetic human evaluators improving metrics well perceived relevance responses bleu scores compared models merely trained large scale internet conversation data also present empirical comparisons several ways improve performance given model leveraging existing models datasets without requiring lengthy training full model\n",
            "output sentence:  improve existing dialogue systems responding people sharing personal stories incorporating emotion prediction representations also release new benchmark dataset empathetic dialogues \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.5263157894736842, 'f': 0.21276595422136718}, 'rouge-2': {'r': 0.047619047619047616, 'p': 0.21052631578947367, 'f': 0.07766989990385534}, 'rouge-l': {'r': 0.08, 'p': 0.3157894736842105, 'f': 0.12765957124264382}}\n",
            "pair:  variational autoencoders vaes proven powerful latent variable models ever form approximate posterior limit expressiveness model categorical distributions flexible useful building blocks example neural memory layers introduce hierarchical discrete variational autoencoder hd vae hi erarchy variational memory layers concrete gumbel softmax relaxation allows maximizing surrogate evidence lower bound stochastic gradient ascent show using limited number latent variables hd vae outperforms gaussian baseline modelling multiple binary image datasets training deep hd vae remains challenge due relaxation bias induced use surrogate objective introduce formal definition conduct preliminary theoretical empirical study bias\n",
            "output sentence:  paper introduce discrete hierarchy categorical latent variables train using concrete gumbel softmax relaxation derive derive upper absolute difference unbiased unbiased \n",
            "\n",
            "{'rouge-1': {'r': 0.15151515151515152, 'p': 0.6666666666666666, 'f': 0.24691357722908097}, 'rouge-2': {'r': 0.05128205128205128, 'p': 0.2857142857142857, 'f': 0.08695651915879024}, 'rouge-l': {'r': 0.12121212121212122, 'p': 0.5333333333333333, 'f': 0.19753086117969826}}\n",
            "pair:  promising class generative models maps points simple distribution complex distribution invertible neural network likelihood based training models requires restricting architectures allow cheap computation jacobian determinants alternatively jacobian trace used transformation specified ordinary differential equation paper use hutchinson trace estimator give scalable unbiased estimate log density result continuous time invertible generative model unbiased density estimation one pass sampling allowing unrestricted neural network architectures demonstrate approach high dimensional density estimation image generation variational inference achieving state art among exact likelihood methods efficient sampling\n",
            "output sentence:  use continuous time dynamics define generative model exact likelihoods efficient sampling parameterized unrestricted neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.13846153846153847, 'p': 0.6428571428571429, 'f': 0.2278480983496235}, 'rouge-2': {'r': 0.0125, 'p': 0.06666666666666667, 'f': 0.02105262891966793}, 'rouge-l': {'r': 0.09230769230769231, 'p': 0.42857142857142855, 'f': 0.1518987312610159}}\n",
            "pair:  deep convolutional network architectures often assumed guarantee generalization small image translations deformations paper show modern cnns vgg resnet inceptionresnetv drastically change output image translated image plane pixels failure generalization also happens realistic small image transformations furthermore see failures generalize frequently modern networks show failures related fact architecture modern cnns ignores classical sampling theorem generalization guaranteed also show biases statistics commonly used image datasets makes unlikely cnns learn invariant transformations taken together results suggest performance cnns object recognition falls far short generalization capabilities humans\n",
            "output sentence:  modern deep cnns invariant translations scalings realistic image transformations lack invariance related operation operation operation image datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.02564102564102564, 'p': 0.2222222222222222, 'f': 0.04597700963931835}, 'rouge-2': {'r': 0.011363636363636364, 'p': 0.125, 'f': 0.02083333180555567}, 'rouge-l': {'r': 0.02564102564102564, 'p': 0.2222222222222222, 'f': 0.04597700963931835}}\n",
            "pair:  ability quantify predict progression disease fundamental selecting appropriate treatment many clinical metrics cannot acquired frequently either cost mri gait analysis inconvenient harmful patient biopsy ray scenarios order estimate individual trajectories disease progression advantageous leverage similarities patients covariance trajectories find latent representation progression existing methods estimating trajectories account events observations dramatically decreases adequacy clinical practice study develop machine learning framework named coordinatewise soft impute csi analyzing disease progression sparse observations presence confounding events csi guaranteed converge global minimum corresponding optimization problem experimental results also demonstrates effectiveness csi using simulated real dataset\n",
            "output sentence:  novel matrix completion based algorithm model disease progression events \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 0.7857142857142857, 'f': 0.3188405764755303}, 'rouge-2': {'r': 0.078125, 'p': 0.35714285714285715, 'f': 0.12820512525969763}, 'rouge-l': {'r': 0.16363636363636364, 'p': 0.6428571428571429, 'f': 0.2608695619827767}}\n",
            "pair:  paper explores use self ensembling visual domain adaptation problems technique derived mean teacher variant tarvainen et al temporal ensembling laine et al technique achieved state art results area semi supervised learning introduce number modifications approach challenging domain adaptation scenarios evaluate effectiveness approach achieves state art results variety benchmarks including winning entry visda visual domain adaptation challenge small image benchmarks algorithm outperforms prior art also achieve accuracy close classifier trained supervised fashion\n",
            "output sentence:  self ensembling based algorithm visual domain adaptation state art results visda image classification domain adaptation challenge \n",
            "\n",
            "{'rouge-1': {'r': 0.045454545454545456, 'p': 0.5714285714285714, 'f': 0.08421052495069253}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03409090909090909, 'p': 0.42857142857142855, 'f': 0.06315789337174517}}\n",
            "pair:  deep neural networks dnns attained surprising achievement last decade due advantages automatic feature learning freedom expressiveness however interpretability remains mysterious dnns complex combinations linear nonlinear transformations even though many models proposed explore interpretability dnns several challenges remain unsolved lack interpretability quantity measures dnns lack theory stability dnns difficulty solve nonconvex dnn problems interpretability constraints address challenges simultaneously paper presents novel intrinsic interpretability evaluation framework dnns specifically four independent properties interpretability defined based existing works moreover investigate theory stability dnns important aspect interpretability prove dnns generally stable given different activation functions finally extended version deep learning alternating direction method multipliers dladmm proposed solve dnn problems interpretability constraints efficiently accurately extensive experiments several benchmark datasets validate several dnns proposed interpretability framework\n",
            "output sentence:  propose novel framework evaluate interpretability neural network \n",
            "\n",
            "{'rouge-1': {'r': 0.017543859649122806, 'p': 0.1111111111111111, 'f': 0.030303027947658587}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.017543859649122806, 'p': 0.1111111111111111, 'f': 0.030303027947658587}}\n",
            "pair:  significant advances made natural language processing nlp modelling since beginning new approaches allow accurate results even little labelled data nlp models benefit training task agnostic task specific unlabelled data however advantages come significant size computational costs workshop paper outlines proposed convolutional student architecture trained distillation process large scale model achieve inference speedup reduction parameter count cases student model performance surpasses teacher studied tasks\n",
            "output sentence:  train small efficient cnn performance openai transformer text classification tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.5555555555555556, 'f': 0.1666666641166667}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0784313725490196, 'p': 0.4444444444444444, 'f': 0.13333333078333337}}\n",
            "pair:  stability key aspect data analysis many applications natural notion stability geometric illustrated example computer vision scattering transforms construct deep convolutional representations certified stable input deformations stability deformations interpreted stability respect changes metric structure domain work show scattering transforms generalized non euclidean domains using diffusion wavelets preserving notion stability respect metric changes domain measured diffusion maps resulting representation stable metric perturbations domain able capture high frequency information akin euclidean scattering\n",
            "output sentence:  stability scattering transform representations graph data deformations underlying graph support \n",
            "\n",
            "{'rouge-1': {'r': 0.08421052631578947, 'p': 0.5714285714285714, 'f': 0.1467889885868193}, 'rouge-2': {'r': 0.008064516129032258, 'p': 0.07692307692307693, 'f': 0.014598538428259568}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.35714285714285715, 'f': 0.09174311702718631}}\n",
            "pair:  weight pruning introduced efficient model compression technique even though pruning removes significant amount weights network memory requirement reduction limited since conventional sparse matrix formats require significant amount memory store index related information moreover computations associated sparse matrix formats slow sequential sparse matrix decoding process utilize highly parallel computing systems efficiently attempt compress index information keeping decoding process parallelizable viterbi based pruning suggested decoding non zero weights however still sequential viterbi based pruning paper propose new sparse matrix format order enable highly parallel decoding process entire sparse matrix proposed sparse matrix constructed combining pruning weight quantization latest rnn models ptb wikitext corpus lstm parameter storage requirement compressed using proposed sparse matrix format compared baseline model compressed weight indices reconstructed dense matrix fast using viterbi encoders simulation results show proposed scheme feed parameters processing elements faster case dense matrix values directly come dram\n",
            "output sentence:  present new weight encoding scheme enables high compression ratio fast sparse dense matrix conversion \n",
            "\n",
            "{'rouge-1': {'r': 0.08823529411764706, 'p': 0.6666666666666666, 'f': 0.1558441537797268}, 'rouge-2': {'r': 0.02247191011235955, 'p': 0.25, 'f': 0.04123711188861734}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.4444444444444444, 'f': 0.10389610183167484}}\n",
            "pair:  generative adversarial networks gans learn map samples noise distribution chosen data distribution recent work demonstrated gans consequently sensitive limited shape noise distribution example single generator struggles map continuous noise uniform distribution discontinuous output separate gaussians complex output intersecting parabolas address problem learning generate multiple models generator output actually combination several distinct networks contribute novel formulation multi generator models learn prior generators conditioned noise parameterized neural network thus network learns optimal rate sample generator also optimally shapes noise received generator resulting noise prior gan npgan achieves expressivity flexibility surpasses single generator models previous multi generator models\n",
            "output sentence:  multi generator gan framework additional network learn prior input noise \n",
            "\n",
            "{'rouge-1': {'r': 0.03614457831325301, 'p': 0.42857142857142855, 'f': 0.0666666652320988}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.16666666666666666, 'f': 0.019999998872000067}, 'rouge-l': {'r': 0.024096385542168676, 'p': 0.2857142857142857, 'f': 0.04444444300987659}}\n",
            "pair:  prepositions among frequent words good prepositional representation great syntactic semantic interest computational linguistics existing methods preposition representation either treat prepositions content words word vec glove depend heavily external linguistic resources including syntactic parsing training task dataset specific representations paper use word triple counts one words preposition capture preposition interaction head children prepositional embeddings derived via tensor decompositions large unlabeled corpus reveal new geometry involving hadamard products empirically demonstrate utility paraphrasing phrasal verbs furthermore prepositional embeddings used simple features two challenging downstream tasks preposition selection prepositional attachment disambiguation achieve comparable better results state art multiple standardized datasets\n",
            "output sentence:  work tensor based method preposition representation training \n",
            "\n",
            "{'rouge-1': {'r': 0.0891089108910891, 'p': 0.75, 'f': 0.15929203349988255}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.3333333333333333, 'f': 0.058823527802768207}, 'rouge-l': {'r': 0.07920792079207921, 'p': 0.6666666666666666, 'f': 0.14159291845563476}}\n",
            "pair:  recent development natural language processing nlp achieved great success using large pre trained models hundreds millions parameters however models suffer heavy model size high latency cannot directly deploy resource limited mobile devices paper propose mobilebert compressing accelerating popular bert model like bert mobilebert task agnostic universally applied various downstream nlp tasks via fine tuning mobilebert slimmed version bert large augmented bottleneck structures carefully designed balance self attentions feed forward networks train mobilebert use bottom top progressive scheme transfer intrinsic knowledge specially designed inverted bottleneck bert large teacher empirical studies show mobilebert smaller faster original bert base achieving competitive results well known nlp benchmarks natural language inference tasks glue mobilebert achieves glue score performance degradation ms latency pixel phone squad question answering task mobilebert achieves dev score higher bert base\n",
            "output sentence:  develop task agnosticlly compressed bert smaller faster bert base achieving competitive performance glue squad \n",
            "\n",
            "{'rouge-1': {'r': 0.0821917808219178, 'p': 0.46153846153846156, 'f': 0.13953488115467824}, 'rouge-2': {'r': 0.03409090909090909, 'p': 0.23076923076923078, 'f': 0.059405938351142114}, 'rouge-l': {'r': 0.0684931506849315, 'p': 0.38461538461538464, 'f': 0.11627906720118988}}\n",
            "pair:  vulnerabilities deep neural networks adversarial examples become significant concern deploying models sensitive domains devising definitive defense attacks proven challenging methods relying detecting adversarial samples valid attacker oblivious detection mechanism paper consider adversarial detection problem robust optimization framework partition input space subspaces train adversarial robust subspace detectors using asymmetrical adversarial training aat integration classifier detectors presents detection mechanism provides performance guarantee adversary considered demonstrate aat promotes learning class conditional distributions gives rise generative detection classification approaches robust interpretable provide comprehensive evaluations methods demonstrate competitive performances compelling properties adversarial detection robust classification problems\n",
            "output sentence:  new generative modeling technique based asymmetrical adversarial training applications adversarial example detection robust classification \n",
            "\n",
            "{'rouge-1': {'r': 0.05084745762711865, 'p': 0.375, 'f': 0.08955223670305197}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.375, 'f': 0.08955223670305197}}\n",
            "pair:  deep learning models rely expressive high dimensional representations achieve good performance tasks classification however high dimensionality representations makes difficult interpret prone fitting propose simple intuitive scalable dimension reduction framework takes account soft probabilistic interpretation standard deep models classification applying framework visualization representations accurately reflect inter class distances standard visualization techniques sne show experimentally framework improves generalization performance unseen categories zero shot learning also provide finite sample error upper bound guarantee method\n",
            "output sentence:  dimensionality reduction cases examples represented soft probability distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.1095890410958904, 'p': 0.8888888888888888, 'f': 0.19512194926531828}, 'rouge-2': {'r': 0.05434782608695652, 'p': 0.625, 'f': 0.09999999852800001}, 'rouge-l': {'r': 0.0821917808219178, 'p': 0.6666666666666666, 'f': 0.14634146146044022}}\n",
            "pair:  convolutional neural networks recurrent neural networks designed network structures well suited nature spacial sequential data respectively however structure standard feed forward neural networks fnns simply stack fully connected layers regardless feature correlations data addition number layers number neurons manually tuned validation data time consuming may lead suboptimal networks paper propose unsupervised structure learning method learning parsimonious deep fnns method determines number layers number neurons layer sparse connectivity adjacent layers automatically data resulting models called backbone skippath neural networks bsnns experiments tasks show comparison fnns bsnns achieve better comparable classification performance much fewer parameters interpretability bsnns also shown better fnns\n",
            "output sentence:  unsupervised structure learning method parsimonious deep feed forward networks \n",
            "\n",
            "{'rouge-1': {'r': 0.03333333333333333, 'p': 0.2222222222222222, 'f': 0.05797101222432271}, 'rouge-2': {'r': 0.014705882352941176, 'p': 0.125, 'f': 0.026315787590027836}, 'rouge-l': {'r': 0.03333333333333333, 'p': 0.2222222222222222, 'f': 0.05797101222432271}}\n",
            "pair:  improve previous end end differentiable neural networks nns fast weight memories gate mechanism updates fast weights every time step sequence two separate outer product based matrices generated slow parts net system trained complex sequence sequence variation associative retrieval problem roughly times temporal memory time varying variables similar sized standard recurrent nns rnns terms accuracy number parameters architecture outperforms variety rnns including long short term memory hypernetworks related fast weight architectures\n",
            "output sentence:  improved fast weight network shows better results general toy task \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.6666666666666666, 'f': 0.1578947347541551}, 'rouge-2': {'r': 0.02702702702702703, 'p': 0.25, 'f': 0.048780486044021486}, 'rouge-l': {'r': 0.07462686567164178, 'p': 0.5555555555555556, 'f': 0.13157894528047093}}\n",
            "pair:  present simple effective algorithm designed address covariate shift problem imitation learning operates training ensemble policies expert demonstration data using variance predictions cost minimized rl together supervised behavioral cloning cost unlike adversarial imitation methods uses fixed reward function easy optimize prove regret bound algorithm tabular setting linear time horizon multiplied coefficient show low certain problems behavioral cloning fails evaluate algorithm empirically across multiple pixel based atari environments continuous control tasks show matches significantly outperforms behavioral cloning generative adversarial imitation learning\n",
            "output sentence:  method addressing covariate shift imitation learning using ensemble uncertainty \n",
            "\n",
            "{'rouge-1': {'r': 0.08139534883720931, 'p': 0.5, 'f': 0.13999999759200002}, 'rouge-2': {'r': 0.009009009009009009, 'p': 0.07692307692307693, 'f': 0.01612903038111364}, 'rouge-l': {'r': 0.05813953488372093, 'p': 0.35714285714285715, 'f': 0.09999999759200007}}\n",
            "pair:  present method train self binarizing neural networks networks evolve weights activations training become binary obtain similar binary networks existing methods rely sign activation function function however gradients non zero values makes standard backpropagation impossible circumvent difficulty training network relying sign activation function methods alternate floating point binary representations network training sub optimal inefficient approach binarization task training unique representation involving smooth activation function iteratively sharpened training becomes binary representation equivalent sign activation function additionally introduce new technique perform binary batch normalization simplifies conventional batch normalization transforming simple comparison operation unlike existing methods forced retain conventional floating point based batch normalization binary networks apart displaying advantages lower memory computation compared conventional floating point binary networks also show higher classification accuracy existing state art methods multiple benchmark datasets\n",
            "output sentence:  method binarize weights activations deep neural network efficient computation memory usage performs better state art \n",
            "\n",
            "{'rouge-1': {'r': 0.18823529411764706, 'p': 0.9411764705882353, 'f': 0.3137254874183007}, 'rouge-2': {'r': 0.14545454545454545, 'p': 0.8888888888888888, 'f': 0.24999999758300784}, 'rouge-l': {'r': 0.18823529411764706, 'p': 0.9411764705882353, 'f': 0.3137254874183007}}\n",
            "pair:  multi hop text based question answering current challenge machine comprehension task requires sequentially integrate facts multiple passages answer complex natural language questions paper propose novel architecture called latent question reformulation network lqr net multi hop parallel attentive network designed question answering tasks require reasoning capabilities lqr net composed association textbf reading modules textbf reformulation modules purpose reading module produce question aware representation document document representation reformulation module extracts essential elements calculate updated representation question updated question passed following hop evaluate architecture hotpotqa question answering dataset designed assess multi hop reasoning capabilities model achieves competitive results public leaderboard outperforms best current textit published models terms exact match em score finally show analysis sequential reformulations provide interpretable reasoning paths\n",
            "output sentence:  paper propose latent question reformulation network lqr net multi hop parallel attentive network designed question answering tasks require reasoning \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.75, 'f': 0.2142857118367347}, 'rouge-2': {'r': 0.03488372093023256, 'p': 0.25, 'f': 0.0612244876468139}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5833333333333334, 'f': 0.1666666642176871}}\n",
            "pair:  monitoring patients icu challenging high cost task hence predicting condition patients icu stay help provide better acute care plan hospital resources continuous progress machine learning research icu management work focused using time series signals recorded icu instruments work show adding clinical notes another modality improves performance model three benchmark tasks hospital mortality prediction modeling decompensation length stay forecasting play important role icu management time series data measured regular intervals doctor notes charted irregular times making challenging model together propose method model jointly achieving considerable improvement across benchmark tasks baseline time series model\n",
            "output sentence:  demostarte using clinical notes conjuntion icu instruments data improves perfomance icu management benchmark tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.1206896551724138, 'p': 0.7777777777777778, 'f': 0.20895522155491203}, 'rouge-2': {'r': 0.05194805194805195, 'p': 0.4444444444444444, 'f': 0.09302325393996759}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.4444444444444444, 'f': 0.11940298274894189}}\n",
            "pair:  existing deep reinforcement learning drl frameworks consider action spaces either discrete continuous space motivated project design game ai king glory kog one world popular mobile game consider scenario discrete continuous hybrid action space directly apply existing dlr frameworks existing approaches either approximate hybrid space discrete set relaxing continuous set usually less efficient robust paper propose parametrized deep network dqn hybrid action space without approximation relaxation algorithm combines dqn ddpg viewed extension dqn hybrid actions empirical study game kog validates efficiency effectiveness method\n",
            "output sentence:  dqn ddpg hybrid algorithm proposed deal discrete continuous hybrid action space \n",
            "\n",
            "{'rouge-1': {'r': 0.05333333333333334, 'p': 0.4, 'f': 0.09411764498269902}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.1111111111111111, 'f': 0.02040816159725128}, 'rouge-l': {'r': 0.05333333333333334, 'p': 0.4, 'f': 0.09411764498269902}}\n",
            "pair:  convolutional neural networks continuously advance progress image object classification steadfast usage algorithm requires constant evaluation upgrading foundational concepts maintain progress network regularization techniques typically focus convolutional layer operations leaving pooling layer operations without suitable options introduce wavelet pooling another alternative traditional neighborhood pooling method decomposes features second level decomposition discards first level subbands reduce feature dimensions method addresses overfitting problem encountered max pooling reducing features structurally compact manner pooling via neighborhood regions experimental results four benchmark classification datasets demonstrate proposed method outperforms performs comparatively methods like max mean mixed stochastic pooling\n",
            "output sentence:  pooling achieved using wavelets instead traditional neighborhood approaches max average etc \n",
            "\n",
            "{'rouge-1': {'r': 0.15476190476190477, 'p': 0.6190476190476191, 'f': 0.24761904441904764}, 'rouge-2': {'r': 0.05357142857142857, 'p': 0.2857142857142857, 'f': 0.09022556125049473}, 'rouge-l': {'r': 0.10714285714285714, 'p': 0.42857142857142855, 'f': 0.17142856822857147}}\n",
            "pair:  model free reinforcement learning rl proven powerful general tool learning complex behaviors however sample efficiency often impractically large solving challenging real world problems even policy algorithms learning limiting factor classic model free rl learning signal consists scalar rewards ignoring much rich information contained state transition tuples model based rl uses information training predictive model often achieve asymptotic performance model free rl due model bias introduce temporal difference models tdms family goal conditioned value functions trained model free learning used model based control tdms combine benefits model free model based rl leverage rich information state transitions learn efficiently still attaining asymptotic performance exceeds direct model based rl methods experimental results show range continuous control tasks tdms provide substantial improvement efficiency compared state art model based model free methods\n",
            "output sentence:  show special goal condition value function trained model free methods used within model based control resulting substantially better sample efficiency performance performance \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.5714285714285714, 'f': 0.13559321824763004}, 'rouge-2': {'r': 0.017543859649122806, 'p': 0.16666666666666666, 'f': 0.03174603002267583}, 'rouge-l': {'r': 0.057692307692307696, 'p': 0.42857142857142855, 'f': 0.10169491316288427}}\n",
            "pair:  abstract stochastic gradient descent sgd adam commonly used optimize deep neural networks choosing one usually means making tradeoffs speed accuracy stability present intuition tradeoffs exist well method unifying two continuous way makes possible control way models trained much greater detail show default parameters new algorithm equals outperforms sgd adam across range models image classification tasks outperforms sgd language modeling tasks\n",
            "output sentence:  algorithm unifying sgd adam empirical study performance \n",
            "\n",
            "{'rouge-1': {'r': 0.16363636363636364, 'p': 0.6923076923076923, 'f': 0.2647058792603807}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}, 'rouge-l': {'r': 0.16363636363636364, 'p': 0.6923076923076923, 'f': 0.2647058792603807}}\n",
            "pair:  paper first identify textit angle bias simple remarkable phenomenon causes vanishing gradient problem multilayer perceptron mlp sigmoid activation functions propose textit linearly constrained weights lcw reduce angle bias neural network train network constraints sum elements weight vector zero reparameterization technique presented efficiently train model lcw embedding constraints weight vectors structure network interestingly batch normalization ioffe szegedy viewed mechanism correct angle bias preliminary experiments show lcw helps train layered mlp efficiently batch normalization\n",
            "output sentence:  identify angle bias causes vanishing gradient problem deep nets propose efficient method reduce bias \n",
            "\n",
            "{'rouge-1': {'r': 0.06451612903225806, 'p': 0.5714285714285714, 'f': 0.11594202716236088}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.42857142857142855, 'f': 0.08695651991598406}}\n",
            "pair:  identifying analogies across domains without supervision key task artificial intelligence recent advances cross domain image mapping concentrated translating images across domains although progress made impressive visual fidelity many times suffice identifying matching sample domain paper tackle task finding exact analogies datasets every image domain find analogous image domain present matching synthesis approach gan show outperforms current techniques show cross domain mapping task broken two parts domain alignment learning mapping function tasks iteratively solved alignment improved unsupervised translation function reaches quality comparable full supervision\n",
            "output sentence:  finding correspondences domains performing matching mapping iterations \n",
            "\n",
            "{'rouge-1': {'r': 0.041237113402061855, 'p': 0.5714285714285714, 'f': 0.07692307566752961}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.3333333333333333, 'f': 0.03124999910644534}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.5714285714285714, 'f': 0.07692307566752961}}\n",
            "pair:  given video sentence goal weakly supervised video moment retrieval locate video segment described sentence without access temporal annotations training instead model must learn identify correct segment moment provided video sentence pairs thus inherent challenge automatically inferring latent correspondence visual language representations facilitate alignment propose weakly supervised moment alignment network wman exploits multi level co attention mechanism learn richer multimodal representations aforementioned mechanism comprised frame word interaction module well novel word conditioned visual graph wcvg approach also incorporates novel application positional encodings commonly used transformers learn visual semantic representations contain contextual information relative positions temporal sequence iterative message passing comprehensive experiments didemo charades sta datasets demonstrate effectiveness learned representations combined wman model outperforms state art weakly supervised method significant margin also better strongly supervised state art methods metrics\n",
            "output sentence:  weakly supervised text based video moment retrieval \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.375, 'f': 0.13636363338842983}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.25, 'f': 0.09090908793388439}}\n",
            "pair:  formulate new problem intersection semi supervised learning contextual bandits motivated several applications including clinical trials dialog systems demonstrate contextual bandit graph convolutional networks adjusted new problem formulation take best approaches develop multi gcn embedded contextual bandit algorithms verified several real world datasets\n",
            "output sentence:  synthesis gcn linucb algorithms online learning missing feedbacks \n",
            "\n",
            "{'rouge-1': {'r': 0.21568627450980393, 'p': 0.9166666666666666, 'f': 0.349206346122449}, 'rouge-2': {'r': 0.14285714285714285, 'p': 0.9090909090909091, 'f': 0.2469135778997104}, 'rouge-l': {'r': 0.21568627450980393, 'p': 0.9166666666666666, 'f': 0.349206346122449}}\n",
            "pair:  propose rapp new methodology novelty detection utilizing hidden space activation values obtained deep autoencoder precisely rapp compares input autoencoder reconstruction input space also hidden spaces show feed reconstructed input autoencoder activated values hidden space equivalent corresponding reconstruction hidden space given original input order aggregate hidden space activation values propose two metrics enhance novelty detection performance extensive experiments using diverse datasets validate rapp improves novelty detection performances autoencoder based approaches besides show rapp outperforms recent novelty detection methods evaluated popular benchmarks\n",
            "output sentence:  new methodology novelty detection utilizing hidden space activation values obtained deep autoencoder \n",
            "\n",
            "{'rouge-1': {'r': 0.07777777777777778, 'p': 0.7777777777777778, 'f': 0.14141413976124886}, 'rouge-2': {'r': 0.018691588785046728, 'p': 0.25, 'f': 0.034782607401134265}, 'rouge-l': {'r': 0.03333333333333333, 'p': 0.3333333333333333, 'f': 0.06060605895316809}}\n",
            "pair:  predicting properties nodes graph important problem applications variety domains graph based semi supervised learning ssl methods aim address problem labeling small subset nodes seeds utilizing graph structure predict label scores rest nodes graph recently graph convolutional networks gcns achieved impressive performance graph based ssl task addition label scores also desirable confidence score associated unfortunately confidence estimation context gcn previously explored fill important gap paper propose confgcn estimates labels scores along confidences jointly gcn based setting confgcn uses estimated confidences determine influence one node another neighborhood aggregation thereby acquiring anisotropic capabilities extensive analysis experiments standard benchmarks find confgcn able significantly outperform state art baselines made confgcn source code available encourage reproducible research\n",
            "output sentence:  propose confidence based graph convolutional network semi supervised learning \n",
            "\n",
            "{'rouge-1': {'r': 0.1038961038961039, 'p': 0.6153846153846154, 'f': 0.1777777753061729}, 'rouge-2': {'r': 0.061224489795918366, 'p': 0.46153846153846156, 'f': 0.10810810604009419}, 'rouge-l': {'r': 0.1038961038961039, 'p': 0.6153846153846154, 'f': 0.1777777753061729}}\n",
            "pair:  propose novel framework generate clean video frames single motion blurred image broad range literature focuses recovering single image blurred image work tackle challenging task video restoration blurred image formulate video restoration single blurred image inverse problem setting clean image sequence respective motion latent factors blurred image observation framework based encoder decoder structure spatial transformer network modules restore video sequence underlying motion end end manner design loss function regularizers complementary properties stabilize training analyze variant models proposed network effectiveness transferability network highlighted large set experiments two different types datasets camera rotation blurs generated panorama scenes dynamic motion blurs high speed videos code models publicly available\n",
            "output sentence:  present novel unified architecture restores video frames single motion blurred image end end manner \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.6666666666666666, 'f': 0.09302325451595458}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0375, 'p': 0.5, 'f': 0.06976744056246621}}\n",
            "pair:  several first order stochastic optimization methods commonly used euclidean domain stochastic gradient descent sgd accelerated gradient descent variance reduced methods already adapted certain riemannian settings however popular optimization tools namely adam adagrad recent amsgrad remain generalized riemannian manifolds discuss difficulty generalizing adaptive schemes agnostic riemannian setting provide algorithms convergence proofs geodesically convex objectives particular case product riemannian manifolds adaptivity implemented across manifolds cartesian product generalization tight sense choosing euclidean space riemannian manifold yields algorithms regret bounds already known standard algorithms experimentally show faster convergence lower train loss value riemannian adaptive methods corresponding baselines realistic task embedding wordnet taxonomy poincare ball\n",
            "output sentence:  adapting adam amsgrad adagrad riemannian manifolds \n",
            "\n",
            "{'rouge-1': {'r': 0.11320754716981132, 'p': 0.631578947368421, 'f': 0.19199999742208002}, 'rouge-2': {'r': 0.025423728813559324, 'p': 0.16666666666666666, 'f': 0.044117644762110844}, 'rouge-l': {'r': 0.05660377358490566, 'p': 0.3157894736842105, 'f': 0.09599999742208007}}\n",
            "pair:  achieving machine intelligence requires smooth integration perception reasoning yet models developed date tend specialize one sophisticated manipulation symbols acquired rich perceptual spaces far proved elusive consider visual arithmetic task goal carry simple arithmetical algorithms digits presented natural conditions hand written placed randomly propose two tiered architecture tackling kind problem lower tier consists heterogeneous collection information processing modules include pre trained deep neural networks locating extracting characters image well modules performing symbolic transformations representations extracted perception higher tier consists controller trained using reinforcement learning coordinates modules order solve high level task instance controller may learn contexts execute perceptual networks symbolic transformations apply outputs resulting model able solve variety tasks visual arithmetic domain several advantages standard architecturally homogeneous feedforward networks including improved sample efficiency\n",
            "output sentence:  use reinforcement learning train agent solve set visual arithmetic tasks using provided pre trained perceptual modules transformations internal representations \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.8, 'f': 0.22222221983024693}, 'rouge-2': {'r': 0.06756756756756757, 'p': 0.5555555555555556, 'f': 0.1204819257773262}, 'rouge-l': {'r': 0.12903225806451613, 'p': 0.8, 'f': 0.22222221983024693}}\n",
            "pair:  present neural rendering architecture helps variational autoencoders vaes learn disentangled representations instead deconvolutional network typically used decoder vaes tile broadcast latent vector across space concatenate fixed coordinate channels apply fully convolutional network stride provides architectural prior dissociating positional non positional features latent space yet without providing explicit supervision effect show architecture term spatial broadcast decoder improves disentangling reconstruction accuracy generalization held regions data space show spatial broadcast decoder complementary state art sota disentangling techniques incorporated improves performance\n",
            "output sentence:  introduce neural rendering architecture helps vaes learn disentangled latent representations \n",
            "\n",
            "{'rouge-1': {'r': 0.09722222222222222, 'p': 0.4375, 'f': 0.15909090611570254}, 'rouge-2': {'r': 0.011904761904761904, 'p': 0.06666666666666667, 'f': 0.02020201763085432}, 'rouge-l': {'r': 0.06944444444444445, 'p': 0.3125, 'f': 0.11363636066115711}}\n",
            "pair:  wide adoption complex rnn based models hindered inference performance cost memory requirements address issue develop antman combining structured sparsity low rank decomposition synergistically reduce model computation size execution time rnns attaining desired accuracy antman extends knowledge distillation based training learn compressed models efficiently evaluation shows antman offers computation reduction less pt accuracy drop language machine reading comprehension models evaluation also shows given accuracy target antman produces smaller models state art lastly show antman offers super linear speed gains compared theoretical speedup demonstrating practical value commodity hardware\n",
            "output sentence:  reducing computation memory complexity rnn models using sparse low rank compression modules trained via knowledge distillation \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.6363636363636364, 'f': 0.20588235022923876}, 'rouge-2': {'r': 0.03225806451612903, 'p': 0.18181818181818182, 'f': 0.05479451798836567}, 'rouge-l': {'r': 0.07017543859649122, 'p': 0.36363636363636365, 'f': 0.11764705611159176}}\n",
            "pair:  batch normalization batch norm often used attempt stabilize accelerate training deep neural networks many cases indeed decreases number parameter updates required achieve low training error however also reduces robustness small adversarial input perturbations noise double digit percentages show five standard datasets furthermore substituting weight decay batch norm sufficient nullify relationship adversarial vulnerability input dimension work consistent mean field analysis found batch norm causes exploding gradients\n",
            "output sentence:  batch normalization reduces adversarial robustness well general robustness many cases particularly corruptions \n",
            "\n",
            "{'rouge-1': {'r': 0.08490566037735849, 'p': 0.6923076923076923, 'f': 0.15126050225549045}, 'rouge-2': {'r': 0.024, 'p': 0.25, 'f': 0.04379561883957595}, 'rouge-l': {'r': 0.05660377358490566, 'p': 0.46153846153846156, 'f': 0.10084033418826356}}\n",
            "pair:  discovering causal structure among set variables fundamental problem many empirical sciences traditional score based casual discovery methods rely various local heuristics search directed acyclic graph dag according predefined score function methods greedy equivalence search may attractive results infinite samples certain model assumptions less satisfactory practice due finite data possible violation assumptions motivated recent advances neural combinatorial optimization propose use reinforcement learning rl search dag best scoring encoder decoder model takes observable data input generates graph adjacency matrices used compute rewards reward incorporates predefined score function two penalty terms enforcing acyclicity contrast typical rl applications goal learn policy use rl search strategy final output would graph among graphs generated training achieves best reward conduct experiments synthetic real datasets show proposed approach improved search ability also allows flexible score function acyclicity constraint\n",
            "output sentence:  apply reinforcement learning score based causal discovery achieve promising results synthetic real datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.16981132075471697, 'p': 0.75, 'f': 0.27692307391242604}, 'rouge-2': {'r': 0.08196721311475409, 'p': 0.45454545454545453, 'f': 0.13888888630015434}, 'rouge-l': {'r': 0.1509433962264151, 'p': 0.6666666666666666, 'f': 0.2461538431431953}}\n",
            "pair:  unsupervised domain adaptation aims generalize hypothesis trained source domain unlabeled target domain one popular approach problem learn domain invariant embeddings domains work study theoretically empirically effect embedding complexity generalization target domain particular complexity affects upper bound target risk reflected experiments next specify theoretical framework multilayer neural networks result develop strategy mitigates sensitivity embedding complexity empirically achieves performance par better best layer dependent complexity tradeoff\n",
            "output sentence:  study effect embedding complexity learning domain invariant representations develop strategy mitigates sensitivity \n",
            "\n",
            "{'rouge-1': {'r': 0.09574468085106383, 'p': 0.6428571428571429, 'f': 0.1666666644101509}, 'rouge-2': {'r': 0.04, 'p': 0.3125, 'f': 0.07092198380363167}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.5714285714285714, 'f': 0.1481481458916324}}\n",
            "pair:  neural style transfer become popular technique generating images distinct artistic styles using convolutional neural networks recent success image style transfer raised question whether similar methods leveraged alter style musical audio work attempt long time scale high quality audio transfer texture synthesis time domain captures harmonic rhythmic timbral elements related musical style using examples may different lengths musical keys demonstrate ability use randomly initialized convolutional neural networks transfer aspects musical style one piece onto another using different representations audio log magnitude short time fourier transform stft mel spectrogram constant transform spectrogram propose using representations way generating modifying perceptually significant characteristics musical audio content demonstrate representation shortcomings advantages others carefully designing neural network structures complement nature musical audio finally show compelling style transfer examples make use ensemble representations help capture varying desired characteristics audio signals\n",
            "output sentence:  present long time scale musical audio style transfer algorithm synthesizes audio time domain uses time frequency representations audio \n",
            "\n",
            "{'rouge-1': {'r': 0.03333333333333333, 'p': 0.2857142857142857, 'f': 0.05970149066607268}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03333333333333333, 'p': 0.2857142857142857, 'f': 0.05970149066607268}}\n",
            "pair:  increasing use neural networks music information retrieval tasks paper empirically investigate different ways improving performance convolutional neural networks cnns spectral audio features specifically explore three aspects cnn design depth network use residual blocks along use grouped convolution global aggregation time application context singer classification singing performance embedding believe conclusions extend types music analysis using convolutional neural networks results show global time aggregation helps improve performance cnns another contribution paper release singing recording dataset used training evaluation\n",
            "output sentence:  using deep learning techniques singing voice related tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.16883116883116883, 'p': 0.9285714285714286, 'f': 0.2857142831107355}, 'rouge-2': {'r': 0.11904761904761904, 'p': 0.7692307692307693, 'f': 0.2061855646891274}, 'rouge-l': {'r': 0.15584415584415584, 'p': 0.8571428571428571, 'f': 0.2637362611327135}}\n",
            "pair:  various gradient compression schemes proposed mitigate communication cost distributed training large scale machine learning models sign based methods signsgd bernstein et al recently gaining popularity simple compression rule connection adaptive gradient methods like adam paper perform general analysis sign based methods non convex optimization analysis built intuitive bounds success probabilities rely special noise distributions boundedness variance stochastic gradients extending theory distributed setting within parameter server framework assure exponentially fast variance reduction respect number nodes maintaining bit compression directions using small mini batch sizes validate theoretical findings experimentally\n",
            "output sentence:  general analysis sign based methods signsgd non convex optimization built intuitive bounds success probabilities \n",
            "\n",
            "{'rouge-1': {'r': 0.1262135922330097, 'p': 0.6190476190476191, 'f': 0.20967741654136318}, 'rouge-2': {'r': 0.02586206896551724, 'p': 0.15, 'f': 0.044117644550173155}, 'rouge-l': {'r': 0.0970873786407767, 'p': 0.47619047619047616, 'f': 0.16129031976716965}}\n",
            "pair:  much work design convolutional networks last five years revolved around empirical investigation importance depth filter sizes number feature channels recent studies shown branching splitting computation along parallel distinct threads aggregating outputs represents new promising dimension significant improvements performance combat complexity design choices multi branch architectures prior work adopted simple strategies fixed branching factor input fed parallel branches additive combination outputs produced branches aggregation points work remove predefined choices propose algorithm learn connections branches network instead chosen priori human designer multi branch connectivity learned simultaneously weights network optimizing single loss function defined respect end task demonstrate approach problem multi class image classification using four different datasets yields consistently higher accuracy compared state art resnext multi branch network given learning capacity\n",
            "output sentence:  paper introduced algorithm learn connectivity deep multi branch networks approach evaluated image categorization consistently yields accuracy gains state art models use fixed \n",
            "\n",
            "{'rouge-1': {'r': 0.0875, 'p': 0.6363636363636364, 'f': 0.15384615172080668}, 'rouge-2': {'r': 0.01020408163265306, 'p': 0.1, 'f': 0.01851851683813458}, 'rouge-l': {'r': 0.05, 'p': 0.36363636363636365, 'f': 0.0879120857867408}}\n",
            "pair:  physical design robot policy controls motion inherently coupled however existing approaches largely ignore coupling instead choosing alternate separate design control phases requires expert intuition throughout risks convergence suboptimal designs work propose method jointly optimizes physical design robot corresponding control policy model free fashion without need expert supervision given arbitrary robot morphology method maintains distribution design parameters uses reinforcement learning train neural network controller throughout training refine robot distribution maximize expected reward results assignment robot parameters neural network policy jointly optimal evaluate approach context legged locomotion demonstrate discovers novel robot designs walking gaits several different morphologies achieving performance comparable better hand crafted designs\n",
            "output sentence:  use deep reinforcement learning design physical attributes robot jointly control policy \n",
            "\n",
            "{'rouge-1': {'r': 0.0392156862745098, 'p': 0.6666666666666666, 'f': 0.07407407302469136}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.4, 'f': 0.029197079588683488}, 'rouge-l': {'r': 0.029411764705882353, 'p': 0.5, 'f': 0.055555554506172854}}\n",
            "pair:  training activation quantized neural networks involves minimizing piecewise constant training loss whose gradient vanishes almost everywhere undesirable standard back propagation chain rule empirical way around issue use straight estimator ste bengio et al backward pass gradient modified chain rule becomes non trivial since unusual gradient certainly gradient loss function following question arises searching negative direction minimizes training loss paper provide theoretical justification concept ste answering question consider problem learning two linear layer network binarized relu activation gaussian input data shall refer unusual gradient given ste modifed chain rule coarse gradient choice ste unique prove ste properly chosen expected coarse gradient correlates positively population gradient available training negation descent direction minimizing population loss show associated coarse gradient descent algorithm converges critical point population loss minimization problem moreover show poor choice ste leads instability training algorithm near certain local minima verified cifar experiments\n",
            "output sentence:  make theoretical justification concept straight estimator \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.5, 'f': 0.12121211908172637}, 'rouge-2': {'r': 0.021897810218978103, 'p': 0.2, 'f': 0.03947368243161366}, 'rouge-l': {'r': 0.04310344827586207, 'p': 0.3125, 'f': 0.07575757362718095}}\n",
            "pair:  owing connection generative adversarial networks gans saddle point problems recently attracted considerable interest machine learning beyond necessity theoretical guarantees revolve around convex concave even linear problems however making theoretical inroads towards efficient gan training depends crucially moving beyond classic framework make piecemeal progress along lines analyze behavior mirror descent md class non monotone problems whose solutions coincide naturally associated variational inequality property call coherence first show ordinary vanilla md converges strict version condition otherwise particular may fail converge even bilinear models unique solution show deficiency mitigated optimism taking extra gradient step optimistic mirror descent omd converges coherent problems analysis generalizes extends results daskalakis et al optimistic gradient descent ogd bilinear problems makes concrete headway provable convergence beyond convex concave games also provide stochastic analogues results validate analysis numerical experiments wide array gan models including gaussian mixture models celeba cifar datasets\n",
            "output sentence:  show inclusion extra gradient step first order gan training methods improve stability lead improved convergence results \n",
            "\n",
            "{'rouge-1': {'r': 0.05194805194805195, 'p': 0.3333333333333333, 'f': 0.08988763811639951}, 'rouge-2': {'r': 0.023529411764705882, 'p': 0.18181818181818182, 'f': 0.041666664637586906}, 'rouge-l': {'r': 0.05194805194805195, 'p': 0.3333333333333333, 'f': 0.08988763811639951}}\n",
            "pair:  high performance deep learning models typically comes cost considerable model size computation time factors limit applicability deployment memory battery constraint devices mobile phones embedded systems work propose novel pruning technique eliminates entire filters neurons according relative norm compared rest network yielding compression decreased redundancy parameters resulting network non sparse however much compact requires special infrastructure deployment prove viability method achieving compression lenet resnet resnet respectively exceeding state art compression results reported resnet without losing performance compared baseline approach exhibit good performance also easy implement many architectures\n",
            "output sentence:  propose novel structured class blind pruning technique produce highly compressed neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.06329113924050633, 'p': 0.4166666666666667, 'f': 0.1098901076005314}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0379746835443038, 'p': 0.25, 'f': 0.06593406364448746}}\n",
            "pair:  deep learning yields great results across many fields speech recognition image classification translation problem getting deep model work well involves research architecture long period tuning present single model yields good results number problems spanning multiple domains particular single model trained concurrently imagenet multiple translation tasks image captioning coco dataset speech recognition corpus english parsing task model architecture incorporates building blocks multiple domains contains convolutional layers attention mechanism sparsely gated layers computational blocks crucial subset tasks train interestingly even block crucial task observe adding never hurts performance cases improves tasks also show tasks less data benefit largely joint training tasks performance large tasks degrades slightly\n",
            "output sentence:  large scale multi task architecture solves imagenet translation together shows transfer learning \n",
            "\n",
            "{'rouge-1': {'r': 0.08955223880597014, 'p': 0.6, 'f': 0.15584415358407824}, 'rouge-2': {'r': 0.0379746835443038, 'p': 0.3333333333333333, 'f': 0.06818181634555791}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.4, 'f': 0.10389610163602636}}\n",
            "pair:  backpropagation driving today artificial neural networks however despite extensive research remains unclear brain implements algorithm among neuroscientists reinforcement learning rl algorithms often seen realistic alternative however convergence rate learning scales poorly number involved neurons propose hybrid learning approach neuron uses rl type strategy learn approximate gradients backpropagation would provide show approach learns approximate gradient match performance gradient based learning fully connected convolutional networks learning feedback weights provides biologically plausible mechanism achieving good performance without need precise pre specified learning rules\n",
            "output sentence:  perturbations used learn feedback weights large fully connected convolutional networks \n",
            "\n",
            "{'rouge-1': {'r': 0.18333333333333332, 'p': 0.9166666666666666, 'f': 0.30555555277777785}, 'rouge-2': {'r': 0.13043478260869565, 'p': 0.8181818181818182, 'f': 0.224999997628125}, 'rouge-l': {'r': 0.18333333333333332, 'p': 0.9166666666666666, 'f': 0.30555555277777785}}\n",
            "pair:  present graph neural network assisted monte carlo tree search approach classical traveling salesman problem tsp adopt greedy algorithm framework construct optimal solution tsp adding nodes successively graph neural network gnn trained capture local global graph structure give prior probability selecting vertex every step prior probability provides heuristics mcts mcts output improved probability selecting successive vertex feedback information fusing prior scouting procedure experimental results tsp nodes demonstrate proposed method obtains shorter tours learning based methods\n",
            "output sentence:  graph neural network assisted monte carlo tree search approach traveling salesman problem \n",
            "\n",
            "{'rouge-1': {'r': 0.136986301369863, 'p': 0.8333333333333334, 'f': 0.23529411522214533}, 'rouge-2': {'r': 0.053763440860215055, 'p': 0.4166666666666667, 'f': 0.09523809321360548}, 'rouge-l': {'r': 0.1232876712328767, 'p': 0.75, 'f': 0.21176470345743945}}\n",
            "pair:  intelligent creatures explore environments learn useful skills without supervision paper propose diversity need diayn method learning useful skills without reward function proposed method learns skills maximizing information theoretic objective using maximum entropy policy variety simulated robotic tasks show simple objective results unsupervised emergence diverse skills walking jumping number reinforcement learning benchmark environments method able learn skill solves benchmark task despite never receiving true task reward show pretrained skills provide good parameter initialization downstream tasks composed hierarchically solve complex sparse reward tasks results suggest unsupervised discovery skills serve effective pretraining mechanism overcoming challenges exploration data efficiency reinforcement learning\n",
            "output sentence:  propose algorithm learning useful skills without reward function show skills used solve downstream tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.04477611940298507, 'p': 0.6, 'f': 0.08333333204089506}, 'rouge-2': {'r': 0.0125, 'p': 0.25, 'f': 0.023809522902494367}, 'rouge-l': {'r': 0.04477611940298507, 'p': 0.6, 'f': 0.08333333204089506}}\n",
            "pair:  reproducibility reinforcement learning research highlighted key challenge area field paper present case study reproducing results one groundbreaking algorithm alphazero reinforcement learning system learns play go superhuman level given rules game describe minigo reproduction alphazero system using publicly available google cloud platform infrastructure google cloud tpus minigo system includes central reinforcement learning loop well auxiliary monitoring evaluation infrastructure ten days training scratch cloud tpus minigo play evenly leelazero elf opengo two strongest publicly available go ais discuss difficulties scaling reinforcement learning system monitoring systems required understand complex interplay hyperparameter configurations\n",
            "output sentence:  reproduced alphazero google cloud platform \n",
            "\n",
            "{'rouge-1': {'r': 0.056179775280898875, 'p': 0.3125, 'f': 0.09523809265487536}, 'rouge-2': {'r': 0.008403361344537815, 'p': 0.06666666666666667, 'f': 0.014925371146135262}, 'rouge-l': {'r': 0.0449438202247191, 'p': 0.25, 'f': 0.07619047360725632}}\n",
            "pair:  present cross view training cvt simple effective method deep semi supervised learning labeled examples model trained standard cross entropy loss unlabeled example model first performs inference acting teacher produce soft targets model learns soft targets acting student deviate prior work adding multiple auxiliary student prediction layers model input student layer sub network full model restricted view input seeing one region image students learn teacher full model teacher sees example concurrently students improve quality representations used teacher learn make predictions limited data combined virtual adversarial training cvt improves upon current state art semi supervised cifar semi supervised svhn also apply cvt train models five natural language processing tasks using hundreds millions sentences unlabeled data tasks cvt substantially outperforms supervised learning alone resulting models improve upon competitive current state art\n",
            "output sentence:  self training different views input gives excellent results semi supervised image recognition sequence tagging dependency parsing \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.6666666666666666, 'f': 0.19047618802721092}, 'rouge-2': {'r': 0.04854368932038835, 'p': 0.45454545454545453, 'f': 0.08771929650200062}, 'rouge-l': {'r': 0.09722222222222222, 'p': 0.5833333333333334, 'f': 0.1666666642176871}}\n",
            "pair:  existing public face image datasets strongly biased toward caucasian faces races latino significantly underrepresented models trained datasets suffer inconsistent classification accuracy limits applicability face analytic systems non white race groups mitigate race bias problem datasets constructed novel face image dataset containing images balanced race define race groups white black indian east asian southeast asian middle eastern latino images collected yfcc flickr dataset labeled race gender age groups evaluations performed existing face attribute datasets well novel image datasets measure generalization performance find model trained dataset substantially accurate novel datasets accuracy consistent across race gender groups also compare several commercial computer vision apis report balanced accuracy across gender race age groups\n",
            "output sentence:  new face image dataset balanced race gender age used bias measurement mitigation \n",
            "\n",
            "{'rouge-1': {'r': 0.09523809523809523, 'p': 0.8, 'f': 0.17021276405613403}, 'rouge-2': {'r': 0.03669724770642202, 'p': 0.36363636363636365, 'f': 0.06666666500138893}, 'rouge-l': {'r': 0.05952380952380952, 'p': 0.5, 'f': 0.10638297682209145}}\n",
            "pair:  differentiable planning network architecture shown powerful solving transfer planning tasks possesses simple end end training feature many great planning architectures proposed later literature inspired design principle recursive network architecture applied emulate backup operations value iteration algorithm however existing frame works learn plan effectively domains lattice structure regular graphs embedded certain euclidean space paper propose general planning network called graph based motion planning networks grmpn able learn plan general irregular graphs hence ii render existing planning network architectures special cases proposed grmpn framework invariant task graph permutation graph isormophism result grmpn possesses generalization strength data efficiency ability demonstrate performance proposed grmpn method baselines three domains ranging mazes regular graph path planning irregular graphs motion planning irregular graph robot configurations\n",
            "output sentence:  propose end end differentiable planning network graphs applicable many motion planning problems \n",
            "\n",
            "{'rouge-1': {'r': 0.13333333333333333, 'p': 0.8571428571428571, 'f': 0.23076922843934913}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.3333333333333333, 'f': 0.06896551538644476}, 'rouge-l': {'r': 0.08888888888888889, 'p': 0.5714285714285714, 'f': 0.1538461515162722}}\n",
            "pair:  plagiarism text reuse become available internet development therefore important check scientific papers fact cheating especially academia existing systems plagiarism detection show good performance huge source databases thus enough copy text source document get original work therefore another type plagiarism become popular cross lingual plagiarism present crosslang system kind plagiarism detection english russian language pair\n",
            "output sentence:  system cross lingual english russian plagiarism detection \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.5555555555555556, 'f': 0.1666666641166667}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0784313725490196, 'p': 0.4444444444444444, 'f': 0.13333333078333337}}\n",
            "pair:  stability key aspect data analysis many applications natural notion stability geometric illustrated example computer vision scattering transforms construct deep convolutional representations certified stable input deformations stability deformations interpreted stability respect changes metric structure domain work show scattering transforms generalized non euclidean domains using diffusion wavelets preserving notion stability respect metric changes domain measured diffusion maps resulting representation stable metric perturbations domain able capture high frequency information akin euclidean scattering\n",
            "output sentence:  stability scattering transform representations graph data deformations underlying graph support \n",
            "\n",
            "{'rouge-1': {'r': 0.0410958904109589, 'p': 0.3333333333333333, 'f': 0.07317072975312319}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.125, 'f': 0.01999999852800011}, 'rouge-l': {'r': 0.0410958904109589, 'p': 0.3333333333333333, 'f': 0.07317072975312319}}\n",
            "pair:  present information theoretic framework understanding trade offs unsupervised learning deep latent variables models using variational inference framework emphasizes need consider latent variable models along two dimensions ability reconstruct inputs distortion communication cost rate derive optimal frontier generative models two dimensional rate distortion plane show standard evidence lower bound objective insufficient select points along frontier however performing targeted optimization learn generative models different rates able learn many models achieve similar generative performance make vastly different trade offs terms usage latent variable experiments mnist omniglot variety architectures show framework sheds light many recent proposed extensions variational autoencoder family\n",
            "output sentence:  provide information theoretic experimental analysis state art variational autoencoders \n",
            "\n",
            "{'rouge-1': {'r': 0.0196078431372549, 'p': 0.3333333333333333, 'f': 0.03703703598765435}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0196078431372549, 'p': 0.3333333333333333, 'f': 0.03703703598765435}}\n",
            "pair:  saliency methods aim explain predictions deep neural networks methods lack reliability explanation sensitive factors contribute model prediction use simple common pre processing step adding mean shift input data show transformation effect model cause numerous methods incorrectly attribute define input invariance requirement saliency method mirror sensitivity model respect transformations input show several examples saliency methods satisfy input invariance property unreliable lead misleading inaccurate attribution\n",
            "output sentence:  attribution sometimes misleading \n",
            "\n",
            "{'rouge-1': {'r': 0.13513513513513514, 'p': 0.6666666666666666, 'f': 0.22471909832091908}, 'rouge-2': {'r': 0.049019607843137254, 'p': 0.35714285714285715, 'f': 0.08620689442925095}, 'rouge-l': {'r': 0.12162162162162163, 'p': 0.6, 'f': 0.20224718820855958}}\n",
            "pair:  present new latent model natural images learned large scale datasets learning process provides latent embedding every image training dataset well deep convolutional network maps latent space image space training new model provides strong universal image prior variety image restoration tasks large hole inpainting superresolution colorization model high resolution natural images approach uses latent spaces high dimensionality one two orders magnitude higher previous latent image models tackle high dimensionality use latent spaces special manifold structure convolutional manifolds parameterized convnet certain architecture experiments compare learned latent models latent models learned autoencoders advanced variants generative adversarial networks strong baseline system using simpler parameterization latent space model outperforms competing approaches range restoration tasks\n",
            "output sentence:  present new deep latent model natural images trained unlabeled datasets utilized solve various image restoration tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.0898876404494382, 'p': 0.47058823529411764, 'f': 0.1509433935332859}, 'rouge-2': {'r': 0.018691588785046728, 'p': 0.11764705882352941, 'f': 0.032258062150104225}, 'rouge-l': {'r': 0.0449438202247191, 'p': 0.23529411764705882, 'f': 0.07547169542007842}}\n",
            "pair:  attacks natural language models difficult compare due different definitions constitutes successful attack present taxonomy constraints categorize attacks constraint present real world use case way measure well generated samples enforce constraint employ framework evaluate two state art attacks fool models synonym substitution attacks claim adversarial perturbations preserve semantics syntactical correctness inputs analysis shows constraints strongly enforced significant portion adversarial examples grammar checker detects increase errors additionally human studies indicate many adversarial examples diverge semantic meaning input appear human written finally highlight need standardized evaluation attacks share constraints without shared evaluation metrics researchers set thresholds determine trade attack quality attack success recommend well designed human studies determine best threshold approximate human judgement\n",
            "output sentence:  present framework evaluating adversarial examples natural language processing demonstrate generated adversarial examples often semantics preserving syntactically correct non suspicious \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.5555555555555556, 'f': 0.1666666641166667}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.125, 'f': 0.02739725832238708}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}}\n",
            "pair:  develop new algorithm imitation learning single expert demonstration contrast many previous one shot imitation learning approaches algorithm assume access one expert demonstration training phase instead leverage exploration policy acquire unsupervised trajectories used train encoder context aware imitation policy optimization procedures encoder imitation learner exploration policy tightly linked linking creates feedback loop wherein exploration policy collects new demonstrations challenge imitation learner encoder attempts help imitation policy best abilities evaluate algorithm mujoco robotics tasks\n",
            "output sentence:  unsupervised self imitation algorithm capable inference single expert demonstration \n",
            "\n",
            "{'rouge-1': {'r': 0.08823529411764706, 'p': 0.46153846153846156, 'f': 0.14814814545343702}, 'rouge-2': {'r': 0.03529411764705882, 'p': 0.25, 'f': 0.06185566793495597}, 'rouge-l': {'r': 0.058823529411764705, 'p': 0.3076923076923077, 'f': 0.09876542940405433}}\n",
            "pair:  using recurrent neural networks rnns sequence modeling tasks promising delivering high quality results challenging meet stringent latency requirements memory bound execution pattern rnns propose big little dual module inference dynamically skip unnecessary memory access computation speedup rnn inference leveraging error resilient feature nonlinear activation functions used rnns propose use lightweight little module approximates original rnn layer referred big module compute activations insensitive region error resilient expensive memory access computation big module reduced results used sensitive region method reduce overall memory access average achieve speedup cpu based server platform negligible impact model quality\n",
            "output sentence:  accelerate rnn inference dynamically reducing redundant memory access using mixture accurate approximate modules \n",
            "\n",
            "{'rouge-1': {'r': 0.03389830508474576, 'p': 0.4, 'f': 0.06249999855957035}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.4, 'f': 0.06249999855957035}}\n",
            "pair:  simultaneous machine translation models start generating target sequence encoded read source sequence recent approach task either apply fixed policy transformer learnable monotonic attention weaker recurrent neural network based structure paper propose new attention mechanism monotonic multihead attention mma introduced monotonic attention mechanism multihead attention also introduced two novel interpretable approaches latency control specifically designed multiple attentions apply mma simultaneous machine translation task demonstrate better latency quality tradeoffs compared milk previous state art approach code released upon publication\n",
            "output sentence:  make transformer streamable monotonic attention \n",
            "\n",
            "{'rouge-1': {'r': 0.12643678160919541, 'p': 0.6470588235294118, 'f': 0.2115384588036243}, 'rouge-2': {'r': 0.04716981132075472, 'p': 0.29411764705882354, 'f': 0.08130081062595025}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.35294117647058826, 'f': 0.11538461264977817}}\n",
            "pair:  recent success neural networks solving difficult decision tasks incentivized incorporating smart decision making edge however work traditionally focused neural network inference rather training due memory compute limitations especially emerging non volatile memory systems writes energetically costly reduce lifespan yet ability train edge becoming increasingly important enables applications real time adaptability device drift environmental variation user customization federated learning across devices work address four key challenges training edge devices non volatile memory low weight update density weight quantization low auxiliary memory online learning present low rank training scheme addresses four challenges maintaining computational efficiency demonstrate technique representative convolutional neural network across several adaptation problems performs standard sgd accuracy number weight updates\n",
            "output sentence:  use kronecker sum approximations low rank training address challenges training neural networks edge devices utilize emerging memory technologies \n",
            "\n",
            "{'rouge-1': {'r': 0.06593406593406594, 'p': 0.5454545454545454, 'f': 0.11764705689926953}, 'rouge-2': {'r': 0.015748031496062992, 'p': 0.2, 'f': 0.029197078938675538}, 'rouge-l': {'r': 0.04395604395604396, 'p': 0.36363636363636365, 'f': 0.07843137062475976}}\n",
            "pair:  myriad kinds segmentation ultimately right segmentation given scene eye annotator standard approaches require large amounts labeled data learn one particular kind segmentation first step towards relieving annotation burden propose problem guided segmentation given varying amounts pixel wise labels segment unannotated pixels propagating supervision locally within image non locally across images propose guided networks extract latent task representation guidance variable amounts classes categories instances etc pixel supervision optimize architecture end end fast accurate data efficient segmentation meta learning span shot many shot learning regimes examine guidance little one pixel per concept much images compare full gradient optimization extremes explore generalization analyze guidance bridge different levels supervision segment classes union instances segmentor concentrates different amounts supervision different types classes efficient latent representation non locally propagates supervision across images updated quickly cumulatively given supervision\n",
            "output sentence:  propose meta learning approach guiding visual segmentation tasks varying amounts supervision \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.875, 'f': 0.19718309659194602}, 'rouge-2': {'r': 0.05063291139240506, 'p': 0.5714285714285714, 'f': 0.09302325431855057}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.75, 'f': 0.16901408250743902}}\n",
            "pair:  use imitation learning learn single policy complex task multiple modes hierarchical structure challenging fact previous work shown modes known learning separate policies mode sub task greatly improve performance imitation learning work discover interaction sub tasks resulting state action trajectory sequences using directed graphical model propose new algorithm based generative adversarial imitation learning framework automatically learns sub task policies unsegmented demonstrations approach maximizes directed information flow graphical model sub task latent variables generated trajectories also show approach connects existing options framework commonly used learn hierarchical policies\n",
            "output sentence:  learning hierarchical policies unsegmented demonstrations using directed information \n",
            "\n",
            "{'rouge-1': {'r': 0.2, 'p': 0.7692307692307693, 'f': 0.31746031418493326}, 'rouge-2': {'r': 0.12903225806451613, 'p': 0.6666666666666666, 'f': 0.21621621349890432}, 'rouge-l': {'r': 0.16, 'p': 0.6153846153846154, 'f': 0.25396825069286977}}\n",
            "pair:  consider reinforcement learning bandit structured prediction problems sparse loss feedback end episode introduce novel algorithm residual loss prediction reslope solves problems automatically learning internal representation denser reward function reslope operates reduction contextual bandits using learned loss representation solve credit assignment problem contextual bandit oracle trade exploration exploitation reslope enjoys regret reduction style theoretical guarantee outperforms state art reinforcement learning algorithms mdp environments bandit structured prediction settings\n",
            "output sentence:  present novel algorithm solving reinforcement learning bandit structured prediction problems sparse loss feedback \n",
            "\n",
            "{'rouge-1': {'r': 0.08888888888888889, 'p': 0.6666666666666666, 'f': 0.15686274302191466}, 'rouge-2': {'r': 0.020833333333333332, 'p': 0.2, 'f': 0.037735847347810686}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.5, 'f': 0.11764705674740487}}\n",
            "pair:  propose extend existing deep reinforcement learning deep rl algorithms allowing additionally choose sequences actions part policy modification forces network anticipate reward action sequences show improves exploration leading better convergence proposal simple flexible easily incorporated deep rl framework show power scheme consistently outperforming state art ga algorithm several popular atari games\n",
            "output sentence:  anticipation improves convergence deep reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.03508771929824561, 'p': 0.2, 'f': 0.05970148999777244}, 'rouge-2': {'r': 0.014705882352941176, 'p': 0.1111111111111111, 'f': 0.02597402390959706}, 'rouge-l': {'r': 0.03508771929824561, 'p': 0.2, 'f': 0.05970148999777244}}\n",
            "pair:  nowadays deep learning one main topics almost every field helped get amazing results great number tasks main problem kind learning consequently neural networks defined deep resource intensive need specialized hardware perform computation reasonable time unfortunately sufficient make deep learning usable real life many tasks mandatory much possible real time needed optimize many components code algorithms numeric accuracy hardware make efficient usable optimizations help us produce incredibly accurate fast learning models\n",
            "output sentence:  embedded architecture deep learning optimized devices face detection emotion recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.056179775280898875, 'p': 0.3125, 'f': 0.09523809265487536}, 'rouge-2': {'r': 0.008403361344537815, 'p': 0.06666666666666667, 'f': 0.014925371146135262}, 'rouge-l': {'r': 0.0449438202247191, 'p': 0.25, 'f': 0.07619047360725632}}\n",
            "pair:  present cross view training cvt simple effective method deep semi supervised learning labeled examples model trained standard cross entropy loss unlabeled example model first performs inference acting teacher produce soft targets model learns soft targets acting student deviate prior work adding multiple auxiliary student prediction layers model input student layer sub network full model restricted view input seeing one region image students learn teacher full model teacher sees example concurrently students improve quality representations used teacher learn make predictions limited data combined virtual adversarial training cvt improves upon current state art semi supervised cifar semi supervised svhn also apply cvt train models five natural language processing tasks using hundreds millions sentences unlabeled data tasks cvt substantially outperforms supervised learning alone resulting models improve upon competitive current state art\n",
            "output sentence:  self training different views input gives excellent results semi supervised image recognition sequence tagging dependency parsing \n",
            "\n",
            "{'rouge-1': {'r': 0.11864406779661017, 'p': 0.875, 'f': 0.20895522177767878}, 'rouge-2': {'r': 0.05714285714285714, 'p': 0.5714285714285714, 'f': 0.10389610224321134}, 'rouge-l': {'r': 0.0847457627118644, 'p': 0.625, 'f': 0.14925372924036534}}\n",
            "pair:  introduce cgnn framework learn functional causal models generative neural networks networks trained using backpropagation minimize maximum mean discrepancy observed data unlike previous approaches cgnn leverages conditional independences distributional asymmetries seamlessly discover bivariate multivariate causal structures without hidden variables cgnn estimate causal structure full differentiable generative model data throughout extensive variety experiments illustrate competitive esults cgnn state art alternatives observational causal discovery simulated real data tasks cause effect inference structure identification multivariate causal discovery\n",
            "output sentence:  discover structure functional causal models generative neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.12631578947368421, 'p': 0.8, 'f': 0.21818181582644633}, 'rouge-2': {'r': 0.08333333333333333, 'p': 0.7142857142857143, 'f': 0.14925372947204277}, 'rouge-l': {'r': 0.12631578947368421, 'p': 0.8, 'f': 0.21818181582644633}}\n",
            "pair:  program verification offers framework ensuring program correctness therefore systematically eliminating different classes bugs inferring loop invariants one main challenges behind automated verification real world programs often contain many loops paper present continuous logic network cln novel neural architecture automatically learning loop invariants directly program execution traces unlike existing neural networks clns learn precise explicit representations formulas satisfiability modulo theories smt loop invariants program execution traces develop new sound complete semantic mapping assigning smt formulas continuous truth values allows clns trained efficiently use clns implement new inference system loop invariants cln inv significantly outperforms existing approaches popular code inv dataset cln inv first tool solve theoretically solvable problems code inv dataset moreover cln inv takes second average problem times faster existing approaches demonstrate cln inv even learn significantly complex loop invariants ones required code inv dataset\n",
            "output sentence:  introduce continuous logic network cln novel neural architecture automatically learning loop invariants general smt formulas \n",
            "\n",
            "{'rouge-1': {'r': 0.03389830508474576, 'p': 0.3333333333333333, 'f': 0.06153845986272194}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.3333333333333333, 'f': 0.06153845986272194}}\n",
            "pair:  reinforcement learning algorithms though successful tend fit training environments thereby hampering application real world paper proposes text text text robust reinforcement learning algorithm significant robust performance low high dimensional control tasks method formalises robust reinforcement learning novel min max game wasserstein constraint correct convergent solver apart formulation also propose efficient scalable solver following novel zero order optimisation method believe useful numerical optimisation general empirically demonstrate significant gains compared standard robust state art algorithms high dimensional mujuco environments\n",
            "output sentence:  rl algorithm learns robust changes dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.07894736842105263, 'p': 0.35294117647058826, 'f': 0.12903225507688756}, 'rouge-2': {'r': 0.03333333333333333, 'p': 0.17647058823529413, 'f': 0.0560747636824178}, 'rouge-l': {'r': 0.05263157894736842, 'p': 0.23529411764705882, 'f': 0.08602150238871556}}\n",
            "pair:  machine learning models used high stakes decisions predict accurately fairly responsibly fulfill three requirements model must able output reject option say know qualified make prediction work propose learning defer method model defer judgment downstream decision maker human user show learning defer generalizes rejection learning framework two ways considering effect agents decision making process allowing optimization complex objectives propose learning algorithm accounts potential biases held decision makerslater pipeline experiments real world datasets demonstrate learning defer make model accurate also less biased even operated highly biased users show deferring models still greatly improve fairness entire pipeline\n",
            "output sentence:  incorporating ability say know improve fairness classifier without sacrificing much accuracy improvement magnifies classifier insight downstream decision making \n",
            "\n",
            "{'rouge-1': {'r': 0.22058823529411764, 'p': 0.7894736842105263, 'f': 0.3448275827929714}, 'rouge-2': {'r': 0.1, 'p': 0.42105263157894735, 'f': 0.16161615851443736}, 'rouge-l': {'r': 0.1323529411764706, 'p': 0.47368421052631576, 'f': 0.20689654831021276}}\n",
            "pair:  humans learn variety concepts skills incrementally course lives exhibiting array desirable properties non forgetting concept rehearsal forward transfer backward transfer knowledge shot learning selective forgetting previous approaches lifelong machine learning demonstrate subsets properties often combining multiple complex mechanisms perspective propose powerful unified framework demonstrate properties utilizing small number weight consolidation parameters deep neural networks addition able draw many parallels behaviours mechanisms proposed framework surrounding human learning memory loss sleep deprivation perspective serves conduit two way inspiration understand lifelong learning machines humans\n",
            "output sentence:  drawing parallels human learning propose unified framework exhibit many lifelong learning abilities neural networks utilizing small number weight consolidation parameters \n",
            "\n",
            "{'rouge-1': {'r': 0.04, 'p': 0.3333333333333333, 'f': 0.07142856951530618}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.125, 'f': 0.019607841691657163}, 'rouge-l': {'r': 0.04, 'p': 0.3333333333333333, 'f': 0.07142856951530618}}\n",
            "pair:  deep learning made remarkable achievement many fields however learning parameters neural networks usually demands large amount labeled data algorithms deep learning therefore encounter difficulties applied supervised learning little data available specific task called shot learning address propose novel algorithm fewshot learning using discrete geometry sense samples class modeled reduced simplex volume simplex used measurement class scatter testing combined test sample points class new simplex formed similarity test sample class quantized ratio volumes new simplex original class simplex moreover present approach constructing simplices using local regions feature maps yielded convolutional neural networks experiments omniglot miniimagenet verify effectiveness simplex algorithm shot learning\n",
            "output sentence:  simplex based geometric method proposed cope shot learning problems \n",
            "\n",
            "{'rouge-1': {'r': 0.24193548387096775, 'p': 0.9375, 'f': 0.3846153813543722}, 'rouge-2': {'r': 0.17721518987341772, 'p': 0.9333333333333333, 'f': 0.29787233774332283}, 'rouge-l': {'r': 0.24193548387096775, 'p': 0.9375, 'f': 0.3846153813543722}}\n",
            "pair:  intrinsic rewards reinforcement learning provide powerful algorithmic capability agents learn interact environment task generic way however increased incentives motivation come cost increased fragility stochasticity introduce method computing intrinsic reward curiosity using metrics derived sampling latent variable model used estimate dynamics ultimately estimate conditional probability observed states used intrinsic reward curiosity experiments video game agent uses model autonomously learn play atari games using curiosity reward combination extrinsic rewards game achieve improved performance games sparse extrinsic rewards stochasticity introduced environment method still demonstrates improved performance baseline\n",
            "output sentence:  introduce method computing intrinsic reward curiosity using metrics derived sampling latent variable model used estimate dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.0975609756097561, 'p': 0.5, 'f': 0.1632653033902541}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.3125, 'f': 0.10204081359433577}}\n",
            "pair:  lifelong machine learning focuses adapting novel tasks without forgetting old tasks whereas shot learning strives learn single task given small amount data two different research areas crucial artificial general intelligence however existing studies somehow assumed impractical settings training models lifelong learning nature quantity incoming tasks inference time assumed known training time shot learning commonly assumed large number tasks available training humans hand perform learning tasks without regard aforementioned assumptions inspired human brain works propose novel model called slow thinking learn stl makes sophisticated slightly slower predictions iteratively considering interactions current previously seen tasks runtime conducted experiments results empirically demonstrate effectiveness stl realistic lifelong shot learning settings\n",
            "output sentence:  paper studies interactions fast learning slow prediction models demonstrate interactions improve machine capability solve joint lifelong learning \n",
            "\n",
            "{'rouge-1': {'r': 0.09090909090909091, 'p': 0.4, 'f': 0.14814814513031557}, 'rouge-2': {'r': 0.041666666666666664, 'p': 0.2, 'f': 0.06896551438763389}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.4, 'f': 0.14814814513031557}}\n",
            "pair:  work provides additional step theoretical understanding neural networks consider neural networks one hidden layer show learning symmetric functions one choose initial conditions standard sgd training efficiently produces generalization guarantees empirically verify show hold initial conditions chosen random proof convergence investigates interaction two layers network results highlight importance using symmetry design neural networks\n",
            "output sentence:  initialized properly neural networks learn simple class symmetric functions initialized randomly fail \n",
            "\n",
            "{'rouge-1': {'r': 0.2222222222222222, 'p': 0.7619047619047619, 'f': 0.3440860180090184}, 'rouge-2': {'r': 0.15730337078651685, 'p': 0.6666666666666666, 'f': 0.25454545145619834}, 'rouge-l': {'r': 0.2222222222222222, 'p': 0.7619047619047619, 'f': 0.3440860180090184}}\n",
            "pair:  understanding object motion one core problems computer vision requires segmenting tracking objects time significant progress made instance segmentation models cannot track objects crucially unable reason space time propose new spatio temporal embedding loss videos generates temporally consistent video instance segmentation model includes temporal network learns model temporal context motion essential produce smooth embeddings time model also estimates monocular depth self supervised loss relative distance object effectively constrains next ensuring time consistent embedding finally show model accurately track segment instances even occlusions missed detections advancing state art kitti multi object tracking dataset\n",
            "output sentence:  introduce new spatio temporal embedding loss videos generates temporally consistent video instance segmentation even occlusions missed detections using appearance geometry temporal context \n",
            "\n",
            "{'rouge-1': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}, 'rouge-2': {'r': 0.022388059701492536, 'p': 0.3333333333333333, 'f': 0.04195804077852221}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}}\n",
            "pair:  graph structured data social networks functional brain networks gene regulatory networks communications networks brought interest generalizing deep learning techniques graph domains paper interested design neural networks graphs variable length order solve learning problems vertex classification graph classification graph regression graph generative tasks existing works focused recurrent neural networks rnns learn meaningful representations graphs recently new convolutional neural networks convnets introduced work want compare rigorously two fundamental families architectures solve graph learning tasks review existing graph rnn convnet architectures propose natural extension lstm convnet graphs arbitrary size design set analytically controlled experiments two basic graph problems subgraph matching graph clustering test different architectures numerical results show proposed graph convnets accurate faster graph rnns graph convnets also accurate variational non learning techniques finally effective graph convnet architecture uses gated edges residuality residuality plays essential role learn multi layer architectures provide gain performance\n",
            "output sentence:  compare graph rnns graph convnets consider generic class graph convnets residuality \n",
            "\n",
            "{'rouge-1': {'r': 0.3076923076923077, 'p': 1.0, 'f': 0.4705882316955018}, 'rouge-2': {'r': 0.27906976744186046, 'p': 1.0, 'f': 0.43636363295206615}, 'rouge-l': {'r': 0.3076923076923077, 'p': 1.0, 'f': 0.4705882316955018}}\n",
            "pair:  paper deep boosting algorithm developed learn discriminative ensemble classifier seamlessly combining set base deep cnns base experts diverse capabilities base deep cnns sequentially trained recognize set object classes easy hard way according learning complexities experimental results demonstrated deep boosting algorithm significantly improve accuracy rates large scale visual recognition\n",
            "output sentence:  deep boosting algorithm developed learn discriminative ensemble classifier seamlessly combining set base deep cnns \n",
            "\n",
            "{'rouge-1': {'r': 0.0684931506849315, 'p': 0.625, 'f': 0.12345678834324036}, 'rouge-2': {'r': 0.036585365853658534, 'p': 0.375, 'f': 0.06666666504691361}, 'rouge-l': {'r': 0.0684931506849315, 'p': 0.625, 'f': 0.12345678834324036}}\n",
            "pair:  temporal point processes dominant paradigm modeling sequences events happening irregular intervals standard way learning models estimating conditional intensity function however parameterizing intensity function usually incurs several trade offs show overcome limitations intensity based approaches directly modeling conditional distribution inter event times draw literature normalizing flows design models flexible efficient additionally propose simple mixture model matches flexibility flow based models also permits sampling computing moments closed form proposed models achieve state art performance standard prediction tasks suitable novel applications learning sequence embeddings imputing missing data\n",
            "output sentence:  learn temporal point processes modeling conditional density conditional intensity \n",
            "\n",
            "{'rouge-1': {'r': 0.12280701754385964, 'p': 0.7777777777777778, 'f': 0.2121212097658402}, 'rouge-2': {'r': 0.030303030303030304, 'p': 0.2222222222222222, 'f': 0.05333333122133342}, 'rouge-l': {'r': 0.12280701754385964, 'p': 0.7777777777777778, 'f': 0.2121212097658402}}\n",
            "pair:  introduce adaptive input representations neural language modeling extend adaptive softmax grave et al input representations variable capacity several choices factorize input output layers whether model words characters sub word units perform systematic comparison popular choices self attentional architecture experiments show models equipped adaptive embeddings twice fast train popular character input cnn lower number parameters wikitext benchmark achieve perplexity improvement perplexity compared previously best published result billion word benchmark achieve perplexity\n",
            "output sentence:  variable capacity input word embeddings sota wikitext billion word benchmarks \n",
            "\n",
            "{'rouge-1': {'r': 0.07865168539325842, 'p': 0.6363636363636364, 'f': 0.139999998042}, 'rouge-2': {'r': 0.034482758620689655, 'p': 0.4, 'f': 0.06349206203073825}, 'rouge-l': {'r': 0.0449438202247191, 'p': 0.36363636363636365, 'f': 0.07999999804200005}}\n",
            "pair:  many real applications show great deal interest learning multiple tasks different data sources modalities unbalanced samples dimensions unfortunately existing cutting edge deep multi task learning mtl approaches cannot directly applied settings due either heterogeneous input dimensions heterogeneity optimal network architectures different tasks thus demanding develop knowledge sharing mechanism handle intrinsic discrepancies among network architectures across tasks end propose flexible knowledge sharing framework jointly learning multiple tasks distinct data sources modalities proposed framework allows task task data specific network design via utilizing compact tensor representation sharing achieved partially shared latent cores providing elaborate sharing control latent cores framework effective transferring task invariant knowledge yet also efficient learning task specific features experiments single multiple data sources modalities settings display promising results proposed method especially favourable insufficient data scenarios\n",
            "output sentence:  distributed latent space based knowledge sharing framework deep multi task learning \n",
            "\n",
            "{'rouge-1': {'r': 0.10576923076923077, 'p': 0.6111111111111112, 'f': 0.18032786633700623}, 'rouge-2': {'r': 0.0078125, 'p': 0.05555555555555555, 'f': 0.013698627975230216}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.4444444444444444, 'f': 0.1311475384681538}}\n",
            "pair:  variational autoencoder vae found success modelling manifold natural images certain datasets allowing meaningful images generated interpolating extrapolating latent code space unclear whether similar capabilities feasible text considering discrete nature work investigate reason unsupervised learning controllable representations fails text find traditional sequence vaes learn disentangled representations latent codes extent often fail properly decode latent factor manipulated manipulated codes often land holes vacant regions aggregated posterior latent space decoding network trained process validation explanation fix problem propose constrain posterior mean learned probability simplex performs manipulation within simplex proposed method mitigates latent vacancy problem achieves first success unsupervised learning controllable representations text empirically method significantly outperforms unsupervised baselines competitive strong supervised approaches text style transfer furthermore switching latent factor topic long sentence generation proposed framework often complete sentence seemingly natural way capability never attempted previous methods\n",
            "output sentence:  previous vaes text cannot learn controllable latent representation images well fix enable first success towards controlled text generation supervision \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}, 'rouge-2': {'r': 0.08860759493670886, 'p': 0.875, 'f': 0.16091953855991545}, 'rouge-l': {'r': 0.12307692307692308, 'p': 0.8888888888888888, 'f': 0.21621621407962022}}\n",
            "pair:  propose new model making generalizable diverse retrosynthetic reaction predictions given target compound task predict likely chemical reactants produce target generative task framed sequence sequence problem using smiles representations molecules building top popular transformer architecture propose two novel pre training methods construct relevant auxiliary tasks plausible reactions problem furthermore incorporate discrete latent variable model architecture encourage model produce diverse set alternative predictions subset reaction examples united states patent literature uspto benchmark dataset model greatly improves performance baseline also generating predictions diverse\n",
            "output sentence:  propose new model making generalizable diverse retrosynthetic reaction predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.8571428571428571, 'f': 0.1411764690768166}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.7142857142857143, 'f': 0.09803921440792003}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.8571428571428571, 'f': 0.1411764690768166}}\n",
            "pair:  humans remarkable ability correctly classify images despite possible degradation many studies suggested hallmark human vision results interaction feedforward signals bottom pathways visual cortex feedback signals provided top pathways motivated interaction propose new neuro inspired model namely convolutional neural networks feedback cnn cnn extends cnn feedback generative network combining bottom top inference perform approximate loopy belief propagation show cnn iterative inference allows disentanglement latent variables across layers validate advantages cnn baseline cnn experimental results suggest cnn robust image degradation pixel noise occlusion blur furthermore show cnn capable restoring original images degraded ones high reconstruction accuracy introducing negligible artifacts\n",
            "output sentence:  cnn extends cnn feedback generative network robust vision \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.8571428571428571, 'f': 0.11650485310208314}, 'rouge-2': {'r': 0.024193548387096774, 'p': 0.42857142857142855, 'f': 0.04580152570596121}, 'rouge-l': {'r': 0.0625, 'p': 0.8571428571428571, 'f': 0.11650485310208314}}\n",
            "pair:  deep neural networks dnns witnessed powerful approach year solving long standing artificial intelligence ai supervised unsupervised tasks exists natural language processing speech processing computer vision others paper attempt apply dnns three different cyber security use cases android malware classification incident detection fraud detection data set use case contains real known benign malicious activities samples use cases part cybersecurity data mining competition cdmc efficient network architecture dnns chosen conducting various trails experiments network parameters network structures experiments chosen efficient configurations dnns run epochs learning rate set range experiments dnns performed well comparison classical machine learning algorithm cases experiments cyber security use cases due fact dnns implicitly extract build better features identifies characteristics data lead better accuracy best accuracy obtained dnns xgboost android malware classification incident detection fraud detection respectively accuracy obtained dnns varies top scored system cdmc tasks\n",
            "output sentence:  deep net deep neural network cyber security use cases \n",
            "\n",
            "{'rouge-1': {'r': 0.1206896551724138, 'p': 0.5833333333333334, 'f': 0.1999999971591837}, 'rouge-2': {'r': 0.039473684210526314, 'p': 0.2727272727272727, 'f': 0.06896551503236895}, 'rouge-l': {'r': 0.08620689655172414, 'p': 0.4166666666666667, 'f': 0.14285714001632657}}\n",
            "pair:  recent studies demonstrated vulnerability deep convolutional neural networks adversarial examples inspired observation intrinsic dimension image data much smaller pixel space dimension vulnerability neural networks grows input dimension propose embed high dimensional input images low dimensional space perform classification however arbitrarily projecting input images low dimensional space without regularization improve robustness deep neural networks propose new framework embedding regularized classifier er classifier improves adversarial robustness classifier embedding regularization experimental results several benchmark datasets show proposed framework achieves state art performance strong adversarial attack methods\n",
            "output sentence:  general easy use framework improves adversarial robustness deep classification models embedding regularization \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.875, 'f': 0.19718309659194602}, 'rouge-2': {'r': 0.05063291139240506, 'p': 0.5714285714285714, 'f': 0.09302325431855057}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.75, 'f': 0.16901408250743902}}\n",
            "pair:  use imitation learning learn single policy complex task multiple modes hierarchical structure challenging fact previous work shown modes known learning separate policies mode sub task greatly improve performance imitation learning work discover interaction sub tasks resulting state action trajectory sequences using directed graphical model propose new algorithm based generative adversarial imitation learning framework automatically learns sub task policies unsegmented demonstrations approach maximizes directed information flow graphical model sub task latent variables generated trajectories also show approach connects existing options framework commonly used learn hierarchical policies\n",
            "output sentence:  learning hierarchical policies unsegmented demonstrations using directed information \n",
            "\n",
            "{'rouge-1': {'r': 0.058823529411764705, 'p': 0.45454545454545453, 'f': 0.10416666463758685}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03529411764705882, 'p': 0.2727272727272727, 'f': 0.06249999797092021}}\n",
            "pair:  recent deep generative models provide photo realistic images well visual textual content embeddings useful address various tasks computer vision natural language processing usefulness nevertheless often limited lack control generative process poor understanding learned representation overcome major issues recent works shown interest studying semantics latent space generative models paper propose advance interpretability latent space generative models introducing new method find meaningful directions latent space generative model along move control precisely specific properties generated image like position scale object image method weakly supervised particularly well suited search directions encoding simple transformations generated image translation zoom color variations demonstrate effectiveness method qualitatively quantitatively gans variational auto encoders\n",
            "output sentence:  model control generation images gan beta vae regard scale position objects \n",
            "\n",
            "{'rouge-1': {'r': 0.1320754716981132, 'p': 0.5384615384615384, 'f': 0.21212120895775943}, 'rouge-2': {'r': 0.046153846153846156, 'p': 0.25, 'f': 0.07792207529094292}, 'rouge-l': {'r': 0.11320754716981132, 'p': 0.46153846153846156, 'f': 0.18181817865472916}}\n",
            "pair:  show output residual cnn appropriate prior weights biases gp limit infinitely many convolutional filters extending similar results dense networks cnn equivalent kernel computed exactly unlike deep kernels parameters hyperparameters original cnn show kernel two properties allow computed efficiently cost evaluating kernel pair images similar single forward pass original cnn one filter per layer kernel equivalent layer resnet obtains classification error mnist new record gp comparable number parameters\n",
            "output sentence:  show cnns resnets appropriate priors parameters gaussian processes limit infinitely many convolutional filters \n",
            "\n",
            "{'rouge-1': {'r': 0.1188118811881188, 'p': 0.75, 'f': 0.20512820276718532}, 'rouge-2': {'r': 0.024, 'p': 0.2, 'f': 0.04285714094387764}, 'rouge-l': {'r': 0.039603960396039604, 'p': 0.25, 'f': 0.06837606601504867}}\n",
            "pair:  open domain question answering qa important problem ai nlp emerging bellwether progress generalizability ai methods techniques much progress open domain qa systems realized advances information retrieval methods corpus construction paper focus recently introduced arc challenge dataset contains multiple choice questions authored grade school science exams questions selected challenging current qa systems current state art performance slightly better random chance present system reformulates given question queries used retrieve supporting text large corpus science related text rewriter able incorporate background knowledge conceptnet tandem generic textual entailment system trained scitail identifies support retrieved results outperforms several strong baselines end end qa task despite trained identify essential terms original source question use generalizable decision methodology retrieved evidence answer candidates select best answer combining query reformulation background knowledge textual entailment system able outperform several strong baselines arc dataset\n",
            "output sentence:  explore using background knowledge query reformulation help retrieve better supporting evidence answering multiple choice science questions \n",
            "\n",
            "{'rouge-1': {'r': 0.05333333333333334, 'p': 1.0, 'f': 0.10126582182342576}, 'rouge-2': {'r': 0.010638297872340425, 'p': 0.25, 'f': 0.02040816248229907}, 'rouge-l': {'r': 0.05333333333333334, 'p': 1.0, 'f': 0.10126582182342576}}\n",
            "pair:  reparameterization trick become one useful tools field variational inference however reparameterization trick based standardization transformation restricts scope application method distributions tractable inverse cumulative distribution functions expressible deterministic transformations distributions paper generalized reparameterization trick allowing general transformation unlike similar works develop generalized transformation based gradient model formally rigorously discover proposed model special case control variate indicating proposed model combine advantages cv generalized reparameterization based proposed gradient model propose new polynomial based gradient estimator better theoretical performance reparameterization trick certain condition applied larger class variational distributions studies synthetic real data show proposed gradient estimator significantly lower gradient variance state art methods thus enabling faster inference procedure\n",
            "output sentence:  generalized transformation generalized transformation model variational model \n",
            "\n",
            "{'rouge-1': {'r': 0.07865168539325842, 'p': 0.7, 'f': 0.14141413959800023}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.4, 'f': 0.06666666513888891}, 'rouge-l': {'r': 0.056179775280898875, 'p': 0.5, 'f': 0.10101009919395981}}\n",
            "pair:  graph neural networks recently achieved great successes predicting quantum mechanical properties molecules models represent molecule graph using distance atoms nodes spatial direction one atom another however directional information plays central role empirical potentials molecules angular potentials alleviate limitation propose directional message passing embed messages passed atoms instead atoms message associated direction coordinate space directional message embeddings rotationally equivariant since associated directions rotate molecule propose message passing scheme analogous belief propagation uses directional information transforming messages based angle additionally use spherical bessel functions construct theoretically well founded orthogonal radial basis achieves better performance currently prevalent gaussian radial basis functions using fewer parameters leverage innovations construct directional message passing neural network dimenet dimenet outperforms previous gnns average md qm\n",
            "output sentence:  directional message passing incorporates spatial directional information improve graph neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.24193548387096775, 'p': 0.8333333333333334, 'f': 0.3749999965125001}, 'rouge-2': {'r': 0.0958904109589041, 'p': 0.4117647058823529, 'f': 0.15555555249135805}, 'rouge-l': {'r': 0.1774193548387097, 'p': 0.6111111111111112, 'f': 0.2749999965125}}\n",
            "pair:  work adopts successful distributional perspective reinforcement learning adapts continuous control setting combine within distributed framework policy learning order develop call distributed distributional deep deterministic policy gradient algorithm pg also combine technique number additional simple improvements use step returns prioritized experience replay experimentally examine contribution individual components show interact well combined contributions results show across wide variety simple control tasks difficult manipulation tasks set hard obstacle based locomotion tasks pg algorithm achieves state art performance\n",
            "output sentence:  develop agent call distributional deterministic deep policy gradient algorithm achieves state art performance number challenging continuous control problems \n",
            "\n",
            "{'rouge-1': {'r': 0.3269230769230769, 'p': 1.0, 'f': 0.4927536194749003}, 'rouge-2': {'r': 0.2, 'p': 0.6666666666666666, 'f': 0.3076923041420118}, 'rouge-l': {'r': 0.25, 'p': 0.7647058823529411, 'f': 0.37681159048939306}}\n",
            "pair:  knowledge graphs structured representations real world facts however typically contain small subset possible facts link prediction task inferring missing facts based existing ones propose tucker relatively simple yet powerful linear model based tucker decomposition binary tensor representation knowledge graph triples using particular decomposition parameters shared relations enabling multi task learning tucker outperforms previous state art models across several standard link prediction datasets\n",
            "output sentence:  propose tucker relatively simple powerful linear model link prediction knowledge graphs based tucker decomposition binary tensor representation triples knowledge graph \n",
            "\n",
            "{'rouge-1': {'r': 0.175, 'p': 0.7368421052631579, 'f': 0.28282827972655855}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.3888888888888889, 'f': 0.11382113571286938}, 'rouge-l': {'r': 0.1125, 'p': 0.47368421052631576, 'f': 0.18181817871645756}}\n",
            "pair:  non autoregressive machine translation nat systems predict sequence output tokens parallel achieving substantial improvements generation speed compared autoregressive models existing nat models usually rely technique knowledge distillation creates training data pretrained autoregressive model better performance knowledge distillation empirically useful leading large gains accuracy nat models reason success yet unclear paper first design systematic experiments investigate knowledge distillation crucial nat training find knowledge distillation reduce complexity data sets help nat model variations output data furthermore strong correlation observed capacity nat model optimal complexity distilled data best translation quality based findings propose several approaches alter complexity data sets improve performance nat models achieve state art performance nat based models close gap autoregressive baseline wmt en de benchmark\n",
            "output sentence:  systematically examine knowledge distillation crucial training non autoregressive translation nat models propose methods improve distilled data best match capacity nat \n",
            "\n",
            "{'rouge-1': {'r': 0.10869565217391304, 'p': 0.29411764705882354, 'f': 0.15873015478961966}, 'rouge-2': {'r': 0.05454545454545454, 'p': 0.16666666666666666, 'f': 0.08219177710639912}, 'rouge-l': {'r': 0.10869565217391304, 'p': 0.29411764705882354, 'f': 0.15873015478961966}}\n",
            "pair:  paper presents gumbelclip set modifications actor critic algorithm policy reinforcement learning gumbelclip uses concepts truncated importance sampling along additive noise produce loss function enabling use policy samples modified algorithm achieves increase convergence speed sample efficiency compared policy algorithms competitive existing policy policy gradient methods significantly simpler implement effectiveness gumbelclip demonstrated existing policy policy actor critic algorithms subset atari domain\n",
            "output sentence:  set modifications loc get policy actor critic outperforms performs similarly acer modifications large batchsizes aggressive clamping policy forcing gumbel \n",
            "\n",
            "{'rouge-1': {'r': 0.2028985507246377, 'p': 0.9333333333333333, 'f': 0.3333333303996599}, 'rouge-2': {'r': 0.15, 'p': 0.8571428571428571, 'f': 0.2553191464010865}, 'rouge-l': {'r': 0.18840579710144928, 'p': 0.8666666666666667, 'f': 0.3095238065901361}}\n",
            "pair:  recent work shown contextualized word representations derived neural machine translation nmt viable alternative simple word predictions tasks internal understanding needs built order able translate one language another much comprehensive unfortunately computational memory limitations present prevent nmt models using large word vocabularies thus alternatives subword units bpe morphological segmentations characters used study impact using different kinds units quality resulting representations used model syntax semantics morphology found representations derived subwords slightly better modeling syntax character based representations superior modeling morphology also robust noisy input\n",
            "output sentence:  study impact using different kinds subword units quality resulting representations used model syntax semantics morphology \n",
            "\n",
            "{'rouge-1': {'r': 0.12790697674418605, 'p': 0.6875, 'f': 0.21568627186466746}, 'rouge-2': {'r': 0.0673076923076923, 'p': 0.4375, 'f': 0.11666666435555559}, 'rouge-l': {'r': 0.11627906976744186, 'p': 0.625, 'f': 0.19607842872741257}}\n",
            "pair:  learning learn meta learning leverages data driven inductive bias increase efficiency learning novel task approach encounters difficulty transfer mutually beneficial instance tasks sufficiently dissimilar change time use connection gradient based meta learning hierarchical bayes propose mixture hierarchical bayesian models parameters arbitrary function approximator neural network generalizing model agnostic meta learning maml algorithm present stochastic expectation maximization procedure jointly estimate parameter initializations gradient descent well latent assignment tasks initializations approach better captures diversity training tasks opposed consolidating inductive biases single set hyperparameters experiments demonstrate better generalization standard miniimagenet benchmark shot classification derive novel scalable non parametric variant method captures evolution task distribution time demonstrated set shot regression tasks\n",
            "output sentence:  use connection gradient based meta learning hierarchical bayes learn mixture meta learners appropriate heterogeneous evolving task distribution \n",
            "\n",
            "{'rouge-1': {'r': 0.09859154929577464, 'p': 1.0, 'f': 0.17948717785338592}, 'rouge-2': {'r': 0.0449438202247191, 'p': 0.6666666666666666, 'f': 0.08421052513240998}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.8571428571428571, 'f': 0.15384615221236028}}\n",
            "pair:  paper present neural phrase based machine translation npmt method explicitly models phrase structures output sequences using sleep wake networks swan recently proposed segmentation based sequence modeling method mitigate monotonic alignment requirement swan introduce new layer perform soft local reordering input sequences different existing neural machine translation nmt approaches npmt use attention based decoding mechanisms instead directly outputs phrases sequential order decode linear time experiments show npmt achieves superior performances iwslt german english english german iwslt english vietnamese machine translation tasks compared strong nmt baselines also observe method produces meaningful phrases output languages\n",
            "output sentence:  neural phrase based machine translation linear decoding time \n",
            "\n",
            "{'rouge-1': {'r': 0.14102564102564102, 'p': 0.9166666666666666, 'f': 0.24444444213333333}, 'rouge-2': {'r': 0.10576923076923077, 'p': 0.7857142857142857, 'f': 0.18644067587474866}, 'rouge-l': {'r': 0.1282051282051282, 'p': 0.8333333333333334, 'f': 0.2222222199111111}}\n",
            "pair:  tasks could come varying number instances classes realistic settings existing meta learning approaches shot classification assume number instances per task class fixed due restriction learn equally utilize meta knowledge across tasks even number instances per task class largely varies moreover consider distributional difference unseen tasks meta knowledge may less usefulness depending task relatedness overcome limitations propose novel meta learning model adaptively balances effect meta learning task specific learning within task learning balancing variables decide whether obtain solution relying meta knowledge task specific learning formulate objective bayesian inference framework tackle using variational inference validate bayesian task adaptive meta learning bayesian taml two realistic task class imbalanced datasets significantly outperforms existing meta learning approaches ablation study confirms effectiveness balancing component bayesian learning framework\n",
            "output sentence:  novel meta learning model adaptively balances effect meta learning task specific learning also class specific learning within task \n",
            "\n",
            "{'rouge-1': {'r': 0.05357142857142857, 'p': 0.6666666666666666, 'f': 0.09917355234205315}, 'rouge-2': {'r': 0.016260162601626018, 'p': 0.25, 'f': 0.0305343499982519}, 'rouge-l': {'r': 0.044642857142857144, 'p': 0.5555555555555556, 'f': 0.08264462672221845}}\n",
            "pair:  field medical diagnostics contains wealth challenges closely resemble classical machine learning problems practical constraints however complicate translation endpoints naively classical architectures many tasks radiology example largely problems multi label classification wherein medical images interpreted indicate multiple present suspected pathologies clinical settings drive necessity high accuracy simultaneously across multitude pathological outcomes greatly limit utility tools consider subset issue exacerbated general scarcity training data maximizes need extract clinically relevant features available samples ideally without use pre trained models may carry forward undesirable biases tangentially related tasks present evaluate partial solution constraints using lstms leverage interdependencies among target labels predicting pathologic patterns chest rays establish state art results largest publicly available chest ray dataset nih without pre training furthermore propose discuss alternative evaluation metrics relevance clinical practice\n",
            "output sentence:  present state art results using neural networks diagnose chest rays \n",
            "\n",
            "{'rouge-1': {'r': 0.04225352112676056, 'p': 0.6, 'f': 0.07894736719182825}, 'rouge-2': {'r': 0.011235955056179775, 'p': 0.25, 'f': 0.021505375520869494}, 'rouge-l': {'r': 0.04225352112676056, 'p': 0.6, 'f': 0.07894736719182825}}\n",
            "pair:  paper explores simplicity learned neural networks various settings learned real vs random data varying size architecture using large minibatch size vs small minibatch size notion simplicity used learnability accurately prediction function neural network learned labeled samples learnability different fact often higher test accuracy results herein suggest strong correlation small generalization errors high learnability work also shows exist significant qualitative differences shallow networks compared popular deep networks broadly paper extends new direction previous work understanding properties learned neural networks hope empirical study understanding learned neural networks might shed light right assumptions made theoretical study deep learning\n",
            "output sentence:  exploring learnability learned neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.04672897196261682, 'p': 0.45454545454545453, 'f': 0.08474576102125829}, 'rouge-2': {'r': 0.008130081300813009, 'p': 0.1, 'f': 0.015037592594267754}, 'rouge-l': {'r': 0.028037383177570093, 'p': 0.2727272727272727, 'f': 0.050847455936512555}}\n",
            "pair:  choice tuning optimizer affects speed ultimately performance deep learning significant past recent research area yet perhaps surprisingly generally agreed upon protocol quantitative reproducible evaluation optimization strategies deep learning suggest routines benchmarks stochastic optimization special focus unique aspects deep learning stochasticity tunability generalization primary contribution present deepobs python package deep learning optimization benchmarks package addresses key challenges quantitative assessment stochastic optimizers automates steps benchmarking library includes wide extensible set ready use realistic optimization problems training residual networks image classification imagenet character level language prediction models well popular classics like mnist cifar package also provides realistic baseline results popular optimizers test problems ensuring fair comparison competition benchmarking new optimizers without run costly experiments comes output back ends directly produce latex code inclusion academic publications supports tensorflow available open source\n",
            "output sentence:  provide software package drastically simplifies automates improves evaluation deep learning optimizers \n",
            "\n",
            "{'rouge-1': {'r': 0.12, 'p': 0.631578947368421, 'f': 0.20168066958548125}, 'rouge-2': {'r': 0.030534351145038167, 'p': 0.21052631578947367, 'f': 0.053333331120888984}, 'rouge-l': {'r': 0.09, 'p': 0.47368421052631576, 'f': 0.15126050151825438}}\n",
            "pair:  tremendous success deep neural networks motivated need better understand fundamental properties networks many theoretical results proposed shallow networks paper study important primitive understanding meaningful input space deep network span recovery let mathbf mathbb times innermost weight matrix arbitrary feed forward neural network mathbb mathbb written sigma mathbf network sigma mathbb mathbb goal recover row span mathbf given oracle access value show multi layered network relu activation functions partial recovery possible namely provably recover linearly independent vectors row span mathbf using poly non adaptive queries furthermore differentiable activation functions demonstrate textit full span recovery possible even output first passed sign thresholding function case algorithm adaptive empirically confirm full span recovery always possible unrealistically thin layers reasonably wide networks obtain full span recovery random networks networks trained mnist data furthermore demonstrate utility span recovery attack inducing neural networks misclassify data obfuscated controlled random noise sensical inputs\n",
            "output sentence:  provably recover span deep multi layered neural network latent structure empirically apply efficient span recovery algorithms attack networks obfuscating inputs \n",
            "\n",
            "{'rouge-1': {'r': 0.10638297872340426, 'p': 0.4166666666666667, 'f': 0.16949152218328073}, 'rouge-2': {'r': 0.017241379310344827, 'p': 0.09090909090909091, 'f': 0.028985504566267837}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.3333333333333333, 'f': 0.13559321709853497}}\n",
            "pair:  paper investigate learning deep neural networks automated optical inspection industrial manufacturing preliminary result shown stunning performance improvement transfer learning completely dissimilar source domain imagenet study demystifying improvement shows transfer learning produces highly compressible network case network learned scratch experimental result shows negligible accuracy drop network learned transfer learning compressed reduction number convolution lters result contrary compression without transfer learning loses accuracy compression rate\n",
            "output sentence:  experimentally show transfer learning makes sparse features network thereby produces compressible network \n",
            "\n",
            "{'rouge-1': {'r': 0.11428571428571428, 'p': 0.7272727272727273, 'f': 0.1975308618503277}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.1, 'f': 0.02197802002173668}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.45454545454545453, 'f': 0.12345678777625364}}\n",
            "pair:  introduce convolutional conditional neural process convcnp new member neural process family models translation equivariance data translation equivariance important inductive bias many learning problems including time series modelling spatial data images model embeds data sets infinite dimensional function space opposed finite dimensional vector spaces formalize notion extend theory neural representations sets include functional representations demonstrate translation equivariant embedding represented using convolutional deep set evaluate convcnps several settings demonstrating achieve state art performance compared existing nps demonstrate building translation equivariance enables zero shot generalization challenging domain tasks\n",
            "output sentence:  extend deep sets functional embeddings neural processes include translation equivariant members \n",
            "\n",
            "{'rouge-1': {'r': 0.06349206349206349, 'p': 0.3333333333333333, 'f': 0.10666666397866674}, 'rouge-2': {'r': 0.0136986301369863, 'p': 0.07692307692307693, 'f': 0.023255811387236626}, 'rouge-l': {'r': 0.047619047619047616, 'p': 0.25, 'f': 0.07999999731200008}}\n",
            "pair:  modern deep neural networks achieve high accuracy training distribution test distribution identically distributed assumption frequently violated practice train test distributions mismatched accuracy plummet currently techniques improve robustness unforeseen data shifts encountered deployment work propose technique improve robustness uncertainty estimates image classifiers propose augmix data processing technique simple implement adds limited computational overhead helps models withstand unforeseen corruptions augmix significantly improves robustness uncertainty measures challenging image classification benchmarks closing gap previous methods best possible performance cases half\n",
            "output sentence:  obtain state art robustness data shifts maintain calibration data shift even though even accuracy drops \n",
            "\n",
            "{'rouge-1': {'r': 0.08045977011494253, 'p': 0.875, 'f': 0.14736841951024932}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.5714285714285714, 'f': 0.07079645901480149}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.75, 'f': 0.12631578793130194}}\n",
            "pair:  worst case training principle minimizes maximal adversarial loss also known adversarial training shown state art approach enhancing adversarial robustness norm ball bounded input perturbations nonetheless min max optimization beyond purpose rigorously explored research adversarial attack defense particular given set risk sources domains minimizing maximal loss induced domain set reformulated general min max problem different examples general formulation include attacking model ensembles devising universal perturbation multiple inputs data transformations generalized different types attack models show problems solved unified theoretically principled min max optimization framework also show self adjusted domain weights learned method provides means explain difficulty level attack defense multiple domains extensive experiments show approach leads substantial performance improvement conventional averaging strategy\n",
            "output sentence:  unified min max optimization framework adversarial attack defense \n",
            "\n",
            "{'rouge-1': {'r': 0.10204081632653061, 'p': 0.9090909090909091, 'f': 0.18348623671744804}, 'rouge-2': {'r': 0.04065040650406504, 'p': 0.5, 'f': 0.07518796853411727}, 'rouge-l': {'r': 0.04081632653061224, 'p': 0.36363636363636365, 'f': 0.07339449359818201}}\n",
            "pair:  ai systems garner widespread public acceptance must develop methods capable explaining decisions black box models neural networks work identify two issues current explanatory methods first show two prevalent perspectives explanations feature additivity feature selection lead fundamentally different instance wise explanations literature explainers different perspectives currently directly compared despite distinct explanation goals second issue current post hoc explainers thoroughly validated simple models linear regression applied real world neural networks explainers commonly evaluated assumption learned models behave reasonably however neural networks often rely unreasonable correlations even producing correct decisions introduce verification framework explanatory methods feature selection perspective framework based non trivial neural network architecture trained real world task able provide guarantees inner workings validate efficacy evaluation showing failure modes current explainers aim framework provide publicly available shelf evaluation feature selection perspective explanations needed\n",
            "output sentence:  evaluation framework based real world neural network post hoc explanatory methods \n",
            "\n",
            "{'rouge-1': {'r': 0.021739130434782608, 'p': 0.15384615384615385, 'f': 0.03809523592562371}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.021739130434782608, 'p': 0.15384615384615385, 'f': 0.03809523592562371}}\n",
            "pair:  hypothesize end end neural image captioning systems work seemingly well exploit learn distributional similarity multimodal feature space mapping test image similar training images space generating caption space validate hypothesis focus image side image captioning vary input image representation keep rnn text generation model cnn rnn constant propose sparse bag objects vector interpretable representation investigate distributional similarity hypothesis found image captioning models capable separating structure noisy input representations ii experience virtually significant performance loss high dimensional representation compressed lower dimensional space iii cluster images similar visual linguistic information together iv heavily reliant test sets similar distribution training set repeatedly generate captions matching images retrieving caption joint visual textual space experiments point one fact distributional similarity hypothesis holds conclude regardless image representation image captioning systems seem match images generate captions learned joint image text semantic subspace\n",
            "output sentence:  paper presents empirical analysis role different types image representations probes properties representations task image captioning \n",
            "\n",
            "{'rouge-1': {'r': 0.049019607843137254, 'p': 0.5, 'f': 0.08928571265943878}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.029411764705882353, 'p': 0.3, 'f': 0.053571426945153114}}\n",
            "pair:  knowledge bases kb automatically manually constructed often incomplete many valid facts inferred kb synthesizing existing information popular approach kb completion infer new relations combinatory reasoning information found along paths connecting pair entities given enormous size kbs exponential number paths previous path based models considered problem predicting missing relation given two entities evaluating truth proposed triple additionally methods traditionally used random paths fixed entity pairs recently learned pick paths propose new algorithm minerva addresses much difficult practical task answering questions relation known one entity since random walks impractical setting unknown destination combinatorially many paths start node present neural reinforcement learning approach learns navigate graph conditioned input query find predictive paths comprehensive evaluation seven knowledge base datasets found minerva competitive many current state art methods\n",
            "output sentence:  present rl agent minerva learns walk knowledge graph answer queries \n",
            "\n",
            "{'rouge-1': {'r': 0.12643678160919541, 'p': 0.8461538461538461, 'f': 0.21999999773800002}, 'rouge-2': {'r': 0.06363636363636363, 'p': 0.5833333333333334, 'f': 0.114754096586939}, 'rouge-l': {'r': 0.11494252873563218, 'p': 0.7692307692307693, 'f': 0.199999997738}}\n",
            "pair:  central capability intelligent systems ability continuously build upon previous experiences speed enhance learning new tasks two distinct research paradigms studied question meta learning views problem learning prior model parameters amenable fast adaptation new task typically assumes set tasks available together batch contrast online regret based learning considers sequential setting problems revealed one conventionally train single model without task specific adaptation work introduces online meta learning setting merges ideas aforementioned paradigms better capture spirit practice continual lifelong learning propose follow meta leader ftml algorithm extends maml algorithm setting theoretically work provides logt regret guarantee ftml algorithm experimental evaluation three different large scale tasks suggest proposed algorithm significantly outperforms alternatives based traditional online learning approaches\n",
            "output sentence:  introduce online meta learning problem setting better capture spirit practice continual lifelong learning \n",
            "\n",
            "{'rouge-1': {'r': 0.1, 'p': 0.6666666666666666, 'f': 0.1739130412098299}, 'rouge-2': {'r': 0.02912621359223301, 'p': 0.23076923076923078, 'f': 0.05172413594084432}, 'rouge-l': {'r': 0.05, 'p': 0.3333333333333333, 'f': 0.0869565194706995}}\n",
            "pair:  referential games offer grounded learning environment neural agents accounts fact language functionally used communicate however take account second constraint considered fundamental shape human language must learnable new language learners thus overcome transmission bottleneck work insert bottleneck referential game introducing changing population agents new agents learn playing experienced agents show mere cultural transmission results substantial improvement language efficiency communicative success measured convergence speed degree structure emerged languages within population consistency language however core contribution show optimal situation co evolve language agents allow agent population evolve genotypical evolution achieve across board improvements considered metrics results stress language emergence studies cultural evolution important also suitability architecture considered\n",
            "output sentence:  enable cultural evolution language genetic evolution agents referential game using new language transmission engine \n",
            "\n",
            "{'rouge-1': {'r': 0.056818181818181816, 'p': 0.625, 'f': 0.1041666651388889}, 'rouge-2': {'r': 0.03418803418803419, 'p': 0.5714285714285714, 'f': 0.06451612796696152}, 'rouge-l': {'r': 0.056818181818181816, 'p': 0.625, 'f': 0.1041666651388889}}\n",
            "pair:  weight initialization activation function deep neural networks crucial impact performance training procedure inappropriate selection lead loss information input forward propagation exponential vanishing exploding gradients back propagation understanding theoretical properties untrained random networks key identifying deep networks may trained successfully recently demonstrated schoenholz et al showed deep feedforward neural networks specific choice hyperparameters known edge chaos lead good performance complete analysis providing quantitative results showing class relu like activation functions information propagates indeed deeper initialization edge chaos extending analysis identify class activation functions improve information propagation relu like functions class includes swish activation phi swish cdot text sigmoid used hendrycks gimpel elfwing et al ramachandran et al provides theoretical grounding excellent empirical performance phi swish observed contributions complement previous results illustrating benefit using random initialization edge chaos context\n",
            "output sentence:  effectively choose initialization activation function deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.07407407407407407, 'p': 0.8, 'f': 0.1355932187877047}, 'rouge-2': {'r': 0.02459016393442623, 'p': 0.3333333333333333, 'f': 0.04580152543791158}, 'rouge-l': {'r': 0.06481481481481481, 'p': 0.7, 'f': 0.11864406624533182}}\n",
            "pair:  batch normalization bn often used attempt stabilize accelerate training deep neural networks many cases indeed decreases number parameter updates required achieve low training error however also reduces robustness small adversarial input perturbations common corruptions double digit percentages show five standard datasets furthermore find substituting weight decay bn sufficient nullify relationship adversarial vulnerability input dimension recent mean field analysis found bn induces gradient explosion used multiple layers cannot fully explain vulnerability observe given occurs already single bn layer argue actual cause tilting decision boundary respect nearest centroid classifier along input dimensions low variance result constant introduced numerical stability bn step acts important hyperparameter tuned recover robustness cost standard test accuracy explain mechanism explicitly linear toy model show experiments still holds nonlinear real world models\n",
            "output sentence:  batch normalization reduces robustness test time common corruptions adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.1506849315068493, 'p': 0.9166666666666666, 'f': 0.2588235269868512}, 'rouge-2': {'r': 0.10989010989010989, 'p': 0.9090909090909091, 'f': 0.19607842944828915}, 'rouge-l': {'r': 0.1506849315068493, 'p': 0.9166666666666666, 'f': 0.2588235269868512}}\n",
            "pair:  introduce systematic framework quantifying robustness classifiers naturally occurring perturbations images found videos part framework construct imagenet vid robust human expert reviewed dataset images grouped sets perceptually similar images derived frames imagenet video object detection dataset evaluate diverse array classifiers trained imagenet including models trained robustness show median classification accuracy drop additionally evaluate faster cnn fcn models detection show natural perturbations induce classification well localization errors leading median drop detection map points analysis shows natural perturbations real world heavily problematic current cnns posing significant challenge deployment safety critical environments require reliable low latency predictions\n",
            "output sentence:  introduce systematic framework quantifying robustness classifiers naturally occurring perturbations images found videos \n",
            "\n",
            "{'rouge-1': {'r': 0.1780821917808219, 'p': 0.8125, 'f': 0.29213482851155154}, 'rouge-2': {'r': 0.08139534883720931, 'p': 0.4666666666666667, 'f': 0.13861385885697486}, 'rouge-l': {'r': 0.1095890410958904, 'p': 0.5, 'f': 0.17977527794975384}}\n",
            "pair:  traditional models question answering optimize using cross entropy loss encourages exact answers cost penalizing nearby overlapping answers sometimes equally accurate propose mixed objective combines cross entropy loss self critical policy learning using rewards derived word overlap solve misalignment evaluation metric optimization objective addition mixed objective introduce deep residual coattention encoder inspired recent work deep self attention residual networks proposals improve model performance across question types input lengths especially long questions requires ability capture long term dependencies stanford question answering dataset model achieves state art results exact match accuracy ensemble obtains exact match accuracy\n",
            "output sentence:  introduce dcn deep residual coattention mixed objective rl achieves state art performance stanford question answering dataset \n",
            "\n",
            "{'rouge-1': {'r': 0.05154639175257732, 'p': 0.45454545454545453, 'f': 0.09259259076303157}, 'rouge-2': {'r': 0.00847457627118644, 'p': 0.1, 'f': 0.015624998559570447}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.36363636363636365, 'f': 0.07407407224451307}}\n",
            "pair:  last decade two competing control strategies emerged solving complex control tasks high efficacy model based control algorithms model predictive control mpc trajectory optimization peer gradients underlying system dynamics order solve control tasks high sample efficiency however like gradient based numerical optimization methods model based control methods sensitive intializations prone becoming trapped local minima deep reinforcement learning drl hand somewhat alleviate issues exploring solution space sampling expense computational cost paper present hybrid method combines best aspects gradient based methods drl base algorithm deep deterministic policy gradients ddpg algorithm propose simple modification uses true gradients differentiable physical simulator increase convergence rate actor critic demonstrate algorithm seven robot control tasks complex one differentiable half cheetah hard contact constraints empirical results show method boosts performance ddpgwithout sacrificing robustness local minima\n",
            "output sentence:  propose novel method leverages gradients differentiable simulators improve performance rl robotics \n",
            "\n",
            "{'rouge-1': {'r': 0.08620689655172414, 'p': 0.35714285714285715, 'f': 0.13888888575617292}, 'rouge-2': {'r': 0.014705882352941176, 'p': 0.07142857142857142, 'f': 0.024390241070791525}, 'rouge-l': {'r': 0.05172413793103448, 'p': 0.21428571428571427, 'f': 0.0833333302006174}}\n",
            "pair:  contrast fully connected networks convolutional neural networks cnns achieve efficiency learning weights associated local filters finite spatial extent implication filter may know looking positioned image information concerning absolute position inherently useful reasonable assume deep cnns may implicitly learn encode information means paper test hypothesis revealing surprising degree absolute position information encoded commonly used neural networks comprehensive set experiments show validity hypothesis shed light information represented offering clues positional information derived deep cnns\n",
            "output sentence:  work shows positional information implicitly encoded network information important detecting position dependent features semantic saliency \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.8571428571428571, 'f': 0.12631578810858723}, 'rouge-2': {'r': 0.043478260869565216, 'p': 0.8333333333333334, 'f': 0.082644627156615}, 'rouge-l': {'r': 0.06818181818181818, 'p': 0.8571428571428571, 'f': 0.12631578810858723}}\n",
            "pair:  predicting future real world settings particularly raw sensory observations images exceptionally challenging real world events stochastic unpredictable high dimensionality complexity natural images requires predictive model build intricate understanding natural world many existing methods tackle problem making simplifying assumptions environment one common assumption outcome deterministic one plausible future lead low quality predictions real world settings stochastic dynamics paper develop stochastic variational video prediction sv method predicts different possible future sample latent variables best knowledge model first provide effective stochastic multi frame prediction real world video demonstrate capability proposed method predicting detailed future frames videos multiple real world datasets action free action conditioned find proposed method produces substantially improved video predictions compared model without stochasticity stochastic video prediction methods sv implementation open sourced upon publication\n",
            "output sentence:  stochastic variational video prediction real world settings \n",
            "\n",
            "{'rouge-1': {'r': 0.109375, 'p': 0.6363636363636364, 'f': 0.18666666416355557}, 'rouge-2': {'r': 0.013888888888888888, 'p': 0.1, 'f': 0.024390241760856822}, 'rouge-l': {'r': 0.078125, 'p': 0.45454545454545453, 'f': 0.13333333083022222}}\n",
            "pair:  obtaining high quality uncertainty estimates essential many applications deep neural networks paper theoretically justify scheme estimating uncertainties based sampling prior distribution crucially uncertainty estimates shown conservative sense never underestimate posterior uncertainty obtained hypothetical bayesian algorithm also show concentration implying uncertainty estimates converge zero get data uncertainty estimates obtained random priors adapted deep network architecture trained using standard supervised learning pipelines provide experimental evaluation random priors calibration distribution detection typical computer vision tasks demonstrating outperform deep ensembles practice\n",
            "output sentence:  provide theoretical support uncertainty estimates deep learning obtained fitting random priors \n",
            "\n",
            "{'rouge-1': {'r': 0.07865168539325842, 'p': 0.6363636363636364, 'f': 0.139999998042}, 'rouge-2': {'r': 0.025423728813559324, 'p': 0.3, 'f': 0.04687499855957036}, 'rouge-l': {'r': 0.0449438202247191, 'p': 0.36363636363636365, 'f': 0.07999999804200005}}\n",
            "pair:  designing accurate efficient convolutional neural architectures vast amount hardware challenging hardware designs complex diverse paper addresses hardware diversity challenge neural architecture search nas unlike previous approaches apply search algorithms small human designed search space without considering hardware diversity propose hurricane explores automatic hardware aware search much larger search space multistep search scheme coordinate ascent framework generate tailored models different types hardware extensive experiments imagenet show algorithm consistently achieves much lower inference latency similar better accuracy state art nas methods three types hardware remarkably hurricane achieves top accuracy imagenet inference latency ms dsp higher accuracy inference speedup fbnet iphonex vpu hurricane achieves higher top accuracy proxyless mobile speedup even well studied mobile cpu hurricane achieves higher top accuracy fbnet iphonex comparable inference latency hurricane also reduces training time average compared singlepath oneshot\n",
            "output sentence:  propose hurricane address challenge hardware diversity one shot neural architecture search \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.6666666666666666, 'f': 0.1481481461728395}, 'rouge-2': {'r': 0.05747126436781609, 'p': 0.5555555555555556, 'f': 0.10416666496744793}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.6666666666666666, 'f': 0.1481481461728395}}\n",
            "pair:  human computer conversation systems attracted much attention natural language processing conversation systems roughly divided two categories retrieval based generation based systems retrieval systems search user issued utterance namely query large conversational repository return reply best matches query generative approaches synthesize new replies ways certain advantages suffer disadvantages propose novel ensemble retrieval based generation based conversation system retrieved candidates addition original query fed reply generator via neural network model aware information generated reply together retrieved ones participates ranking process find final reply output experimental results show ensemble system outperforms single module large margin\n",
            "output sentence:  novel ensemble retrieval based generation based open domain conversation systems \n",
            "\n",
            "{'rouge-1': {'r': 0.24193548387096775, 'p': 0.9375, 'f': 0.3846153813543722}, 'rouge-2': {'r': 0.17721518987341772, 'p': 0.9333333333333333, 'f': 0.29787233774332283}, 'rouge-l': {'r': 0.24193548387096775, 'p': 0.9375, 'f': 0.3846153813543722}}\n",
            "pair:  intrinsic rewards reinforcement learning provide powerful algorithmic capability agents learn interact environment task generic way however increased incentives motivation come cost increased fragility stochasticity introduce method computing intrinsic reward curiosity using metrics derived sampling latent variable model used estimate dynamics ultimately estimate conditional probability observed states used intrinsic reward curiosity experiments video game agent uses model autonomously learn play atari games using curiosity reward combination extrinsic rewards game achieve improved performance games sparse extrinsic rewards stochasticity introduced environment method still demonstrates improved performance baseline\n",
            "output sentence:  introduce method computing intrinsic reward curiosity using metrics derived sampling latent variable model used estimate dynamics \n",
            "\n",
            "{'rouge-1': {'r': 0.12903225806451613, 'p': 0.6153846153846154, 'f': 0.21333333046755557}, 'rouge-2': {'r': 0.04938271604938271, 'p': 0.3333333333333333, 'f': 0.08602150312868545}, 'rouge-l': {'r': 0.11290322580645161, 'p': 0.5384615384615384, 'f': 0.18666666380088892}}\n",
            "pair:  unsupervised representation learning holds promise exploiting large amount available unlabeled data learn general representations promising technique unsupervised learning framework variational auto encoders vaes however unsupervised representations learned vaes significantly outperformed learned supervising recognition hypothesis learn useful representations recognition model needs encouraged learn repeating consistent patterns data drawing inspiration mid level representation discovery work propose patchvae reasons images patch level key contribution bottleneck formulation vae framework encourages mid level style representations experiments demonstrate representations learned method perform much better recognition tasks compared learned vanilla vaes\n",
            "output sentence:  patch based bottleneck formulation vae framework learns unsupervised representations better suited visual recognition \n",
            "\n",
            "{'rouge-1': {'r': 0.07777777777777778, 'p': 0.4666666666666667, 'f': 0.13333333088435376}, 'rouge-2': {'r': 0.04672897196261682, 'p': 0.3333333333333333, 'f': 0.08196721095807584}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.4, 'f': 0.11428571183673475}}\n",
            "pair:  capturing long range feature relations central issue convolutional neural networks cnns tackle attempts integrate end end trainable attention module cnns widespread main goal works adjust feature maps considering spatial channel correlation inside convolution layer paper focus modeling relationships among layers propose novel structure recurrent layer attention network stores hierarchy features recurrent neural networks rnns concurrently propagating cnn adaptively scales feature volumes layers introduce several structural derivatives demonstrating compatibility recent attention modules expandability proposed network semantic understanding learned features also visualize intermediate layers plot curve layer scaling coefficients layer attention recurrent layer attention network achieves significant performance enhancement requiring slight increase parameters image classification task cifar imagenet dataset object detection task microsoft coco dataset\n",
            "output sentence:  propose new type end end trainable attention module applies global weight balances among layers utilizing co rnn \n",
            "\n",
            "{'rouge-1': {'r': 0.12048192771084337, 'p': 0.5555555555555556, 'f': 0.19801979905107345}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.2222222222222222, 'f': 0.06557376797635056}, 'rouge-l': {'r': 0.08433734939759036, 'p': 0.3888888888888889, 'f': 0.13861385845701407}}\n",
            "pair:  intrinsically motivated goal exploration algorithms enable machines discover repertoires policies produce diversity effects complex environments exploration algorithms shown allow real world robots acquire skills tool use high dimensional continuous state action spaces however far assumed self generated goals sampled specifically engineered feature space limiting autonomy work propose approach using deep representation learning algorithms learn adequate goal space developmental stage approach first perceptual learning stage deep learning algorithms use passive raw sensor observations world changes learn corresponding latent space goal exploration happens second stage sampling goals latent space present experiments simulated robot arm interacting object show exploration algorithms using learned representations closely match even sometimes improve performance obtained using engineered representations\n",
            "output sentence:  propose novel intrinsically motivated goal exploration architecture unsupervised learning goal space representations evaluate various implementations enable discovery diversity policies \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.6, 'f': 0.11320754546101817}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.2222222222222222, 'f': 0.03305784986271435}, 'rouge-l': {'r': 0.0625, 'p': 0.6, 'f': 0.11320754546101817}}\n",
            "pair:  fundamental challenging train robust accurate deep neural networks dnns semantically abnormal examples exist although great progress made still one crucial research question thoroughly explored yet training examples focused much emphasised achieve robust learning work study question propose gradient rescaling gr solve gr modifies magnitude logit vector gradient emphasise relatively easier training data points noise becomes severe functions explicit emphasis regularisation improve generalisation performance dnns apart regularisation connect gr examples weighting designing robust loss functions empirically demonstrate gr highly anomaly robust outperforms state art large margin increasing cifar noisy labels also significantly superior standard regularisers clean abnormal settings furthermore present comprehensive ablation studies explore behaviours gr different cases informative applying gr real world scenarios\n",
            "output sentence:  robust discriminative representation learning via gradient rescaling emphasis regularisation perspective \n",
            "\n",
            "{'rouge-1': {'r': 0.0847457627118644, 'p': 0.5, 'f': 0.14492753375341316}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.3, 'f': 0.0869565192606596}}\n",
            "pair:  similar humans animals deep artificial neural networks exhibit critical periods temporary stimulus deficit impair development skill extent impairment depends onset length deficit window animal models size neural network deficits affect low level statistics vertical flipping images lasting effect performance overcome training better understand phenomenon use fisher information weights measure effective connectivity layers network training counterintuitively information rises rapidly early phases training decreases preventing redistribution information resources phenomenon refer loss information plasticity analysis suggests first epochs critical creation strong connections optimal relative input data distribution strong connections created appear change additional training findings suggest initial learning transient scrutinized compared asymptotic behavior plays key role determining outcome training process findings combined recent theoretical results literature also suggest forgetting decrease information weights critical achieving invariance disentanglement representation learning finally critical periods restricted biological systems emerge naturally learning systems whether biological artificial due fundamental constrains arising learning dynamics information processing\n",
            "output sentence:  sensory deficits early training phases lead irreversible performance loss artificial neuronal networks suggesting information phenomena common cause point point initial transient \n",
            "\n",
            "{'rouge-1': {'r': 0.09473684210526316, 'p': 0.6923076923076923, 'f': 0.1666666645490398}, 'rouge-2': {'r': 0.015625, 'p': 0.16666666666666666, 'f': 0.02857142700408172}, 'rouge-l': {'r': 0.06315789473684211, 'p': 0.46153846153846156, 'f': 0.11111110899348427}}\n",
            "pair:  machine learning systems often encounter distribution ood errors dealing testing data coming different distribution one used training growing use critical applications becomes important develop systems able accurately quantify predictive uncertainty screen anomalous inputs however unlike standard learning tasks currently well established guiding principle designing architectures accurately quantify uncertainty moreover commonly used ood detection approaches prone errors even sometimes assign higher likelihoods ood samples address problems first seek identify guiding principles designing uncertainty aware architectures proposing neural architecture distribution search nads unlike standard neural architecture search methods seek single best performing architecture nads searches distribution architectures perform well given task allowing us identify building blocks common among uncertainty aware architectures formulation able optimize stochastic outlier detection objective construct ensemble models perform ood detection perform multiple ood detection experiments observe nads performs favorably compared state art ood detection methods\n",
            "output sentence:  propose architecture search method identify distribution architectures use construct bayesian ensemble outlier detection \n",
            "\n",
            "{'rouge-1': {'r': 0.07608695652173914, 'p': 0.7, 'f': 0.13725490019223377}, 'rouge-2': {'r': 0.031496062992125984, 'p': 0.4444444444444444, 'f': 0.05882352817582182}, 'rouge-l': {'r': 0.05434782608695652, 'p': 0.5, 'f': 0.09803921391772397}}\n",
            "pair:  backpropagation bp algorithm often thought biologically implausible brain one main reasons bp requires symmetric weight matrices feedforward feedback pathways address weight transport problem grossberg two biologically plausible algorithms proposed liao et al lillicrap et al relax bp weight symmetry requirements demonstrate comparable learning capabilities bp small datasets however recent study bartunov et al finds although feedback alignment fa variants target propagation tp perform well mnist cifar perform significantly worse bp imagenet additionally evaluate sign symmetry ss algorithm liao et al differs bp fa feedback feedforward weights share magnitudes share signs examined performance sign symmetry feedback alignment imagenet ms coco datasets using different network architectures resnet alexnet imagenet retinanet ms coco surprisingly networks trained sign symmetry attain classification performance approaching bp trained networks results complement study bartunov et al establish new benchmark future biologically plausible learning algorithms difficult datasets complex architectures\n",
            "output sentence:  biologically plausible learning algorithms particularly sign symmetry work well imagenet \n",
            "\n",
            "{'rouge-1': {'r': 0.08433734939759036, 'p': 0.4375, 'f': 0.1414141387042139}, 'rouge-2': {'r': 0.009174311926605505, 'p': 0.06666666666666667, 'f': 0.016129030131373852}, 'rouge-l': {'r': 0.04819277108433735, 'p': 0.25, 'f': 0.08080807809815335}}\n",
            "pair:  approaches problem inverse reinforcement learning irl focus estimating reward function best explains expert agent policy demonstrated behavior control task often case behavior succinctly represented simple reward combined set hard constraints setting agent attempting maximize cumulative rewards subject given constraints behavior reformulate problem irl markov decision processes mdps given nominal model environment nominal reward function seek estimate state action feature constraints environment motivate agent behavior approach based maximum entropy irl framework allows us reason likelihood expert agent demonstrations given knowledge mdp using method infer constraints added mdp increase likelihood observing demonstrations present algorithm iteratively infers maximum likelihood constraint best explain observed behavior evaluate efficacy using simulated behavior recorded data humans navigating around obstacle\n",
            "output sentence:  method infers constraints task execution leveraging principle maximum entropy quantify demonstrations differ expected un constrained behavior \n",
            "\n",
            "{'rouge-1': {'r': 0.1702127659574468, 'p': 0.6666666666666666, 'f': 0.27118643743751797}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.3333333333333333, 'f': 0.1111111083333334}, 'rouge-l': {'r': 0.1276595744680851, 'p': 0.5, 'f': 0.20338982726802646}}\n",
            "pair:  recent theoretical experimental results suggest possibility using current near future quantum hardware challenging sampling tasks paper introduce free energy based reinforcement learning ferl application quantum hardware propose method processing quantum annealer measured qubit spin configurations approximating free energy quantum boltzmann machine qbm apply method perform reinforcement learning grid world problem using wave quantum annealer experimental results show technique promising method harnessing power quantum sampling reinforcement learning tasks\n",
            "output sentence:  train quantum boltzmann machines using replica stacking method quantum annealer perform reinforcement learning task \n",
            "\n",
            "{'rouge-1': {'r': 0.20833333333333334, 'p': 0.5882352941176471, 'f': 0.30769230382958584}, 'rouge-2': {'r': 0.10909090909090909, 'p': 0.375, 'f': 0.16901408101567159}, 'rouge-l': {'r': 0.14583333333333334, 'p': 0.4117647058823529, 'f': 0.21538461152189353}}\n",
            "pair:  gaussian processes ubiquitous nature engineering case point class neural networks infinite width limit whose priors correspond gaussian processes perturbatively extend correspondence finite width neural networks yielding non gaussian processes priors methodology developed herein allows us track flow preactivation distributions progressively integrating random variables lower higher layers reminiscent renormalization group flow develop perturbative prescription perform bayesian inference weakly non gaussian priors\n",
            "output sentence:  develop analytical method study bayesian inference finite width neural networks find renormalization group flow picture naturally emerges \n",
            "\n",
            "{'rouge-1': {'r': 0.05, 'p': 0.6666666666666666, 'f': 0.09302325451595458}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0375, 'p': 0.5, 'f': 0.06976744056246621}}\n",
            "pair:  several first order stochastic optimization methods commonly used euclidean domain stochastic gradient descent sgd accelerated gradient descent variance reduced methods already adapted certain riemannian settings however popular optimization tools namely adam adagrad recent amsgrad remain generalized riemannian manifolds discuss difficulty generalizing adaptive schemes agnostic riemannian setting provide algorithms convergence proofs geodesically convex objectives particular case product riemannian manifolds adaptivity implemented across manifolds cartesian product generalization tight sense choosing euclidean space riemannian manifold yields algorithms regret bounds already known standard algorithms experimentally show faster convergence lower train loss value riemannian adaptive methods corresponding baselines realistic task embedding wordnet taxonomy poincare ball\n",
            "output sentence:  adapting adam amsgrad adagrad riemannian manifolds \n",
            "\n",
            "{'rouge-1': {'r': 0.07777777777777778, 'p': 0.6363636363636364, 'f': 0.13861385944515245}, 'rouge-2': {'r': 0.03418803418803419, 'p': 0.4, 'f': 0.0629921245334491}, 'rouge-l': {'r': 0.06666666666666667, 'p': 0.5454545454545454, 'f': 0.11881187924713266}}\n",
            "pair:  propose framework understand unprecedented performance robustness deep neural networks using field theory correlations weights within layer described symmetries layer networks generalize better symmetries broken reduce redundancies weights using two parameter field theory find network break symmetries towards end training process commonly known physics spontaneous symmetry breaking corresponds network generalizing without user input layers break symmetry communication adjacent layers layer decoupling limit applicable residual networks et al show remnant symmetries survive non linear layers spontaneously broken based empirical results lagrangian non linear weight layers together striking similarities one quantum field theory scalar using results quantum field theory show framework able explain many experimentally observed phenomena training random labels zero error zhang et al information bottleneck phase transition shwartz ziv tishby shattered gradients balduzzi et al many\n",
            "output sentence:  closed form results deep learning layer decoupling limit applicable residual networks \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.75, 'f': 0.1999999976888889}, 'rouge-2': {'r': 0.029411764705882353, 'p': 0.2727272727272727, 'f': 0.05309734337536226}, 'rouge-l': {'r': 0.08974358974358974, 'p': 0.5833333333333334, 'f': 0.15555555324444445}}\n",
            "pair:  shot image classification aims learning classifier limited labeled data generating classification weights applied many meta learning approaches shot image classification due simplicity effectiveness however argue difficult generate exact universal classification weights diverse query samples training samples work introduce attentive weights generation shot learning via information maximization awgim addresses current issues two novel contributions awgim generates different classification weights different query samples letting query samples attends whole support set ii guarantee generated weights adaptive different query sample formulate problem maximize lower bound mutual information generated weights query well support data far see first attempt unify information maximization shot learning two contributions proved effective extensive experiments show awgim able achieve state art performance benchmark datasets\n",
            "output sentence:  novel shot learning method generate query specific classification weights via information maximization \n",
            "\n",
            "{'rouge-1': {'r': 0.15853658536585366, 'p': 0.9285714285714286, 'f': 0.27083333084201394}, 'rouge-2': {'r': 0.08823529411764706, 'p': 0.6428571428571429, 'f': 0.15517241167063023}, 'rouge-l': {'r': 0.15853658536585366, 'p': 0.9285714285714286, 'f': 0.27083333084201394}}\n",
            "pair:  selection initial parameter values gradient based optimization deep neural networks one impactful hyperparameter choices deep learning systems affecting convergence times model performance yet despite significant empirical theoretical analysis relatively little proved concrete effects different initialization schemes work analyze effect initialization deep linear networks provide first time rigorous proof drawing initial weights orthogonal group speeds convergence relative standard gaussian initialization iid weights show deep networks width needed efficient convergence global minimum orthogonal initializations independent depth whereas width needed efficient convergence gaussian initializations scales linearly depth results demonstrate benefits good initialization persist throughout learning suggesting explanation recent empirical successes found initializing deep non linear networks according principle dynamical isometry\n",
            "output sentence:  provide first time rigorous proof orthogonal initialization speeds convergence relative gaussian initialization deep linear networks \n",
            "\n",
            "{'rouge-1': {'r': 0.18333333333333332, 'p': 0.7333333333333333, 'f': 0.2933333301333333}, 'rouge-2': {'r': 0.07042253521126761, 'p': 0.35714285714285715, 'f': 0.11764705607197239}, 'rouge-l': {'r': 0.1, 'p': 0.4, 'f': 0.15999999680000007}}\n",
            "pair:  temporal logics useful describing dynamic system behavior successfully used language goal definitions task planning prior works inferring temporal logic specifications focused summarizing input dataset finding specifications satisfied plan traces belonging given set paper examine problem inferring specifications describe temporal differences two sets plan traces formalize concept providing contrastive explanations present bayesian probabilistic model inferring contrastive explanations linear temporal logic specifications demonstrate efficacy scalability robustness model inferring correct specifications across various benchmark planning domains simulated air combat mission\n",
            "output sentence:  present bayesian inference model infer contrastive explanations ltl specifications describing two sets plan traces differ \n",
            "\n",
            "{'rouge-1': {'r': 0.18032786885245902, 'p': 0.9166666666666666, 'f': 0.3013698602664665}, 'rouge-2': {'r': 0.13253012048192772, 'p': 0.9166666666666666, 'f': 0.23157894516121888}, 'rouge-l': {'r': 0.18032786885245902, 'p': 0.9166666666666666, 'f': 0.3013698602664665}}\n",
            "pair:  introduce new memory architecture navigation previously unseen environments inspired landmark based navigation animals proposed semi parametric topological memory sptm consists non parametric graph nodes corresponding locations environment parametric deep network capable retrieving nodes graph based observations graph stores metric information connectivity locations corresponding nodes use sptm planning module navigation system given minutes footage previously unseen maze sptm based navigation agent build topological map environment use confidently navigate towards goals average success rate sptm agent goal directed navigation across test environments higher best performing baseline factor three\n",
            "output sentence:  introduce new memory architecture navigation previously unseen environments inspired landmark based navigation animals \n",
            "\n",
            "{'rouge-1': {'r': 0.021505376344086023, 'p': 0.3333333333333333, 'f': 0.04040403926538112}, 'rouge-2': {'r': 0.00909090909090909, 'p': 0.2, 'f': 0.01739130351606809}, 'rouge-l': {'r': 0.021505376344086023, 'p': 0.3333333333333333, 'f': 0.04040403926538112}}\n",
            "pair:  passage time unmanned autonomous vehicles uavs especially autonomous flying drones grabbed lot attention artificial intelligence since electronic technology getting smaller cheaper efficient huge advancement study uavs observed recently monitoring floods discerning spread algae water bodies detecting forest trail application far wide work mainly focused autonomous flying drones establish case study towards efficiency robustness accuracy uavs showed results well supported experiments provide details software hardware architecture used study discuss implementation algorithms present experiments provide comparison three different state art algorithms namely trailnet inceptionresnet mobilenet terms accuracy robustness power consumption inference time study shown mobilenet produced better results less computational requirement power consumption also reported challenges faced work well brief discussion future work improve safety features performance\n",
            "output sentence:  case study optimal deep learning model uavs \n",
            "\n",
            "{'rouge-1': {'r': 0.07476635514018691, 'p': 0.8888888888888888, 'f': 0.13793103305142687}, 'rouge-2': {'r': 0.030534351145038167, 'p': 0.4444444444444444, 'f': 0.05714285593979594}, 'rouge-l': {'r': 0.056074766355140186, 'p': 0.6666666666666666, 'f': 0.10344827443073723}}\n",
            "pair:  stochastic gradient descent sgd methods using randomly selected batches widely used train neural network nn models performing design exploration find best nn particular task often requires extensive training different models large dataset computationally expensive straightforward method accelerate computation distribute batch sgd multiple processors however large batch training often times leads degradation accuracy poor generalization even poor robustness adversarial attacks existing solutions large batch training either work require massive hyper parameter tuning address issue propose novel large batch training method combines recent results adversarial training regularize sharp minima second order optimization use curvature information change batch size adaptively training extensively evaluate method cifar svhn tinyimagenet imagenet datasets using multiple nns including residual networks well compressed networks squeezenext new approach exceeds performance existing solutions terms accuracy number sgd iterations times respectively emphasize achieved without additional hyper parameter tuning tailor method experiments\n",
            "output sentence:  large batch size training using adversarial training second order information \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.7, 'f': 0.16279069561925366}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.2222222222222222, 'f': 0.03883494986143847}, 'rouge-l': {'r': 0.09210526315789473, 'p': 0.7, 'f': 0.16279069561925366}}\n",
            "pair:  gradient clipping widely used technique training deep networks generally motivated optimisation lens informally controls dynamics iterates thus enhancing rate convergence local minimum intuition made precise line recent works show suitable clipping yield significantly faster convergence vanilla gradient descent paper propose new lens studying gradient clipping namely robustness informally one expects clipping provide robustness noise since one overly trust single sample surprisingly prove common problem label noise classification standard gradient clipping general provide robustness hand show simple variant gradient clipping provably robust corresponds suitably modifying underlying loss function yields simple noise robust alternative standard cross entropy loss performs well empirically\n",
            "output sentence:  gradient clipping endow robustness label noise simple loss based variant \n",
            "\n",
            "{'rouge-1': {'r': 0.18571428571428572, 'p': 0.8125, 'f': 0.3023255783666847}, 'rouge-2': {'r': 0.09411764705882353, 'p': 0.47058823529411764, 'f': 0.15686274232026148}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.625, 'f': 0.2325581365062196}}\n",
            "pair:  generative modeling high dimensional data like images notoriously difficult ill defined problem particular evaluate learned generative model unclear paper argue adversarial learning pioneered generative adversarial networks gans provides interesting framework implicitly define meaningful task losses unsupervised tasks generating visually realistic images relating gans structured prediction framework statistical decision theory put light links recent advances structured prediction theory choice divergence gans argue insights notions hard easy learn losses analogously extended adversarial divergences also discuss attractive properties parametric adversarial divergences generative modeling perform experiments show importance choosing divergence reflects final task\n",
            "output sentence:  parametric adversarial divergences implicitly define meaningful task losses generative modeling make parallels structured prediction study properties divergences task task \n",
            "\n",
            "{'rouge-1': {'r': 0.0707070707070707, 'p': 0.7777777777777778, 'f': 0.12962962810185186}, 'rouge-2': {'r': 0.015748031496062992, 'p': 0.2222222222222222, 'f': 0.029411763469939498}, 'rouge-l': {'r': 0.06060606060606061, 'p': 0.6666666666666666, 'f': 0.11111110958333334}}\n",
            "pair:  consider problem weakly supervised structured prediction sp reinforcement learning rl example given database table question perform sequence computation actions table generates response receives binary success failure reward line research successful leveraging rl directly optimizes desired metrics sp tasks example accuracy question answering bleu score machine translation however different common rl settings environment dynamics deterministic sp fully utilized model freerl methods usually applied since sp models usually full access environment dynamics propose apply model based rl methods rely planning primary model component demonstrate effectiveness planning based sp neural program planner npp given set candidate programs pretrained search policy decides program promising considering information generated executing programs evaluate npp weakly supervised program synthesis natural language semantic parsing stacked learning planning module based pretrained search policies wikitablequestions benchmark npp achieves new state art accuracy\n",
            "output sentence:  model based planning component improves rl based semantic parsing wikitablequestions \n",
            "\n",
            "{'rouge-1': {'r': 0.10638297872340426, 'p': 0.4166666666666667, 'f': 0.16949152218328073}, 'rouge-2': {'r': 0.017241379310344827, 'p': 0.09090909090909091, 'f': 0.028985504566267837}, 'rouge-l': {'r': 0.0851063829787234, 'p': 0.3333333333333333, 'f': 0.13559321709853497}}\n",
            "pair:  paper investigate learning deep neural networks automated optical inspection industrial manufacturing preliminary result shown stunning performance improvement transfer learning completely dissimilar source domain imagenet study demystifying improvement shows transfer learning produces highly compressible network case network learned scratch experimental result shows negligible accuracy drop network learned transfer learning compressed reduction number convolution lters result contrary compression without transfer learning loses accuracy compression rate\n",
            "output sentence:  experimentally show transfer learning makes sparse features network thereby produces compressible network \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.7, 'f': 0.2692307661242604}, 'rouge-2': {'r': 0.058823529411764705, 'p': 0.3333333333333333, 'f': 0.09999999745000007}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.3, 'f': 0.11538461227810658}}\n",
            "pair:  peripheral nervous system represents input output system brain cuff electrodes implanted peripheral nervous system allow observation control system however data produced electrodes low signal noise ratio complex signal content paper consider analysis neural data recorded vagus nerve animal models develop unsupervised learner based convolutional neural networks able simultaneously de noise cluster regions data signal content\n",
            "output sentence:  unsupervised analysis data recorded peripheral nervous system denoises categorises signals \n",
            "\n",
            "{'rouge-1': {'r': 0.054945054945054944, 'p': 0.38461538461538464, 'f': 0.0961538439663462}, 'rouge-2': {'r': 0.008928571428571428, 'p': 0.08333333333333333, 'f': 0.016129030509885726}, 'rouge-l': {'r': 0.04395604395604396, 'p': 0.3076923076923077, 'f': 0.076923074735577}}\n",
            "pair:  interpretability small labelled datasets key issues practical application deep learning particularly areas medicine paper present semi supervised technique addresses issues simultaneously learn dense representations large unlabelled image datasets use representations learn classifiers small labeled sets generate visual rationales explaining predictions using chest radiography diagnosis motivating application show method good generalization ability learning represent chest radiography dataset training classifier separate set different institution method identifies heart failure thoracic diseases prediction generate visual rationales positive classifications optimizing latent representation minimize probability disease constrained similarity measure image space decoding resultant latent representation produces image without apparent disease difference original altered image forms interpretable visual rationale algorithm prediction method simultaneously produces visual rationales compare favourably previous techniques classifier outperforms current state art\n",
            "output sentence:  propose method using gans generate high quality visual rationales help explain model predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 0.7692307692307693, 'f': 0.24390243635633554}, 'rouge-2': {'r': 0.05952380952380952, 'p': 0.4166666666666667, 'f': 0.1041666644791667}, 'rouge-l': {'r': 0.10144927536231885, 'p': 0.5384615384615384, 'f': 0.1707317046490185}}\n",
            "pair:  demonstrate possible train large recurrent language models user level differential privacy guarantees negligible cost predictive accuracy work builds recent advances training deep networks user partitioned data privacy accounting stochastic gradient descent particular add user level privacy protection federated averaging algorithm makes large step updates user level data work demonstrates given dataset sufficiently large number users requirement easily met even small internet scale datasets achieving differential privacy comes cost increased computation rather decreased utility prior work find private lstm language models quantitatively qualitatively similar un noised models trained large dataset\n",
            "output sentence:  user level differential privacy recurrent neural network language models possible sufficiently large dataset \n",
            "\n",
            "{'rouge-1': {'r': 0.04225352112676056, 'p': 0.42857142857142855, 'f': 0.0769230752892834}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.04225352112676056, 'p': 0.42857142857142855, 'f': 0.0769230752892834}}\n",
            "pair:  ranking central task machine learning information retrieval task especially important present user slate items appealing whole turn requires taking account interactions items since intuitively placing item slate affects decision items chosen alongside work propose sequence sequence model ranking called seq slate step model predicts next item place slate given items already chosen recurrent nature model allows complex dependencies items captured directly flexible scalable way show learn model end end weak supervision form easily obtained click data demonstrate usefulness approach experiments standard ranking benchmarks well real world recommendation system\n",
            "output sentence:  pointer network architecture ranking items learned click logs \n",
            "\n",
            "{'rouge-1': {'r': 0.05154639175257732, 'p': 0.625, 'f': 0.09523809383038549}, 'rouge-2': {'r': 0.00819672131147541, 'p': 0.14285714285714285, 'f': 0.015503874942611691}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.5, 'f': 0.07619047478276646}}\n",
            "pair:  variance reduction methods use mixture large small batch gradients svrg johnson zhang spiderboost wang et al require significantly computational resources per update sgd robbins monro reduce computational cost per update variance reduction methods introducing sparse gradient operator blending top operator stich et al aji heafield randomized coordinate descent operator computational cost computing derivative model parameter constant make observation gains variance reduction proportional magnitude derivative paper show sparse gradient based magnitude past gradients reduces computational cost model updates without significant loss variance reduction theoretically algorithm least good best available algorithm spiderboost appropriate settings parameters much efficient algorithm succeeds capturing sparsity gradients empirically algorithm consistently outperforms spiderboost using various models solve various image classification tasks also provide empirical evidence support intuition behind algorithm via simple gradient entropy computation serves quantify gradient sparsity every iteration\n",
            "output sentence:  use sparsity improve computational complexity variance reduction methods \n",
            "\n",
            "{'rouge-1': {'r': 0.07692307692307693, 'p': 0.6666666666666666, 'f': 0.13793103262782402}, 'rouge-2': {'r': 0.019801980198019802, 'p': 0.25, 'f': 0.0366972463462672}, 'rouge-l': {'r': 0.0641025641025641, 'p': 0.5555555555555556, 'f': 0.1149425268806976}}\n",
            "pair:  graph neural networks gnns effective framework representation learning graphs gnns follow neighborhood aggregation scheme representation vector node computed recursively aggregating transforming representation vectors neighboring nodes many gnn variants proposed achieved state art results node graph classification tasks however despite gnns revolutionizing graph representation learning limited understanding representational properties limitations present theoretical framework analyzing expressive power gnns capture different graph structures results characterize discriminative power popular gnn variants graph convolutional networks graphsage show cannot learn distinguish certain simple graph structures develop simple architecture provably expressive among class gnns powerful weisfeiler lehman graph isomorphism test empirically validate theoretical findings number graph classification benchmarks demonstrate model achieves state art performance\n",
            "output sentence:  develop theoretical foundations expressive power gnns design provably powerful gnn \n",
            "\n",
            "{'rouge-1': {'r': 0.054945054945054944, 'p': 0.45454545454545453, 'f': 0.09803921376201463}, 'rouge-2': {'r': 0.009433962264150943, 'p': 0.1, 'f': 0.017241377734839622}, 'rouge-l': {'r': 0.054945054945054944, 'p': 0.45454545454545453, 'f': 0.09803921376201463}}\n",
            "pair:  recent studies attention modules enabled higher performance computer vision tasks capturing global contexts accordingly attending important features paper propose simple highly parametrically efficient module named tree structured attention module tam recursively encourages neighboring channels collaborate order produce spatial attention map output unlike attention modules try capture long range dependencies channel module focuses imposing non linearities tween channels utilizing point wise group convolution module strengthens representational power model also acts gate controls signal flow module allows model achieve higher performance highly parameter efficient manner empirically validate effectiveness module extensive experiments cifar svhn datasets proposed attention module employed resnet resnet models gain accuracy improvement less parameter head pytorch implementation code publicly available\n",
            "output sentence:  paper proposes attention module captures inter channel relationships offers large performance gains \n",
            "\n",
            "{'rouge-1': {'r': 0.12941176470588237, 'p': 0.8461538461538461, 'f': 0.22448979361724286}, 'rouge-2': {'r': 0.0660377358490566, 'p': 0.5833333333333334, 'f': 0.11864406596954902}, 'rouge-l': {'r': 0.08235294117647059, 'p': 0.5384615384615384, 'f': 0.14285714055601834}}\n",
            "pair:  despite advances deep learning artificial neural networks learn way humans today neural networks learn multiple tasks trained jointly cannot maintain performance learnt tasks tasks presented one time phenomenon called catastrophic forgetting fundamental challenge overcome neural networks learn continually incoming data work derive inspiration human memory develop architecture capable learning continuously sequentially incoming tasks averting catastrophic forgetting specifically model consists dual memory architecture emulate complementary learning systems hippocampus neocortex human brain maintains consolidated long term memory via generative replay past experiences substantiate claim replay generative ii show benefits generative replay dual memory via experiments iii demonstrate improved performance retention even small models low capacity architecture displays many important characteristics human memory provides insights connection sleep learning humans\n",
            "output sentence:  dual memory architecture inspired human brain learn sequentially incoming tasks averting catastrophic forgetting \n",
            "\n",
            "{'rouge-1': {'r': 0.12264150943396226, 'p': 0.65, 'f': 0.20634920367850845}, 'rouge-2': {'r': 0.025423728813559324, 'p': 0.15789473684210525, 'f': 0.04379561804891057}, 'rouge-l': {'r': 0.0660377358490566, 'p': 0.35, 'f': 0.11111110844041328}}\n",
            "pair:  achieving machine intelligence requires smooth integration perception reasoning yet models developed date tend specialize one sophisticated manipulation symbols acquired rich perceptual spaces far proved elusive consider visual arithmetic task goal carry simple arithmetical algorithms digits presented natural conditions hand written placed randomly propose two tiered architecture tackling kind problem lower tier consists heterogeneous collection information processing modules include pre trained deep neural networks locating extracting characters image well modules performing symbolic transformations representations extracted perception higher tier consists controller trained using reinforcement learning coordinates modules order solve high level task instance controller may learn contexts execute perceptual networks symbolic transformations apply outputs resulting model able solve variety tasks visual arithmetic domain several advantages standard architecturally homogeneous feedforward networks including improved sample efficiency\n",
            "output sentence:  use reinforcement learning train agent solve set visual arithmetic tasks using provided pre trained perceptual modules transformations internal representations representations \n",
            "\n",
            "{'rouge-1': {'r': 0.04081632653061224, 'p': 0.3333333333333333, 'f': 0.07272727078347112}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02040816326530612, 'p': 0.16666666666666666, 'f': 0.03636363441983481}}\n",
            "pair:  propose new framework entity event extraction based generative adversarial imitation learning inverse reinforcement learning method using generative adversarial network gan assume instances labels yield various extents difficulty gains penalties rewards expected diverse utilize discriminators estimate proper rewards according difference labels committed ground truth expert extractor agent experiments also demonstrate proposed framework outperforms state art methods\n",
            "output sentence:  use dynamic rewards train event extractors \n",
            "\n",
            "{'rouge-1': {'r': 0.189873417721519, 'p': 0.8333333333333334, 'f': 0.30927834749282607}, 'rouge-2': {'r': 0.056074766355140186, 'p': 0.35294117647058826, 'f': 0.09677419118236218}, 'rouge-l': {'r': 0.11392405063291139, 'p': 0.5, 'f': 0.1855670072866405}}\n",
            "pair:  work aim solve data driven optimization problems goal find input maximizes unknown score function given access dataset input score pairs inputs may lie extremely thin manifolds high dimensional spaces making optimization prone falling manifold evaluating unknown function may expensive algorithm able exploit static offline data propose model inversion networks mins approach solve problems unlike prior work mins scale extremely high dimensional input spaces efficiently leverage offline logged datasets optimization contextual non contextual settings show mins also extended active setting commonly studied prior work via simple novel effective scheme active data collection experiments show mins act powerful optimizers range contextual non contextual static active problems including optimization images protein designs learning logged bandit feedback\n",
            "output sentence:  propose novel approach solve data driven model based optimization problems passive active settings scale high dimensional input spaces \n",
            "\n",
            "{'rouge-1': {'r': 0.02586206896551724, 'p': 0.5, 'f': 0.049180326933620004}, 'rouge-2': {'r': 0.007042253521126761, 'p': 0.16666666666666666, 'f': 0.013513512735573458}, 'rouge-l': {'r': 0.02586206896551724, 'p': 0.5, 'f': 0.049180326933620004}}\n",
            "pair:  machine learning workloads often expensive train taking weeks converge current generation frameworks relies custom back ends order achieve efficiency making impractical train models less common hardware back ends exist knossos builds recent work avoids need hand written libraries instead compiles machine learning models much way one would compile kinds software order make resulting code efficient knossos complier directly optimises abstract syntax tree program however contrast traditional compilers employ hand written optimisation passes take rewriting approach driven star search algorithm learn value function evaluates future potential cost reduction taking various rewriting actions program show knossos automatically learned optimisations past compliers implement hand furthermore demonstrate knossos achieve wall time reduction compared hand tuned compiler suite machine learning programs including basic linear algebra convolutional networks knossos compiler minimal dependencies used architecture supports cpp toolchain since cost model proposed algorithm optimises tailored particular hardware architecture proposed approach potentially applied variety hardware\n",
            "output sentence:  combine search reinforcement learning speed machine learning code \n",
            "\n",
            "{'rouge-1': {'r': 0.08139534883720931, 'p': 0.7, 'f': 0.1458333314670139}, 'rouge-2': {'r': 0.028037383177570093, 'p': 0.3333333333333333, 'f': 0.051724136499702776}, 'rouge-l': {'r': 0.06976744186046512, 'p': 0.6, 'f': 0.12499999813368057}}\n",
            "pair:  machine learning models including traditional models neural networks easily fooled adversarial examples generated natural examples small perturbations poses critical challenge machine learning security impedes wide application machine learning many important domains computer vision malware detection unfortunately even state art defense approaches adversarial training defensive distillation still suffer major limitations circumvented unique angle propose investigate two important research questions paper adversarial examples distinguishable natural examples adversarial examples generated different methods distinguishable two questions concern distinguishability adversarial examples answering potentially lead simple yet effective approach termed defensive distinction paper formulation multi label classification protecting adversarial examples design perform experiments using mnist dataset investigate two questions obtain highly positive results demonstrating strong distinguishability adversarial examples recommend unique defensive distinction approach seriously considered complement defense approaches\n",
            "output sentence:  propose defensive distinction protection approach demonstrate strong distinguishability adversarial examples \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.5454545454545454, 'f': 0.26373626007003986}, 'rouge-2': {'r': 0.05434782608695652, 'p': 0.21739130434782608, 'f': 0.08695651853913056}, 'rouge-l': {'r': 0.14492753623188406, 'p': 0.45454545454545453, 'f': 0.21978021611399595}}\n",
            "pair:  meta reinforcement learning approaches aim develop learning procedures adapt quickly distribution tasks help examples developing efficient exploration strategies capable finding useful samples becomes critical settings existing approaches finding efficient exploration strategies add auxiliary objectives promote exploration pre update policy however makes adaptation using gradient steps difficult pre update exploration post update exploitation policies quite different instead propose explicitly model separate exploration policy task distribution two different policies gives flexibility training exploration policy also makes adaptation specific task easier show using self supervised supervised learning objectives adaptation stabilizes training process also demonstrate superior performance model compared prior works domain\n",
            "output sentence:  propose use separate exploration policy collect pre adaptation trajectories maml also show using self supervised objective inner loop leads training training better better performance \n",
            "\n",
            "{'rouge-1': {'r': 0.17777777777777778, 'p': 0.9411764705882353, 'f': 0.2990654178880252}, 'rouge-2': {'r': 0.13675213675213677, 'p': 0.9411764705882353, 'f': 0.23880596793383835}, 'rouge-l': {'r': 0.17777777777777778, 'p': 0.9411764705882353, 'f': 0.2990654178880252}}\n",
            "pair:  resemblance methods used studying quantum many body physics machine learning drawn considerable attention particular tensor networks tns deep learning architectures bear striking similarities extent tns used machine learning previous results used one dimensional tns image recognition showing limited scalability request high bond dimension work train two dimensional hierarchical tns solve image recognition problems using training algorithm derived multipartite entanglement renormalization ansatz mera approach overcomes scalability issues implies novel mathematical connections among quantum many body physics quantum information theory machine learning keeping tn unitary training phase tn states defined optimally encodes class images quantum many body state study quantum features tn states including quantum entanglement fidelity suggest quantities could novel properties characterize image classes well machine learning tasks work could applied identifying possible quantum properties certain artificial intelligence methods\n",
            "output sentence:  approach overcomes scalability issues implies novel mathematical connections among quantum many body physics quantum information theory machine learning \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.7777777777777778, 'f': 0.24137930772294885}, 'rouge-2': {'r': 0.08771929824561403, 'p': 0.625, 'f': 0.15384615168757396}, 'rouge-l': {'r': 0.12244897959183673, 'p': 0.6666666666666666, 'f': 0.20689654910225921}}\n",
            "pair:  propose method joint image per pixel annotation synthesis gan demonstrate gan good high level representation target data easily projected semantic segmentation masks method used create training dataset teaching separate semantic segmentation network experiments show segmentation network successfully generalizes real data additionally method outperforms supervised training number training samples small works variety different scenes classes source code proposed method publicly available\n",
            "output sentence:  gan based method joint image per pixel annotation synthesis \n",
            "\n",
            "{'rouge-1': {'r': 0.041237113402061855, 'p': 0.5714285714285714, 'f': 0.07692307566752961}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.3333333333333333, 'f': 0.03124999910644534}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.5714285714285714, 'f': 0.07692307566752961}}\n",
            "pair:  given video sentence goal weakly supervised video moment retrieval locate video segment described sentence without access temporal annotations training instead model must learn identify correct segment moment provided video sentence pairs thus inherent challenge automatically inferring latent correspondence visual language representations facilitate alignment propose weakly supervised moment alignment network wman exploits multi level co attention mechanism learn richer multimodal representations aforementioned mechanism comprised frame word interaction module well novel word conditioned visual graph wcvg approach also incorporates novel application positional encodings commonly used transformers learn visual semantic representations contain contextual information relative positions temporal sequence iterative message passing comprehensive experiments didemo charades sta datasets demonstrate effectiveness learned representations combined wman model outperforms state art weakly supervised method significant margin also better strongly supervised state art methods metrics\n",
            "output sentence:  weakly supervised text based video moment retrieval \n",
            "\n",
            "{'rouge-1': {'r': 0.09302325581395349, 'p': 0.5, 'f': 0.15686274245290277}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.2222222222222222, 'f': 0.06249999758300791}, 'rouge-l': {'r': 0.09302325581395349, 'p': 0.5, 'f': 0.15686274245290277}}\n",
            "pair:  representations sets challenging learn operations sets permutation invariant end propose permutation optimisation module learns permute set end end permuted set processed learn permutation invariant representation set avoiding bottleneck traditional set models demonstrate model ability learn permutations set representations either explicit implicit supervision four datasets achieve state art results number sorting image mosaics classification image mosaics visual question answering\n",
            "output sentence:  learn permute set encode permuted set rnn obtain set representation \n",
            "\n",
            "{'rouge-1': {'r': 0.07894736842105263, 'p': 0.75, 'f': 0.14285714113378684}, 'rouge-2': {'r': 0.02040816326530612, 'p': 0.2857142857142857, 'f': 0.03809523685079368}, 'rouge-l': {'r': 0.06578947368421052, 'p': 0.625, 'f': 0.11904761732426304}}\n",
            "pair:  multi view learning provide self supervision different views available data distributional hypothesis provides another form useful self supervision adjacent sentences plentiful large unlabelled corpora motivated asymmetry two hemispheres human brain well observation different learning architectures tend emphasise different aspects sentence meaning present two multi view frameworks learning sentence representations unsupervised fashion one framework uses generative objective discriminative one frameworks final representation ensemble two views one view encodes input sentence recurrent neural network rnn view encodes simple linear model show learning vectors produced multi view frameworks provide improved representations single view learnt counterparts combination different views gives representational improvement view demonstrates solid transferability standard downstream tasks\n",
            "output sentence:  multi view learning improves unsupervised sentence representation learning \n",
            "\n",
            "{'rouge-1': {'r': 0.06779661016949153, 'p': 0.5, 'f': 0.11940298297170863}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.375, 'f': 0.08955223670305197}}\n",
            "pair:  contextualized word representations elmo bert shown perform well various semantic structural syntactic task work tackle task unsupervised disentanglement semantics structure neural language representations aim learn transformation contextualized vectors discards lexical semantics keeps structural information end automatically generate groups sentences structurally similar semantically different use metric learning approach learn transformation emphasizes structural component encoded vectors demonstrate transformation clusters vectors space structural properties rather lexical semantics finally demonstrate utility distilled representations showing outperform original contextualized representations shot parsing setting\n",
            "output sentence:  distill language models representations syntax unsupervised metric learning \n",
            "\n",
            "{'rouge-1': {'r': 0.05952380952380952, 'p': 0.625, 'f': 0.10869565058601136}, 'rouge-2': {'r': 0.02727272727272727, 'p': 0.375, 'f': 0.05084745636311408}, 'rouge-l': {'r': 0.05952380952380952, 'p': 0.625, 'f': 0.10869565058601136}}\n",
            "pair:  order efficiently learn small amount data new tasks meta learning transfers knowledge learned previous tasks new ones however critical challenge meta learning task heterogeneity cannot well handled traditional globally shared meta learning methods addition current task specific meta learning methods may either suffer hand crafted structure design lack capability capture complex relations tasks paper motivated way knowledge organization knowledge bases propose automated relational meta learning arml framework automatically extracts cross task relations constructs meta knowledge graph new task arrives quickly find relevant structure tailor learned structure knowledge meta learner result proposed framework addresses challenge task heterogeneity learned meta knowledge graph also increases model interpretability conduct extensive experiments toy regression shot image classification results demonstrate superiority arml state art baselines\n",
            "output sentence:  addressing task heterogeneity problem meta learning introducing meta knowledge graph \n",
            "\n",
            "{'rouge-1': {'r': 0.06097560975609756, 'p': 0.38461538461538464, 'f': 0.10526315553241003}, 'rouge-2': {'r': 0.009708737864077669, 'p': 0.08333333333333333, 'f': 0.017391302478639143}, 'rouge-l': {'r': 0.06097560975609756, 'p': 0.38461538461538464, 'f': 0.10526315553241003}}\n",
            "pair:  training larger number parameters keeping fast iterations increasingly adopted strategy trend developing better performing deep neural network dnn models necessitates increased memory footprint computational requirements training introduce novel methodology training deep neural networks using bit floating point fp numbers reduced bit precision allows larger effective memory increased computational speed name method shifted squeezed fp fp show unlike previous bit precision training methods proposed method works box representative models resnet transformer ncf method maintain model accuracy without requiring fine tuning loss scaling parameters keeping certain layers single precision introduce two learnable statistics dnn tensors shifted squeezed factors used optimally adjust range tensors bits thus minimizing loss information due quantization\n",
            "output sentence:  propose novel bit format eliminates need loss scaling stochastic rounding low precision techniques \n",
            "\n",
            "{'rouge-1': {'r': 0.041666666666666664, 'p': 0.1, 'f': 0.05882352525951587}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.1, 'f': 0.05882352525951587}}\n",
            "pair:  data arise multiple latent subpopulations machine learning frameworks typically estimate parameter values independently sub population paper propose overcome limits considering samples tasks multitask learning framework\n",
            "output sentence:  present method estimate collections regression models model personalized single sample \n",
            "\n",
            "{'rouge-1': {'r': 0.1590909090909091, 'p': 1.0, 'f': 0.2745098015532488}, 'rouge-2': {'r': 0.07547169811320754, 'p': 0.6666666666666666, 'f': 0.13559321851192188}, 'rouge-l': {'r': 0.1590909090909091, 'p': 1.0, 'f': 0.2745098015532488}}\n",
            "pair:  propose fixed grouping layer fgl novel feedforward layer designed incorporate inductive bias structured smoothness deep learning model fgl achieves goal connecting nodes across layers based spatial similarity use structured smoothness implemented fgl motivated applications structured spatial data turn motivated domain knowledge proposed model architecture outperforms conventional neural network architectures across variety simulated real datasets structured smoothness\n",
            "output sentence:  feedforward layer incorporate structured smoothness deep learning model \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.75, 'f': 0.1999999976888889}, 'rouge-2': {'r': 0.019801980198019802, 'p': 0.18181818181818182, 'f': 0.03571428394292101}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.5, 'f': 0.13333333102222228}}\n",
            "pair:  training generative models like generative adversarial network gan challenging noisy data novel curriculum learning algorithm pertaining clustering proposed address issue paper curriculum construction based centrality underlying clusters data points data points high centrality takes priority fed generative models training make algorithm scalable large scale data active set devised sense every round training proceeds active subset containing small fraction already trained data incremental data lower centrality moreover geometric analysis presented interpret necessity cluster curriculum generative models experiments cat human face data validate algorithm able learn optimal generative models progan respect specified quality metrics noisy data interesting finding optimal cluster curriculum closely related critical point geometric percolation process formulated paper\n",
            "output sentence:  novel cluster based algorithm curriculum learning proposed solve robust training generative models \n",
            "\n",
            "{'rouge-1': {'r': 0.07462686567164178, 'p': 0.7142857142857143, 'f': 0.13513513342220598}, 'rouge-2': {'r': 0.012345679012345678, 'p': 0.16666666666666666, 'f': 0.022988504462941015}, 'rouge-l': {'r': 0.05970149253731343, 'p': 0.5714285714285714, 'f': 0.108108106395179}}\n",
            "pair:  learning gaussian process models occurs adaptation hyperparameters mean covariance function classical approach entails maximizing marginal likelihood yielding fixed point estimates approach called type ii maximum likelihood ml ii alternative learning procedure infer posterior hyperparameters hierarchical specification gps call fully bayesian gaussian process regression gpr work considers two approximations intractable hyperparameter posterior hamiltonian monte carlo hmc yielding sampling based approximation variational inference vi posterior hyperparameters approximated factorized gaussian mean field full rank gaussian accounting correlations hyperparameters analyse predictive performance fully bayesian gpr range benchmark data sets\n",
            "output sentence:  analysis bayesian hyperparameter inference gaussian process regression \n",
            "\n",
            "{'rouge-1': {'r': 0.08888888888888889, 'p': 0.7272727272727273, 'f': 0.15841583964317224}, 'rouge-2': {'r': 0.02727272727272727, 'p': 0.3, 'f': 0.049999998472222265}, 'rouge-l': {'r': 0.05555555555555555, 'p': 0.45454545454545453, 'f': 0.09900989904911285}}\n",
            "pair:  paper consider specific problem word level language modeling investigate strategies regularizing optimizing lstm based models propose weight dropped lstm uses dropconnect hidden hidden weights form recurrent regularization introduce nt asgd non monotonically triggered nt variant averaged stochastic gradient method asgd wherein averaging trigger determined using nt condition opposed tuned user using regularization strategies asgd weight dropped lstm awd lstm achieves state art word level perplexities two data sets penn treebank wikitext exploring effectiveness neural cache conjunction proposed model achieve even lower state art perplexity penn treebank wikitext also explore viability proposed regularization optimization strategies context quasi recurrent neural network qrnn demonstrate comparable performance awd lstm counterpart code reproducing results open sourced available https github com salesforce awd lstm lm\n",
            "output sentence:  effective regularization optimization strategies lstm based language models achieves sota ptb wt \n",
            "\n",
            "{'rouge-1': {'r': 0.14814814814814814, 'p': 0.7272727272727273, 'f': 0.24615384334201185}, 'rouge-2': {'r': 0.015151515151515152, 'p': 0.1, 'f': 0.026315787188365854}, 'rouge-l': {'r': 0.09259259259259259, 'p': 0.45454545454545453, 'f': 0.15384615103431953}}\n",
            "pair:  data augmentation da fundamental overfitting large convolutional neural networks especially limited training dataset images da usually based heuristic transformations like geometric color transformations instead using predefined transformations work learns data augmentation directly training data learning transform images encoder decoder architecture combined spatial transformer network transformed images still belong class new complex samples classifier experiments show approach better previous generative data augmentation methods comparable predefined transformation methods training image classifier\n",
            "output sentence:  automatic learning data augmentation using gan based architecture improve image classifier \n",
            "\n",
            "{'rouge-1': {'r': 0.0196078431372549, 'p': 0.3333333333333333, 'f': 0.03703703598765435}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0196078431372549, 'p': 0.3333333333333333, 'f': 0.03703703598765435}}\n",
            "pair:  saliency methods aim explain predictions deep neural networks methods lack reliability explanation sensitive factors contribute model prediction use simple common pre processing step adding mean shift input data show transformation effect model cause numerous methods incorrectly attribute define input invariance requirement saliency method mirror sensitivity model respect transformations input show several examples saliency methods satisfy input invariance property unreliable lead misleading inaccurate attribution\n",
            "output sentence:  attribution sometimes misleading \n",
            "\n",
            "{'rouge-1': {'r': 0.05434782608695652, 'p': 0.625, 'f': 0.09999999852800001}, 'rouge-2': {'r': 0.007874015748031496, 'p': 0.14285714285714285, 'f': 0.014925372144130162}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.5, 'f': 0.07999999852800002}}\n",
            "pair:  graph neural networks gnns received tremendous attention recently due power handling graph data different downstream tasks across different application domains key gnn graph convolutional filters recently various kinds filters designed however still lacks depth analysis whether exists best filter perform best graph data graph properties influence optimal choice graph filter design appropriate filter adaptive graph data paper focus addressing three questions first propose novel assessment tool evaluate effectiveness graph convolutional filters given graph using assessment tool find single filter silver bullet perform best possible graphs addition different graph structure properties influence optimal graph convolutional filter design choice based findings develop adaptive filter graph neural network afgnn simple powerful model adaptively learn task specific filter given graph leverages graph filter assessment regularization learns combine set base filters experiments synthetic real world benchmark datasets demonstrate proposed model indeed learn appropriate filter perform well graph tasks\n",
            "output sentence:  propose assessment framework analyze learn graph convolutional filter \n",
            "\n",
            "{'rouge-1': {'r': 0.04, 'p': 0.3, 'f': 0.07058823321799315}, 'rouge-2': {'r': 0.010526315789473684, 'p': 0.1111111111111111, 'f': 0.019230767649778234}, 'rouge-l': {'r': 0.02666666666666667, 'p': 0.2, 'f': 0.04705882145328729}}\n",
            "pair:  disentangling underlying generative factors data distribution important interpretability generalizable representations paper introduce two novel disentangling methods first method unlabeled disentangling gan ud gan unsupervised decomposes latent noise generating similar dissimilar image pairs learns distance metric pairs siamese networks contrastive loss pairwise approach provides consistent representations similar data points second method ud gan weakly supervised modifies ud gan user defined guidance functions restrict information goes siamese networks constraint helps ud gan focus desired semantic variations data show methods outperform existing unsupervised approaches quantitative metrics measure semantic accuracy learned representations addition illustrate simple guidance functions use ud gan allow us directly capture desired variations data\n",
            "output sentence:  use siamese networks guide disentangle generation process gans without labeled data \n",
            "\n",
            "{'rouge-1': {'r': 0.09803921568627451, 'p': 0.5555555555555556, 'f': 0.1666666641166667}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0784313725490196, 'p': 0.4444444444444444, 'f': 0.13333333078333337}}\n",
            "pair:  stability key aspect data analysis many applications natural notion stability geometric illustrated example computer vision scattering transforms construct deep convolutional representations certified stable input deformations stability deformations interpreted stability respect changes metric structure domain work show scattering transforms generalized non euclidean domains using diffusion wavelets preserving notion stability respect metric changes domain measured diffusion maps resulting representation stable metric perturbations domain able capture high frequency information akin euclidean scattering\n",
            "output sentence:  stability scattering transform representations graph data deformations underlying graph support \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.42857142857142855, 'f': 0.1818181784756658}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.3333333333333333, 'f': 0.1111111083333334}, 'rouge-l': {'r': 0.11538461538461539, 'p': 0.42857142857142855, 'f': 0.1818181784756658}}\n",
            "pair:  area explainable ai xai explainable ai planning xaip matures ability agents generate curate explanations likewise grow propose new challenge area form rebellious deceptive explanations discuss explanations might generated briefly discuss evaluation criteria\n",
            "output sentence:  position paper proposing rebellious deceptive explanations agents \n",
            "\n",
            "{'rouge-1': {'r': 0.14492753623188406, 'p': 1.0, 'f': 0.25316455475084126}, 'rouge-2': {'r': 0.05952380952380952, 'p': 0.5555555555555556, 'f': 0.10752687997225113}, 'rouge-l': {'r': 0.11594202898550725, 'p': 0.8, 'f': 0.20253164335843615}}\n",
            "pair:  paper focused investigating demystifying intriguing robustness phenomena parameterized neural network training particular provide empirical theoretical evidence first order methods gradient descent provably robust noise corruption constant fraction labels despite parameterization rich dataset model particular first show first iterations updates still vicinity initialization algorithms fit correct labels essentially ignoring noisy labels ii secondly prove start overfit noisy labels algorithms must stray rather far initial model occur many iterations together show gradient descent early stopping provably robust label noise shed light empirical robustness deep networks well commonly adopted early stopping heuristics\n",
            "output sentence:  prove gradient descent robust label corruption despite parameterization rich dataset model \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.7272727272727273, 'f': 0.19512194889649023}, 'rouge-2': {'r': 0.021739130434782608, 'p': 0.18181818181818182, 'f': 0.03883494954849665}, 'rouge-l': {'r': 0.07042253521126761, 'p': 0.45454545454545453, 'f': 0.12195121718917315}}\n",
            "pair:  standard image captioning tasks coco flickr factual neutral tone human state obvious man playing guitar tasks useful verify machine understands content image engaging humans captions mind define new task personality captions goal engaging humans possible incorporating controllable style personality traits collect release large dataset captions conditioned possible traits build models combine existing work sentence representations mazar et al transformers trained billion dialogue examples ii image representations mahajan et al resnets trained billion social media images obtain state art performance flickr coco strong performance new task finally online evaluations validate task models engaging humans best model close human performance\n",
            "output sentence:  develop engaging image captioning models conditioned personality also state art regular captioning tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.09210526315789473, 'p': 0.5833333333333334, 'f': 0.15909090673553722}, 'rouge-2': {'r': 0.043478260869565216, 'p': 0.2857142857142857, 'f': 0.07547169582057679}, 'rouge-l': {'r': 0.06578947368421052, 'p': 0.4166666666666667, 'f': 0.11363636128099178}}\n",
            "pair:  neural message passing algorithms semi supervised classification graphs recently achieved great success however classifying node methods consider nodes propagation steps away size utilized neighborhood hard extend paper use relationship graph convolutional networks gcn pagerank derive improved propagation scheme based personalized pagerank utilize propagation procedure construct simple model personalized propagation neural predictions ppnp fast approximation appnp model training time par faster number parameters par lower previous models leverages large adjustable neighborhood classification easily combined neural network show model outperforms several recently proposed methods semi supervised classification thorough study done far gcn like models implementation available online\n",
            "output sentence:  personalized propagation neural predictions ppnp improves graph neural networks separating prediction propagation via personalized pagerank \n",
            "\n",
            "{'rouge-1': {'r': 0.13114754098360656, 'p': 0.7272727272727273, 'f': 0.22222221963348768}, 'rouge-2': {'r': 0.04285714285714286, 'p': 0.3, 'f': 0.07499999781250007}, 'rouge-l': {'r': 0.11475409836065574, 'p': 0.6363636363636364, 'f': 0.1944444418557099}}\n",
            "pair:  regularization based continual learning approaches generally prevent catastrophic forgetting augmenting training loss auxiliary objective however practical optimization scenarios noisy data gradients possible stochastic gradient descent inadvertently change critical parameters paper argue importance regularizing optimization trajectories directly derive new co natural gradient update rule continual learning whereby new task gradients preconditioned empirical fisher information previously learnt tasks show using co natural gradient systematically reduces forgetting continual learning moreover helps combat overfitting learning new task low resource scenario\n",
            "output sentence:  regularizing optimization trajectory fisher information old tasks reduces catastrophic forgetting greatly \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6470588235294118, 'f': 0.20952380680997734}, 'rouge-2': {'r': 0.05042016806722689, 'p': 0.3333333333333333, 'f': 0.0875912385934254}, 'rouge-l': {'r': 0.09090909090909091, 'p': 0.47058823529411764, 'f': 0.1523809496671202}}\n",
            "pair:  sometimes srs stereotactic radio surgery requires using sphere packing region interest roi cancer determine treatment plan developed sphere packing algorithm packs non intersecting spheres inside roi region interest case voxels identified cancer tissues paper analyze rotational invariant properties sphere packing algorithm based distance transformations epsilon rotation invariant means ability arbitrary rotate roi keeping volume properties remaining almost within limit epsilon applied rotations produce spherical packing remains highly correlated analyze geometrically properties sphere packing rotation volume data roi novel sphere packing algorithm high degree rotation invariance within range epsilon method used shape descriptor derived values disjoint set spheres form distance based sphere packing algorithm extract invariant descriptor roi demonstrated implementing ideas using slicer platform available research data based sing mri stereotactic images presented several performance results different benchmarks data patients slicer platform\n",
            "output sentence:  packing region interest roi cancerous regions identified volume data packing spheres inside roi rotating measures difference sphere rotation sphere \n",
            "\n",
            "{'rouge-1': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.02631578947368421, 'p': 0.25, 'f': 0.04761904589569167}}\n",
            "pair:  introduce concept channel aggregation convnet architecture novel compact representation cnn features useful explicitly modeling nonlinear channels encoding especially new unit embedded inside deep architectures action recognition channel aggregation based multiple channels features convnet aims spot finding optical convergence path fast speed name proposed convolutional architecture nonlinear channels aggregation networks ncan new layer nonlinear channels aggregation layer ncal theoretically motivate channels aggregation functions empirically study effect convergence speed classification accuracy another contribution work efficient effective implementation ncal speeding orders magnitude evaluate performance standard benchmarks ucf hmdb experimental results demonstrate formulation obtains fast convergence stronger generalization capability without sacrificing performance\n",
            "output sentence:  architecture enables cnn trained video sequences converging rapidly \n",
            "\n",
            "{'rouge-1': {'r': 0.0898876404494382, 'p': 0.47058823529411764, 'f': 0.1509433935332859}, 'rouge-2': {'r': 0.018691588785046728, 'p': 0.11764705882352941, 'f': 0.032258062150104225}, 'rouge-l': {'r': 0.0449438202247191, 'p': 0.23529411764705882, 'f': 0.07547169542007842}}\n",
            "pair:  attacks natural language models difficult compare due different definitions constitutes successful attack present taxonomy constraints categorize attacks constraint present real world use case way measure well generated samples enforce constraint employ framework evaluate two state art attacks fool models synonym substitution attacks claim adversarial perturbations preserve semantics syntactical correctness inputs analysis shows constraints strongly enforced significant portion adversarial examples grammar checker detects increase errors additionally human studies indicate many adversarial examples diverge semantic meaning input appear human written finally highlight need standardized evaluation attacks share constraints without shared evaluation metrics researchers set thresholds determine trade attack quality attack success recommend well designed human studies determine best threshold approximate human judgement\n",
            "output sentence:  present framework evaluating adversarial examples natural language processing demonstrate generated adversarial examples often semantics preserving syntactically correct non suspicious \n",
            "\n",
            "{'rouge-1': {'r': 0.14606741573033707, 'p': 0.7222222222222222, 'f': 0.2429906514071098}, 'rouge-2': {'r': 0.05454545454545454, 'p': 0.35294117647058826, 'f': 0.09448818665757337}, 'rouge-l': {'r': 0.11235955056179775, 'p': 0.5555555555555556, 'f': 0.1869158850519696}}\n",
            "pair:  machine learning ml gets applied security critical sensitive domains growing need integrity privacy outsourced ml computations pragmatic solution comes trusted execution environments tees use hardware software protections isolate sensitive computations untrusted software stack however isolation guarantees come price performance compared untrusted alternatives paper initiates study high performance execution deep neural networks dnns tees efficiently partitioning dnn computations trusted untrusted devices building upon efficient outsourcing scheme matrix multiplication propose slalom framework securely delegates execution linear layers dnn tee intel sgx sanctum faster yet untrusted co located processor evaluate slalom running dnns intel sgx enclave selectively delegates work untrusted gpu canonical dnns vgg mobilenet resnet variants obtain increases throughput verifiable inference verifiable private inference\n",
            "output sentence:  accelerate secure dnn inference trusted execution environments factor selectively outsourcing computation linear layers faster yet untrusted co processor \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.4166666666666667, 'f': 0.1818181784066116}, 'rouge-2': {'r': 0.0392156862745098, 'p': 0.16666666666666666, 'f': 0.06349206040816341}, 'rouge-l': {'r': 0.09302325581395349, 'p': 0.3333333333333333, 'f': 0.14545454204297528}}\n",
            "pair:  introduce simple efficient algorithms computing minhash probability distribution suitable sparse dense data equivalent running times state art cases collision probability algorithms new measure similarity positive vectors investigate detail describe sense collision probability optimal locality sensitive hash based sampling argue similarity measure useful probability distributions similarity pursued algorithms weighted minhash natural generalization jaccard index\n",
            "output sentence:  minimum set exponentially distributed hashes useful collision probability generalizes jaccard index probability distributions \n",
            "\n",
            "{'rouge-1': {'r': 0.0847457627118644, 'p': 0.2777777777777778, 'f': 0.12987012628773834}, 'rouge-2': {'r': 0.014084507042253521, 'p': 0.05263157894736842, 'f': 0.022222218891358527}, 'rouge-l': {'r': 0.05084745762711865, 'p': 0.16666666666666666, 'f': 0.07792207433968645}}\n",
            "pair:  field shot learning recently seen substantial advancements advancements came casting shot learning meta learning problem model agnostic meta learning maml currently one best approaches shot learning via meta learning maml simple elegant powerful however variety issues sensitive neural network architectures often leading instability training requiring arduous hyperparameter searches stabilize training achieve high generalization computationally expensive training inference times paper propose various modifications maml stabilize system also substantially improve generalization performance convergence speed computational overhead maml call maml\n",
            "output sentence:  maml great many problems solve many problems result learn hyper parameters end end speed training inference set new sota shot learning \n",
            "\n",
            "{'rouge-1': {'r': 0.11764705882352941, 'p': 0.5714285714285714, 'f': 0.1951219483878644}, 'rouge-2': {'r': 0.0375, 'p': 0.23076923076923078, 'f': 0.06451612662735585}, 'rouge-l': {'r': 0.08823529411764706, 'p': 0.42857142857142855, 'f': 0.14634146058298636}}\n",
            "pair:  likelihood based generative models promising resource detect distribution ood inputs could compromise robustness reliability machine learning system however likelihoods derived models shown problematic detecting certain types inputs significantly differ training data paper pose problem due excessive influence input complexity generative models likelihoods report set experiments supporting hypothesis use estimate input complexity derive efficient parameter free ood score seen likelihood ratio akin bayesian model comparison find score perform comparably even better existing ood detection approaches wide range data sets models model sizes complexity estimates\n",
            "output sentence:  pose generative models likelihoods excessively influenced input complexity propose way compensate detecting distribution inputs \n",
            "\n",
            "{'rouge-1': {'r': 0.09876543209876543, 'p': 0.6666666666666666, 'f': 0.1720430085050295}, 'rouge-2': {'r': 0.03125, 'p': 0.2727272727272727, 'f': 0.05607476451043766}, 'rouge-l': {'r': 0.08641975308641975, 'p': 0.5833333333333334, 'f': 0.15053763216094346}}\n",
            "pair:  class labels empirically shown useful improving sample quality generative adversarial nets gans paper mathematically study properties current variants gans make use class label information class aware gradient cross entropy decomposition reveal class labels associated losses influence gan training based propose activation maximization generative adversarial networks gan advanced solution comprehensive experiments conducted validate analysis evaluate effectiveness solution gan outperforms strong baselines achieves state art inception score cifar addition demonstrate inception imagenet classifier inception score mainly tracks diversity generator however reliable evidence reflect true sample quality thus propose new metric called score provide accurate estimation sample quality proposed model also outperforms baseline methods new metric\n",
            "output sentence:  understand class labels help gan training propose new evaluation metric generative models \n",
            "\n",
            "{'rouge-1': {'r': 0.16417910447761194, 'p': 0.8461538461538461, 'f': 0.2749999972781251}, 'rouge-2': {'r': 0.1282051282051282, 'p': 0.7692307692307693, 'f': 0.21978021733124017}, 'rouge-l': {'r': 0.16417910447761194, 'p': 0.8461538461538461, 'f': 0.2749999972781251}}\n",
            "pair:  reinforcement learning agent needs pursue different goals across episodes requires goal conditional policy addition potential generalize desirable behavior unseen goals policies may also enable higher level planning based subgoals sparse reward environments capacity exploit information degree arbitrary goal achieved another goal intended appears crucial enable sample efficient learning however reinforcement learning agents recently endowed capacity hindsight paper demonstrate hindsight introduced policy gradient methods generalizing idea broad class successful algorithms experiments diverse selection sparse reward environments show hindsight leads remarkable increase sample efficiency\n",
            "output sentence:  introduce capacity exploit information degree arbitrary goal achieved another goal intended policy gradient methods \n",
            "\n",
            "{'rouge-1': {'r': 0.05333333333333334, 'p': 1.0, 'f': 0.10126582182342576}, 'rouge-2': {'r': 0.02127659574468085, 'p': 0.5, 'f': 0.04081632574760517}, 'rouge-l': {'r': 0.05333333333333334, 'p': 1.0, 'f': 0.10126582182342576}}\n",
            "pair:  reparameterization trick become one useful tools field variational inference however reparameterization trick based standardization transformation restricts scope application method distributions tractable inverse cumulative distribution functions expressible deterministic transformations distributions paper generalized reparameterization trick allowing general transformation unlike similar works develop generalized transformation based gradient model formally rigorously discover proposed model special case control variate indicating proposed model combine advantages cv generalized reparameterization based proposed gradient model propose new polynomial based gradient estimator better theoretical performance reparameterization trick certain condition applied larger class variational distributions studies synthetic real data show proposed gradient estimator significantly lower gradient variance state art methods thus enabling faster inference procedure\n",
            "output sentence:  generalized transformation generalized transformation based variational model \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.5, 'f': 0.11999999788800003}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.3333333333333333, 'f': 0.07999999788800007}}\n",
            "pair:  paper puts forward new text tensor representation relies information compression techniques assign shorter codes frequently used characters representation language independent need pretraining produces encoding information loss provides adequate description morphology text able represent prefixes declensions inflections similar vectors able represent even unseen words training dataset similarly compact yet sparse ideal speed training times using tensor processing libraries part paper show technique especially effective coupled convolutional neural networks cnns text classification character level apply two variants cnn coupled experimental results show drastically reduces number parameters optimized resulting competitive classification accuracy values fraction time spent one hot encoding representations thus enabling training commodity hardware\n",
            "output sentence:  using compressing tecniques encoding words possibility faster training cnn dimensionality reduction representation \n",
            "\n",
            "{'rouge-1': {'r': 0.029850746268656716, 'p': 0.4, 'f': 0.05555555426311731}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.014925373134328358, 'p': 0.2, 'f': 0.027777776485339568}}\n",
            "pair:  neural conversational models widely used applications like personal assistants chat bots models seem give better performance operating word level however fusion languages like french russian polish vocabulary size sometimes become infeasible since words lots word forms propose neural network architecture transforming normalized text grammatically correct one model efficiently employs correspondence normalized target words significantly outperforms character level models faster training faster evaluation also propose new pipeline building conversational models first generate normalized answer transform grammatically correct one using network proposed pipeline gives better performance character level conversational models according assessor testing\n",
            "output sentence:  proposed architecture solve morphological agreement task \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.7272727272727273, 'f': 0.19512194889649023}, 'rouge-2': {'r': 0.053763440860215055, 'p': 0.5, 'f': 0.09708737688754834}, 'rouge-l': {'r': 0.11267605633802817, 'p': 0.7272727272727273, 'f': 0.19512194889649023}}\n",
            "pair:  imitation learning aims inversely learn policy expert demonstrations extensively studied literature single agent setting markov decision process mdp model multi agent setting markov game mg model however existing approaches general multi agent markov games applicable multi agent extensive markov games agents make asynchronous decisions following certain order rather simultaneous decisions propose novel framework asynchronous multi agent generative adversarial imitation learning amagail general extensive markov game settings learned expert policies proven guarantee subgame perfect equilibrium spe general stronger equilibrium nash equilibrium ne experiment results demonstrate compared state art baselines amagail model better infer policy expert agent using demonstration data collected asynchronous decision making scenarios extensive markov games\n",
            "output sentence:  paper extends multi agent generative adversarial imitation learning extensive form markov games \n",
            "\n",
            "{'rouge-1': {'r': 0.11428571428571428, 'p': 0.8888888888888888, 'f': 0.20253164355071304}, 'rouge-2': {'r': 0.08045977011494253, 'p': 0.875, 'f': 0.14736841951024932}, 'rouge-l': {'r': 0.11428571428571428, 'p': 0.8888888888888888, 'f': 0.20253164355071304}}\n",
            "pair:  paper consider problem detecting object occlusion object detectors formulate bounding box regression unimodal task regressing single set bounding box coordinates independently however observe bounding box borders occluded object multiple plausible configurations also occluded bounding box borders correlations visible ones motivated two observations propose deep multivariate mixture gaussians model bounding box regression occlusion mixture components potentially learn different configurations occluded part covariances variates help learn relationship occluded parts visible ones quantitatively model improves ap baselines crowdhuman ms coco respectively almost computational memory overhead qualitatively model enjoys explainability since interpret resulting bounding boxes via covariance matrices mixture components\n",
            "output sentence:  deep multivariate mixture gaussians model bounding box regression occlusion \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.75, 'f': 0.1499999982}, 'rouge-2': {'r': 0.046511627906976744, 'p': 0.5714285714285714, 'f': 0.08602150398427566}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.75, 'f': 0.1499999982}}\n",
            "pair:  study benefit sharing representations among tasks enable effective use deep neural networks multi task reinforcement learning leverage assumption learning different tasks sharing common properties helpful generalize knowledge resulting effective feature extraction compared learning single task intuitively resulting set features offers performance benefits used reinforcement learning algorithms prove providing theoretical guarantees highlight conditions convenient share representations among tasks extending well known finite time bounds approximate value iteration multi task setting addition complement analysis proposing multi task extensions three reinforcement learning algorithms empirically evaluate widely used reinforcement learning benchmarks showing significant improvements single task counterparts terms sample efficiency performance\n",
            "output sentence:  study benefit sharing representation multi task reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.0851063829787234, 'p': 0.5, 'f': 0.14545454296859506}, 'rouge-2': {'r': 0.01694915254237288, 'p': 0.125, 'f': 0.02985074416573862}, 'rouge-l': {'r': 0.07446808510638298, 'p': 0.4375, 'f': 0.12727272478677687}}\n",
            "pair:  implementing correct method invocation important task software developers however challenging work since structure method invocation complicated paper propose invocmap code completion tool allows developers obtain implementation multiple method invocations list method names inside code context invocmap able predict nested method invocations names appear list input method names given developers achieve analyze method invocations four levels abstraction build machine translation engine learn mapping first level third level abstraction multiple method invocations requires developers manually add local variables generated expression get final code evaluate proposed approach six popular libraries jdk android gwt joda time hibernate xstream training corpus million method invocations extracted java github projects testing corpus extracted online forums code snippets invocmap achieves accuracy rate score depending much information context provided along method names shows potential auto code completion\n",
            "output sentence:  paper proposes theory classifying method invocations different abstraction levels conducting statistical approach code completion method name method \n",
            "\n",
            "{'rouge-1': {'r': 0.2028985507246377, 'p': 0.9333333333333333, 'f': 0.3333333303996599}, 'rouge-2': {'r': 0.17073170731707318, 'p': 0.9333333333333333, 'f': 0.28865979119991503}, 'rouge-l': {'r': 0.2028985507246377, 'p': 0.9333333333333333, 'f': 0.3333333303996599}}\n",
            "pair:  operating deep neural networks devices limited resources requires reduction memory footprints computational requirements paper introduce training method called look table quantization lut learns dictionary assigns weight one dictionary values show method flexible many techniques seen special cases lut example constrain dictionary trained lut generate networks pruned weight matrices restrict dictionary powers two avoid need multiplications order obtain fully multiplier less networks also introduce multiplier less version batch normalization extensive experiments image recognition object detection tasks show lut consistently achieves better performance methods quantization bitwidth\n",
            "output sentence:  paper introduce training method called look table quantization lut learns dictionary assigns weight one dictionary values \n",
            "\n",
            "{'rouge-1': {'r': 0.08108108108108109, 'p': 0.75, 'f': 0.1463414616537775}, 'rouge-2': {'r': 0.044642857142857144, 'p': 0.7142857142857143, 'f': 0.08403361233811173}, 'rouge-l': {'r': 0.08108108108108109, 'p': 0.75, 'f': 0.1463414616537775}}\n",
            "pair:  conventional deep learning classifiers static sense trained predefined set classes learning classify novel class typically requires training work address problem low shot network expansion learning introduce learning framework enables expanding pre trained base deep network classify novel classes number examples novel classes particularly small present simple yet powerful distillation method base network augmented additional weights classify novel classes keeping weights base network unchanged term learning hard distillation since preserve response network old classes equal base expanded network show since small number weights needs trained hard distillation excels low shot training scenarios furthermore hard distillation avoids detriment classification performance base classes finally show low shot network expansion done small memory footprint using compact generative model base classes training data negligible degradation relative learning full training set\n",
            "output sentence:  paper address problem low shot network expansion learning \n",
            "\n",
            "{'rouge-1': {'r': 0.109375, 'p': 0.6363636363636364, 'f': 0.18666666416355557}, 'rouge-2': {'r': 0.013888888888888888, 'p': 0.1, 'f': 0.024390241760856822}, 'rouge-l': {'r': 0.078125, 'p': 0.45454545454545453, 'f': 0.13333333083022222}}\n",
            "pair:  obtaining high quality uncertainty estimates essential many applications deep neural networks paper theoretically justify scheme estimating uncertainties based sampling prior distribution crucially uncertainty estimates shown conservative sense never underestimate posterior uncertainty obtained hypothetical bayesian algorithm also show concentration implying uncertainty estimates converge zero get data uncertainty estimates obtained random priors adapted deep network architecture trained using standard supervised learning pipelines provide experimental evaluation random priors calibration distribution detection typical computer vision tasks demonstrating outperform deep ensembles practice\n",
            "output sentence:  provide theoretical support uncertainty estimates deep learning obtained fitting random priors \n",
            "\n",
            "{'rouge-1': {'r': 0.16129032258064516, 'p': 0.7894736842105263, 'f': 0.2678571400398597}, 'rouge-2': {'r': 0.0546875, 'p': 0.3684210526315789, 'f': 0.09523809298718133}, 'rouge-l': {'r': 0.13978494623655913, 'p': 0.6842105263157895, 'f': 0.23214285432557397}}\n",
            "pair:  zero shot learning zsl classification task classes referred unseen classes labeled training images instead side information description seen unseen classes often form semantic descriptive attributes lack training images set classes restricts use standard classification techniques losses including popular cross entropy loss key step tackling zsl problem bridging visual semantic space via learning nonlinear embedding well established approach obtain semantic representation visual information perform classification semantic space paper propose novel architecture casting zsl fully connected neural network cross entropy loss embed visual space semantic space training order introduce unseen visual information network utilize soft labeling based semantic similarities seen unseen classes best knowledge similarity based soft labeling explored cross modal transfer zsl evaluate proposed model five benchmark datasets zero shot learning awa awa apy sun cub datasets show despite simplicity approach achieves state art performance generalized zsl setting datasets outperforms state art datasets\n",
            "output sentence:  use cross entropy loss zero shot learning soft labeling unseen classes simple effective solution achieves state performance zsl performance datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.03389830508474576, 'p': 0.4, 'f': 0.06249999855957035}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03389830508474576, 'p': 0.4, 'f': 0.06249999855957035}}\n",
            "pair:  simultaneous machine translation models start generating target sequence encoded read source sequence recent approach task either apply fixed policy transformer learnable monotonic attention weaker recurrent neural network based structure paper propose new attention mechanism monotonic multihead attention mma introduced monotonic attention mechanism multihead attention also introduced two novel interpretable approaches latency control specifically designed multiple attentions apply mma simultaneous machine translation task demonstrate better latency quality tradeoffs compared milk previous state art approach code released upon publication\n",
            "output sentence:  make transformer streamable monotonic attention \n",
            "\n",
            "{'rouge-1': {'r': 0.15384615384615385, 'p': 0.5333333333333333, 'f': 0.23880596667409226}, 'rouge-2': {'r': 0.01639344262295082, 'p': 0.07142857142857142, 'f': 0.02666666363022257}, 'rouge-l': {'r': 0.057692307692307696, 'p': 0.2, 'f': 0.08955223533080878}}\n",
            "pair:  generative networks promising models specifying visual transformations unfortunately certification generative models challenging one needs capture sufficient non convexity produce precise bounds output existing verification methods either fail scale generative networks capture enough non convexity work present new verifier called approxline certify non trivial properties generative networks approxline performs deterministic probabilistic abstract interpretation captures infinite sets outputs generative networks show approxline verify interesting interpolations network latent space\n",
            "output sentence:  verify deterministic probabilistic properties neural networks using non convex relaxations visible transformations specified generative models \n",
            "\n",
            "{'rouge-1': {'r': 0.05660377358490566, 'p': 0.6, 'f': 0.10344827428656364}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.03773584905660377, 'p': 0.4, 'f': 0.068965515665874}}\n",
            "pair:  present deep neural network spike assisted feature extraction safe dnn improve robustness classification stochastic perturbation inputs proposed network augments dnn unsupervised learning low level features using spiking neuron network snn spike time dependent plasticity stdp complete network learns ignore local perturbation performing global feature detection classification experimental results cifar imagenet subset demonstrate improved noise robustness multiple dnn architectures without sacrificing accuracy clean images\n",
            "output sentence:  noise robust deep learning architecture \n",
            "\n",
            "{'rouge-1': {'r': 0.07246376811594203, 'p': 0.5, 'f': 0.1265822762698286}, 'rouge-2': {'r': 0.012658227848101266, 'p': 0.1111111111111111, 'f': 0.022727270891012546}, 'rouge-l': {'r': 0.057971014492753624, 'p': 0.4, 'f': 0.10126582057362607}}\n",
            "pair:  reinforcement learning rl frequently used increase performance text generation tasks including machine translation mt notably use minimum risk training mrt generative adversarial networks gan however little known methods learn context mt prove one common rl methods mt optimize expected reward well show methods take infeasibly long time converge fact results suggest rl practices mt likely improve performance pre trained parameters already close yielding correct translation findings suggest observed gains may due effects unrelated training signal concretely changes shape distribution curve\n",
            "output sentence:  reinforcment practices machine translation performance gains might come better predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.06896551724137931, 'p': 0.8571428571428571, 'f': 0.12765957308963333}, 'rouge-2': {'r': 0.02830188679245283, 'p': 0.5, 'f': 0.05357142755739797}, 'rouge-l': {'r': 0.05747126436781609, 'p': 0.7142857142857143, 'f': 0.10638297734495247}}\n",
            "pair:  worst case training principle minimizes maximal adversarial loss also known adversarial training shown state art approach enhancing adversarial robustness norm ball bounded input perturbations nonetheless min max optimization beyond purpose rigorously explored research adversarial attack defense particular given set risk sources domains minimizing maximal loss induced domain set reformulated general min max problem different examples general formulation include attacking model ensembles devising universal perturbation multiple inputs data transformations generalized different types attack models show problems solved unified theoretically principled min max optimization framework also show self adjusted domain weights learned method provides means explain difficulty level attack defense multiple domains extensive experiments show approach leads substantial performance improvement conventional averaging strategy\n",
            "output sentence:  unified min max optimization framework adversarial attack \n",
            "\n",
            "{'rouge-1': {'r': 0.16279069767441862, 'p': 0.7777777777777778, 'f': 0.26923076636834326}, 'rouge-2': {'r': 0.06140350877192982, 'p': 0.4117647058823529, 'f': 0.10687022674902401}, 'rouge-l': {'r': 0.13953488372093023, 'p': 0.6666666666666666, 'f': 0.23076922790680476}}\n",
            "pair:  hashing based collaborative filtering learns binary vector representations hash codes users items recommendations computed efficiently using hamming distance simply sum differing bits two hash codes problem hashing based collaborative filtering using hamming distance bit equally weighted distance computation practice bits might encode important properties bits importance depends user end propose end end trainable variational hashing based collaborative filtering approach uses novel concept self masking user hash code acts mask items using boolean operation learns encode bits important user rather user preference towards underlying item property bits represent allows binary user level importance weighting item without need store additional weights user experimentally evaluate approach state art baselines datasets obtain significant gains ndcg also make available efficient implementation self masking experimentally yields runtime overhead compared standard hamming distance\n",
            "output sentence:  propose new variational hashing based collaborative filtering approach optimized novel self mask variant hamming distance outperforms state art ndcg \n",
            "\n",
            "{'rouge-1': {'r': 0.07142857142857142, 'p': 0.5, 'f': 0.12499999781250003}, 'rouge-2': {'r': 0.0125, 'p': 0.1111111111111111, 'f': 0.02247190829440742}, 'rouge-l': {'r': 0.04285714285714286, 'p': 0.3, 'f': 0.07499999781250007}}\n",
            "pair:  inverse problems ubiquitous natural sciences refer challenging task inferring complex potentially multi modal posterior distributions hidden parameters given set observations typically model physical process form differential equations available leads intractable inference parameters forward propagation parameters model simulates evolution system inverse problem finding parameters given sequence states unique work propose generalisation bayesian optimisation framework approximate inference resulting method learns approximations posterior distribution applying stein variational gradient descent top estimates gaussian process model preliminary results demonstrate method performance likelihood free inference reinforcement learning environments\n",
            "output sentence:  approach combine variational inference bayesian optimisation solve complicated inverse problems \n",
            "\n",
            "{'rouge-1': {'r': 0.08045977011494253, 'p': 0.875, 'f': 0.14736841951024932}, 'rouge-2': {'r': 0.03773584905660377, 'p': 0.5714285714285714, 'f': 0.07079645901480149}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.75, 'f': 0.12631578793130194}}\n",
            "pair:  worst case training principle minimizes maximal adversarial loss also known adversarial training shown state art approach enhancing adversarial robustness norm ball bounded input perturbations nonetheless min max optimization beyond purpose rigorously explored research adversarial attack defense particular given set risk sources domains minimizing maximal loss induced domain set reformulated general min max problem different examples general formulation include attacking model ensembles devising universal perturbation multiple inputs data transformations generalized different types attack models show problems solved unified theoretically principled min max optimization framework also show self adjusted domain weights learned method provides means explain difficulty level attack defense multiple domains extensive experiments show approach leads substantial performance improvement conventional averaging strategy\n",
            "output sentence:  unified min max optimization framework adversarial attack defense \n",
            "\n",
            "{'rouge-1': {'r': 0.03669724770642202, 'p': 0.4444444444444444, 'f': 0.06779660876041371}, 'rouge-2': {'r': 0.0070921985815602835, 'p': 0.1111111111111111, 'f': 0.013333332205333429}, 'rouge-l': {'r': 0.027522935779816515, 'p': 0.3333333333333333, 'f': 0.050847456218040835}}\n",
            "pair:  artificial intelligence ai becomes integral part life development explainable ai embodied decision making process ai robotic agent becomes imperative robotic teammate ability generate explanations explain behavior one key requirements explainable agency prior work explanation generation focuses supporting reasoning behind robot behavior approaches however fail consider mental workload needed understand received explanation words human teammate expected understand explanation provided often task execution matter much information presented explanation work argue explanation especially complex ones made online fashion execution helps spread information explained thus reducing mental workload humans however challenge different parts explanation dependent must taken account generating online explanations end general formulation online explanation generation presented along three different implementations satisfying different online properties base explanation generation method model reconciliation setting introduced prior work approaches evaluated human subjects standard planning competition ipc domain using nasa task load index tlx well simulation ten different problems across two ipc domains\n",
            "output sentence:  introduce online explanation consider cognitive requirement human understanding generated explanation agent \n",
            "\n",
            "{'rouge-1': {'r': 0.16363636363636364, 'p': 0.9, 'f': 0.2769230743195267}, 'rouge-2': {'r': 0.039473684210526314, 'p': 0.3, 'f': 0.06976743980530022}, 'rouge-l': {'r': 0.14545454545454545, 'p': 0.8, 'f': 0.24615384355029585}}\n",
            "pair:  propose end end framework training domain specific models dsms obtain high accuracy computational efficiency object detection tasks dsms trained distillation focus achieving high accuracy limited domain fixed view intersection argue dsms capture essential features well even small model size enabling higher accuracy efficiency traditional techniques addition improve training efficiency reducing dataset size culling easy classify images training set limited domain observed compact dsms significantly surpass accuracy coco trained models size training compact dataset show accuracy drop training time reduced\n",
            "output sentence:  high object detection accuracy obtained training domain specific compact models training short \n",
            "\n",
            "{'rouge-1': {'r': 0.07936507936507936, 'p': 0.625, 'f': 0.14084506842293198}, 'rouge-2': {'r': 0.012987012987012988, 'p': 0.125, 'f': 0.023529410059515695}, 'rouge-l': {'r': 0.06349206349206349, 'p': 0.5, 'f': 0.11267605433842494}}\n",
            "pair:  propose cooperative training cot training generative models measure tractable density discrete data cot coordinately trains generator auxiliary predictive mediator training target estimate mixture density learned distribution target distribution minimize jensen shannon divergence estimated cot achieves independent success without necessity pre training via maximum likelihood estimation involving high variance algorithms like reinforce low variance algorithm theoretically proved superior sample generation likelihood prediction also theoretically empirically show superiority cot previous algorithms terms generative quality diversity predictive generalization ability computational cost\n",
            "output sentence:  proposed cooperative training novel training algorithm generative modeling discrete data \n",
            "\n",
            "{'rouge-1': {'r': 0.10309278350515463, 'p': 0.7142857142857143, 'f': 0.18018017797581368}, 'rouge-2': {'r': 0.03007518796992481, 'p': 0.2857142857142857, 'f': 0.05442176698412704}, 'rouge-l': {'r': 0.061855670103092786, 'p': 0.42857142857142855, 'f': 0.10810810590374163}}\n",
            "pair:  stochastic gradient descent sgd trades noisy gradient updates computational efficiency de facto optimization algorithm solve large scale machine learning problems sgd make rapid learning progress performing updates using subsampled training data noisy updates also lead slow asymptotic convergence several variance reduction algorithms svrg introduce control variates obtain lower variance gradient estimate faster convergence despite appealing asymptotic guarantees svrg like algorithms widely adopted deep learning traditional asymptotic analysis stochastic optimization provides limited insight training deep learning models fixed number epochs paper present non asymptotic analysis svrg noisy least squares regression problem primary focus compare exact loss svrg sgd iteration show learning dynamics regression model closely matches neural networks mnist cifar underparameterized overparameterized models analysis experimental results suggest trade computational cost convergence speed underparametrized neural networks svrg outperforms sgd epochs regime however sgd shown always outperform svrg overparameterized regime\n",
            "output sentence:  non asymptotic analysis sgd svrg showing strength algorithm convergence speed computational cost parametrized parametrized settings \n",
            "\n",
            "{'rouge-1': {'r': 0.10526315789473684, 'p': 0.8571428571428571, 'f': 0.18749999805175782}, 'rouge-2': {'r': 0.03076923076923077, 'p': 0.3333333333333333, 'f': 0.05633802662170209}, 'rouge-l': {'r': 0.08771929824561403, 'p': 0.7142857142857143, 'f': 0.15624999805175782}}\n",
            "pair:  large transformer models routinely achieve state art results number tasks training models prohibitively costly especially long sequences introduce two techniques improve efficiency transformers one replace dot product attention one uses locality sensitive hashing changing complexity length sequence furthermore use reversible residual layers instead standard residuals allows storing activations training process instead times number layers resulting model reformer performs par transformer models much memory efficient much faster long sequences\n",
            "output sentence:  efficient transformer locality sensitive hashing reversible layers \n",
            "\n",
            "{'rouge-1': {'r': 0.12941176470588237, 'p': 0.55, 'f': 0.20952380643990937}, 'rouge-2': {'r': 0.06862745098039216, 'p': 0.35, 'f': 0.11475409561945722}, 'rouge-l': {'r': 0.11764705882352941, 'p': 0.5, 'f': 0.19047618739229027}}\n",
            "pair:  stochastic auc maximization garnered increasing interest due better fit imbalanced data classification however existing works limited stochastic auc maximization linear predictive model restricts predictive power dealing extremely complex data paper consider stochastic auc maximization problem deep neural network predictive model building saddle point reformulation surrogated loss auc problem cast non convex concave min max problem main contribution made paper make stochastic auc maximization practical deep neural networks big data theoretical insights well particular propose explore polyak ojasiewicz pl condition proved observed deep learning enables us develop new stochastic algorithms even faster convergence rate practical step size scheme adagrad style algorithm also analyzed pl condition adaptive convergence rate experimental results demonstrate effectiveness proposed algorithms\n",
            "output sentence:  paper designs two algorithms stochastic auc maximization problem state art complexities using deep neural network predictive model verified empirical empirical studies \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.75, 'f': 0.11538461396449705}, 'rouge-2': {'r': 0.022900763358778626, 'p': 0.375, 'f': 0.043165466541069335}, 'rouge-l': {'r': 0.041666666666666664, 'p': 0.5, 'f': 0.0769230755029586}}\n",
            "pair:  unpaired image image translation among category domains achieved remarkable success past decades recent studies mainly focus two challenges one thing translation inherently multimodal due variations domain specific information domain house cat multiple fine grained subcategories another existing multimodal approaches limitations handling two domains independently build one model every pair domains address problems propose hierarchical image image translation hit method jointly formulates multimodal multi domain problem semantic hierarchy structure control uncertainty multimodal specifically regard domain specific variations result multi granularity property domains one control granularity multimodal translation dividing domain large variations multiple subdomains capture local fine grained variations assumption gaussian prior variations domains modeled common space translations done among multiple domains within one model learn complicated space propose leverage inclusion relation among domains constrain distributions parent children nested experiments several datasets validate promising results competitive performance state arts\n",
            "output sentence:  granularity controled multi domain multimodal image image translation method \n",
            "\n",
            "{'rouge-1': {'r': 0.12307692307692308, 'p': 0.7272727272727273, 'f': 0.21052631331371197}, 'rouge-2': {'r': 0.036585365853658534, 'p': 0.3, 'f': 0.06521738936672974}, 'rouge-l': {'r': 0.1076923076923077, 'p': 0.6363636363636364, 'f': 0.18421052384002776}}\n",
            "pair:  paper propose new loss function performing principal component analysis pca using linear autoencoders laes optimizing standard loss results decoder matrix spans principal subspace sample covariance data fails identify exact eigenvectors downside originates invariance cancels global map prove loss function eliminates issue decoder converges exact ordered unnormalized eigenvectors sample covariance matrix new loss establish local minima global optima also show computing new loss also gradients order complexity classical loss report numerical results synthetic simulations real data pca experiment mnist matrix demonstrating approach practically applicable rectify previous laes downsides\n",
            "output sentence:  new loss function pca linear autoencoders provably yields ordered exact eigenvectors \n",
            "\n",
            "{'rouge-1': {'r': 0.11475409836065574, 'p': 0.7, 'f': 0.1971830961713946}, 'rouge-2': {'r': 0.057971014492753624, 'p': 0.4444444444444444, 'f': 0.10256410052268247}, 'rouge-l': {'r': 0.08196721311475409, 'p': 0.5, 'f': 0.1408450680023805}}\n",
            "pair:  paper propose view acceptance rate metropolis hastings algorithm universal objective learning sample target distribution given either set samples form unnormalized density point view unifies goals approaches markov chain monte carlo mcmc generative adversarial networks gans variational inference reveal connection derive lower bound acceptance rate treat objective learning explicit implicit samplers form lower bound allows doubly stochastic gradient optimization case target distribution factorizes data points empirically validate approach bayesian inference neural networks generative models images\n",
            "output sentence:  learning sample via lower bounding acceptance rate metropolis hastings algorithm \n",
            "\n",
            "{'rouge-1': {'r': 0.08247422680412371, 'p': 0.8888888888888888, 'f': 0.1509433946724813}, 'rouge-2': {'r': 0.03508771929824561, 'p': 0.5, 'f': 0.06557376926632627}, 'rouge-l': {'r': 0.041237113402061855, 'p': 0.4444444444444444, 'f': 0.07547169655927378}}\n",
            "pair:  ever increasing demand resultant reduced quality services focus shifted towards easing network congestion enable efficient flow systems like traffic supply chains electrical grids step direction imagine traditional heuristics based training systems approach incapable modelling involved dynamics one apply multi agent reinforcement learning marl problems considering vertex network agent marl based models assume agents independent many real world tasks agents need behave group rather collection individuals paper propose framework induces cooperation coordination amongst agents connected via underlying network using emergent communication marl based setup formulate problem general network setting demonstrate utility communication networks help case study traffic systems furthermore study emergent communication protocol show formation distinct communities grounded vocabulary best knowledge work studies emergent language networked marl setting\n",
            "output sentence:  framework studying emergent communication networked multi agent reinforcement learning setup \n",
            "\n",
            "{'rouge-1': {'r': 0.0625, 'p': 0.375, 'f': 0.1071428546938776}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0625, 'p': 0.375, 'f': 0.1071428546938776}}\n",
            "pair:  key equatorial climate phenomena qbo enso never adequately explained deterministic processes spite recent research showing growing evidence predictable behavior study applies fundamental laplace tidal equations simplifying assumptions along equator coriolis force small angle approximation solutions partial differential equations highly non linear related navier stokes search approaches used fit data\n",
            "output sentence:  analytical formulation equatorial standing wave phenomena application qbo enso \n",
            "\n",
            "{'rouge-1': {'r': 0.0379746835443038, 'p': 0.375, 'f': 0.06896551557140974}, 'rouge-2': {'r': 0.008849557522123894, 'p': 0.14285714285714285, 'f': 0.016666665568055627}, 'rouge-l': {'r': 0.0379746835443038, 'p': 0.375, 'f': 0.06896551557140974}}\n",
            "pair:  generative networks known difficult assess recent works generative models especially generative adversarial networks produce nice samples varied categories images validation quality highly dependent method used good generator generate data contain meaningful varied information fit distribution dataset paper presents new method assess generator approach based training classifier mixture real generated samples train generative model labeled training set use generative model sample new data points mix original training data mixture real generated data thus used train classifier afterwards tested given labeled test dataset compare result score classifier trained real training data mixed noise computing classifier accuracy different ratios samples distributions real generated able estimate generator successfully fits able generalize distribution dataset experiments compare result different generators vae gan framework mnist fashion mnist dataset\n",
            "output sentence:  evaluating generative networks data augmentation capacity discrimative models \n",
            "\n",
            "{'rouge-1': {'r': 0.2127659574468085, 'p': 0.5555555555555556, 'f': 0.307692303687574}, 'rouge-2': {'r': 0.04918032786885246, 'p': 0.17647058823529413, 'f': 0.07692307351413559}, 'rouge-l': {'r': 0.1702127659574468, 'p': 0.4444444444444444, 'f': 0.24615384214911246}}\n",
            "pair:  reservoir computing powerful tool explain brain learns temporal sequences movements existing learning schemes either biologically implausible inefficient explain animal performance show network learn complicated sequences reward modulated hebbian learning rule network reservoir neurons combined second network serves dynamic working memory provides spatio temporal backbone signal reservoir combination working memory reward modulated hebbian learning readout neurons performs well force learning advantage biologically plausible interpretation learning rule learning paradigm\n",
            "output sentence:  show working memory input reservoir network makes local reward modulated hebbian rule perform well recursive least squares aka force \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.6, 'f': 0.21428571135204083}, 'rouge-2': {'r': 0.018867924528301886, 'p': 0.1111111111111111, 'f': 0.03225806203433942}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.4, 'f': 0.14285713992346943}}\n",
            "pair:  neural networks known produce unexpected results inputs far training distribution one approach tackle problem detect samples trained network answer reliably odin recently proposed method distribution detection modify trained network achieves good performance various image classification tasks paper adapt odin sentence classification word tagging tasks show scores produced odin used confidence measure predictions distribution distribution datasets\n",
            "output sentence:  recent distribution detection method helps measure confidence rnn predictions nlp tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.45454545454545453, 'f': 0.1851851819410151}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.2, 'f': 0.07407407105624156}, 'rouge-l': {'r': 0.06976744186046512, 'p': 0.2727272727272727, 'f': 0.11111110786694109}}\n",
            "pair:  transforming graphical user interface screenshot created designer computer code typical task conducted developer order build customized software websites mobile applications paper show deep learning methods leveraged train model end end automatically generate code single input image accuracy three different platforms ios android web based technologies\n",
            "output sentence:  cnn lstm generate markup like code describing graphical user interface images \n",
            "\n",
            "{'rouge-1': {'r': 0.11594202898550725, 'p': 0.6666666666666666, 'f': 0.1975308616735254}, 'rouge-2': {'r': 0.025, 'p': 0.18181818181818182, 'f': 0.04395604183069689}, 'rouge-l': {'r': 0.10144927536231885, 'p': 0.5833333333333334, 'f': 0.17283950364883405}}\n",
            "pair:  recently progress made towards improving relational reasoning machine learning field among existing models graph neural networks gnns one effective approaches multi hop relational reasoning fact multi hop relational reasoning indispensable many natural language processing tasks relation extraction paper propose generate parameters graph neural networks gp gnns according natural language sentences enables gnns process relational reasoning unstructured text inputs verify gp gnns relation extraction text experimental results human annotated dataset two distantly supervised datasets show model achieves significant improvements compared baselines also perform qualitative analysis demonstrate model could discover accurate relations multi hop relational reasoning\n",
            "output sentence:  graph neural network model parameters generated natural languages perform multi hop reasoning \n",
            "\n",
            "{'rouge-1': {'r': 0.12048192771084337, 'p': 0.625, 'f': 0.20202019931027448}, 'rouge-2': {'r': 0.019230769230769232, 'p': 0.13333333333333333, 'f': 0.03361344317491717}, 'rouge-l': {'r': 0.0963855421686747, 'p': 0.5, 'f': 0.1616161589062341}}\n",
            "pair:  attentional rnn based encoder decoder models abstractive summarization achieved good performance short input output sequences longer documents summaries however models often include repetitive incoherent phrases introduce neural network model novel intra attention attends input continuously generated output separately new training method combines standard supervised word prediction reinforcement learning rl models trained supervised learning often exhibit exposure bias assume ground truth provided step training however standard word prediction combined global sequence prediction training rl resulting summaries become readable evaluate model cnn daily mail new york times datasets model obtains rouge score cnn daily mail dataset improvement previous state art models human evaluation also shows model produces higher quality summaries\n",
            "output sentence:  summarization model combining new intra attention reinforcement learning method increase summary rouge scores quality long sequences \n",
            "\n",
            "{'rouge-1': {'r': 0.08333333333333333, 'p': 0.5384615384615384, 'f': 0.14432989458603465}, 'rouge-2': {'r': 0.05309734513274336, 'p': 0.42857142857142855, 'f': 0.09448818701469405}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.5384615384615384, 'f': 0.14432989458603465}}\n",
            "pair:  clustering central task unsupervised learning data mining means one widely used clustering algorithms unfortunately generally non trivial extend means cluster data points beyond gaussian distribution particularly clusters non convex shapes beliakov king end first time introduce extreme value theory evt improve clustering ability means particularly euclidean space transformed novel probability space denoted extreme value space evt thus propose novel algorithm called extreme value means ev means including gev means gpd means addition also introduce tricks accelerate euclidean distance computation improving computational efficiency classical means furthermore ev means extended online version online extreme value means utilizing mini batch means cluster streaming data extensive experiments conducted validate ev means online ev means synthetic datasets real datasets experimental results show algorithms significantly outperform competitors cases\n",
            "output sentence:  paper introduces extreme value theory means measure similarity proposes novel algorithm called extreme value means clustering \n",
            "\n",
            "{'rouge-1': {'r': 0.14516129032258066, 'p': 0.6, 'f': 0.2337662306291112}, 'rouge-2': {'r': 0.04285714285714286, 'p': 0.21428571428571427, 'f': 0.07142856865079376}, 'rouge-l': {'r': 0.11290322580645161, 'p': 0.4666666666666667, 'f': 0.18181817868105923}}\n",
            "pair:  paper show simple coloring scheme improve theoretically empirically expressive power message passing neural networks mpnns specifically introduce graph neural network called colored local iterative procedure clip uses colors disambiguate identical node attributes show representation universal approximator continuous functions graphs node attributes method relies separability key topological characteristic allows extend well chosen neural networks universal representations finally show experimentally clip capable capturing structural characteristics traditional mpnns fail distinguish state art benchmark graph classification datasets\n",
            "output sentence:  paper introduces coloring scheme node disambiguation graph neural networks based separability proven universal mpnn extension \n",
            "\n",
            "{'rouge-1': {'r': 0.1935483870967742, 'p': 0.8, 'f': 0.31168830855118906}, 'rouge-2': {'r': 0.1125, 'p': 0.6, 'f': 0.18947368155124658}, 'rouge-l': {'r': 0.1935483870967742, 'p': 0.8, 'f': 0.31168830855118906}}\n",
            "pair:  self attention based transformer demonstrated state art performances number natural language processing tasks self attention able model long term dependencies may suffer extraction irrelevant information context tackle problem propose novel model called sparse transformer sparse transformer able improve concentration attention global context explicit selection relevant segments extensive experimental results series natural language processing tasks including neural machine translation image captioning language modeling demonstrate advantages sparse transformer model performance sparse transformer reaches state art performances iwslt english vietnamese translation iwslt german english translation addition conduct qualitative analysis account sparse transformer superior performance\n",
            "output sentence:  work propose sparse transformer improve concentration attention global context explicit selection relevant segments sequence sequence learning \n",
            "\n",
            "{'rouge-1': {'r': 0.0547945205479452, 'p': 0.5, 'f': 0.09876543031854902}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.0410958904109589, 'p': 0.375, 'f': 0.07407407229385768}}\n",
            "pair:  conventionally convolutional neural networks cnns process different images set filters however variations images pose challenge fashion paper propose generate sample specific filters convolutional layers forward pass since filters generated fly model becomes flexible better fit training data compared traditional cnns order obtain sample specific features extract intermediate feature maps autoencoder filters usually high dimensional propose learn set coefficients instead set filters coefficients used linearly combine base filters filter repository generate final filters cnn proposed method evaluated mnist mtfl cifar datasets experiment results demonstrate classification accuracy baseline model improved using proposed filter generation method\n",
            "output sentence:  dynamically generate filters conditioned input image cnns forward pass \n",
            "\n",
            "{'rouge-1': {'r': 0.06818181818181818, 'p': 0.5, 'f': 0.11999999788800003}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.045454545454545456, 'p': 0.3333333333333333, 'f': 0.07999999788800007}}\n",
            "pair:  paper puts forward new text tensor representation relies information compression techniques assign shorter codes frequently used characters representation language independent need pretraining produces encoding information loss provides adequate description morphology text able represent prefixes declensions inflections similar vectors able represent even unseen words training dataset similarly compact yet sparse ideal speed training times using tensor processing libraries part paper show technique especially effective coupled convolutional neural networks cnns text classification character level apply two variants cnn coupled experimental results show drastically reduces number parameters optimized resulting competitive classification accuracy values fraction time spent one hot encoding representations thus enabling training commodity hardware\n",
            "output sentence:  using compressing tecniques encoding words possibility faster training cnn dimensionality reduction representation \n",
            "\n",
            "{'rouge-1': {'r': 0.16417910447761194, 'p': 0.8461538461538461, 'f': 0.2749999972781251}, 'rouge-2': {'r': 0.1282051282051282, 'p': 0.7692307692307693, 'f': 0.21978021733124017}, 'rouge-l': {'r': 0.16417910447761194, 'p': 0.8461538461538461, 'f': 0.2749999972781251}}\n",
            "pair:  reinforcement learning agent needs pursue different goals across episodes requires goal conditional policy addition potential generalize desirable behavior unseen goals policies may also enable higher level planning based subgoals sparse reward environments capacity exploit information degree arbitrary goal achieved another goal intended appears crucial enable sample efficient learning however reinforcement learning agents recently endowed capacity hindsight paper demonstrate hindsight introduced policy gradient methods generalizing idea broad class successful algorithms experiments diverse selection sparse reward environments show hindsight leads remarkable increase sample efficiency\n",
            "output sentence:  introduce capacity exploit information degree arbitrary goal achieved another goal intended policy gradient methods \n",
            "\n",
            "{'rouge-1': {'r': 0.21428571428571427, 'p': 0.6666666666666666, 'f': 0.324324320642805}, 'rouge-2': {'r': 0.07936507936507936, 'p': 0.23809523809523808, 'f': 0.11904761529761916}, 'rouge-l': {'r': 0.08928571428571429, 'p': 0.2777777777777778, 'f': 0.1351351314536159}}\n",
            "pair:  recent advances illustrated often possible learn solve linear inverse problems imaging using training data outperform traditional regularized least squares solutions along lines present extensions neumann network recently introduced end end learned architecture inspired truncated neumann series expansion solution map regularized least squares problem summarize neumann network approach show form compatible optimal reconstruction function given inverse problem also investigate extension neumann network incorporates sample efficient patch based regularization approach\n",
            "output sentence:  neumann networks end end sample efficient learning approach solving linear inverse problems imaging compatible mse optimal approach admit admit extension learning learning \n",
            "\n",
            "{'rouge-1': {'r': 0.14285714285714285, 'p': 0.5263157894736842, 'f': 0.22471909776543367}, 'rouge-2': {'r': 0.03614457831325301, 'p': 0.16666666666666666, 'f': 0.05940593766493495}, 'rouge-l': {'r': 0.1, 'p': 0.3684210526315789, 'f': 0.15730336742835507}}\n",
            "pair:  study problem learning similarity functions large corpora using neural network embedding models models typically trained using sgd random sampling unobserved pairs sample size grows quadratically corpus size making expensive scale propose new efficient methods train models without sample unobserved pairs inspired matrix factorization approach relies adding global quadratic penalty expressing term inner product two generalized gramians show gradient term efficiently computed maintaining estimates gramians develop variance reduction schemes improve quality estimates conduct large scale experiments show significant improvement training time generalization performance compared sampling methods\n",
            "output sentence:  develop efficient methods train neural embedding models dot product structure reformulating objective function terms generalized gram matrices estimates matrices \n",
            "\n",
            "{'rouge-1': {'r': 0.0410958904109589, 'p': 0.3333333333333333, 'f': 0.07317072975312319}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.125, 'f': 0.01999999852800011}, 'rouge-l': {'r': 0.0410958904109589, 'p': 0.3333333333333333, 'f': 0.07317072975312319}}\n",
            "pair:  present information theoretic framework understanding trade offs unsupervised learning deep latent variables models using variational inference framework emphasizes need consider latent variable models along two dimensions ability reconstruct inputs distortion communication cost rate derive optimal frontier generative models two dimensional rate distortion plane show standard evidence lower bound objective insufficient select points along frontier however performing targeted optimization learn generative models different rates able learn many models achieve similar generative performance make vastly different trade offs terms usage latent variable experiments mnist omniglot variety architectures show framework sheds light many recent proposed extensions variational autoencoder family\n",
            "output sentence:  provide information theoretic experimental analysis state art variational autoencoders \n",
            "\n",
            "{'rouge-1': {'r': 0.0958904109589041, 'p': 0.5, 'f': 0.16091953752939625}, 'rouge-2': {'r': 0.010869565217391304, 'p': 0.07692307692307693, 'f': 0.019047616878004783}, 'rouge-l': {'r': 0.0958904109589041, 'p': 0.5, 'f': 0.16091953752939625}}\n",
            "pair:  multilingual neural machine translation nmt systems capable translating multiple source target languages within single system important indicator generalization within systems quality zero shot translation translating language pairs system never seen training however zero shot performance multilingual models lagged far behind quality achieved using two step translation process pivots intermediate language usually english work diagnose multilingual models perform zero shot settings propose explicit language invariance losses guide nmt encoder towards learning language agnostic representations proposed strategies significantly improve zero shot translation performance wmt english french german iwslt shared task first time match performance pivoting approaches maintaining performance supervised directions\n",
            "output sentence:  simple similarity constraints top multilingual nmt enables high quality translation unseen language pairs first time \n",
            "\n",
            "{'rouge-1': {'r': 0.21052631578947367, 'p': 0.9411764705882353, 'f': 0.3440860185177477}, 'rouge-2': {'r': 0.16279069767441862, 'p': 0.875, 'f': 0.27450980127643215}, 'rouge-l': {'r': 0.21052631578947367, 'p': 0.9411764705882353, 'f': 0.3440860185177477}}\n",
            "pair:  deep neural networks shown incredible performance inference tasks variety domains unfortunately current deep networks enormous cloud based structures require significant storage space limits scaling deep learning service dlaas use device augmented intelligence paper finds algorithms directly use lossless compressed representations deep feedforward networks synaptic weights drawn discrete sets perform inference without full decompression basic insight allows less rate naive approaches recognition bipartite graph layers feedforward networks kind permutation invariance labeling nodes terms inferential operation inference operation depends locally edges directly connected also provide experimental results approach mnist dataset\n",
            "output sentence:  paper finds algorithms directly use lossless compressed representations deep feedforward networks perform inference without full decompression decompression \n",
            "\n",
            "{'rouge-1': {'r': 0.07446808510638298, 'p': 0.5384615384615384, 'f': 0.13084111936064288}, 'rouge-2': {'r': 0.01680672268907563, 'p': 0.16666666666666666, 'f': 0.030534349480799576}, 'rouge-l': {'r': 0.06382978723404255, 'p': 0.46153846153846156, 'f': 0.11214953057559615}}\n",
            "pair:  large scale pre trained language model bert recently achieved great success wide range language understanding tasks however remains open question utilize bert text generation tasks paper present novel approach addressing challenge generic sequence sequence seq seq setting first propose new task conditional masked language modeling mlm enable fine tuning bert target text generation dataset fine tuned bert teacher exploited extra supervision improve conventional seq seq models student text generation leveraging bert idiosyncratic bidirectional nature distilling knowledge learned bert encourage auto regressive seq seq models plan ahead imposing global sequence level supervision coherent text generation experiments show proposed approach significantly outperforms strong baselines transformer multiple text generation tasks including machine translation mt text summarization proposed model also achieves new state art results iwslt german english english vietnamese mt datasets\n",
            "output sentence:  propose model agnostic way leverage bert text generation achieve improvements transformer tasks datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.13924050632911392, 'p': 0.7333333333333333, 'f': 0.23404255050928024}, 'rouge-2': {'r': 0.028846153846153848, 'p': 0.21428571428571427, 'f': 0.05084745553576567}, 'rouge-l': {'r': 0.11392405063291139, 'p': 0.6, 'f': 0.1914893590199185}}\n",
            "pair:  work attempt answer critical question whether exists input sequence cause well trained discrete space neural network sequence sequence seq seq model generate egregious outputs aggressive malicious attacking etc inputs exist find efficiently adopt empirical methodology first create lists egregious output sequences design discrete optimization algorithm find input sequences cause model generate moreover optimization algorithm enhanced large vocabulary search constrained search input sequences likely input real world users experiments apply approach dialogue response generation models trained three real world dialogue data sets ubuntu switchboard opensubtitles testing whether model generate malicious responses demonstrate given trigger inputs algorithm finds significant number malicious sentences assigned large probability model reveals undesirable consequence standard seq seq training\n",
            "output sentence:  paper aims provide empirical answer question whether well trained dialogue response model output malicious responses \n",
            "\n",
            "{'rouge-1': {'r': 0.17543859649122806, 'p': 0.7142857142857143, 'f': 0.28169013767903195}, 'rouge-2': {'r': 0.078125, 'p': 0.35714285714285715, 'f': 0.12820512525969763}, 'rouge-l': {'r': 0.15789473684210525, 'p': 0.6428571428571429, 'f': 0.25352112359452494}}\n",
            "pair:  improve robustness deep neural nets adversarial attacks using interpolating function output activation data dependent activation function remarkably improves classification accuracy stability adversarial perturbations together total variation minimization adversarial images augmented training strongest attack achieve accuracy improvement fast gradient sign method iterative fast gradient sign method carlini wagnerl attacks respectively defense strategy additive many existing methods give intuitive explanation defense strategy via analyzing geometry feature space reproducibility code available github\n",
            "output sentence:  proposal strategies adversarial defense based data dependent activation function total variation minimization training data augmentation \n",
            "\n",
            "{'rouge-1': {'r': 0.1978021978021978, 'p': 0.9473684210526315, 'f': 0.32727272441487604}, 'rouge-2': {'r': 0.1391304347826087, 'p': 0.8888888888888888, 'f': 0.24060150141896097}, 'rouge-l': {'r': 0.1978021978021978, 'p': 0.9473684210526315, 'f': 0.32727272441487604}}\n",
            "pair:  work present new agent architecture called reactor combines multiple algorithmic architectural contributions produce agent higher sample efficiency prioritized dueling dqn wang et al categorical dqn bellemare et al giving better run time performance mnih et al first contribution new policy evaluation algorithm called distributional retrace brings multi step policy updates distributional reinforcement learning setting approach used convert several classes multi step policy evaluation algorithms designed expected value evaluation distributional ones next introduce leaveone policy gradient algorithm improves trade variance bias using action values baseline final algorithmic contribution new prioritized replay algorithm sequences exploits temporal locality neighboring observations efficient replay prioritization using atari benchmarks show innovations contribute sample efficiency final agent performance finally demonstrate reactor reaches state art performance million frames less day training\n",
            "output sentence:  reactor combines multiple algorithmic architectural contributions produce agent higher sample efficiency prioritized dueling dqn giving better run time performance \n",
            "\n",
            "{'rouge-1': {'r': 0.05982905982905983, 'p': 0.6363636363636364, 'f': 0.1093749984289551}, 'rouge-2': {'r': 0.031496062992125984, 'p': 0.36363636363636365, 'f': 0.0579710130256249}, 'rouge-l': {'r': 0.03418803418803419, 'p': 0.36363636363636365, 'f': 0.06249999842895513}}\n",
            "pair:  recent direction unpaired image image translation one hand exciting alleviates big burden obtaining label intensive pixel pixel supervision hand fully satisfactory due presence artifacts degenerated transformations paper take manifold view problem introducing smoothness term sample graph attain harmonic functions enforce consistent mappings translation develop harmonicgan learn bi directional translations source target domains help similarity consistency inherent self consistency property samples maintained distance metrics defined two types features including histogram cnn exploited identical problem setting cyclegan without additional manual inputs small training time cost harmonicgan demonstrates significant qualitative quantitative improvement state art well improved interpretability show experimental results number applications including medical imaging object transfiguration semantic labeling outperform competing methods tasks medical imaging task particular method turns cyclegan failure success halving mean squared error generating images radiologists prefer competing methods cases\n",
            "output sentence:  smooth regularization sample graph unpaired image image translation results significantly improved consistency \n",
            "\n",
            "{'rouge-1': {'r': 0.049019607843137254, 'p': 0.5, 'f': 0.08928571265943878}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.029411764705882353, 'p': 0.3, 'f': 0.053571426945153114}}\n",
            "pair:  knowledge bases kb automatically manually constructed often incomplete many valid facts inferred kb synthesizing existing information popular approach kb completion infer new relations combinatory reasoning information found along paths connecting pair entities given enormous size kbs exponential number paths previous path based models considered problem predicting missing relation given two entities evaluating truth proposed triple additionally methods traditionally used random paths fixed entity pairs recently learned pick paths propose new algorithm minerva addresses much difficult practical task answering questions relation known one entity since random walks impractical setting unknown destination combinatorially many paths start node present neural reinforcement learning approach learns navigate graph conditioned input query find predictive paths comprehensive evaluation seven knowledge base datasets found minerva competitive many current state art methods\n",
            "output sentence:  present rl agent minerva learns walk knowledge graph answer queries \n",
            "\n",
            "{'rouge-1': {'r': 0.039603960396039604, 'p': 0.5, 'f': 0.07339449405268918}, 'rouge-2': {'r': 0.008264462809917356, 'p': 0.14285714285714285, 'f': 0.015624998966064523}, 'rouge-l': {'r': 0.039603960396039604, 'p': 0.5, 'f': 0.07339449405268918}}\n",
            "pair:  paper studies undesired phenomena sensitivity representations learned deep networks semantically irrelevant changes data identify cause shortcoming classical variational auto encoder vae objective evidence lower bound elbo show elbo fails control behaviour encoder support empirical data distribution behaviour vae lead extreme errors learned representation key hurdle effective use representations data efficient learning transfer address problem propose augment data specifications enforce insensitivity representation respect families transformations incorporate specifications propose regularization method based selection mechanism creates fictive data point explicitly perturbing observed true data point certain choices parameters formulation naturally leads minimization entropy regularized wasserstein distance representations illustrate approach standard datasets experimentally show significant improvements downstream adversarial accuracy achieved learning robust representations completely unsupervised manner without reference particular downstream task without costly supervised adversarial training procedure\n",
            "output sentence:  propose method computing adversarially robust representations entirely unsupervised way \n",
            "\n",
            "{'rouge-1': {'r': 0.14893617021276595, 'p': 0.9333333333333333, 'f': 0.2568807315714166}, 'rouge-2': {'r': 0.125, 'p': 0.7647058823529411, 'f': 0.21487603064271568}, 'rouge-l': {'r': 0.14893617021276595, 'p': 0.9333333333333333, 'f': 0.2568807315714166}}\n",
            "pair:  established diverse behaviors spanning controllable subspace markov decision process trained rewarding policy distinguishable policies however one limitation formulation difficulty generalize beyond finite set behaviors explicitly learned may needed subsequent tasks successor features provide appealing solution generalization problem require defining reward function linear grounded feature space paper show two techniques combined method solves primary limitation introduce variational intrinsic successor features visr novel algorithm learns controllable features leveraged provide enhanced generalization fast task inference successor features framework empirically validate visr full atari suite novel setup wherein rewards exposed briefly long unsupervised phase achieving human level performance games beating baselines believe visr represents step towards agents rapidly learn limited feedback\n",
            "output sentence:  introduce variational intrinsic successor features visr novel algorithm learns controllable features leveraged provide fast task task successor features framework \n",
            "\n",
            "{'rouge-1': {'r': 0.19148936170212766, 'p': 0.6923076923076923, 'f': 0.2999999966055556}, 'rouge-2': {'r': 0.07547169811320754, 'p': 0.3333333333333333, 'f': 0.12307692006627227}, 'rouge-l': {'r': 0.14893617021276595, 'p': 0.5384615384615384, 'f': 0.23333332993888892}}\n",
            "pair:  transfer learning uses trained weights source model initial weightsfor training target dataset well chosen source large numberof labeled data leads significant improvement accuracy demonstrate atechnique automatically labels large unlabeled datasets trainsource models transfer learning experimentally evaluate method usinga baseline dataset human annotated imagenet labels five variationsof technique show performance automatically trainedmodels come within baseline average\n",
            "output sentence:  technique automatically labeling large unlabeled datasets train source models transfer learning experimental evaluation \n",
            "\n",
            "{'rouge-1': {'r': 0.13636363636363635, 'p': 0.8, 'f': 0.23300970624941086}, 'rouge-2': {'r': 0.09183673469387756, 'p': 0.6, 'f': 0.15929203309577888}, 'rouge-l': {'r': 0.13636363636363635, 'p': 0.8, 'f': 0.23300970624941086}}\n",
            "pair:  coding theory central discipline underpinning wireline wireless modems workhorses information age progress coding theory largely driven individual human ingenuity sporadic breakthroughs past century paper study whether possible automate discovery decoding algorithms via deep learning study family sequential codes parametrized recurrent neural network rnn architectures show cre atively designed trained rnn architectures decode well known sequential codes convolutional turbo codes close optimal performance additive white gaussian noise awgn channel achieved breakthrough algorithms times viterbi bcjr decoders representing dynamic programing forward backward algorithms show strong gen eralizations train specific signal noise ratio block length test wide range quantities well robustness adaptivity deviations awgn setting\n",
            "output sentence:  show creatively designed trained rnn architectures decode well known sequential codes achieve close close optimal performances \n",
            "\n",
            "{'rouge-1': {'r': 0.1935483870967742, 'p': 0.8, 'f': 0.31168830855118906}, 'rouge-2': {'r': 0.1125, 'p': 0.6, 'f': 0.18947368155124658}, 'rouge-l': {'r': 0.1935483870967742, 'p': 0.8, 'f': 0.31168830855118906}}\n",
            "pair:  self attention based transformer demonstrated state art performances number natural language processing tasks self attention able model long term dependencies may suffer extraction irrelevant information context tackle problem propose novel model called sparse transformer sparse transformer able improve concentration attention global context explicit selection relevant segments extensive experimental results series natural language processing tasks including neural machine translation image captioning language modeling demonstrate advantages sparse transformer model performance sparse transformer reaches state art performances iwslt english vietnamese translation iwslt german english translation addition conduct qualitative analysis account sparse transformer superior performance\n",
            "output sentence:  work propose sparse transformer improve concentration attention global context explicit selection relevant segments sequence sequence learning \n",
            "\n",
            "{'rouge-1': {'r': 0.053763440860215055, 'p': 0.625, 'f': 0.0990098995314185}, 'rouge-2': {'r': 0.008333333333333333, 'p': 0.14285714285714285, 'f': 0.01574803045446098}, 'rouge-l': {'r': 0.03225806451612903, 'p': 0.375, 'f': 0.05940593913537892}}\n",
            "pair:  consider dictionary learning problem aim model given data linear combination columns matrix known dictionary sparse weights forming linear combination known coefficients since dictionary coefficients parameterizing linear model unknown corresponding optimization inherently non convex major challenge recently provable algorithms dictionary learning proposed yet provide guarantees recovery dictionary without explicit recovery guarantees coefficients moreover estimation error dictionary adversely impacts ability successfully localize estimate coefficients potentially limits utility existing provable dictionary learning methods applications coefficient recovery interest end develop noodl simple neurally plausible alternating optimization based online dictionary learning algorithm recovers dictionary coefficients exactly geometric rate initialized appropriately algorithm noodl also scalable amenable large scale distributed implementations neural architectures mean involves simple linear non linear operations finally corroborate theoretical results via experimental evaluation proposed algorithm current state art techniques\n",
            "output sentence:  present provable algorithm exactly recovering factors dictionary learning model \n",
            "\n",
            "{'rouge-1': {'r': 0.04878048780487805, 'p': 0.5714285714285714, 'f': 0.08988763900012627}, 'rouge-2': {'r': 0.01, 'p': 0.16666666666666666, 'f': 0.01886792346030622}, 'rouge-l': {'r': 0.036585365853658534, 'p': 0.42857142857142855, 'f': 0.06741572888776673}}\n",
            "pair:  recent advances deep reinforcement learning made significant strides performance applications go atari games however developing practical methods balance exploration exploitation complex domains remains largely unsolved thompson sampling extension reinforcement learning provide elegant approach exploration requires access posterior samples model time advances approximate bayesian methods made posterior approximation flexible neural network models practical thus attractive consider approximate bayesian neural networks thompson sampling framework understand impact using approximate posterior thompson sampling benchmark well established recently developed methods approximate posterior sampling combined thompson sampling series contextual bandit problems found many approaches successful supervised learning setting underperformed sequential decision making scenario particular highlight challenge adapting slowly converging uncertainty estimates online setting\n",
            "output sentence:  empirical comparison bayesian deep networks thompson sampling \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6666666666666666, 'f': 0.21052631313019393}, 'rouge-2': {'r': 0.024390243902439025, 'p': 0.18181818181818182, 'f': 0.04301075060238188}, 'rouge-l': {'r': 0.078125, 'p': 0.4166666666666667, 'f': 0.13157894470914133}}\n",
            "pair:  typical amortized inference variational autoencoders specialized single probabilistic query propose inference network architecture generalizes unseen probabilistic queries instead encoder decoder pair train single inference network directly data using cost function stochastic samples also queries use network perform inference tasks would undirected graphical model hidden variables without deal intractable partition function results mapped learning actual undirected model notoriously hard problem network also marginalizes nuisance variables required show approach generalizes unseen probabilistic queries also unseen test data providing fast flexible inference experiments show approach outperforms matches pcd advil benchmark datasets\n",
            "output sentence:  instead learning parameters graphical model data learn inference network answer probabilistic queries \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.875, 'f': 0.21874999781250004}, 'rouge-2': {'r': 0.07246376811594203, 'p': 0.625, 'f': 0.12987012800809583}, 'rouge-l': {'r': 0.125, 'p': 0.875, 'f': 0.21874999781250004}}\n",
            "pair:  despite existing work ensuring generalization neural networks terms scale sensitive complexity measures norms margin sharpness complexity measures offer explanation neural networks generalize better parametrization work suggest novel complexity measure based unit wise capacities resulting tighter generalization bound two layer relu networks capacity bound correlates behavior test error increasing network sizes within range reported experiments could partly explain improvement generalization parametrization present matching lower bound rademacher complexity improves previous capacity lower bounds neural networks\n",
            "output sentence:  suggest generalization bound could partly explain improvement generalization parametrization \n",
            "\n",
            "{'rouge-1': {'r': 0.037037037037037035, 'p': 1.0, 'f': 0.07142857073979593}, 'rouge-2': {'r': 0.013513513513513514, 'p': 1.0, 'f': 0.02666666640355556}, 'rouge-l': {'r': 0.037037037037037035, 'p': 1.0, 'f': 0.07142857073979593}}\n",
            "pair:  paper design harmonic acoustic model pitch detection model arranges conventional convolution sparse convolution way global harmonic patterns captured sparse convolution composed enough number local patterns captured layers conventional convolution trained maps dataset harmonic model outperforms existing pitch detection systems trained dataset impressively trained maps simple data augmentation harmonic model lstm layer top surpasses date complex pitch detection system trained maestro dataset complicated data augmentation applied whose training split order magnitude larger training split maps harmonic model demonstrated potential used advanced automatic music transcription amt systems\n",
            "output sentence:  harmonic acoustic model \n",
            "\n",
            "{'rouge-1': {'r': 0.17391304347826086, 'p': 0.8888888888888888, 'f': 0.2909090881719009}, 'rouge-2': {'r': 0.09523809523809523, 'p': 0.75, 'f': 0.16901408250743902}, 'rouge-l': {'r': 0.17391304347826086, 'p': 0.8888888888888888, 'f': 0.2909090881719009}}\n",
            "pair:  modeling informal inference natural language challenging recent availability large annotated data become feasible train complex models neural networks perform natural language inference nli achieved state art performance although exist relatively large annotated data machines learn knowledge needed perform nli data nli models benefit external knowledge build nli models leverage paper aim answer questions enriching state art neural natural language inference models external knowledge demonstrate proposed models external knowledge improve state art stanford natural language inference snli dataset\n",
            "output sentence:  proposed models external knowledge improve state art snli dataset \n",
            "\n",
            "{'rouge-1': {'r': 0.078125, 'p': 0.5, 'f': 0.13513513279766257}, 'rouge-2': {'r': 0.02666666666666667, 'p': 0.2222222222222222, 'f': 0.04761904570578239}, 'rouge-l': {'r': 0.046875, 'p': 0.3, 'f': 0.08108107874360854}}\n",
            "pair:  recent trend training neural networks replace data structures crafted hand aim faster execution better accuracy greater compression setting neural data structure instantiated training network many epochs inputs convergence many applications expensive initialization practical example streaming algorithms inputs ephemeral inspected small number times paper explore learning approximate set membership stream data one shot via meta learning propose novel memory architecture neural bloom filter show compressive bloom filters several existing memory augmented neural networks scenarios skewed data structured sets\n",
            "output sentence:  investigate space efficiency memory augmented neural nets learning set membership \n",
            "\n",
            "{'rouge-1': {'r': 0.07547169811320754, 'p': 0.8, 'f': 0.13793103290725328}, 'rouge-2': {'r': 0.014492753623188406, 'p': 0.25, 'f': 0.027397259238131022}, 'rouge-l': {'r': 0.05660377358490566, 'p': 0.6, 'f': 0.10344827428656364}}\n",
            "pair:  learning optimize recently proposed framework learning optimization algorithms using reinforcement learning paper explore learning optimization algorithm training shallow neural nets high dimensional stochastic optimization problems present interesting challenges existing reinforcement learning algorithms develop extension suited learning optimization algorithms setting demonstrate learned optimization algorithm consistently outperforms known optimization algorithms even unseen tasks robust changes stochasticity gradients neural net architecture specifically show optimization algorithm trained proposed method problem training neural net mnist generalizes problems training neural nets toronto faces dataset cifar cifar\n",
            "output sentence:  learn optimization algorithm generalizes unseen tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.12222222222222222, 'p': 0.9166666666666666, 'f': 0.2156862724336794}, 'rouge-2': {'r': 0.056910569105691054, 'p': 0.6363636363636364, 'f': 0.10447761043328138}, 'rouge-l': {'r': 0.1, 'p': 0.75, 'f': 0.1764705861591696}}\n",
            "pair:  deep generative modeling using flows gained popularity owing tractable exact log likelihood estimation efficient training synthesis process however flow models suffer challenge high dimensional latent space dimension input space effective solution challenge proposed dinh et al multi scale architecture based iterative early factorization part total dimensions regular intervals prior works generative flows involving multi scale architecture perform dimension factorization based static masking propose novel multi scale architecture performs data dependent factorization decide dimensions pass flow layers facilitate introduce heuristic based contribution dimension total log likelihood encodes importance dimensions proposed heuristic readily obtained part flow training process enabling versatile implementation likelihood contribution based multi scale architecture generic flow models present implementation original flow introduced dinh et al demonstrate improvements log likelihood score sampling quality standard image benchmarks also conduct ablation studies compare proposed method options dimension factorization\n",
            "output sentence:  data dependent factorization dimensions multi scale architecture based contribution total log likelihood \n",
            "\n",
            "{'rouge-1': {'r': 0.16417910447761194, 'p': 0.8461538461538461, 'f': 0.2749999972781251}, 'rouge-2': {'r': 0.1282051282051282, 'p': 0.7692307692307693, 'f': 0.21978021733124017}, 'rouge-l': {'r': 0.16417910447761194, 'p': 0.8461538461538461, 'f': 0.2749999972781251}}\n",
            "pair:  reinforcement learning agent needs pursue different goals across episodes requires goal conditional policy addition potential generalize desirable behavior unseen goals policies may also enable higher level planning based subgoals sparse reward environments capacity exploit information degree arbitrary goal achieved another goal intended appears crucial enable sample efficient learning however reinforcement learning agents recently endowed capacity hindsight paper demonstrate hindsight introduced policy gradient methods generalizing idea broad class successful algorithms experiments diverse selection sparse reward environments show hindsight leads remarkable increase sample efficiency\n",
            "output sentence:  introduce capacity exploit information degree arbitrary goal achieved another goal intended policy gradient methods \n",
            "\n",
            "{'rouge-1': {'r': 0.11627906976744186, 'p': 0.9090909090909091, 'f': 0.20618556499946858}, 'rouge-2': {'r': 0.08653846153846154, 'p': 0.6923076923076923, 'f': 0.1538461518708452}, 'rouge-l': {'r': 0.10465116279069768, 'p': 0.8181818181818182, 'f': 0.1855670082984377}}\n",
            "pair:  long term video prediction highly challenging since entails simultaneously capturing spatial temporal information across long range image frames standard recurrent models ineffective since prone error propagation cannot effectively capture higher order correlations potential solution extend higher order spatio temporal recurrent models however model requires large number parameters operations making intractable learn practice prone overfitting work propose convolutional tensor train lstm conv tt lstm learns higher orderconvolutional lstm convlstm efficiently using convolutional tensor train decomposition cttd proposed model naturally incorporates higher order spatio temporal information small cost memory computation using efficient low rank tensor representations evaluate model moving mnist kth datasets show improvements standard convlstm better comparable results convlstm based approaches much fewer parameters\n",
            "output sentence:  propose convolutional tensor train lstm learns higher order convolutional lstm efficiently using convolutional tensor train decomposition \n",
            "\n",
            "{'rouge-1': {'r': 0.07368421052631578, 'p': 0.7777777777777778, 'f': 0.1346153830343935}, 'rouge-2': {'r': 0.026785714285714284, 'p': 0.375, 'f': 0.04999999875555558}, 'rouge-l': {'r': 0.06315789473684211, 'p': 0.6666666666666666, 'f': 0.11538461380362428}}\n",
            "pair:  real world machine learning applications large outliers pervasive noise commonplace access clean training data required standard deep autoencoders unlikely reliably detecting anomalies given set images task high practical relevance visual quality inspection surveillance medical image analysis autoencoder neural networks learn reconstruct normal images hence classify images anomalous reconstruction error exceeds threshold paper proposed unsupervised method based subset scanning autoencoder activations contributions work threefold first propose novel method combining detection reconstruction error subset scanning scores improve anomaly score current autoencoders without requiring retraining second provide ability inspect visualize set anomalous nodes reconstruction error space make sample noised third show subset scanning used anomaly detection inner layers autoencoder provide detection power results several untargeted adversarial noise models standard datasets\n",
            "output sentence:  unsupervised method detect adversarial samples autoencoder activations reconstruction error space \n",
            "\n",
            "{'rouge-1': {'r': 0.054945054945054944, 'p': 0.38461538461538464, 'f': 0.0961538439663462}, 'rouge-2': {'r': 0.008928571428571428, 'p': 0.08333333333333333, 'f': 0.016129030509885726}, 'rouge-l': {'r': 0.04395604395604396, 'p': 0.3076923076923077, 'f': 0.076923074735577}}\n",
            "pair:  interpretability small labelled datasets key issues practical application deep learning particularly areas medicine paper present semi supervised technique addresses issues simultaneously learn dense representations large unlabelled image datasets use representations learn classifiers small labeled sets generate visual rationales explaining predictions using chest radiography diagnosis motivating application show method good generalization ability learning represent chest radiography dataset training classifier separate set different institution method identifies heart failure thoracic diseases prediction generate visual rationales positive classifications optimizing latent representation minimize probability disease constrained similarity measure image space decoding resultant latent representation produces image without apparent disease difference original altered image forms interpretable visual rationale algorithm prediction method simultaneously produces visual rationales compare favourably previous techniques classifier outperforms current state art\n",
            "output sentence:  propose method using gans generate high quality visual rationales help explain model predictions \n",
            "\n",
            "{'rouge-1': {'r': 0.057971014492753624, 'p': 0.5714285714285714, 'f': 0.10526315622229918}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.043478260869565216, 'p': 0.42857142857142855, 'f': 0.07894736674861498}}\n",
            "pair:  existing neural networks learning graphs deal issue permutation invariance conceiving network message passing scheme node sums feature vectors coming neighbors argue imposes limitation representation power instead propose new general architecture representing objects consisting hierarchy parts call covariant compositional networks ccns covariance means activation neuron must transform specific way permutations similarly steerability cnns achieve covariance making activation transform according tensor representation permutation group derive corresponding tensor aggregation rules neuron must implement experiments show ccns outperform competing methods standard graph learning benchmarks\n",
            "output sentence:  general framework creating covariant graph neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.42857142857142855, 'f': 0.1818181784756658}, 'rouge-2': {'r': 0.06666666666666667, 'p': 0.3333333333333333, 'f': 0.1111111083333334}, 'rouge-l': {'r': 0.11538461538461539, 'p': 0.42857142857142855, 'f': 0.1818181784756658}}\n",
            "pair:  area explainable ai xai explainable ai planning xaip matures ability agents generate curate explanations likewise grow propose new challenge area form rebellious deceptive explanations discuss explanations might generated briefly discuss evaluation criteria\n",
            "output sentence:  position paper proposing rebellious deceptive explanations agents \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.6, 'f': 0.16666666427469137}, 'rouge-2': {'r': 0.02564102564102564, 'p': 0.2222222222222222, 'f': 0.04597700963931835}, 'rouge-l': {'r': 0.04838709677419355, 'p': 0.3, 'f': 0.08333333094135809}}\n",
            "pair:  generative adversarial networks made data generation possible various use cases case complex high dimensional distributions difficult train convergence problems appearance mode collapse sliced wasserstein gans especially application max sliced wasserstein distance made possible approximate wasserstein distance training efficient stable way helped ease convergence problems architectures method transforms sample assignment distance calculation sorting one dimensional projection samples results sufficient approximation high dimensional wasserstein distance paper demonstrate approximation wasserstein distance sorting samples always optimal approach greedy assignment real fake samples result faster convergence better approximation original distribution\n",
            "output sentence:  apply greedy assignment projected samples instead sorting approximate wasserstein distance \n",
            "\n",
            "{'rouge-1': {'r': 0.03260869565217391, 'p': 0.3333333333333333, 'f': 0.0594059389706892}, 'rouge-2': {'r': 0.008849557522123894, 'p': 0.1111111111111111, 'f': 0.016393441256382806}, 'rouge-l': {'r': 0.03260869565217391, 'p': 0.3333333333333333, 'f': 0.0594059389706892}}\n",
            "pair:  paper interested two seemingly different concepts textit adversarial training textit generative adversarial networks gans particularly techniques work improve end analyze limitation adversarial training defense method starting questioning well robustness model generalize successfully improve generalizability via data augmentation fake images sampled generative adversarial network surprised see resulting robust classifier leads better generator free intuitively explain interesting phenomenon leave theoretical analysis future work motivated observations propose system combines generator discriminator adversarial attacker together single network end end training fine tuning method simultaneously improve robustness classifiers measured accuracy strong adversarial attacks quality generators evaluated aesthetically quantitatively terms classifier achieve better robustness state art adversarial training algorithm proposed madry textit et al generator achieves competitive performance compared sn gan miyato koyama\n",
            "output sentence:  found adversarial training speeds gan training also increases image quality \n",
            "\n",
            "{'rouge-1': {'r': 0.10256410256410256, 'p': 0.6666666666666666, 'f': 0.17777777546666668}, 'rouge-2': {'r': 0.04807692307692308, 'p': 0.4166666666666667, 'f': 0.08620689469678958}, 'rouge-l': {'r': 0.07692307692307693, 'p': 0.5, 'f': 0.13333333102222228}}\n",
            "pair:  existing unsupervised video video translation methods fail produce translated videos frame wise realistic semantic information preserving video level consistent work propose novel unsupervised video video translation model model decomposes style content uses specialized encoder decoder structure propagates inter frame information bidirectional recurrent neural network rnn units style content decomposition mechanism enables us achieve long term style consistent video translation results well provides us good interface modality flexible translation addition changing input frames style codes incorporated translation propose video interpolation loss captures temporal information within sequence train building blocks self supervised manner model produce photo realistic spatio temporal consistent translated videos multimodal way subjective objective experimental results validate superiority model existing methods\n",
            "output sentence:  temporally consistent modality flexible unsupervised video video translation framework trained self supervised manner \n",
            "\n",
            "{'rouge-1': {'r': 0.109375, 'p': 0.6363636363636364, 'f': 0.18666666416355557}, 'rouge-2': {'r': 0.013888888888888888, 'p': 0.1, 'f': 0.024390241760856822}, 'rouge-l': {'r': 0.078125, 'p': 0.45454545454545453, 'f': 0.13333333083022222}}\n",
            "pair:  obtaining high quality uncertainty estimates essential many applications deep neural networks paper theoretically justify scheme estimating uncertainties based sampling prior distribution crucially uncertainty estimates shown conservative sense never underestimate posterior uncertainty obtained hypothetical bayesian algorithm also show concentration implying uncertainty estimates converge zero get data uncertainty estimates obtained random priors adapted deep network architecture trained using standard supervised learning pipelines provide experimental evaluation random priors calibration distribution detection typical computer vision tasks demonstrating outperform deep ensembles practice\n",
            "output sentence:  provide theoretical support uncertainty estimates deep learning obtained fitting random priors \n",
            "\n",
            "{'rouge-1': {'r': 0.16666666666666666, 'p': 0.8, 'f': 0.2758620661117717}, 'rouge-2': {'r': 0.05263157894736842, 'p': 0.3333333333333333, 'f': 0.09090908855371907}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.4, 'f': 0.13793103162901313}}\n",
            "pair:  improved generative adversarial network improved gan successful method using generative adversarial models solve problem semi supervised learning however suffers problem unstable training paper found instability mostly due vanishing gradients generator remedy issue propose new method use collaborative training improve stability semi supervised gan combination wasserstein gan experiments shown proposed method stable original improved gan achieves comparable classification accuracy different data sets\n",
            "output sentence:  improve training stability semi supervised generative adversarial networks collaborative training \n",
            "\n",
            "{'rouge-1': {'r': 0.10465116279069768, 'p': 0.5294117647058824, 'f': 0.17475727879724767}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.23529411764705882, 'f': 0.06299212366544742}, 'rouge-l': {'r': 0.09302325581395349, 'p': 0.47058823529411764, 'f': 0.1553398030690923}}\n",
            "pair:  one notable contributions deep learning application convolutional neural networks convnets structured signal classification particular image classification beyond impressive performances supervised learning structure networks inspired development deep filter banks referred scattering transforms transforms apply cascade wavelet transforms complex modulus operators extract features invariant group operations stable deformations furthermore convnets inspired recent advances geometric deep learning aim generalize networks graph data applying notions graph signal processing learn deep graph filter cascades advance lines research proposing geometric scattering transform using graph wavelets defined terms random walks graph demonstrate utility features extracted designed deep filter bank graph classification biochemistry social network data incl state art results latter case data exploration enable inference ec exchange preferences enzyme evolution\n",
            "output sentence:  present new feed forward graph convnet based generalizing wavelet scattering transform mallat demonstrate utility graph classification data exploration tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.10344827586206896, 'p': 0.46153846153846156, 'f': 0.16901408151557235}, 'rouge-2': {'r': 0.03125, 'p': 0.16666666666666666, 'f': 0.05263157628808878}, 'rouge-l': {'r': 0.06896551724137931, 'p': 0.3076923076923077, 'f': 0.11267605334655831}}\n",
            "pair:  application deep recurrent networks audio transcription led impressive gains automatic speech recognition asr systems many demonstrated small adversarial perturbations fool deep neural networks incorrectly predicting specified target high confidence current work fooling asr systems focused white box attacks model architecture parameters known paper adopt black box approach adversarial generation combining approaches genetic algorithms gradient estimation solve task achieve targeted attack similarity generations maintaining audio file similarity\n",
            "output sentence:  present novel black box targeted attack able fool state art speech text transcription \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.8571428571428571, 'f': 0.2264150920469918}, 'rouge-2': {'r': 0.042735042735042736, 'p': 0.38461538461538464, 'f': 0.07692307512307696}, 'rouge-l': {'r': 0.09782608695652174, 'p': 0.6428571428571429, 'f': 0.1698113184620862}}\n",
            "pair:  smallest eigenvectors graph laplacian well known provide succinct representation geometry weighted graph reinforcement learning rl weighted graph may interpreted state transition process induced behavior policy acting environment approximating eigenvectors laplacian provides promising approach state representation learning however existing methods performing approximation ill suited general rl settings two main reasons first computationally expensive often requiring operations large matrices second methods lack adequate justification beyond simple tabular finite state settings paper present fully general scalable method approximating eigenvectors laplacian model free rl context systematically evaluate approach empirically show generalizes beyond tabular finite state setting even tabular finite state settings ability approximate eigenvectors outperforms previous proposals finally show potential benefits using laplacian representation learned using method goal achieving rl tasks providing evidence technique used significantly improve performance rl agent\n",
            "output sentence:  propose scalable method approximate eigenvectors laplacian reinforcement learning context show learned representations improve performance agent \n",
            "\n",
            "{'rouge-1': {'r': 0.05263157894736842, 'p': 0.42857142857142855, 'f': 0.09374999805175785}, 'rouge-2': {'r': 0.015384615384615385, 'p': 0.16666666666666666, 'f': 0.028169012537195087}, 'rouge-l': {'r': 0.03508771929824561, 'p': 0.2857142857142857, 'f': 0.06249999805175788}}\n",
            "pair:  neural networks commonly used models classification wide variety tasks typically learned affine transformation placed end models yielding per class value used classification classifier vast number parameters grows linearly number possible classes thus requiring increasingly resources work argue classifier fixed global scale constant little loss accuracy tasks allowing memory computational benefits moreover show initializing classifier hadamard matrix speed inference well discuss implications current understanding neural network models\n",
            "output sentence:  fix classifier neural networks without losing accuracy \n",
            "\n",
            "{'rouge-1': {'r': 0.02631578947368421, 'p': 0.4, 'f': 0.049382714891022736}, 'rouge-2': {'r': 0.01020408163265306, 'p': 0.25, 'f': 0.019607842383698604}, 'rouge-l': {'r': 0.02631578947368421, 'p': 0.4, 'f': 0.049382714891022736}}\n",
            "pair:  learning rank one interested optimising global ordering list items according utility users popular approaches learn scoring function scores items individually without context items list optimising pointwise pairwise listwise loss list sorted descending order scores possible interactions items present list taken account training phase loss level however inference items scored individually possible interactions considered paper propose context aware neural network model learns item scores applying self attention mechanism relevance given item thus determined context items present list training inference finally empirically demonstrate significant performance gains self attention based neural architecture multi layer perceptron baselines effect consistent across popular pointwise pairwise listwise losses datasets implicit explicit relevance feedback\n",
            "output sentence:  learning rank using transformer architecture \n",
            "\n",
            "{'rouge-1': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}, 'rouge-2': {'r': 0.030612244897959183, 'p': 0.25, 'f': 0.054545452601652965}, 'rouge-l': {'r': 0.1111111111111111, 'p': 0.6153846153846154, 'f': 0.18823529152664362}}\n",
            "pair:  learning policy using observational data challenging distribution states induces execution time may differ distribution observed training work propose train policy explicitly penalizing mismatch two distributions fixed time horizon using learned model environment dynamics unrolled multiple time steps training policy network minimize differentiable cost rolled trajectory cost contains two terms policy cost represents objective policy seeks optimize uncertainty cost represents divergence states trained propose measure second cost using uncertainty dynamics model predictions using recent ideas uncertainty estimation deep networks evaluate approach using large scale observational dataset driving behavior recorded traffic cameras show able learn effective driving policies purely observational data environment interaction\n",
            "output sentence:  model based rl approach uses differentiable uncertainty penalty learn driving policies purely observational data \n",
            "\n",
            "{'rouge-1': {'r': 0.1111111111111111, 'p': 0.75, 'f': 0.1935483848491155}, 'rouge-2': {'r': 0.045454545454545456, 'p': 0.42857142857142855, 'f': 0.08219177908800905}, 'rouge-l': {'r': 0.09259259259259259, 'p': 0.625, 'f': 0.1612903203329865}}\n",
            "pair:  community detection graphs central importance graph mining machine learning network science detecting overlapping communities especially challenging remains open problem motivated success graph based deep learning graph related tasks study applicability framework overlapping community detection propose probabilistic model overlapping community detection based graph neural network architecture despite simplicity model outperforms existing approaches community recovery task large margin moreover due inductive formulation proposed model able perform sample community detection nodes present training time\n",
            "output sentence:  detecting overlapping communities graphs using graph neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.22666666666666666, 'p': 0.8947368421052632, 'f': 0.3617021244341331}, 'rouge-2': {'r': 0.14545454545454545, 'p': 0.8, 'f': 0.24615384355029585}, 'rouge-l': {'r': 0.22666666666666666, 'p': 0.8947368421052632, 'f': 0.3617021244341331}}\n",
            "pair:  collaborative personalization learned user representations embeddings improve prediction accuracy neural network based models significantly propose federated user representation learning furl simple scalable privacy preserving resource efficient way utilize existing neural personalization techniques federated learning fl setting furl divides model parameters federated private parameters private parameters private user embeddings trained locally unlike federated parameters transferred averaged server show theoretically parameter split affect training model personalization approaches storing user embeddings locally preserves user privacy also improves memory locality personalization compared server training evaluate furl two datasets demonstrating significant improvement model quality performance increases approximately level performance centralized training reductions furthermore show user embeddings learned fl centralized setting similar structure indicating furl learn collaboratively shared parameters preserving user privacy\n",
            "output sentence:  propose federated user representation learning furl simple scalable privacy preserving bandwidth efficient way utilize existing neural personalization techniques federated federated federated federated \n",
            "\n",
            "{'rouge-1': {'r': 0.14634146341463414, 'p': 0.7058823529411765, 'f': 0.24242423957963474}, 'rouge-2': {'r': 0.05825242718446602, 'p': 0.3333333333333333, 'f': 0.09917355118639443}, 'rouge-l': {'r': 0.0975609756097561, 'p': 0.47058823529411764, 'f': 0.16161615877155397}}\n",
            "pair:  training agent solve control tasks directly high dimensional images model free reinforcement learning rl proven difficult agent needs learn latent representation together control policy perform task fitting high capacity encoder using scarce reward signal extremely sample inefficient also prone suboptimal convergence two ways improve sample efficiency learn good feature representation use policy algorithms dissect various approaches learning good latent features conclude image reconstruction loss essential ingredient enables efficient stable representation learning image based rl following findings devise policy actor critic algorithm auxiliary decoder trains end end matches state art performance across model free model based algorithms many challenging control tasks release code encourage future research image based rl\n",
            "output sentence:  design simple efficient model free policy method image based reinforcement learning matches state art model based methods sample efficiency \n",
            "\n",
            "{'rouge-1': {'r': 0.24193548387096775, 'p': 0.8333333333333334, 'f': 0.3749999965125001}, 'rouge-2': {'r': 0.0958904109589041, 'p': 0.4117647058823529, 'f': 0.15555555249135805}, 'rouge-l': {'r': 0.1774193548387097, 'p': 0.6111111111111112, 'f': 0.2749999965125}}\n",
            "pair:  work adopts successful distributional perspective reinforcement learning adapts continuous control setting combine within distributed framework policy learning order develop call distributed distributional deep deterministic policy gradient algorithm pg also combine technique number additional simple improvements use step returns prioritized experience replay experimentally examine contribution individual components show interact well combined contributions results show across wide variety simple control tasks difficult manipulation tasks set hard obstacle based locomotion tasks pg algorithm achieves state art performance\n",
            "output sentence:  develop agent call distributional deterministic deep policy gradient algorithm achieves state art performance number challenging continuous control problems \n",
            "\n",
            "{'rouge-1': {'r': 0.08974358974358974, 'p': 0.5833333333333334, 'f': 0.15555555324444445}, 'rouge-2': {'r': 0.020833333333333332, 'p': 0.18181818181818182, 'f': 0.03738317572539095}, 'rouge-l': {'r': 0.05128205128205128, 'p': 0.3333333333333333, 'f': 0.08888888657777784}}\n",
            "pair:  paper addresses problem incremental domain adaptation ida assume domain comes sequentially could access data current domain goal ida build unified model performing well encountered domains propose augment recurrent neural network rnn directly parameterized memory bank retrieved attention mechanism step rnn transition memory bank provides natural way ida adapting model new domain progressively add new slots memory bank increases model capacity learn new memory slots fine tune existing parameters back propagation experiments show approach significantly outperforms naive fine tuning previous work ida including elastic weight consolidation progressive neural network compared expanding hidden states approach robust old domains shown empirical theoretical results\n",
            "output sentence:  present neural memory based architecture incremental domain adaptation provide theoretical empirical results \n",
            "\n",
            "{'rouge-1': {'r': 0.050505050505050504, 'p': 0.7142857142857143, 'f': 0.09433962140797438}, 'rouge-2': {'r': 0.008064516129032258, 'p': 0.16666666666666666, 'f': 0.015384614504142063}, 'rouge-l': {'r': 0.04040404040404041, 'p': 0.5714285714285714, 'f': 0.07547169687967249}}\n",
            "pair:  spectral clustering leading popular technique unsupervised data analysis two major limitations scalability generalization spectral embedding sample extension paper introduce deep learning approach spectral clustering overcomes shortcomings network call spectralnet learns map embeds input data points eigenspace associated graph laplacian matrix subsequently clusters train spectralnet using procedure involves constrained stochastic optimization stochastic optimization allows scale large datasets constraints implemented using special purpose output layer allow us keep network output orthogonal moreover map learned spectralnet naturally generalizes spectral embedding unseen data points improve quality clustering replace standard pairwise gaussian affinities affinities leaned unlabeled data using siamese network additional improvement achieved applying network code representations produced standard autoencoders end end learning procedure fully unsupervised addition apply vc dimension theory derive lower bound size spectralnet state art clustering results reported mnist reuters datasets\n",
            "output sentence:  unsupervised spectral clustering using deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.10204081632653061, 'p': 0.7142857142857143, 'f': 0.1785714263839286}, 'rouge-2': {'r': 0.05, 'p': 0.5, 'f': 0.09090908925619837}, 'rouge-l': {'r': 0.10204081632653061, 'p': 0.7142857142857143, 'f': 0.1785714263839286}}\n",
            "pair:  present simple approach based pixel wise nearest neighbors understand interpret functioning state art neural networks pixel level tasks aim understand uncover synthesis prediction mechanisms state art convolutional neural networks end primarily analyze synthesis process generative models prediction mechanism discriminative models main hypothesis work convolutional neural networks pixel level tasks learn fast compositional nearest neighbor synthesis prediction function experiments semantic segmentation image image translation show qualitative quantitative evidence supporting hypothesis\n",
            "output sentence:  convolutional neural networks behave compositional nearest neighbors \n",
            "\n",
            "{'rouge-1': {'r': 0.0392156862745098, 'p': 0.2857142857142857, 'f': 0.06896551511890613}, 'rouge-2': {'r': 0.009009009009009009, 'p': 0.07692307692307693, 'f': 0.01612903038111364}, 'rouge-l': {'r': 0.029411764705882353, 'p': 0.21428571428571427, 'f': 0.05172413580856132}}\n",
            "pair:  brain computer interfaces bci may help patients faltering communication abilities due neurodegenerative diseases produce text speech direct neural processing however practical realization proven difficult due limitations speed accuracy generalizability existing interfaces end aim create bci decodes text directly neural signals implement framework initially isolates frequency bands input signal encapsulating differential information regarding production various phonemic classes bands form feature set feeds lstm discerns time point probability distributions across phonemes uttered subject finally particle filtering algorithm temporally smooths probabilities incorporating prior knowledge english language output text corresponding decoded word producing output abstain constraining reconstructed word given bag words unlike previous studies empirical success proposed approach offers promise employment interface patients unfettered naturalistic environments\n",
            "output sentence:  present open loop brain machine interface whose performance unconstrained traditionally used bag words approach \n",
            "\n",
            "{'rouge-1': {'r': 0.3548387096774194, 'p': 0.9166666666666666, 'f': 0.5116279029529476}, 'rouge-2': {'r': 0.19736842105263158, 'p': 0.625, 'f': 0.29999999635200003}, 'rouge-l': {'r': 0.25806451612903225, 'p': 0.6666666666666666, 'f': 0.3720930192320173}}\n",
            "pair:  interactive fiction games text based simulations agent interacts world purely natural language ideal environments studying extend reinforcement learning agents meet challenges natural language understanding partial observability action generation combinatorially large text based action spaces present kg agent builds dynamic knowledge graph exploring generates actions using template based action space contend dual uses knowledge graph reason game state constrain natural language generation keys scalable exploration combinatorially large natural language actions results across wide variety games show kg outperforms current agents despite exponential increase action space size\n",
            "output sentence:  present kg reinforcement learning agent builds dynamic knowledge graph exploring generates natural language using template based action space outperforming current agents wide set text based games \n",
            "\n",
            "{'rouge-1': {'r': 0.1518987341772152, 'p': 0.8571428571428571, 'f': 0.2580645135715112}, 'rouge-2': {'r': 0.11956521739130435, 'p': 0.8461538461538461, 'f': 0.20952380735419504}, 'rouge-l': {'r': 0.1518987341772152, 'p': 0.8571428571428571, 'f': 0.2580645135715112}}\n",
            "pair:  natural language processing models lack unified approach robustness testing paper introduce wildnlp framework testing model stability natural setting text corruptions keyboard errors misspelling occur compare robustness models popular nlp tasks nli ner sentiment analysis testing performance aspects introduced framework particular focus comparison recent state art text representations non contextualized word embeddings order improve robust ness perform adversarial training se lected aspects check transferability improvement models various cor ruption types find high perfor mance models ensure sufficient robustness although modern embedding tech niques help improve release cor rupted datasets code wildnlp frame work community\n",
            "output sentence:  compare robustness models popular nlp tasks nli ner sentiment analysis testing performance perturbed inputs \n",
            "\n",
            "{'rouge-1': {'r': 0.03125, 'p': 0.4, 'f': 0.05797101314849825}, 'rouge-2': {'r': 0.013513513513513514, 'p': 0.25, 'f': 0.025641024667981634}, 'rouge-l': {'r': 0.03125, 'p': 0.4, 'f': 0.05797101314849825}}\n",
            "pair:  generative adversarial networks gans extremely effective approximating complex distributions high dimensional input data samples substantial progress made understanding improving gan performance terms theory application however currently lack quantitative methods model assessment many gan variants proposed relatively little understanding relative abilities paper evaluate performance various types gans using divergence distance functions typically used training observe consistency across various proposed metrics interestingly test time metrics favour networks use training time criterion also compare proposed metrics human perceptual scores\n",
            "output sentence:  empirical evaluation generative adversarial networks \n",
            "\n",
            "{'rouge-1': {'r': 0.05102040816326531, 'p': 0.45454545454545453, 'f': 0.091743117451393}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.030612244897959183, 'p': 0.2727272727272727, 'f': 0.05504586974497103}}\n",
            "pair:  neural networks trained map one specific dataset another usually learn generalized transformation extrapolate accurately outside space training instance generative adversarial network gan exclusively trained transform images cars light dark might effect images horses neural networks good generation within manifold data trained however generating new samples outside manifold extrapolating sample much harder problem less well studied address introduce technique called neuron editing learns neurons encode edit particular transformation latent space use autoencoder decompose variation within dataset activations different neurons generate transformed data defining editing transformation neurons performing transformation latent trained space encode fairly complex non linear transformations data much simpler distribution shifts neuron activations showcase technique image domain style transfer two biological applications removal batch artifacts representing unwanted noise modeling effect drug treatments predict synergy drugs\n",
            "output sentence:  reframe generation problem one editing existing points result extrapolate better traditional gans \n",
            "\n",
            "{'rouge-1': {'r': 0.08928571428571429, 'p': 0.5, 'f': 0.15151514894398535}, 'rouge-2': {'r': 0.015873015873015872, 'p': 0.1111111111111111, 'f': 0.027777775590277953}, 'rouge-l': {'r': 0.07142857142857142, 'p': 0.4, 'f': 0.12121211864095506}}\n",
            "pair:  great progress made making neural networks effective across wide range tasks many surprisingly vulnerable small carefully chosen perturbations input known adversarial examples paper advocate experimentally investigate use logit regularization techniques adversarial defense used conjunction methods creating adversarial robustness little cost demonstrate much effectiveness one recent adversarial defense mechanism attributed logit regularization show improve defense white box black box attacks process creating stronger black box attacks pgd based models\n",
            "output sentence:  logit regularization methods help explain improve state art adversarial defenses \n",
            "\n",
            "{'rouge-1': {'r': 0.17543859649122806, 'p': 0.7142857142857143, 'f': 0.28169013767903195}, 'rouge-2': {'r': 0.078125, 'p': 0.35714285714285715, 'f': 0.12820512525969763}, 'rouge-l': {'r': 0.15789473684210525, 'p': 0.6428571428571429, 'f': 0.25352112359452494}}\n",
            "pair:  improve robustness deep neural nets adversarial attacks using interpolating function output activation data dependent activation function remarkably improves classification accuracy stability adversarial perturbations together total variation minimization adversarial images augmented training strongest attack achieve accuracy improvement fast gradient sign method iterative fast gradient sign method carlini wagnerl attacks respectively defense strategy additive many existing methods give intuitive explanation defense strategy via analyzing geometry feature space reproducibility code available github\n",
            "output sentence:  proposal strategies adversarial defense based data dependent activation function total variation minimization training data augmentation \n",
            "\n",
            "{'rouge-1': {'r': 0.1896551724137931, 'p': 0.6875, 'f': 0.29729729390796206}, 'rouge-2': {'r': 0.057971014492753624, 'p': 0.26666666666666666, 'f': 0.09523809230442186}, 'rouge-l': {'r': 0.1206896551724138, 'p': 0.4375, 'f': 0.18918918579985394}}\n",
            "pair:  propose novel score based approach learning directed acyclic graph dag observational data adapt recently proposed continuous constrained optimization formulation allow nonlinear relationships variables using neural networks extension allows model complex interactions global search compared greedy approaches addition comparing method existing continuous optimization methods provide missing empirical comparisons nonlinear greedy search methods synthetic real world data sets new method outperforms current continuous methods tasks competitive existing greedy search methods important metrics causal inference\n",
            "output sentence:  proposing new score based approach structure causal learning leveraging neural networks recent continuous constrained formulation problem \n",
            "\n",
            "{'rouge-1': {'r': 0.08791208791208792, 'p': 0.6153846153846154, 'f': 0.1538461516586539}, 'rouge-2': {'r': 0.008547008547008548, 'p': 0.07142857142857142, 'f': 0.015267173663539661}, 'rouge-l': {'r': 0.04395604395604396, 'p': 0.3076923076923077, 'f': 0.076923074735577}}\n",
            "pair:  generative priors become highly effective solving inverse problems including denoising inpainting reconstruction noisy measurements generative model represent image much lower dimensional latent codes context compressive sensing unknown image belongs range pretrained generative network recover image estimating underlying compact latent code available measurements however recent studies revealed even untrained deep neural networks work prior recovering natural images approaches update network weights keeping latent codes fixed reconstruct target image given measurements paper optimize network weights latent codes use untrained generative network prior video compressive sensing problem show optimizing latent code additionally get concise representation frames retain structural similarity video frames also apply low rank constraint latent codes represent video sequences even lower dimensional latent space empirically show proposed methods provide better comparable accuracy low computational complexity compared existing methods\n",
            "output sentence:  recover videos compressive measurements learning low dimensional low rank representation directly measurements training deep generator \n",
            "\n",
            "{'rouge-1': {'r': 0.06741573033707865, 'p': 0.4, 'f': 0.11538461291605034}, 'rouge-2': {'r': 0.017857142857142856, 'p': 0.13333333333333333, 'f': 0.03149606090892196}, 'rouge-l': {'r': 0.056179775280898875, 'p': 0.3333333333333333, 'f': 0.09615384368528113}}\n",
            "pair:  prohibitive energy cost running high performance convolutional neural networks cnns limiting deployment resource constrained platforms including mobile wearable devices propose cnn energy aware dynamic routing called energynet achieves adaptive complexity inference based inputs leading overall reduction run time energy cost without noticeably losing even improving accuracy achieved proposing energy loss captures computational data movement costs combine accuracy oriented loss learn dynamic routing policy skipping certain layers networks optimizes hybrid loss empirical results demonstrate compared baseline cnns energynetcan trim energy cost inference cifar tiny imagenet testing sets respectively maintaining testing accuracies encouraging observe energy awareness might serve training regularization even improve prediction accuracy models achieve higher top testing accuracy baseline cifar saving energy higher top testing accuracy tiny imagenet saving energy respectively\n",
            "output sentence:  paper proposes new cnn model combines energy cost dynamic routing strategy enable adaptive energy efficient inference \n",
            "\n",
            "{'rouge-1': {'r': 0.06329113924050633, 'p': 0.3333333333333333, 'f': 0.10638297604119518}, 'rouge-2': {'r': 0.009900990099009901, 'p': 0.07142857142857142, 'f': 0.01739130220945206}, 'rouge-l': {'r': 0.05063291139240506, 'p': 0.26666666666666666, 'f': 0.08510638029651435}}\n",
            "pair:  introduce novel method enables parameter efficient transfer multi task learning deep neural networks basic approach learn model patch small set parameters specialize task instead fine tuning last layer entire network instance show learning set scales biases sufficient convert pretrained network perform well qualitatively different problems converting single shot multibox detection ssd model class image classification model reusing parameters ssd feature extractor similarly show learning existing low parameter layers depth wise convolutions keeping rest network frozen also improves transfer learning accuracy significantly approach allows simultaneous multi task well sequential transfer learning several multi task learning problems despite using much fewer parameters traditional logits fine tuning match single task performance\n",
            "output sentence:  novel practically effective method adapt pretrained neural networks new tasks retraining minimal less number parameters \n",
            "\n",
            "{'rouge-1': {'r': 0.10714285714285714, 'p': 0.5625, 'f': 0.17999999731200003}, 'rouge-2': {'r': 0.018691588785046728, 'p': 0.125, 'f': 0.03252032294004907}, 'rouge-l': {'r': 0.08333333333333333, 'p': 0.4375, 'f': 0.13999999731200002}}\n",
            "pair:  propose automating science journalism asj process producing press release scientific paper novel task serve new benchmark neural abstractive summarization asj challenging task requires long source texts summarized long target texts also paraphrasing complex scientific concepts understood general audience purpose introduce specialized dataset asj contains scientific papers press releases science daily state art sequence sequence seq seq models could easily generate convincing press releases asj generally nonfactual deviate source address issue improve seq seq generation via transfer learning co training new targets scientific abstracts sources ii partitioned press releases design measure factuality scores pertinent scientific papers press releases seq seq models quantitative qualitative evaluation shows sizable improvements strong baseline suggesting proposed framework could improve seq seq summarization beyond asj\n",
            "output sentence:  new application seq seq modelling automating sciene journalism highly abstractive dataset transfer learning tricks automatic evaluation measure \n",
            "\n",
            "{'rouge-1': {'r': 0.1388888888888889, 'p': 0.7692307692307693, 'f': 0.23529411505605538}, 'rouge-2': {'r': 0.04395604395604396, 'p': 0.3333333333333333, 'f': 0.07766990085399195}, 'rouge-l': {'r': 0.125, 'p': 0.6923076923076923, 'f': 0.2117647032913495}}\n",
            "pair:  information bottleneck method provides information theoretic method representation learning training encoder retain information relevant predicting label minimizing amount superfluous information representation original formulation however requires labeled data order identify information superfluous work extend ability multi view unsupervised setting two views underlying entity provided label unknown enables us identify superfluous information shared views theoretical analysis leads definition new multi view model produces state art results sketchy dataset label limited versions mir flickr dataset also extend theory single view setting taking advantage standard data augmentation techniques empirically showing better generalization capabilities compared traditional unsupervised approaches representation learning\n",
            "output sentence:  extend information bottleneck method unsupervised multiview setting show state art results standard datasets \n",
            "\n",
            "{'rouge-1': {'r': 0.11538461538461539, 'p': 0.6666666666666666, 'f': 0.19672130895995704}, 'rouge-2': {'r': 0.06153846153846154, 'p': 0.5, 'f': 0.10958903914430478}, 'rouge-l': {'r': 0.11538461538461539, 'p': 0.6666666666666666, 'f': 0.19672130895995704}}\n",
            "pair:  paper propose two methods namely trace norm regression tnr stable trace norm analysis statna improve performances recommender systems side information trace norm regression approach extracts low rank latent factors underlying side information drives user preference different context furthermore novel recommender framework statna captures latent low rank common drivers user preferences also considers idiosyncratic taste individual users compare performances tnr statna movielens datasets state art models demonstrate statna tnr general outperforms methods\n",
            "output sentence:  methodologies recommender systems side information based trace norm regularization \n",
            "\n",
            "{'rouge-1': {'r': 0.0967741935483871, 'p': 0.5, 'f': 0.16216215944485027}, 'rouge-2': {'r': 0.0547945205479452, 'p': 0.3333333333333333, 'f': 0.0941176446339101}, 'rouge-l': {'r': 0.0967741935483871, 'p': 0.5, 'f': 0.16216215944485027}}\n",
            "pair:  standard variational lower bounds used train latent variable models produce biased estimates quantities interest introduce unbiased estimator log marginal likelihood gradients latent variable models based randomized truncation infinite series parameterized encoder decoder architecture parameters encoder optimized minimize variance estimator show models trained using estimator give better test set likelihoods standard importance sampling based approach average computational cost estimator also allows use latent variable models tasks unbiased estimators rather marginal likelihood lower bounds preferred minimizing reverse kl divergences estimating score functions\n",
            "output sentence:  create unbiased estimator log probability latent variable models extending models larger scope applications \n",
            "\n",
            "{'rouge-1': {'r': 0.13095238095238096, 'p': 0.7857142857142857, 'f': 0.2244897934693878}, 'rouge-2': {'r': 0.05660377358490566, 'p': 0.46153846153846156, 'f': 0.10084033418826356}, 'rouge-l': {'r': 0.09523809523809523, 'p': 0.5714285714285714, 'f': 0.1632653036734694}}\n",
            "pair:  recent research efforts enable study natural language grounded navigation photo realistic environments following natural language instructions dialog however existing methods tend overfit training data seen environments fail generalize well previously unseen environments order close gap seen unseen environments aim learning generalizable navigation model two novel perspectives introduce multitask navigation model seamlessly trained vision language navigation vln navigation dialog history ndh tasks benefits richer natural language guidance effectively transfers knowledge across tasks propose learn environment agnostic representations navigation policy invariant among environments thus generalizing better unseen environments extensive experiments show environment agnostic multitask navigation model significantly reduces performance gap seen unseen environments outperforms baselines unseen environments relative measure success rate vln goal progress ndh establishing new state art ndh task\n",
            "output sentence:  propose learn generalized policy natural language grounded navigation tasks via environment agnostic multitask learning \n",
            "\n",
            "{'rouge-1': {'r': 0.16981132075471697, 'p': 0.8181818181818182, 'f': 0.28124999715332033}, 'rouge-2': {'r': 0.07246376811594203, 'p': 0.45454545454545453, 'f': 0.12499999762812504}, 'rouge-l': {'r': 0.1320754716981132, 'p': 0.6363636363636364, 'f': 0.21874999715332033}}\n",
            "pair:  large deep neural networks powerful exhibit undesirable behaviors memorization sensitivity adversarial examples work propose mixup simple learning principle alleviate issues essence mixup trains neural network convex combinations pairs examples labels mixup regularizes neural network favor simple linear behavior training examples experiments imagenet cifar cifar google commands uci datasets show mixup improves generalization state art neural network architectures also find mixup reduces memorization corrupt labels increases robustness adversarial examples stabilizes training generative adversarial networks\n",
            "output sentence:  training convex combinations random training examples labels improves generalization deep neural networks \n",
            "\n",
            "{'rouge-1': {'r': 0.09859154929577464, 'p': 1.0, 'f': 0.17948717785338592}, 'rouge-2': {'r': 0.0449438202247191, 'p': 0.6666666666666666, 'f': 0.08421052513240998}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.8571428571428571, 'f': 0.15384615221236028}}\n",
            "pair:  paper present neural phrase based machine translation npmt method explicitly models phrase structures output sequences using sleep wake networks swan recently proposed segmentation based sequence modeling method mitigate monotonic alignment requirement swan introduce new layer perform soft local reordering input sequences different existing neural machine translation nmt approaches npmt use attention based decoding mechanisms instead directly outputs phrases sequential order decode linear time experiments show npmt achieves superior performances iwslt german english english german iwslt english vietnamese machine translation tasks compared strong nmt baselines also observe method produces meaningful phrases output languages\n",
            "output sentence:  neural phrase based machine translation linear decoding time \n",
            "\n",
            "{'rouge-1': {'r': 0.1368421052631579, 'p': 0.8666666666666667, 'f': 0.23636363400826452}, 'rouge-2': {'r': 0.06722689075630252, 'p': 0.47058823529411764, 'f': 0.11764705663602944}, 'rouge-l': {'r': 0.12631578947368421, 'p': 0.8, 'f': 0.21818181582644633}}\n",
            "pair:  inspired recent successes deep generative models text speech tts wavenet van den oord et al tacotron wang et al article proposes use deep generative model tailored automatic speech recognition asr primary acoustic model overall recognition system separate language model lm two dimensions depth considered use mixture density networks autoregressive non autoregressive generate density functions capable modeling acoustic input sequences much powerful conditioning first generation generative models asr gaussian mixture models hidden markov models gmm hmms use standard lstms spirit original tandem approach produce discriminative feature vectors generative modeling combining mixture density networks deep discriminative features leads novel dual stack lstm architecture directly related rnn transducer graves explicit functional form density combining naturally separate language model using bayes rule generative models discussed compared experimentally terms log likelihoods frame accuracies\n",
            "output sentence:  paper proposes use deep generative acoustic model automatic speech recognition combining naturally deep sequence sequence sequence using bayes using bayes \n",
            "\n",
            "{'rouge-1': {'r': 0.13043478260869565, 'p': 0.5, 'f': 0.20689654844233057}, 'rouge-2': {'r': 0.03636363636363636, 'p': 0.18181818181818182, 'f': 0.060606057828282954}, 'rouge-l': {'r': 0.08695652173913043, 'p': 0.3333333333333333, 'f': 0.13793103120095132}}\n",
            "pair:  well known neural networks universal approximators deeper networks tend practice powerful shallower ones shed light proving total number neurons required approximate natural classes multivariate polynomials variables grows linearly deep neural networks grows exponentially merely single hidden layer allowed also provide evidence number hidden layers increased neuron requirement grows exponentially suggesting minimum number layers required practical expressibility grows logarithmically\n",
            "output sentence:  prove deep neural networks exponentially efficient shallow ones approximating sparse multivariate polynomials \n",
            "\n",
            "{'rouge-1': {'r': 0.017543859649122806, 'p': 0.1111111111111111, 'f': 0.030303027947658587}, 'rouge-2': {'r': 0.0, 'p': 0.0, 'f': 0.0}, 'rouge-l': {'r': 0.017543859649122806, 'p': 0.1111111111111111, 'f': 0.030303027947658587}}\n",
            "pair:  significant advances made natural language processing nlp modelling since beginning new approaches allow accurate results even little labelled data nlp models benefit training task agnostic task specific unlabelled data however advantages come significant size computational costs workshop paper outlines proposed convolutional student architecture trained distillation process large scale model achieve inference speedup reduction parameter count cases student model performance surpasses teacher studied tasks\n",
            "output sentence:  train small efficient cnn performance openai transformer text classification tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.16981132075471697, 'p': 0.75, 'f': 0.27692307391242604}, 'rouge-2': {'r': 0.08196721311475409, 'p': 0.45454545454545453, 'f': 0.13888888630015434}, 'rouge-l': {'r': 0.1509433962264151, 'p': 0.6666666666666666, 'f': 0.2461538431431953}}\n",
            "pair:  unsupervised domain adaptation aims generalize hypothesis trained source domain unlabeled target domain one popular approach problem learn domain invariant embeddings domains work study theoretically empirically effect embedding complexity generalization target domain particular complexity affects upper bound target risk reflected experiments next specify theoretical framework multilayer neural networks result develop strategy mitigates sensitivity embedding complexity empirically achieves performance par better best layer dependent complexity tradeoff\n",
            "output sentence:  study effect embedding complexity learning domain invariant representations develop strategy mitigates sensitivity \n",
            "\n",
            "{'rouge-1': {'r': 0.15714285714285714, 'p': 0.5789473684210527, 'f': 0.24719100787779325}, 'rouge-2': {'r': 0.04081632653061224, 'p': 0.21052631578947367, 'f': 0.06837606565563602}, 'rouge-l': {'r': 0.14285714285714285, 'p': 0.5263157894736842, 'f': 0.22471909776543367}}\n",
            "pair:  shot learning process learning novel classes using examples remains challenging task machine learning many sophisticated shot learning algorithms proposed based notion networks easily overfit novel examples simply fine tuned using examples study show commonly used low resolution mini imagenet dataset fine tuning method achieves higher accuracy common shot learning algorithms shot task nearly accuracy state art algorithm shot task evaluate method practical tasks namely high resolution single domain cross domain tasks tasks show method achieves higher accuracy common shot learning algorithms analyze experimental results show retraining process stabilized employing low learning rate using adaptive gradient optimizers fine tuning increase test accuracy test accuracy improved updating entire network large domain shift exists base novel classes\n",
            "output sentence:  empirical study provides novel perspective shot learning fine tuning method shows comparable accuracy complex state art several methods several tasks tasks \n",
            "\n",
            "{'rouge-1': {'r': 0.10144927536231885, 'p': 0.7, 'f': 0.17721518766223365}, 'rouge-2': {'r': 0.038461538461538464, 'p': 0.3333333333333333, 'f': 0.06896551538644476}, 'rouge-l': {'r': 0.057971014492753624, 'p': 0.4, 'f': 0.10126582057362607}}\n",
            "pair:  propose studying gan training dynamics regret minimization contrast popular view consistent minimization divergence real generated distributions analyze convergence gan training new point view understand mode collapse happens hypothesize existence undesirable local equilibria non convex game responsible mode collapse observe local equilibria often exhibit sharp gradients discriminator function around real data points demonstrate degenerate local equilibria avoided gradient penalty scheme called dragan show dragan enables faster training achieves improved stability fewer mode collapses leads generator networks better modeling performance across variety architectures objective functions\n",
            "output sentence:  analysis convergence mode collapse studying gan training process regret minimization \n",
            "\n",
            "{'rouge-1': {'r': 0.1917808219178082, 'p': 0.7368421052631579, 'f': 0.3043478228095463}, 'rouge-2': {'r': 0.06382978723404255, 'p': 0.2608695652173913, 'f': 0.10256409940536206}, 'rouge-l': {'r': 0.1232876712328767, 'p': 0.47368421052631576, 'f': 0.1956521706356333}}\n",
            "pair:  end end task oriented dialogue challenging since knowledge bases usually large dynamic hard incorporate learning framework propose global local memory pointer glmp networks address issue model global memory encoder local memory decoder proposed share external knowledge encoder encodes dialogue history modifies global contextual representation generates global memory pointer decoder first generates sketch response unfilled slots next passes global memory pointer filter external knowledge relevant information instantiates slots via local memory pointers empirically show model improve copy accuracy mitigate common vocabulary problem result glmp able improve previous state art models simulated babi dialogue dataset human human stanford multi domain dialogue dataset automatic human evaluation\n",
            "output sentence:  glmp global memory encoder context rnn global pointer local memory decoder sketch rnn local pointer share external knowledge memnn proposed strengthen response task generation task \n",
            "\n",
            "{'rouge-1': {'r': 0.16393442622950818, 'p': 0.6666666666666666, 'f': 0.26315789156855957}, 'rouge-2': {'r': 0.05970149253731343, 'p': 0.2857142857142857, 'f': 0.0987654292394453}, 'rouge-l': {'r': 0.11475409836065574, 'p': 0.4666666666666667, 'f': 0.18421052314750697}}\n",
            "pair:  soundness optimality plan depends correctness domain model real world applications specifying complete domain models difficult interactions agent environment quite complex propose framework learn ppddl representation model incrementally multiple planning problems using experiences current planning problem suits non stationary environments introduce novel concept reliability intrinsic motivation reinforcement learning means learning failure prevent repeated instances similar failures motivation improve learning efficiency goal directedness evaluate work experimental results three planning domains\n",
            "output sentence:  introduce approach allow agents learn ppddl action models incrementally multiple planning problems framework reinforcement learning \n",
            "\n",
            "{'rouge-1': {'r': 0.028169014084507043, 'p': 0.2857142857142857, 'f': 0.05128204964825777}, 'rouge-2': {'r': 0.012658227848101266, 'p': 0.16666666666666666, 'f': 0.023529410452595226}, 'rouge-l': {'r': 0.028169014084507043, 'p': 0.2857142857142857, 'f': 0.05128204964825777}}\n",
            "pair:  introduce lyceum high performance computational ecosystem robotlearning lyceum built top julia programming language themujoco physics simulator combining ease use high level program ming language performance native lyceum xfaster compared popular abstractions like openai sgymand deep mind sdm control substantially reduces training time various inforcement learning algorithms also fast enough support real timemodel predictive control physics simulators lyceum straightfor ward api supports parallel computation across multiple cores machines code base tutorials demonstration videos found https sites google com view lyceum anon\n",
            "output sentence:  high performance robotics simulation algorithm development framework \n",
            "\n",
            "{'rouge-1': {'r': 0.11267605633802817, 'p': 0.5, 'f': 0.183908042975294}, 'rouge-2': {'r': 0.010309278350515464, 'p': 0.05555555555555555, 'f': 0.0173913017073728}, 'rouge-l': {'r': 0.08450704225352113, 'p': 0.375, 'f': 0.13793103148104116}}\n",
            "pair:  many environments tiny subset states yield high reward cases interactions environment provide relevant learning signal hence may want preferentially train high reward states probable trajectories leading end advocate use textit backtracking model predicts preceding states terminate given high reward state train model starting high value state one estimated high value predicts samples state action tuples may led high value state traces state action pairs refer recall traces sampled backtracking model starting high value state informative terminate good states hence use traces improve policy provide variational interpretation idea practical algorithm backtracking model samples approximate posterior distribution trajectories lead large rewards method improves sample efficiency policy rl algorithms across several environments tasks\n",
            "output sentence:  backward model previous state action given next state used simulate additional trajectories terminating states improves improves rl rl efficiency \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-123d9fb647b7>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateRandomlyprint_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_decoder1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-45-efe5772afd9a>\u001b[0m in \u001b[0;36mevaluateRandomlyprint_1\u001b[0;34m(encoder, decoder, n)\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0mtokenized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0;31m#print(len(tokenized_input))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m           \u001b[0moutput_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0moutput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-229d13fc485a>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(encoder, decoder, sentence, max_length)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mei\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mei\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-505305a0b3ae>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[1;32m    813\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[1;32m    814\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}